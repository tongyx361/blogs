<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Shawn/Yuxuan Tong 童雨轩</title>
<link>https://tongyx361.github.io/blogs/</link>
<atom:link href="https://tongyx361.github.io/blogs/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.7.31</generator>
<lastBuildDate>Sat, 24 May 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title></title>
  <dc:creator>Yuxuan Tong (童雨轩)</dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/verl-intro/</link>
  <description><![CDATA[ undefined ]]></description>
  <guid>https://tongyx361.github.io/blogs/posts/verl-intro/</guid>
  <pubDate>Sat, 24 May 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>重新思考 RL 中的 KL 梯度优化</title>
  <dc:creator>童雨轩 </dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh/</link>
  <description><![CDATA[ 






<section id="sec-grpo-kl-misunderstanding" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</h1>
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(Shao et al. 2024)</span> 的优化目标公式为：</p>
<p><span id="eq-grpo-obj"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%5Cleft%5C%7B%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D-%5Cbeta%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B1%7D"></span></p>
<p>其中</p>
<p><span id="eq-grpo-obj-kl-term"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7Br%20e%20f%7D%5Cright%5D=%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D-%5Clog%20%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20i%7D%20%5Cmid%20q,%20o_%7Bi,%20%5Calpha%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%20e%20t%7D%5Cright)%7D-1%0A%5Ctag%7B2%7D"></span></p>
<p>首先，Equation&nbsp;1 中出现了 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%5Ctext%7Bold%7D%7D">，这意味着其考虑了 off-policy 设置，但 Equation&nbsp;2 中却没有相应的处理，只适用于 <img src="https://latex.codecogs.com/png.latex?o_i%20%5Csim%20%5Cpi_%7B%5Ctheta%7D">，无法正确处理 <img src="https://latex.codecogs.com/png.latex?o_i%20%5Csim%20%5Cpi_%7B%5Ctheta_%5Ctext%7Bold%7D%7D">。</p>
<p>其次，Equation&nbsp;2 将估计样本量 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D-%5Clog%20%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20i%7D%20%5Cmid%20q,%20o_%7Bi,%20%5Calpha%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%20e%20t%7D%5Cright)%7D-1"> 写成 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7Br%20e%20f%7D%5Cright%5D"> 也并不十分恰当，因为 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7Br%20e%20f%7D%5Cright%5D"> 通常表示 KL 散度的真实值。</p>
<p>而目前流行的 LLM RL 框架，在实现 KL 优化时，通常也忽略了 off-policy 问题，同时还存在其他一系列问题：</p>
<ol type="1">
<li>误认为前向传播估计出 KL 散度，再反向传播就能得到其梯度（但实际上通常并非如此）；</li>
<li>忽略了先对动作动作对数条件似然应用 KL 估计样本量再求和并非良好定义的行为，导致梯度错误；</li>
<li>忽略了同一轨迹上 KL 对数概率必须求和以获得轨迹联合概率，而不能求平均；</li>
<li>错误地计算了平均操作。</li>
</ol>
<p>由于 on-policy 设置更加简单，但也已经暴露了上述大部分问题，我们可以先从 on-policy 设置开始讨论，后续再考虑 off-policy 设置。</p>
<aside id="footnotes" class="footnotes footnotes-end-of-section">
<hr>
<ol>
<li id="fn1"><p>http://joschu.net/blog/kl-approx.html↩︎</p></li>
</ol>
</aside>
</section>
<section id="sec-popular-llm-rl-kl-optim" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</h1>
<p>我们可以先回顾目前流行的 LLM RL 框架中对于 KL 优化的实现。以下我们以</p>
<ol type="1">
<li>TRL<sup>2</sup>，</li>
<li>OpenRLHF<sup>3</sup> <span class="citation" data-cites="hu2024openrlhf">(Hu et al. 2024)</span></li>
<li>verl<sup>4</sup> <span class="citation" data-cites="sheng2024hybridflow">(Sheng et al. 2024)</span></li>
</ol>
<p>为例。</p>
<p>熟悉这些框架的读者可以跳过本节，直接从 Section&nbsp;3 开始阅读。</p>
<section id="trlkl-reward-项" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</h2>
<p>TRL 计算 KL 定义中的样本值 <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D">，并将其从 reward 中减去。对应代码可见 Listing&nbsp;1。</p>
<div id="lst-trl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: TRL 计算 KL 样本值 <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D"> 并从 reward 中减去<sup>5</sup>
</figcaption>
<div aria-describedby="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. compute rewards</span></span>
<span id="cb1-2">kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprobs</span>
<span id="cb1-3">non_score_reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>args.kl_coef <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> kl</span>
<span id="cb1-4">rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> non_score_reward.clone()</span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb1-6">rewards[[actual_start, actual_end]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> scores</span></code></pre></div>
</div>
</figure>
</div>
<p>这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 Section&nbsp;2.4。</p>
<aside id="footnotes-2" class="footnotes footnotes-end-of-section">
<hr>
<ol start="2">
<li id="fn2"><p>https://github.com/huggingface/trl↩︎</p></li>
<li id="fn3"><p>https://github.com/OpenRLHF/OpenRLHF↩︎</p></li>
<li id="fn4"><p>https://github.com/volcengine/verl↩︎</p></li>
<li id="fn5"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506↩︎</p></li>
</ol>
</aside>
</section>
<section id="openrlhf" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</h2>
<section id="sec-openrlhf-kl-reward" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</h3>
<p>与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 Listing&nbsp;2。</p>
<div id="lst-openrlhf-calc-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 <sup>6</sup>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_approx_kl(</span>
<span id="cb2-2">    log_probs: torch.Tensor,</span>
<span id="cb2-3">    log_probs_base: torch.Tensor,</span>
<span id="cb2-4">    action_mask: Optional[torch.Tensor] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb2-5">    kl_estimator: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k1"</span>,</span>
<span id="cb2-6">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> torch.Tensor:</span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Compute the approximate KL divergence between two distributions.</span></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Schulman blog: http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-10"></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Args:</span></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        log_probs: Log probabilities of the new distribution.</span></span>
<span id="cb2-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        log_probs_base: Log probabilities of the base distribution.</span></span>
<span id="cb2-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        action_mask: Mask for actions.</span></span>
<span id="cb2-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb2-16"></span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k1"</span>:</span>
<span id="cb2-18">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_probs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_probs_base.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb2-19">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-20">            log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> action_mask</span>
<span id="cb2-21"></span>
<span id="cb2-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The $k_2$ estimator is the non negative kl approximation in</span></span>
<span id="cb2-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The k2_loss is approximately equivalent to the</span></span>
<span id="cb2-25">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># one-step KL divergence penalty with the $k_1$ estimator</span></span>
<span id="cb2-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># used in https://arxiv.org/abs/2310.10505.</span></span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k2"</span>:</span>
<span id="cb2-28">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_probs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_probs_base.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb2-29">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-30">            log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> action_mask</span>
<span id="cb2-31">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span></span>
<span id="cb2-32"></span>
<span id="cb2-33">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The $k_3$ estimator is the non negative kl approximation in</span></span>
<span id="cb2-34">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-35">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k3"</span>:</span>
<span id="cb2-36">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_probs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_probs_base.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb2-37">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-38">            log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> action_mask</span>
<span id="cb2-39">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log_ratio</span>
<span id="cb2-40">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio.exp() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_ratio</span>
<span id="cb2-41"></span>
<span id="cb2-42">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> log_ratio</span>
<span id="cb2-43"></span>
<span id="cb2-44"></span>
<span id="cb2-45"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_reward(</span>
<span id="cb2-46">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-47">    kl_coef: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-48">    kl: Union[torch.Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[torch.Tensor]],</span>
<span id="cb2-49">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-50">    num_actions: Optional[Union[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb2-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-52">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Union[torch.Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[torch.Tensor]]:</span>
<span id="cb2-53">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-54">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-55">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-56">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb2-57">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-58">        reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-59">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, (kl_seg, action_len) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(kl, num_actions)):</span>
<span id="cb2-60">            kl_reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>kl_coef <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> kl_seg</span>
<span id="cb2-61">            kl_reward[action_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> r[i]</span>
<span id="cb2-62">            reward.append(kl_reward)</span>
<span id="cb2-63"></span>
<span id="cb2-64">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> reward</span></code></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-3" class="footnotes footnotes-end-of-section">
<hr>
<ol start="6">
<li id="fn6"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88↩︎</p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</h3>
<p>此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 Listing&nbsp;3。</p>
<div id="lst-openrlhf-calc-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 <sup>7</sup>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> training_step_actor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, experience: Experience) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>]:</span>
<span id="cb3-2">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor.train()</span>
<span id="cb3-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(experience.sequences, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>):</span>
<span id="cb3-5">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-7">        sequences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.sequences</span>
<span id="cb3-8">        old_action_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.action_log_probs</span>
<span id="cb3-9">        advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.advantages</span>
<span id="cb3-10">        num_actions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.action_mask.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-11">        packed_seq_lens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-12">        attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.attention_mask</span>
<span id="cb3-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.use_kl_loss <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> experience.base_action_log_probs <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-14">            base_action_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.base_action_log_probs</span>
<span id="cb3-15"></span>
<span id="cb3-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># actor loss</span></span>
<span id="cb3-17">    action_log_probs, output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor(</span>
<span id="cb3-18">        sequences,</span>
<span id="cb3-19">        num_actions,</span>
<span id="cb3-20">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-21">    )</span>
<span id="cb3-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># loss function</span></span>
<span id="cb3-24">    actor_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_loss_fn(</span>
<span id="cb3-25">        action_log_probs,</span>
<span id="cb3-26">        old_action_log_probs,</span>
<span id="cb3-27">        advantages,</span>
<span id="cb3-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-29">    )</span>
<span id="cb3-30"></span>
<span id="cb3-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.use_kl_loss:</span>
<span id="cb3-32">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_model <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-33">            kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_approx_kl(</span>
<span id="cb3-34">                action_log_probs,</span>
<span id="cb3-35">                base_action_log_probs,</span>
<span id="cb3-36">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-37">                kl_estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.kl_estimator,</span>
<span id="cb3-38">            )</span>
<span id="cb3-39">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-40">            kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros_like(action_log_probs, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>action_log_probs.dtype, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>action_log_probs.device)</span>
<span id="cb3-41"></span>
<span id="cb3-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.packing_samples:</span>
<span id="cb3-43">            kl_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(kl, experience.action_mask, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-44">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-45">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-46"></span>
<span id="cb3-47">        kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_mean.mean()</span>
<span id="cb3-48">        experience.info[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"kl"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_loss.item()</span>
<span id="cb3-49">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-50">        kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-52">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.strategy.optimizer_step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_optim, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_scheduler, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>)</span>
<span id="cb3-53">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-4" class="footnotes footnotes-end-of-section">
<hr>
<ol start="7">
<li id="fn7"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="verl" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="verl"><span class="header-section-number">2.3</span> verl</h2>
<section id="kl-reward-项" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</h3>
<p>verl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 Listing&nbsp;4。</p>
<div id="lst-verl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: verl 将 KL 估计样本值从 reward 中减去 <sup>8</sup>
</figcaption>
<div aria-describedby="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kl'</span>):</span>
<span id="cb4-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb4-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute kl between ref_policy and current policy</span></span>
<span id="cb4-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ref_log_prob'</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> data.batch.keys():</span>
<span id="cb4-5">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> core_algos.kl_penalty(data.batch[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'old_log_probs'</span>], data.batch[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ref_log_prob'</span>],</span>
<span id="cb4-6">                                    kl_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>kl_penalty)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (batch_size, response_length)</span></span>
<span id="cb4-7">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> response_mask</span>
<span id="cb4-8">        beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_ctrl.value</span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb4-10">        beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb4-11">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros_like(response_mask, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb4-12"></span>
<span id="cb4-13">    token_level_rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> token_level_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> kld</span>
<span id="cb4-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-5" class="footnotes footnotes-end-of-section">
<hr>
<ol start="8">
<li id="fn8"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160↩︎</p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项-1" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</h3>
<p>verl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 Listing&nbsp;5。</p>
<div id="lst-verl-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 <sup>9</sup>
</figcaption>
<div aria-describedby="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_policy(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, data: DataProto):</span>
<span id="cb5-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># make sure we are in training mode</span></span>
<span id="cb5-3">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_module.train()</span>
<span id="cb5-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.ppo_epochs):</span>
<span id="cb5-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> batch_idx, data <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dataloader):</span>
<span id="cb5-7">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-8">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_optimizer.zero_grad()</span>
<span id="cb5-9"></span>
<span id="cb5-10">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> data <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> micro_batches:</span>
<span id="cb5-11">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-12">                responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'responses'</span>]</span>
<span id="cb5-13">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-14">                old_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'old_log_probs'</span>]</span>
<span id="cb5-15">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-16"></span>
<span id="cb5-17">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># all return: (bsz, response_length)</span></span>
<span id="cb5-18">                entropy, log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._forward_micro_batch(micro_batch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>temperature)</span>
<span id="cb5-19"></span>
<span id="cb5-20">                pg_loss, pg_clipfrac, ppo_kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> core_algos.compute_policy_loss(old_log_prob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>old_log_prob,</span>
<span id="cb5-21">                                                                                log_prob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>log_prob,</span>
<span id="cb5-22">                                                                                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-23">                                                                                )</span>
<span id="cb5-24">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-25"></span>
<span id="cb5-26">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute policy loss</span></span>
<span id="cb5-27">                policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> entropy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> entropy_coeff</span>
<span id="cb5-28"></span>
<span id="cb5-29">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.use_kl_loss:</span>
<span id="cb5-30">                    ref_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ref_log_prob'</span>]</span>
<span id="cb5-31">                    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute kl loss</span></span>
<span id="cb5-32">                    kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> core_algos.kl_penalty(logprob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>log_prob,</span>
<span id="cb5-33">                                                ref_logprob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ref_log_prob,</span>
<span id="cb5-34">                                                kl_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.kl_loss_type)</span>
<span id="cb5-35">                    kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(kld, response_mask)</span>
<span id="cb5-36"></span>
<span id="cb5-37">                    policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.kl_loss_coef</span>
<span id="cb5-38">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-39">                loss.backward()</span>
<span id="cb5-40">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-41">            grad_norm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._optimizer_step()</span>
<span id="cb5-42">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-43">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_optimizer.zero_grad()</span>
<span id="cb5-44">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-6" class="footnotes footnotes-end-of-section">
<hr>
<ol start="9">
<li id="fn9"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="sec-why-kl-reward" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</h2>
<p>将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT <span class="citation" data-cites="ouyang2022instructgpt">(Ouyang et al. 2022)</span>。</p>
<section id="kl-reward-的流行应当源自-rlhf-与-instructgpt" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</h3>
<p>InstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。</p>
<blockquote class="blockquote">
<p>… In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models “PPO.”</p>
<p>…</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Ctext%20%7B%20objective%20%7D(%5Cphi)=%20&amp;%20E_%7B(x,%20y)%20%5Csim%20D_%5Cpi%5E%7B%5Cmathrm%7BRL%7D%7D%7D%5Cleft%5Br_%5Ctheta(x,%20y)-%5Cbeta%20%5Clog%20%5Cleft(%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D(y%20%5Cmid%20x)%20/%20%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D(y%20%5Cmid%20x)%5Cright)%5Cright%5D+%20%5C%5C%0A&amp;%20%5Cgamma%20E_%7Bx%20%5Csim%20D_%7B%5Ctext%20%7Bremin%20%7D%7D%7D%5Cleft%5B%5Clog%20%5Cleft(%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D(x)%5Cright)%5Cright%5D%0A%5Cend%7Baligned%7D%0A"></p>
<blockquote class="blockquote">
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D">is the learned RL policy,<img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D"> is the supervised trained model, and<img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%20%7Bpretrain%20%7D%7D">is the pretraining distribution. The KL reward coefficient, <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and the pretraining loss coefficient, <img src="https://latex.codecogs.com/png.latex?%5Cgamma">, control the strength of the KL penalty and pretraining gradients respectively. For “PPO” models, <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</p>
</blockquote>
</section>
<section id="sec-oai-kl-reward-src" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</h3>
<p>然而，在OpenAI 早期的一篇论文 “Learning to summarize from human feedback” <span class="citation" data-cites="stiennon2020summarize">(Stiennon et al. 2020)</span> 中，他们就已经采用了 KL reward，并提及了出处：</p>
<blockquote class="blockquote">
<p>… <strong>Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D"> with parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> and this original supervised model <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D">, as previously done in [25].</strong> The full reward <img src="https://latex.codecogs.com/png.latex?R"> can be written as:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(x,%20y)=r_%5Ctheta(x,%20y)-%5Cbeta%20%5Clog%20%5Cleft%5B%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D(y%20%5Cmid%20x)%20/%20%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D(y%20%5Cmid%20x)%5Cright%5D%0A"></p>
<blockquote class="blockquote">
<p>This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn’t learn to produce outputs that are too different from those that the reward model has seen during training.</p>
</blockquote>
</section>
<section id="kl-reward-最早的出处" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</h3>
<p>Section&nbsp;2.4.2 中 OpenAI 引用的 KL reward 出处 [25] 是 “Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog” <span class="citation" data-cites="jaques2019wayoffpolicy">(Jaques et al. 2019)</span>。</p>
<p>实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：</p>
<blockquote class="blockquote">
<p>Rather than simply sample from the prior, we would like the <img src="https://latex.codecogs.com/png.latex?Q">-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior <img src="https://latex.codecogs.com/png.latex?p(y%20%5Cmid%20x)">, and the <img src="https://latex.codecogs.com/png.latex?Q">-network policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">, while still maximizing reward. Given a trajectory of actions, <img src="https://latex.codecogs.com/png.latex?%5Ctau=%5Cleft%5C%7Ba_1,%20a_2,%20%5Cldots%20a_%7Bt-1%7D%5Cright%5C%7D">, let <img src="https://latex.codecogs.com/png.latex?q(%5Ctau)=%5Cprod_%7Bt=1%7D%5ET%20%5Cpi_%5Ctheta%5Cleft(a_t,%20s_t%5Cright)">be the policy of our<img src="https://latex.codecogs.com/png.latex?Q">-learning algorithm at the trajectory level. Similarly, let <img src="https://latex.codecogs.com/png.latex?p(%5Ctau)=%5Cprod_%7Bt=1%7D%5ET%20p%5Cleft(a_t%20%5Cmid%20s_t%5Cright)">be the prior distribution over the trajectory, and<img src="https://latex.codecogs.com/png.latex?r(%5Ctau)"> be the rewards. We seek to maximize the following KL-regularized objective:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(q)=%5Cmathbb%7BE%7D_%7Bq(%5Ctau)%7D%5Br(%5Ctau)%5D%20/%20c-D_%7B%5Ctext%7BKL%7D%7D%5Bq(%5Ctau)%20%5Cmid%20p(%5Ctau)%5D%0A"></p>
<blockquote class="blockquote">
<p>Since <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D%5Bq%20%5Cmid%20p%5D=%5Csum_x%20q(x)(%5Clog%20q(x)-%5Clog%20p(x))">, we can see that this is equivalent to maximizing the following expected value function of the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> at the action level:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ%5E%5Cpi%5Cleft(s_t,%20a_t%5Cright)=%5Cmathbb%7BE%7D_%5Cpi%5Cleft%5B%5Csum%5ET%20r%5Cleft(s_%7Bt%5E%7B%5Cprime%7D%7D,%20a_%7Bt%5E%7B%5Cprime%7D%7D%5Cright)%20/%20c+%5Clog%20p%5Cleft(a_%7Bt%5E%7B%5Cprime%7D%7D%20%5Cmid%20s_%7Bt%5E%7B%5Cprime%7D%7D%5Cright)-%5Clog%20%5Cpi%5Cleft(a_%7Bt%5E%7B%5Cprime%7D%7D%20%5Cmid%20s_%7Bt%5E%7B%5Cprime%7D%7D%5Cright)%5Cright%5D%0A"></p>
<blockquote class="blockquote">

</blockquote>
</section>
</section>
</section>
<section id="sec-rl-kl-optim-formulation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</h1>
<p>为了进一步分析这些 LLM RL 框架中的实现是否正确，我们需要先形式化 LLM RL 中 KL 散度的优化。</p>
<section id="rl-中的-kl-散度通常定义在轨迹分布上" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</h2>
<p>GRPO 公式 (Equation&nbsp;1) 中的 KL 项可以定义为：</p>
<p><span id="eq-def-kl-theta-ref"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D"> 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 与参考策略整体分布 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bref%7D%7D"> 的 KL 散度。</p>
</section>
<section id="将轨迹展开为状态-动作序列" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</h2>
<p>RL 文献中通常会将轨迹 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D"> 展开为状态-动作序列 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D">：<sup>10</sup></p>
<p><span id="eq-def-kl-theta-ref-state-action-ag"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%7D%7Bp(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B4%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7B%5Ctau%7D%7C"> 为轨迹动作数的随机变量。</p>
<p>此处利用了联合概率的展开，以 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 为例：</p>
<p><span id="eq-dp-expansion"><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Ctheta%7D(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20=%20p(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%0A%5Ctag%7B5%7D"></span></p>
<p>注意区分整体概率分布 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D">、策略（条件）概率分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 与状态转移概率分布 <img src="https://latex.codecogs.com/png.latex?p">。</p>
<aside id="footnotes-7" class="footnotes footnotes-end-of-section">
<hr>
<ol start="10">
<li id="fn10"><p>这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D"> 对应于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_1">，而 <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Bo%7D%7D"> 对应于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_T,%20%5Cmathbf%7Ba%7D_T%7D">。↩︎</p></li>
</ol>
</aside>
</section>
<section id="markov-决策过程中的-kl-散度" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</h2>
<p>实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP<sup>11</sup>。</p>
<p>Markov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 <img src="https://latex.codecogs.com/png.latex?n"> 个历史状态和动作，而非全部的历史信息，对应的过程称为 <img src="https://latex.codecogs.com/png.latex?n"> 阶 Markov 过程。以 <img src="https://latex.codecogs.com/png.latex?n=1"> 为例：</p>
<p><span id="eq-def-markov-prop"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cpi(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20&amp;%20=%20%5Cpi(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%20%5C%5C%0Ap(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20&amp;%20=%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B6%7D"></span></p>
<p>则 Equation&nbsp;5 中的联合概率可以进一步简化为：</p>
<p><span id="eq-dp-expansion-markov-1"><img src="https://latex.codecogs.com/png.latex?%0Ap(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20=%20p(s_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%0A%5Ctag%7B7%7D"></span></p>
<p>如果考虑一阶 Markov 过程，则 Equation&nbsp;4 中的 KL 可以进一步简化为：</p>
<p><span id="eq-def-kl-theta-ref-state-action-markov-1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20=%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B8%7D"></span></p>
<aside id="footnotes-8" class="footnotes footnotes-end-of-section">
<hr>
<ol start="11">
<li id="fn11"><p>https://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B↩︎</p></li>
</ol>
</aside>
</section>
<section id="sec-lm-as-dp" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</h2>
<p>目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。</p>
<p>尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 <img src="https://latex.codecogs.com/png.latex?s_1"> 表示 prompt 中的所有 token，对于 <img src="https://latex.codecogs.com/png.latex?t%20%3E1">，如果令 <img src="https://latex.codecogs.com/png.latex?s_t"> 表示第 <img src="https://latex.codecogs.com/png.latex?t"> 个动作 token 前的所有 token，则自回归模型满足 Markov 性质，否则不一定。</p>
<p>接下来，我们先令 <img src="https://latex.codecogs.com/png.latex?s_t"> 表示前 <img src="https://latex.codecogs.com/png.latex?t"> 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。</p>
</section>
<section id="估计-kl-散度" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</h2>
<section id="几乎不可能直接计算-kl-散度的真实值" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</h3>
<p>实际实现中，我们几乎不可能直接计算出 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%5Cmathcal%7BT%7D%5Cright%7C"> 与轨迹最大长度 <img src="https://latex.codecogs.com/png.latex?T%20=%20%5Cmax_%7B%5Cmathbf%7B%5Ctau%7D%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%7C%5Cmathbf%7B%5Ctau%7D%7C"> 成指数关系： <span id="eq-def-rl-kl-avg-over-traj"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D%20(%5Cmathbf%7B%5Ctau%7D)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B9%7D"></span></p>
</section>
<section id="通常使用-monte-carlo-方法估计-kl-散度" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</h3>
<p>所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法<sup>12</sup>来估计 RL 中的 KL 散度，例如：</p>
<p><span id="eq-def-rl-kl-mc-k1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D%20(%5Cmathbf%7B%5Ctau%7D)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5C%5C%0A&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau_%7Bi%20%7D%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B10%7D"></span></p>
<p>其中，<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%20=%20%5Cleft(%5Cmathbf%7Bs%7D_%7Bi,1%7D,%20%5Cmathbf%7Ba%7D_%7Bi,1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bi,%7C%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7Bi,%7C%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D">，<img src="https://latex.codecogs.com/png.latex?N"> 为估计使用的轨迹样本数量。</p>
<aside id="footnotes-9" class="footnotes footnotes-end-of-section">
<hr>
<ol start="12">
<li id="fn12"><p>https://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95↩︎</p></li>
</ol>
</aside>
</section>
<section id="不同的-kl-估计量" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</h3>
<p>实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。</p>
<p>设 KL 估计量为 <img src="https://latex.codecogs.com/png.latex?k">，则对应的 KL 估计值为</p>
<p><span id="eq-def-rl-kl-mc-general"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20k(%5Ctau_i)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B11%7D"></span></p>
<p>例如 Section&nbsp;2.2.1 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 <code>k1</code>, <code>k2</code>, <code>k3</code>，这应该是主要参考了 John Schulman 的博客 “Approximating KL Divergence”。</p>
<p>verl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度<sup>13</sup>，但目前还没有实现。对应代码可见 Listing&nbsp;6。</p>
<div id="lst-verl-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: verl 的 KL 散度 Monte Carlo 估计样本值<sup>14</sup>
</figcaption>
<div aria-describedby="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> torch.FloatTensor:</span>
<span id="cb6-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"kl"</span>:</span>
<span id="cb6-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprob</span>
<span id="cb6-5"></span>
<span id="cb6-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"abs"</span>:</span>
<span id="cb6-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprob).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>()</span>
<span id="cb6-8"></span>
<span id="cb6-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mse"</span>:</span>
<span id="cb6-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprob).square()</span>
<span id="cb6-11"></span>
<span id="cb6-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># J. Schulman. Approximating kl divergence, 2020.</span></span>
<span id="cb6-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># # URL http://joschu.net/blog/kl-approx.html.</span></span>
<span id="cb6-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'low_var_kl'</span>:</span>
<span id="cb6-15">        kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ref_logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> logprob</span>
<span id="cb6-16">        ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(kl)</span>
<span id="cb6-17">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).contiguous()</span>
<span id="cb6-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.clamp(kld, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb6-19"></span>
<span id="cb6-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"full"</span>:</span>
<span id="cb6-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># so, here logprob and ref_logprob should contain the logits for every token in vocabulary</span></span>
<span id="cb6-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">NotImplementedError</span></span>
<span id="cb6-23"></span>
<span id="cb6-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">NotImplementedError</span></span></code></pre></div>
</div>
</figure>
</div>
<p>由于 <img src="https://latex.codecogs.com/png.latex?k_1">、<img src="https://latex.codecogs.com/png.latex?k_2">、<img src="https://latex.codecogs.com/png.latex?k_3"> 三种估计量最为流行，我们将以这三种估计量为例展开分析。</p>
<p>考虑 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20k_j(%5Ctau_i)">，其中 <img src="https://latex.codecogs.com/png.latex?%5Ctau_i%20%5Csim%20p_%7B%5Ctheta%7D">，令 <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Ctau_i)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Ctau_i)%7D">，注意，此处 <img src="https://latex.codecogs.com/png.latex?r"> 并非 KL 定义中的样本量，而是其倒数，则：</p>
<p><span id="eq-def-kl-estimators"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ak_%7B1%7D%20&amp;%20=%20-%20%5Clog%20r%20%5C%5C%0Ak_%7B2%7D%20&amp;%20=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Clog%20r)%5E2%20%5C%5C%0Ak_%7B3%7D%20&amp;%20=%20(r%20-%201)%20-%20%5Clog%20r%0A%5Cend%7Baligned%7D%0A%5Ctag%7B12%7D"></span></p>
<aside id="footnotes-10" class="footnotes footnotes-end-of-section">
<hr>
<ol start="13">
<li id="fn13"><p>这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。↩︎</p></li>
<li id="fn14"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383↩︎</p></li>
</ol>
</aside>
</section>
</section>
</section>
<section id="流行-on-policy-kl-优化实现的数学形式化" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</h1>
<p>神经网络模型普遍使用梯度法优化，因此，我们主要关注这些 KL 优化实现导出的梯度。</p>
<p>而由于 reward 项优化的实现涉及到基线（Baseline）、折扣（Discounting）、GAE <span class="citation" data-cites="schulman2018gae">(Schulman et al. 2018)</span> 等内容，较为复杂，我们可以先分析 KL loss 项实现。</p>
<section id="sec-kl-loss-impl" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</h2>
<p>上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。</p>
<p>然而，如 Section&nbsp;1 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。</p>
<section id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</h3>
<p>观察 Listing&nbsp;3 计算 “KL loss” 项的部分。</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-2">kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_approx_kl(</span>
<span id="cb7-3">    action_log_probs,</span>
<span id="cb7-4">    base_action_log_probs,</span>
<span id="cb7-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-6">    kl_estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.kl_estimator,</span>
<span id="cb7-7">)</span>
<span id="cb7-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-9">kl_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(kl, experience.action_mask, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-11">kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_mean.mean()</span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div>
<p>这些代码：</p>
<ol type="1">
<li>计算了 <code>kl</code>，对应对每个动作 token <img src="https://latex.codecogs.com/png.latex?a_%7Bi,t%7D"> 计算 “KL 估计量” <img src="https://latex.codecogs.com/png.latex?k">。</li>
<li>计算了 <code>kl_mean</code>，对应对每个轨迹 <img src="https://latex.codecogs.com/png.latex?%5Ctau_i"> 计算均值 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20k">。</li>
<li>计算了 <code>kl_loss</code>，对应对所有轨迹样本计算均值 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20k">。</li>
</ol>
<p>由于其没有去除任何梯度，因此其导出的梯度估计值为</p>
<p><span id="eq-def-kl-loss-grad-estim-openrlhf"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20k%20%5Cright)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%0A%5Cend%7Baligned%7D%0A%5Ctag%7B13%7D"></span></p>
<p>Listing&nbsp;5 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：</p>
<p><span id="eq-def-kl-loss-grad-estim-verl"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7BN%7D%20%7C%5Ctau_i%7C%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20k%20%5Cright)%20=%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7BN%7D%20%7C%5Ctau_i%7C%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cnabla_%7B%5Ctheta%7D%20k%0A%5Cend%7Baligned%7D%0A%5Ctag%7B14%7D"></span></p>
<p>我们将平均操作一般化为权重 <img src="https://latex.codecogs.com/png.latex?w_%7B%5Cmathbf%7B%5Ctau%7D%7D"> 与 <img src="https://latex.codecogs.com/png.latex?w_%7Bt%7D">，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：</p>
<p><span id="eq-def-kl-loss-grad-estim-general"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Csum_%7Bi=1%7D%5E%7BN%7D%20w_%7B%5Cmathbf%7B%5Ctau%7D_i%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20w_%7Bt%7D%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B15%7D"></span></p>
<p>则</p>
<ul>
<li>OpenRLHF 对应 <img src="https://latex.codecogs.com/png.latex?w_%7B%5Cmathbf%7B%5Ctau%7D%7D%20=%20%5Cfrac%7B1%7D%7BN%7D,%20w_%7Bt%7D%20=%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D">；</li>
<li>verl 对应 <img src="https://latex.codecogs.com/png.latex?w_%7B%5Cmathbf%7B%5Ctau%7D%7D%20=%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7BN%7D%20%7C%5Ctau_i%7C%7D,%20w_%7Bt%7D%20=%201">。</li>
</ul>
<p>此处，我们先以 OpenRLHF 的梯度估计 (Equation&nbsp;13) 为例，分析不同 KL 估计量导出的梯度估计，其满足：</p>
<p><span id="eq-def-kl-loss-grad-expect-openrlhf"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D_i%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5Cright%5D%0A%5Ctag%7B16%7D"></span></p>
<p>我们会在 Section&nbsp;5 中推导正确的 KL 梯度估计。</p>
</section>
<section id="k_1-导出的梯度期望为-0" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <img src="https://latex.codecogs.com/png.latex?k_1"> 导出的梯度：期望为 0</h3>
<p>向 Equation&nbsp;16 代入 <img src="https://latex.codecogs.com/png.latex?k%20=%20k_1%20=%20-%20%5Clog%20r%20=%20%5Clog%20%5Cfrac%7B1%7D%7Br%7D%20=%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D">，导出的梯度估计为</p>
<p><span id="eq-kl-loss-grad-sample-k1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cleft(%20p(%5Cmathbf%7Bs%7D_%7B1%7D)%20%5Cright)%20%5Cright)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cleft(%20p(%5Cmathbf%7Bs%7D_%7B1%7D)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D)%20%5Cright)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%5Ctheta(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B17%7D"></span></p>
<p>则其导出的梯度期望满足：</p>
<p><span id="eq-kl-loss-grad-expect-k1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A&amp;%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20%5C%5C%0A&amp;%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B18%7D"></span></p>
<p>此处利用了 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20=%20%5Cfrac%7B1%7D%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20=%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)">。</p>
<p>所以 <img src="https://latex.codecogs.com/png.latex?k_1"> loss 项优化的量是 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cright%5D">。这意味着该优化过程会降低采样轨迹的长度。</p>
<p>特别地，当不对同一轨迹中的 “<img src="https://latex.codecogs.com/png.latex?k_1"> 估计量”求均值，而是求和时，可以直接将 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D"> 这一项替换为 <img src="https://latex.codecogs.com/png.latex?1">，得到 <span id="eq-kl-loss-grad-expect-k1-no-intra-traj-mean"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5Cright%5D%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D%20=%20%5Cnabla_%7B%5Ctheta%7D%201%20=%200%0A%5Ctag%7B19%7D"></span><sup>15</sup></p>
<p>这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。</p>
<p>无论哪种情况，<img src="https://latex.codecogs.com/png.latex?k_1"> 导出的优化量都非常奇怪，不太可能出于实现者的本意。</p>
<p>同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。接下来，我们将忽略这一操作，即将 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D"> 一项替换为 <img src="https://latex.codecogs.com/png.latex?1">。</p>
<aside id="footnotes-11" class="footnotes footnotes-end-of-section">
<hr>
<ol start="15">
<li id="fn15"><p>此处对数似然的梯度的期望值为 0，是一个著名的性质，会在接下来频繁用到。↩︎</p></li>
</ol>
</aside>
</section>
<section id="k_2-导出的梯度" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <img src="https://latex.codecogs.com/png.latex?k_2"> 导出的梯度</h3>
<p>向 Equation&nbsp;16 代入 <img src="https://latex.codecogs.com/png.latex?k%20=%20k_2%20=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Clog%20r)%5E2%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%5Cright)%5E2">，导出的单条轨迹 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D"> 的梯度为 <span id="eq-kl-loss-grad-sample-k2"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%5Cright)%5E2%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B20%7D"></span></p>
<p>显然，</p>
<p><span id="eq-kl-loss-grad-sample-k2-wrong"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%20%5C%5C%0A%5Cneq%20&amp;%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%20%5Cright)%20%5C%5C%0A=&amp;%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B21%7D"></span></p>
<p>然而，</p>
<p><span id="eq-kl-loss-grad-expect-k2-wrong"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cleft%5B%20%5Cleft(%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20-%20%5Cleft(%20%5Clog%20p_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%20%5Cleft%5B%20%5Cnabla_%7B%5Ctheta%7D%20(%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20-%201)%20p_%7B%5Ctheta%7D(%5Ctau)%20-%20%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%20%5Cleft%5B%20(%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20-%201)%20p_%7B%5Ctheta%7D(%5Ctau)%20-%20%5Clog%20p_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%20p_%7B%5Ctheta%7D%20%5Cleft%5B%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%7D%20-%201%20%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20-%201%20%5Cright)%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B22%7D"></span></p>
<p>此处利用了 <img src="https://latex.codecogs.com/png.latex?%5Clog%20p(x)%20%5Cnabla_%7B%5Ctheta%7D%20p(x)%20=%20%5Cnabla_%7B%5Ctheta%7D%20(%5Clog%20p(x)%20-%201)%20p(x)"></p>
<p>因此，最小化 <img src="https://latex.codecogs.com/png.latex?k_2"> loss 项 (Equation&nbsp;21) ，并非在优化 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。</p>
</section>
<section id="k_3-导出的梯度" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <img src="https://latex.codecogs.com/png.latex?k_3"> 导出的梯度</h3>
<p>向 Equation&nbsp;16 代入 <img src="https://latex.codecogs.com/png.latex?k%20=%20k_3%20=%20(r%20-%201)%20-%20%5Clog%20r%20=%20(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20-%201)%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D">，导出的单条轨迹 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D"> 的梯度为 <span id="eq-kl-loss-grad-sample-k3"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20-%201%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20-%20%5Cfrac%7B%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D%5E%7B2%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20-%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5C%5C%0A=&amp;%20-%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D%5E%7B2%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright)%20-%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5C%5C%0A=&amp;%20-%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D%5E%7B2%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B23%7D"></span></p>
<p>其中，根据 Equation&nbsp;19，<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5Cright%5D%20=%200">，不妨直接省略。</p>
<p>而剩余部分似乎很难通过消去 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)"> 来提出 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D"> 并准确分析。但显然也并非在优化 KL 散度。</p>
</section>
<section id="小结流行的-kl-loss-项-实现并不合理" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</h3>
<p>综上所述，对于 OpenRLHF 实现的 “KL loss 项”，</p>
<ol type="1">
<li>对同一轨迹内的 “KL 估计量” 求均值这一操作很可能是错误的，正确操作应当为求和，对应于根据对数条件概率求对数联合概率。</li>
<li><img src="https://latex.codecogs.com/png.latex?k_1"> 导出的梯度
<ol type="1">
<li>若对同一轨迹内的 “KL 估计量” 求均值，则会导致输出长度减小，</li>
<li>而如果修正为求和，则其期望为 0，在平均意义上不改分布。</li>
</ol></li>
<li><img src="https://latex.codecogs.com/png.latex?k_2">，<img src="https://latex.codecogs.com/png.latex?k_3"> 导出的梯度则十分复杂，难以分析，但都并非在优化 KL 散度，这可能是因为其错误地将 KL 估计样本量应用于动作对数条件似然并求和。回顾 KL 估计量公式 (Equation&nbsp;12) ，应当注意到这些估计量是直接作用于似然 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)">，而没有保证作用于概率后求积/对数和仍然有意义。</li>
</ol>
</section>
</section>
<section id="分析流行的-kl-reward-项-实现" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</h2>
<section id="sec-analogy-pg-kl" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</h3>
<p>由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是： <span id="eq-pg-est-adv"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5Br(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Chat%7BA%7D_t%20%5Cright%5D%0A%5Ctag%7B24%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t"> 为优势（Advantage）的估计量。</p>
<p>为了方便观察 KL reward 项发挥的作用，我们将 <img src="https://latex.codecogs.com/png.latex?r_%7B%5Cmathbf%7B%5Ctau%7D%7D"> 展开，并不妨考虑一个更简单的估计，例如：</p>
<p><span id="eq-pg-est-ret"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20r(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Csum_%7Bt'=1%7D%5E%7B%7C%5Ctau%7C%7D%20r(s_%7Bt'%7D,%20a_%7Bt'%7D)%20%5Cright%5D%0A%5Ctag%7B25%7D"></span></p>
<p>简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 “Policy Gradient” 一讲<sup>16</sup>。</p>
<p>类比 <img src="https://latex.codecogs.com/png.latex?r_%7Bt'%7D"> 导出的梯度期望，将负的 KL 样本量 <img src="https://latex.codecogs.com/png.latex?-%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%20%5Cright)%7D"> 加入 reward <img src="https://latex.codecogs.com/png.latex?r_%7Bt'%7D"> 代入其中，导出的梯度期望为：</p>
<p><span id="eq-kl-grad-est-markov-1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%20%5Cleft(%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%20%5Cright)%20%5Cright)%20%5Csum_%7Bt'=1%7D%5E%7B%7C%5Ctau%7C%7D%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_%7Bt'%7D%20%5Cmid%20s_%7Bt'%7D%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(a_%7Bt'%7D%20%5Cmid%20s_%7Bt'%7D%20%5Cright)%7D%20%5Cright%5D%20=%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%5Cright%5D%0A%5Ctag%7B26%7D"></span></p>
<p>注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (Equation&nbsp;6)。</p>
<p>实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：</p>
<p><span id="eq-kl-grad-est-dp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A%5Cto&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20%5Clog%20%5Cfrac%7B%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20%5Clog%20%5Cfrac%7B%20p(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%7D%7B%20p(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20%5Clog%20%5Cfrac%7B%20p_%5Ctheta%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cright)%7D%7B%20p_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B27%7D"></span></p>
<p>可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。</p>
<aside id="footnotes-12" class="footnotes footnotes-end-of-section">
<hr>
<ol start="16">
<li id="fn16"><p>https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf↩︎</p></li>
</ol>
</aside>
</section>
<section id="不同-kl-估计量导出的-reward-项的作用" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</h3>
<p>不难注意到，Section&nbsp;4.2.1 中的 KL 样本量对应于 <img src="https://latex.codecogs.com/png.latex?k_1"> 估计量。</p>
<p>一个自然的问题是，如果对动作条件似然使用 <img src="https://latex.codecogs.com/png.latex?k_2"> 或 <img src="https://latex.codecogs.com/png.latex?k_3"> 等其他估计量，会得到什么结果？</p>
<p><img src="https://latex.codecogs.com/png.latex?k_2"> 或 <img src="https://latex.codecogs.com/png.latex?k_3"> 等其他估计量导致的一个问题，求和时通常无法得到联合概率。具体来说，其他估计量分别在优化</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?k_2">: <img src="https://latex.codecogs.com/png.latex?-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright)%5E%7B2%7D%20%5Cright%5D"></li>
<li><img src="https://latex.codecogs.com/png.latex?k_3">: <img src="https://latex.codecogs.com/png.latex?-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20(%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%20%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20-%201%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D)%20%5Cright%5D"></li>
</ul>
<p>显然，这里的求和无法得到联合概率，也就无法实现类似 Equation&nbsp;27 中的效果了。</p>
</section>
<section id="小结在-on-policy-设置下修正-grpo-目标的-kl-项" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</h3>
<p>若对动作对数条件似然计算 KL 估计样本量，则由于涉及到求和，<img src="https://latex.codecogs.com/png.latex?k_1"> 之外的估计量通常没有良好定义。</p>
<p>但是若放弃对动作条件似然计算 KL 估计样本量，而是对求和之后的对数（条件）似然进行计算，则只需满足</p>
<p><span id="eq-kl-reward-grad-expect-general"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20k%5Cleft(%5Cfrac%7B%20p_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%7B%20p_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%5Cright)%20%5Cright%5D%0A%5Capprox%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cfrac%7B1%7D%7BN%7D%20k%5Cleft(%5Cfrac%7B%20p_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%7B%20p_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%5Cright)%0A%5Capprox%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%0A%5Ctag%7B28%7D"></span></p>
<p>暂时不考虑 off-policy 问题，根据 Equation&nbsp;28, GRPO 公式 (Equation&nbsp;1, Equation&nbsp;2) 应当修正 KL 项如下：</p>
<p><span id="eq-grpo-obj-kl-fixed"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cleft%5C%7B%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D%20%20%5Cright%5C%7D%20%20-%5Cbeta%20k%5Cleft(%20%5Cfrac%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B29%7D"></span></p>
</section>
</section>
</section>
<section id="sec-derive-kld-grad" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</h1>
<p>前文中，我们分析了流行的 LLM RL 框架中对 KL 散度优化的实现，并得出了结论。另一种思路是直接推导出 KL 散度的梯度估计表达式，并据此实现代码。</p>
<p>由于我们使用的是梯度法，为了优化 KL 散度，我们需要准确估计的是 KL 散度的梯度而非其本身。类似地，在 PG 中，我们需要最大化 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Br(%5Cmathbf%7B%5Ctau%7D)%5D">，估计的是其梯度 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Br(%5Cmathbf%7B%5Ctau%7D)%5D=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Br(%5Cmathbf%7B%5Ctau%7D)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%5D">而不是<img src="https://latex.codecogs.com/png.latex?r(%5Cmathbf%7B%5Ctau%7D)"> 本身。</p>
<p>同时，如 Section&nbsp;4.1 所述，先前向传播估计 KL 散度，再直接反向传播，通常是无法直接得到 KL 散度的梯度的。所以，我们需要直接估计 KL 散度的梯度。</p>
<p>首先，展开 KL 散度的表达式：</p>
<p><span id="eq-kl-expansion-sum"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20%5Cpropto%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B30%7D"></span></p>
<p>再计算其梯度：</p>
<p><span id="eq-kl-grad-expansion"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Cpropto%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s_1)%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cright)%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C-1%7D%20p(s_%7Bt+1%7D%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%5Cright)%20%20%5C%5C%0A&amp;%20%5Ccdot%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s_1)%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%20-%201%7D%20p(s_%7Bt+1%7D%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%5Cright)%20%5C%5C%0A&amp;%20%5Ccdot%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cright)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B31%7D"></span></p>
<p>Equation&nbsp;31 中的梯度相当复杂，难以直接计算。接下来，我们将引入一系列合理的假设来简化它。</p>
<section id="在已知环境中简化-kl-梯度估计" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</h2>
<p>实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。</p>
<p>当状态转移概率分布已知时，<img src="https://latex.codecogs.com/png.latex?%5Cforall%20t,%20p_%7B%5Ctheta%7D(a_1,%20%5Ccdots,%20s_t,%20a_t%20%5Cmid%20s_1)"> 都是可以计算的，则 KL 散度可以直接写成：</p>
<p><span id="eq-kl-grad-expansion-known-transition"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Csum_%7B%5Cmathbf%7B%5Ctau%7D%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(%5Cmathbf%7Bs%7D_1)%20p_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1)%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1)%7D%20%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B32%7D"></span></p>
</section>
<section id="简写为-contextual-bandit" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</h2>
<p>为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_1%20=%20%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathcal%7BP%7D,%20(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_T,%20%5Cmathbf%7Ba%7D_T)%20=%20%5Cmathbf%7By%7D%20%5Cin%20%5Cmathcal%7BR%7D">，其中 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D,%20%5Cmathcal%7BR%7D"> 分别表示 prompt / response 空间，则 KL 散度变为：</p>
<p><span id="eq-def-kl-cb"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(x,%20y)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B33%7D"></span></p>
<p>其梯度变为：</p>
<p><span id="eq-def-kl-grad-cb"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B34%7D"></span></p>
<p>其中梯度项可以进一步展开为：</p>
<p><span id="eq-def-kl-grad-cb-grad-term"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%5Cright)%20%5C%5C%0A=&amp;%20%5Cleft(%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20+%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5C%5C%0A=&amp;%20%5Cleft(%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20+%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cfrac%7B1%7D%7B%5Cpi_%5Ctheta(y%20%5Cmid%20x)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Cleft(%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B35%7D"></span></p>
<p>代入回 KL 梯度表达式：</p>
<p><span id="eq-def-kl-grad-cb-expect"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20+%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B36%7D"></span></p>
<p>这里为了重新获得期望形式，引入了 <img src="https://latex.codecogs.com/png.latex?1%20=%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20/%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)">，并利用了 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20=%20%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D"> 和 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20=%200">。</p>
<p>进行 Monte Carlo 估计：</p>
<p><span id="eq-def-kl-grad-cb-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y_i%20%5Cmid%20x_i)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y_i%20%5Cmid%20x_i)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y_i%20%5Cmid%20x_i)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B37%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7Bx%7D_i,%20%5Cmathbf%7By%7D_i)%20%5Csim%20p_%7B%5Ctheta%7D">。</p>
</section>
<section id="还原为已知环境决策过程" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</h2>
<p>将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：</p>
<p><span id="eq-def-kl-grad-kt-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7BT%7D,%20%5Cmathbf%7Ba%7D_%7BT%7D)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_t)%7D%5Cright)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_t)%5Cright)%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B38%7D"></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7B1,%20t%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7B1,%20t%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%7D%5Cright)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7B1,%20t%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B39%7D"></span></p>
</section>
<section id="利用因果性技巧化简-kl-梯度估计" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计<sup>17</sup></h2>
<p>因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。</p>
<p>对于任何 <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20%7C%5Ctau%7C">，我们有 <span id="eq-score-expect-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%7D%5Cleft%5B%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7Ba_t%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5C%5C%0A=&amp;%20%5Csum_%7Ba_j%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cpi_%5Ctheta(a_j%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_j)%20%5Ccdot%200%20%5C%5C%0A=&amp;%200%0A%5Cend%7Baligned%7D%0A%5Ctag%7B40%7D"></span></p>
<p>更进一步，如果 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D"> 是一个与 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D_t,%20%5Cmathbf%7Bs%7D_%7Bt+1%7D,%20%5Cmathbf%7Ba%7D_%7Bt+1%7D,%20%5Cldots"> 独立的随机变量，那么 <span id="eq-score-indep-mul-expect-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Ba%7D_t,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20%5Csim%20p_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D%20)%7D%20%5Cleft%5B%20%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Cright%5D%0A%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D%20)%7D%20%5Cleft%5B%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Cmathbb%7BE%7D_%7B%0A%20%20%20%20(%5Cmathbf%7Bs%7D_%7Bt+1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20%5Csim%20p_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D)%7D%20%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cright%5D%20%5Cright%5D%0A%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D%20)%7D%20%5Cleft%5B%20%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Cright%5D%0A%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%0A%20%20%20%20%20%20%20%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Ccdot%200%20%5Cright%5D%20%5C%5C%0A=&amp;%200%0A%5Cend%7Baligned%7D%0A%5Ctag%7B41%7D"></span></p>
<p>其中，为了利用 Equation&nbsp;40 的结论，我们利用了全期望定律，即</p>
<p><span id="eq-law-of-total-expectation"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20%5Csim%20p%7D%20%5Cleft%5B%5Cmathbf%7Bx%7D%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7By%7D%20%5Csim%20p%7D%20%5Cleft%5B%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D%20%5Csim%20p(%5Ccdot%20%5Cmid%20%5Cmathbf%7By%7D)%7D%20%5B%5Cmathbf%7Bx%7D%5D%20%5Cright%5D%0A%5Ctag%7B42%7D"></span></p>
<p>来引入我们想要的期望。</p>
<p><span id="eq-score-indep-mul-expect-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_i%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5CPsi_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%5Ctheta(s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20p_%5Ctheta(s_%7Bt+1%7D,%20%5Ccdots,%20s_%7B%7C%5Ctau%7C%7D,%20a_%7B%7C%5Ctau%7C%7D%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%20%5CPsi_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7B(s_%7B1%7D,%20a_%7B1%7D,%20%5Ccdots,%20s_%7Bt%7D)%7D%20p_%5Ctheta(s_1,%20a_1,%20%5Ccdots,%20s_t)%20%20%5Csum_%7B(a_%7Bt%7D,%20s_%7Bt+1%7D,%20%5Ccdots,%20s_%7B%7C%5Ctau%7C%7D,%20a_%7B%7C%5Ctau%7C%7D)%7D%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5CPsi_%7Bt'%7D%20%5Cnabla_%5Ctheta%20p_%5Ctheta(s_%7Bt+1%7D,%20%5Ccdots,%20a_%7B%7C%5Ctau%7C%7D%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%20%5C%5C%0A=&amp;%20%5Csum_%7B(s_%7B1%7D,%20a_%7B1%7D,%20%5Ccdots,%20s_%7Bt%7D)%7D%20p_%5Ctheta(s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Csum_%7Ba_t%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Csum_%7B(s_%7Bt+1%7D,%20%5Ccdots,%20s_%7B%7C%5Ctau%7C%7D,%20a_%7B%7C%5Ctau%7C%7D)%7D%20%20p_%5Ctheta(s_%7Bt+1%7D,%20%5Ccdots,%20a_%7B%7C%5Ctau%7C%7D%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%20%5CPsi_%7Bt'%7D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B43%7D"></span></p>
<p>考虑 Monte Carlo 估计式 Equation&nbsp;39 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss-estimator-one-grad"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%0A%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Cright%5D%0A%5Ctag%7B44%7D"></span></p>
<p>由于序列决策过程满足因果性，即 <img src="https://latex.codecogs.com/png.latex?%5Cforall%20t'%20%3C%20t">，<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bt'%7D,%20%5Cmathbf%7Ba%7D_%7Bt'%7D"> 独立于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D">，则可令 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D">，其独立于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bi,%20t%7D,%20%5Cmathbf%7Ba%7D_%7Bi,%20t%7D,%20%5Cldots">，利用 Equation&nbsp;43 的性质，则有 <span id="eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cforall%20t'%20%3C%20t,%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%0A%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Cright%5D%20=%200%0A%5Ctag%7B45%7D"></span></p>
<p>将 Equation&nbsp;45 代入 KL 梯度表达式 (Equation&nbsp;38) ，即可简化得到：</p>
<p><span id="eq-def-kl-grad-kt-reduce"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20=%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7BT%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%0A%5Ctag%7B46%7D"></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%0A%5Ctag%7B47%7D"></span></p>
<p>同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7BKL%7D_%7B%5Ctheta%7D%20=%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Ctext%7Bnograd%7D%5Cleft%20(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%20%5Cright)%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%0A%5Ctag%7B48%7D"></span></p>
<p>这里也可以看到，KL loss 项正确的实现要求：</p>
<ol type="1">
<li>在序列内 token 间，对对数条件似然先求和，得到 KL 样本值，</li>
<li>再在序列间求均值。</li>
</ol>
<p>因此 OpenRLHF (Equation&nbsp;13) 与 verl (Equation&nbsp;14) 的权重都是错误的。</p>
<aside id="footnotes-13" class="footnotes footnotes-end-of-section">
<hr>
<ol start="17">
<li id="fn17"><p>https://www.wikiwand.com/en/articles/Policy_gradient_method↩︎</p></li>
</ol>
</aside>
</section>
<section id="sec-kl-grad-as-kl-reward" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</h2>
<p>在 Equation&nbsp;46 中，令 <img src="https://latex.codecogs.com/png.latex?k%5Cleft(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'%7D,%20%5Cmathbf%7Ba%7D_%7Bt'%7D%5Cright)%20=%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D">，则有： <span id="eq-def-kl-grad-kt-reduce-k"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20=%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7BT%7D%20k%5Cleft(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'%7D,%20%5Cmathbf%7Ba%7D_%7Bt'%7D%5Cright)%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%0A%5Ctag%7B49%7D"></span></p>
<p>不难注意到 Equation&nbsp;49 中 <img src="https://latex.codecogs.com/png.latex?k"> 与 Equation&nbsp;25 中 reward <img src="https://latex.codecogs.com/png.latex?r"> 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。</p>
<p>类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS285<sup>18</sup> 等材料。</p>
<aside id="footnotes-14" class="footnotes footnotes-end-of-section">
<hr>
<ol start="18">
<li id="fn18"><p>https://rail.eecs.berkeley.edu/deeprlcourse/↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="off-policy-设置下如何估计-kl-散度的梯度" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</h1>
<p>上面的推导中，我们假设了 RL 是 on-policy 设置，即采样策略即为最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">。</p>
<p>在这一节，我们进一步考虑 off-policy 设置，即一次采样获得样本会用于多次更新，除了第一次更新，采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 与最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> 都会不同。off-policy 设置给 KL 散度优化带来的问题在于，我们需要优化最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> 的 KL 散度，但却没有来自 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 的样本，这意味着我们无法直接使用梯度估计式 Equation&nbsp;47。</p>
<section id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</h2>
<p>遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20%5Cneq%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，尽管没有来自最新策略 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 的样本，却仍然在使用基于 on-policy 设置的优化方式。</p>
<section id="trl" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="trl"><span class="header-section-number">6.1.1</span> TRL</h3>
<p>TRL 在 Listing&nbsp;1 中计算 KL 样本值使用的 <code>logprobs</code> 及其对应的轨迹样本均来自采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">。对应代码可见 Listing&nbsp;7。</p>
<div id="lst-trl-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: TRL 使用采样样本并使用 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算对数似然<sup>19</sup>
</figcaption>
<div aria-describedby="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1">queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_ids"</span>].to(device)</span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> unwrap_model_for_generation(</span>
<span id="cb8-5">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model, <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#...</span></span>
<span id="cb8-6">) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> unwrapped_model:</span>
<span id="cb8-7">    query_responses, logitss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> batch_generation(</span>
<span id="cb8-8">        unwrapped_model.policy,</span>
<span id="cb8-9">        queries,</span>
<span id="cb8-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb8-11">    )</span>
<span id="cb8-12"></span>
<span id="cb8-13"></span>
<span id="cb8-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, queries.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], args.local_rollout_forward_batch_size):</span>
<span id="cb8-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb8-16">    logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logitss[i : i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.local_rollout_forward_batch_size]</span>
<span id="cb8-17">    logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> selective_log_softmax(logits, response)</span></code></pre></div>
</div>
</figure>
</div>
<p>注意，基于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D%20%5Csim%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算的 KL 样本值可以用于估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">，在第一次更新时，由于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20=%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，所以也可以用于估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。但问题在于，从第二次更新开始，<img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20%5Cneq%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，而我们仍然希望估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。</p>
<p>随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 重新估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。对应代码可见 Listing&nbsp;8。</p>
<div id="lst-trl-ppo-update" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: TRL PPO 多轮更新
</figcaption>
<div aria-describedby="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span>
<span id="cb9-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ppo_epoch_idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(args.num_ppo_epochs):</span>
<span id="cb9-3">    b_inds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.permutation(args.local_batch_size)</span>
<span id="cb9-4">    minibatch_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> mini_batch_start <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, args.local_batch_size, args.local_mini_batch_size):</span>
<span id="cb9-6">        mini_batch_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mini_batch_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.local_mini_batch_size</span>
<span id="cb9-7">        mini_batch_inds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> b_inds[mini_batch_start:mini_batch_end]</span>
<span id="cb9-8">        gradient_accumulation_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> micro_batch_start <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, args.local_mini_batch_size, args.per_device_train_batch_size):</span>
<span id="cb9-10">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> accelerator.accumulate(model):</span>
<span id="cb9-11">                micro_batch_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> micro_batch_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.per_device_train_batch_size</span>
<span id="cb9-12">                micro_batch_inds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mini_batch_inds[micro_batch_start:micro_batch_end]</span>
<span id="cb9-13">                mb_advantage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> advantages[micro_batch_inds]</span>
<span id="cb9-14">                mb_responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> responses[micro_batch_inds]</span>
<span id="cb9-15">                mb_query_responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> query_responses[micro_batch_inds]</span>
<span id="cb9-16">                mb_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logprobs[micro_batch_inds]</span>
<span id="cb9-17">                mb_return <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns[micro_batch_inds]</span>
<span id="cb9-18">                mb_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> values[micro_batch_inds]</span>
<span id="cb9-19"></span>
<span id="cb9-20"></span>
<span id="cb9-21">                output, vpred_temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> forward(model, mb_query_responses, processing_class.pad_token_id)</span>
<span id="cb9-22">                logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output.logits[:, context_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> : <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb9-23">                logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> args.temperature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span></span>
<span id="cb9-24">                new_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> selective_log_softmax(logits, mb_responses)</span>
<span id="cb9-25">                new_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.masked_fill(</span>
<span id="cb9-26">                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB</span>
<span id="cb9-27">                )</span>
<span id="cb9-28">                vpred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vpred_temp[:, context_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> : <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].squeeze(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-29">                vpred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb9-30">                vpredclipped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.clamp(</span>
<span id="cb9-31">                    vpred,</span>
<span id="cb9-32">                    mb_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> args.cliprange_value,</span>
<span id="cb9-33">                    mb_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.cliprange_value,</span>
<span id="cb9-34">                )</span>
<span id="cb9-35">                vf_losses1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.square(vpred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mb_return)</span>
<span id="cb9-36">                vf_losses2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.square(vpredclipped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mb_return)</span>
<span id="cb9-37">                vf_loss_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(vf_losses1, vf_losses2)</span>
<span id="cb9-38">                vf_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> masked_mean(vf_loss_max, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>padding_mask_p1[micro_batch_inds])</span>
<span id="cb9-39">                vf_clipfrac <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(</span>
<span id="cb9-40">                    (vf_losses2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> vf_losses1).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>padding_mask_p1[micro_batch_inds]</span>
<span id="cb9-41">                )</span>
<span id="cb9-42">                logprobs_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> new_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mb_logprobs</span>
<span id="cb9-43">                ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(logprobs_diff)</span>
<span id="cb9-44">                pg_losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mb_advantage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ratio</span>
<span id="cb9-45">                pg_losses2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mb_advantage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.clamp(ratio, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> args.cliprange, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.cliprange)</span>
<span id="cb9-46">                pg_loss_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(pg_losses, pg_losses2)</span>
<span id="cb9-47">                pg_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(pg_loss_max, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>padding_mask[micro_batch_inds])</span>
<span id="cb9-48">                loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.vf_coef <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> vf_loss</span>
<span id="cb9-49">                accelerator.backward(loss)</span>
<span id="cb9-50">                optimizer.step()</span>
<span id="cb9-51">                optimizer.zero_grad()</span></code></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-15" class="footnotes footnotes-end-of-section">
<hr>
<ol start="19">
<li id="fn19"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432↩︎</p></li>
</ol>
</aside>
</section>
<section id="openrlhf-1" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</h3>
<p>类似地，OpenRLHF 在 Listing&nbsp;2 中计算 KL 样本值使用的 <code>log_probs</code> 在 <code>make_experience</code> 时被计算，和对应的样本 <code>sequences</code> 都来自采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，而非当前策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D">。对应代码可见 Listing&nbsp;9。</p>
<div id="lst-openrlhf-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;9: OpenRLHF 采样样本并使用 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算对数似然
</figcaption>
<div aria-describedby="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595</span></span>
<span id="cb10-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> make_experience(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, samples: Samples) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Experience:</span>
<span id="cb10-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb10-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.</span></span>
<span id="cb10-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb10-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680</span></span>
<span id="cb10-8">    action_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor(</span>
<span id="cb10-9">        sequences,</span>
<span id="cb10-10">        num_actions,</span>
<span id="cb10-11">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-12">    )</span>
<span id="cb10-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709</span></span>
<span id="cb10-15">    kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_approx_kl(</span>
<span id="cb10-16">        action_log_probs,</span>
<span id="cb10-17">        base_action_log_probs,</span>
<span id="cb10-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-19">    )</span></code></pre></div>
</div>
</figure>
</div>
<p>从 Listing&nbsp;3 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 计算的对数似然，但如 Section&nbsp;4.1 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。</p>
</section>
<section id="verl-1" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="verl-1"><span class="header-section-number">6.1.3</span> verl</h3>
<p>从 Listing&nbsp;4 可见，verl 同样使用 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算 KL 样本值。</p>
<p>从 Listing&nbsp;5 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 的 KL 样本值。</p>
</section>
</section>
<section id="利用重要性采样处理-off-policy-设置" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</h2>
<p>off-policy 设置下，我们没有来自最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 的样本，而只能使用来自采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 的样本，但我们仍然希望估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%20%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。</p>
<p>熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即</p>
<p><span id="eq-is-off-policy-kl"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5Bf(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20f(%5Ctau)%20%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Ctau)%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Ctau)%7D%20f(%5Ctau)%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%20%5Cleft%5B%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20f(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%0A%5Ctag%7B50%7D"></span></p>
<p>此处，重要性采样系数 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D"> 可以仿照 Equation&nbsp;5 展开为：</p>
<p><span id="eq-is-coef-expansion"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20=%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%0A%5Ctag%7B51%7D"></span> <sup>20</sup></p>
<p>利用重要性采样 (Equation&nbsp;50, Equation&nbsp;51) ，KL 梯度表达式 Equation&nbsp;46 可以转化为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%20%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A=&amp;%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7BT%7D,%20%5Cmathbf%7Ba%7D_%7BT%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7BT%7D,%20%5Cmathbf%7Ba%7D_%7BT%7D)%7D%20%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%20%5Cright%5D%20%5C%5C%0A=&amp;%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%5Cright)%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B52%7D"></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A%5Capprox&amp;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%20%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%20%5C%5C%0A=&amp;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Cleft(%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%20%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%20%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B53%7D"></span></p>
<p>对应的 loss 函数为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7BKL%7D_%7B%5Ctheta%7D%20=%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%20%5Ctext%7Bnograd%7D%5Cleft(%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%20%5Cright)%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Ctag%7B54%7D"></span></p>
<p>类似 Equation&nbsp;49，我们可以令</p>
<p><span id="eq-def-kl-reward-is"><img src="https://latex.codecogs.com/png.latex?%0Ak(%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%20=%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%20%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%0A%5Ctag%7B55%7D"></span></p>
<p>注意，Equation&nbsp;55 中的 <img src="https://latex.codecogs.com/png.latex?k"> 需要对于每个新的 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 重新计算。</p>
<aside id="footnotes-16" class="footnotes footnotes-end-of-section">
<hr>
<ol start="20">
<li id="fn20"><p>实际计算中，Equation&nbsp;51 由于涉及到 <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7B%5Ctau%7D%7C"> 次连乘，方差大且数值稳定性差，需要利用因果性、近似等技术来化简。本文目前省略该部分，后续将会更新相关内容。↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="结论如何正确地在-rl-中优化-kl-散度" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</h1>
<section id="修正-grpo-公式中的-kl-项" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</h2>
<p>GRPO 公式 (Equation&nbsp;1, Equation&nbsp;2) 对于 KL 优化主要存在两个错误：</p>
<ol type="1">
<li>忽略了 KL 优化的 off-policy 问题</li>
<li>先将 <img src="https://latex.codecogs.com/png.latex?k_%7B3%7D"> 估计样本量应用于动作条件似然再求和，导致得到异常的梯度</li>
</ol>
<p>对于这两个问题，在 Equation&nbsp;29 的基础上，仿照 Equation&nbsp;55，我们可以按如下方式修正：</p>
<p><span id="eq-grpo-obj-kl-fixed-is"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Cleft%5C%7B%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D%20%5Cright%5C%7D%20-%5Cbeta%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7Co_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(o_%7Bi,%20t%7D%20%7C%20q,%20o_%7Bi,%5Clt%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(o_%7Bi,%20t%7D%20%7C%20q,%20o_%7Bi,%5Clt%20t%7D)%7D%5Cright)%20k%5Cleft(%20%5Cfrac%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B56%7D"></span></p>
</section>
<section id="修正流行-llm-rl-框架中的-kl-优化实现" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</h2>
<p>目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：</p>
<ol type="1">
<li>实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）</li>
<li>错误地实现了平均操作</li>
</ol>
<p>对于这些问题，可以按照如下思路修正：</p>
<ol type="1">
<li>为 KL 项添加重要性采样，这需要从第二轮更新开始，每次基于新的 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> 重新计算 KL loss / reward 项，包括重要性采样系数</li>
<li>应用 KL 估计样本量时，先对于序列内 token 间的对数条件似然求和，得到轨迹联合概率，再代入公式</li>
<li>如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 Equation&nbsp;55 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）</li>
<li>如果不希望应用 reward 优化的其他技术，可以按 Equation&nbsp;54 实现为 KL loss 项</li>
</ol>
</section>
</section>
<section id="讨论" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> 讨论</h1>
<section id="对于-kl-梯度更好的估计样本量" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</h2>
<p>如 Section&nbsp;5.5 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？</p>
<p>此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？</p>
</section>
<section id="kl-regularized-rl-的理论优势" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</h2>
<p>最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。</p>
<p>然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 <span class="citation" data-cites="zhao2025logregretkl">Zhao et al. (2025)</span> 证明了 KL-regularized RL 的 regret 只有 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Clog%20T)">，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Csqrt%7BT%7D)">。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="附录" class="level1 appendix" data-number="9"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9</span> 附录</h2><div class="quarto-appendix-contents">

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>本文的作者（童雨轩）仍在寻求北美的 Ph.D.&nbsp;或 RA 机会。如果你觉得本文对你有帮助，欢迎浏览其主页<sup>21</sup>来获取进一步了解。</p>
</div>
</div>




</div></section><section id="相关工作" class="level2 appendix" data-number="9.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.1</span> 相关工作</h2><div class="quarto-appendix-contents">

<p>与本文同期也有许多精彩的讨论，由于笔者还没能通读全文，此处仅提供链接，不作概括，欢迎感兴趣的读者自行阅读：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/28440962040">GRPO 中的 KL Loss 实现细节问题 - Hongyu Zang @ 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28735759256">k2 loss就是比k3 loss好！以及GRPO_off-policy - Yiming Liu @ 知乎</a></li>
</ul>
<aside id="footnotes-17" class="footnotes footnotes-end-of-section">
<hr>
<ol start="21">
<li id="fn21"><p>https://tongyx361.github.io↩︎</p></li>
</ol>
</aside>
</div></section><section id="写作契机trpoppo-与-grpo-中的-kl-为什么不一样" class="level2 appendix" data-number="9.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.2</span> 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”</h2><div class="quarto-appendix-contents">

<p>笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题<sup>22</sup>：</p>
<blockquote class="blockquote">
<p>A small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?</p>
<p>TRPO <span class="citation" data-cites="schulman2015trpo">(Schulman et al. 2015)</span></p>
</blockquote>
<p><span id="eq-trpo"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cunderset%7B%5Ctheta%7D%7B%5Ctext%7Bmaximize%7D%7D~L_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D(%5Ctheta)%20%5C%5C%0A&amp;%20%5Ctext%20%7B%20subject%20to%20%7D%20%5Cbar%7BD%7D_%7B%5Cmathrm%7BKL%7D%7D%5E%7B%5Crho_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%7D%5Cleft(%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D,%20%5Ctheta%5Cright)%20%5Cleq%20%5Cdelta%0A%5Cend%7Baligned%7D%0A%5Ctag%7B57%7D"></span></p>
<blockquote class="blockquote">
<p>PPO <span class="citation" data-cites="schulman2017ppo">(Schulman et al. 2017)</span></p>
</blockquote>
<p><span id="eq-ppo-klpen"><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7BK%20L%20P%20E%20N%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7By%7D_t%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(%5Cmathbf%7By%7D_t%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright)%7D%20%5Chat%7BA%7D_t-%5Cbeta%20%5Cmathrm%7BKL%7D%5Cleft%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright),%20%5Cpi_%5Ctheta%5Cleft(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright)%5Cright%5D%5Cright%5D%0A%5Ctag%7B58%7D"></span></p>
<blockquote class="blockquote">
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(Shao et al. 2024)</span></p>
</blockquote>
<p><span id="eq-grpo"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%5Cleft%5C%7B%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D-%5Cbeta%20%5Cmathbb%7BD%7D_%7BK%20L%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B59%7D"></span></p>
<p>这个问题本身的答案是非常简单的。</p>
<p>首先，这个问题混淆了两种不同的 KL 惩罚项：</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BKL%7D%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D,%5Cpi_%7B%5Ctheta%7D%5D">，其作用是约束最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D">不要离采样策略<img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BKL%7D%5B%5Cpi_%7B%5Ctheta%7D,%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D%5D">，其作用是约束最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D">不要离参考策略<img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D"> 太远，从而更充分地利用参考策略中的先验。</li>
</ol>
<p>另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BKL%7D%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D,%5Cpi_%7B%5Ctheta%7D%5D">。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：</p>
<blockquote class="blockquote">
<p>Let <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> denote the probability ratio <img src="https://latex.codecogs.com/png.latex?r_%7Bt%7D(%5Ctheta)=%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cleft(%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft%7Ca_t%5Cright%7C%20s_t%5Cright)%7D">, so <img src="https://latex.codecogs.com/png.latex?r%5Cleft(%5Ctheta_%7B%5Ctext%7Bold%7D%7D%5Cright)=1">. TRPO maximizes a “surrogate” objective</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%20%5Chat%7BA%7D_t%5Cright%5D=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5Br_t(%5Ctheta)%20%5Chat%7BA%7D_t%5Cright%5D%20.%0A"></p>
<blockquote class="blockquote">
<p>…</p>
<p>The main objective we propose is the following:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCLIP%7D%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cmin%20%5Cleft(r_t(%5Ctheta)%20%5Chat%7BA%7D_t,%20%5Ctext%7Bclip%7D%5Cleft(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon%5Cright)%20%5Chat%7BA%7D_t%5Cright)%5Cright%5D%0A"></p>
<blockquote class="blockquote">
<p>where epsilon is a hyperparameter, say, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon=0.2">. The motivation for this objective is as follows. The first term inside the <img src="https://latex.codecogs.com/png.latex?%5Cmin"> is <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D">. The second term, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bclip%7D%5Cleft(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon%5Cright)%20%5Chat%7BA%7D_t">, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving <img src="https://latex.codecogs.com/png.latex?r_t"> outside of the interval <img src="https://latex.codecogs.com/png.latex?%5B1-%5Cepsilon,%201+%5Cepsilon%5D">.</p>
<p>…</p>
<p><strong>Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence</strong>, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence <img src="https://latex.codecogs.com/png.latex?d_%7B%5Ctext%7Btarg%7D%7D"> each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve included it here because it’s an important baseline.</p>
<p>In the simplest instantiation of this algorithm, we perform the following steps in each policy update:</p>
<ul>
<li>Using several epochs of minibatch SGD, optimize the KL-penalized objective</li>
</ul>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BKLPEN%7D%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%20%5Chat%7BA%7D_t-%5Cbeta%20%5Cmathrm%7BKL%7D%5Cleft%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(%5Ccdot%20%5Cmid%20s_t%5Cright),%20%5Cpi_%5Ctheta%5Cleft(%5Ccdot%20%5Cmid%20s_t%5Cright)%5Cright%5D%5Cright%5D%0A"></p>
<blockquote class="blockquote">

</blockquote>
<p>顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)=%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D">就是<img src="https://latex.codecogs.com/png.latex?K%20L%5Cleft%5B%5Cpi_%7B%5Ctheta_%7Bd%20d%7D%7D,%20%5Cpi_%5Ctheta%5Cright%5D=%5Cmathbb%7BE%7D_%7Ba_t%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bd%20t%7D%7D%5Cleft(%5Ccdot%20%5Cmid%20s_t%5Cright)%7D%5Cleft%5B%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta_%7Bd%20t%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%5Cright%5D"> 中对单个样本 <img src="https://latex.codecogs.com/png.latex?(s_t,%20a_t)"> 的值中 <img src="https://latex.codecogs.com/png.latex?%5Clog"> 的真数。</p>
<aside id="footnotes-18" class="footnotes footnotes-end-of-section">
<hr>
<ol start="22">
<li id="fn22"><p>https://x.com/pufanyi/status/1888845956684370202↩︎</p></li>
</ol>
</aside>
</div></section><section id="致谢" class="level2 appendix" data-number="9.3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.3</span> 致谢</h2><div class="quarto-appendix-contents">

<p>感谢王浩然、YuMS 对本文提供的重要反馈。</p>
<p>感谢生广明、Wei Xiong、刘仁彪、刘威、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论以及对于本文的有益反馈。</p>
<p>感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。</p>
</div></section><section id="引用" class="level2 appendix" data-number="9.4"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.4</span> 引用</h2><div class="quarto-appendix-contents">

<p>BibTeX:</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode numberSource bibtex number-lines code-with-copy"><code class="sourceCode bibtex"><span id="cb11-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">@online</span>{<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">tong2025kl</span>,</span>
<span id="cb11-2">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">author</span> = {童雨轩},</span>
<span id="cb11-3">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span> = {重新思考 {RL} 中的 {KL} 梯度优化},</span>
<span id="cb11-4">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">year</span> = {2025},</span>
<span id="cb11-5">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">url</span> = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},</span>
<span id="cb11-6">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">urldate</span> = {2025-03-09},</span>
<span id="cb11-7">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">language</span> = {Chinese},</span>
<span id="cb11-8">}</span></code></pre></div>
<p>文本：</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb12-1">童雨轩. 2025. “重新思考 RL 中的 KL 梯度优化.” https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh.</span></code></pre></div>


<!-- -->


</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-hu2024openrlhf" class="csl-entry">
Hu, Jian, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. <span>“OpenRLHF: An Easy-to-Use, Scalable and High-Performance RLHF Framework.”</span> <em>arXiv Preprint arXiv:2405.11143</em>.
</div>
<div id="ref-jaques2019wayoffpolicy" class="csl-entry">
Jaques, Natasha, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. <span>“Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.”</span> <a href="https://arxiv.org/abs/1907.00456">https://arxiv.org/abs/1907.00456</a>.
</div>
<div id="ref-ouyang2022instructgpt" class="csl-entry">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. <a href="https://openreview.net/forum?id=TG8KACxEON">https://openreview.net/forum?id=TG8KACxEON</a>.
</div>
<div id="ref-schulman2015trpo" class="csl-entry">
Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 1889–97. PMLR.
</div>
<div id="ref-schulman2018gae" class="csl-entry">
Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2018. <span>“High-Dimensional Continuous Control Using Generalized Advantage Estimation.”</span> <a href="https://arxiv.org/abs/1506.02438">https://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-schulman2017ppo" class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv:1707.06347</em>.
</div>
<div id="ref-shao2024deepseekmath" class="csl-entry">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, et al. 2024. <span>“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.”</span> <em>arXiv Preprint arXiv:2402.03300</em>.
</div>
<div id="ref-sheng2024hybridflow" class="csl-entry">
Sheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. <span>“HybridFlow: A Flexible and Efficient RLHF Framework.”</span> <em>arXiv Preprint arXiv: 2409.19256</em>.
</div>
<div id="ref-stiennon2020summarize" class="csl-entry">
Stiennon, Nisan, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020. <span>“Learning to Summarize with Human Feedback.”</span> In <em>NeurIPS</em>. <a href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html</a>.
</div>
<div id="ref-zhao2025logregretkl" class="csl-entry">
Zhao, Heyang, Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. 2025. <span>“Logarithmic Regret for Online KL-Regularized Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2502.07460">https://arxiv.org/abs/2502.07460</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div> ]]></description>
  <category>Chinese 中文</category>
  <category>Technical 技术</category>
  <guid>https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh/</guid>
  <pubDate>Sun, 09 Mar 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title></title>
  <dc:creator>Yuxuan Tong</dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/verl-tutorial/</link>
  <description><![CDATA[ undefined ]]></description>
  <guid>https://tongyx361.github.io/blogs/posts/verl-tutorial/</guid>
  <pubDate>Fri, 23 May 2025 11:32:50 GMT</pubDate>
</item>
</channel>
</rss>
