<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="童雨轩">

<title>重新思考 RL 中的 KL 梯度优化 – Shawn/Yuxuan Tong 童雨轩</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d7e1bcfc089a344b07019795ab637039.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-42b010fded3c64b7df49bf84b47d11a3.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shawn/Yuxuan Tong 童雨轩</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://tongyx361.github.io"> <i class="bi bi-person" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/tongyx361"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tongyx361"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">重新思考 RL 中的 KL 梯度优化</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">修正 GRPO 公式与流行 LLM RL 框架</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Chinese 中文</div>
                <div class="quarto-category">Technical 技术</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>童雨轩 <a href="mailto:tongyuxuan361@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">2025/03/09</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Takeways</div>
      <p>对于 LLM RL 中相对于参考策略的 KL 优化，GRPO 公式</p>
      <ol type="1">
      <li>没有处理 KL 项的 off-policy 问题，这可以通过在多轮更新时重新计算 KL 项并添加重要性采样系数解决</li>
      <li>先将 KL 估计样本量应用于动作对数条件似然再求和，而非先求和得到概率再应用估计样本量，与 John Schulman “Approximating KL Divergence”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> 分析不符（对应导出的梯度也可能因此而错误）</li>
      </ol>
      <p>目前流行的 LLM RL 框架（TRL，OpenRLHF，verl）也没有避免上述问题，且存在其他问题：</p>
      <ol start="3" type="1">
      <li>在计算 KL loss 项时默认不去除任何梯度，实际得到的梯度通常不是在优化 KL 散度</li>
      <li>KL loss 项的平均操作存在错误。</li>
      </ol>
      <p>本文基于序列决策过程（而非 bandit）建模分析了上述问题，并提供了正确的 KL loss / reward 项实现的数学推导与上述问题的修正思路。</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#sec-grpo-kl-misunderstanding" id="toc-sec-grpo-kl-misunderstanding" class="nav-link active" data-scroll-target="#sec-grpo-kl-misunderstanding"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</a></li>
  <li><a href="#sec-popular-llm-rl-kl-optim" id="toc-sec-popular-llm-rl-kl-optim" class="nav-link" data-scroll-target="#sec-popular-llm-rl-kl-optim"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</a>
  <ul class="collapse">
  <li><a href="#trlkl-reward-项" id="toc-trlkl-reward-项" class="nav-link" data-scroll-target="#trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</a></li>
  <li><a href="#openrlhf" id="toc-openrlhf" class="nav-link" data-scroll-target="#openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</a>
  <ul class="collapse">
  <li><a href="#sec-openrlhf-kl-reward" id="toc-sec-openrlhf-kl-reward" class="nav-link" data-scroll-target="#sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项" id="toc-kl-loss-项" class="nav-link" data-scroll-target="#kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#verl" id="toc-verl" class="nav-link" data-scroll-target="#verl"><span class="header-section-number">2.3</span> verl</a>
  <ul class="collapse">
  <li><a href="#kl-reward-项" id="toc-kl-reward-项" class="nav-link" data-scroll-target="#kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项-1" id="toc-kl-loss-项-1" class="nav-link" data-scroll-target="#kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#sec-why-kl-reward" id="toc-sec-why-kl-reward" class="nav-link" data-scroll-target="#sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</a>
  <ul class="collapse">
  <li><a href="#kl-reward-的流行应当源自-rlhf-与-instructgpt" id="toc-kl-reward-的流行应当源自-rlhf-与-instructgpt" class="nav-link" data-scroll-target="#kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</a></li>
  <li><a href="#sec-oai-kl-reward-src" id="toc-sec-oai-kl-reward-src" class="nav-link" data-scroll-target="#sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</a></li>
  <li><a href="#kl-reward-最早的出处" id="toc-kl-reward-最早的出处" class="nav-link" data-scroll-target="#kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-rl-kl-optim-formulation" id="toc-sec-rl-kl-optim-formulation" class="nav-link" data-scroll-target="#sec-rl-kl-optim-formulation"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</a>
  <ul class="collapse">
  <li><a href="#rl-中的-kl-散度通常定义在轨迹分布上" id="toc-rl-中的-kl-散度通常定义在轨迹分布上" class="nav-link" data-scroll-target="#rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</a></li>
  <li><a href="#将轨迹展开为状态-动作序列" id="toc-将轨迹展开为状态-动作序列" class="nav-link" data-scroll-target="#将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</a></li>
  <li><a href="#markov-决策过程中的-kl-散度" id="toc-markov-决策过程中的-kl-散度" class="nav-link" data-scroll-target="#markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</a></li>
  <li><a href="#sec-lm-as-dp" id="toc-sec-lm-as-dp" class="nav-link" data-scroll-target="#sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</a></li>
  <li><a href="#估计-kl-散度" id="toc-估计-kl-散度" class="nav-link" data-scroll-target="#估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</a>
  <ul class="collapse">
  <li><a href="#几乎不可能直接计算-kl-散度的真实值" id="toc-几乎不可能直接计算-kl-散度的真实值" class="nav-link" data-scroll-target="#几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</a></li>
  <li><a href="#通常使用-monte-carlo-方法估计-kl-散度" id="toc-通常使用-monte-carlo-方法估计-kl-散度" class="nav-link" data-scroll-target="#通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</a></li>
  <li><a href="#不同的-kl-估计量" id="toc-不同的-kl-估计量" class="nav-link" data-scroll-target="#不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#流行-on-policy-kl-优化实现的数学形式化" id="toc-流行-on-policy-kl-优化实现的数学形式化" class="nav-link" data-scroll-target="#流行-on-policy-kl-优化实现的数学形式化"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</a>
  <ul class="collapse">
  <li><a href="#sec-kl-loss-impl" id="toc-sec-kl-loss-impl" class="nav-link" data-scroll-target="#sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</a>
  <ul class="collapse">
  <li><a href="#不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" id="toc-不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" class="nav-link" data-scroll-target="#不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</a></li>
  <li><a href="#k_1-导出的梯度期望为-0" id="toc-k_1-导出的梯度期望为-0" class="nav-link" data-scroll-target="#k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <span class="math inline">\(k_1\)</span> 导出的梯度：期望为 0</a></li>
  <li><a href="#k_2-导出的梯度" id="toc-k_2-导出的梯度" class="nav-link" data-scroll-target="#k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <span class="math inline">\(k_2\)</span> 导出的梯度</a></li>
  <li><a href="#k_3-导出的梯度" id="toc-k_3-导出的梯度" class="nav-link" data-scroll-target="#k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <span class="math inline">\(k_3\)</span> 导出的梯度</a></li>
  <li><a href="#小结流行的-kl-loss-项-实现并不合理" id="toc-小结流行的-kl-loss-项-实现并不合理" class="nav-link" data-scroll-target="#小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</a></li>
  </ul></li>
  <li><a href="#分析流行的-kl-reward-项-实现" id="toc-分析流行的-kl-reward-项-实现" class="nav-link" data-scroll-target="#分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</a>
  <ul class="collapse">
  <li><a href="#sec-analogy-pg-kl" id="toc-sec-analogy-pg-kl" class="nav-link" data-scroll-target="#sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</a></li>
  <li><a href="#不同-kl-估计量导出的-reward-项的作用" id="toc-不同-kl-估计量导出的-reward-项的作用" class="nav-link" data-scroll-target="#不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</a></li>
  <li><a href="#小结在-on-policy-设置下修正-grpo-目标的-kl-项" id="toc-小结在-on-policy-设置下修正-grpo-目标的-kl-项" class="nav-link" data-scroll-target="#小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-derive-kld-grad" id="toc-sec-derive-kld-grad" class="nav-link" data-scroll-target="#sec-derive-kld-grad"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</a>
  <ul class="collapse">
  <li><a href="#在已知环境中简化-kl-梯度估计" id="toc-在已知环境中简化-kl-梯度估计" class="nav-link" data-scroll-target="#在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</a></li>
  <li><a href="#简写为-contextual-bandit" id="toc-简写为-contextual-bandit" class="nav-link" data-scroll-target="#简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</a></li>
  <li><a href="#还原为已知环境决策过程" id="toc-还原为已知环境决策过程" class="nav-link" data-scroll-target="#还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</a></li>
  <li><a href="#利用因果性技巧化简-kl-梯度估计" id="toc-利用因果性技巧化简-kl-梯度估计" class="nav-link" data-scroll-target="#利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计</a></li>
  <li><a href="#sec-kl-grad-as-kl-reward" id="toc-sec-kl-grad-as-kl-reward" class="nav-link" data-scroll-target="#sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</a></li>
  </ul></li>
  <li><a href="#off-policy-设置下如何估计-kl-散度的梯度" id="toc-off-policy-设置下如何估计-kl-散度的梯度" class="nav-link" data-scroll-target="#off-policy-设置下如何估计-kl-散度的梯度"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</a>
  <ul class="collapse">
  <li><a href="#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" id="toc-流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" class="nav-link" data-scroll-target="#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</a>
  <ul class="collapse">
  <li><a href="#trl" id="toc-trl" class="nav-link" data-scroll-target="#trl"><span class="header-section-number">6.1.1</span> TRL</a></li>
  <li><a href="#openrlhf-1" id="toc-openrlhf-1" class="nav-link" data-scroll-target="#openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</a></li>
  <li><a href="#verl-1" id="toc-verl-1" class="nav-link" data-scroll-target="#verl-1"><span class="header-section-number">6.1.3</span> verl</a></li>
  </ul></li>
  <li><a href="#利用重要性采样处理-off-policy-设置" id="toc-利用重要性采样处理-off-policy-设置" class="nav-link" data-scroll-target="#利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</a></li>
  </ul></li>
  <li><a href="#结论如何正确地在-rl-中优化-kl-散度" id="toc-结论如何正确地在-rl-中优化-kl-散度" class="nav-link" data-scroll-target="#结论如何正确地在-rl-中优化-kl-散度"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</a>
  <ul class="collapse">
  <li><a href="#修正-grpo-公式中的-kl-项" id="toc-修正-grpo-公式中的-kl-项" class="nav-link" data-scroll-target="#修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</a></li>
  <li><a href="#修正流行-llm-rl-框架中的-kl-优化实现" id="toc-修正流行-llm-rl-框架中的-kl-优化实现" class="nav-link" data-scroll-target="#修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</a></li>
  </ul></li>
  <li><a href="#讨论" id="toc-讨论" class="nav-link" data-scroll-target="#讨论"><span class="header-section-number">8</span> 讨论</a>
  <ul class="collapse">
  <li><a href="#对于-kl-梯度更好的估计样本量" id="toc-对于-kl-梯度更好的估计样本量" class="nav-link" data-scroll-target="#对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</a></li>
  <li><a href="#kl-regularized-rl-的理论优势" id="toc-kl-regularized-rl-的理论优势" class="nav-link" data-scroll-target="#kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#sec-grpo-kl-misunderstanding" id="toc-sec-grpo-kl-misunderstanding"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</a></li>
  <li><a href="#sec-popular-llm-rl-kl-optim" id="toc-sec-popular-llm-rl-kl-optim"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</a>
  <ul>
  <li><a href="#trlkl-reward-项" id="toc-trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</a></li>
  <li><a href="#openrlhf" id="toc-openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</a>
  <ul>
  <li><a href="#sec-openrlhf-kl-reward" id="toc-sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项" id="toc-kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#verl" id="toc-verl"><span class="header-section-number">2.3</span> verl</a>
  <ul>
  <li><a href="#kl-reward-项" id="toc-kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项-1" id="toc-kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#sec-why-kl-reward" id="toc-sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</a>
  <ul>
  <li><a href="#kl-reward-的流行应当源自-rlhf-与-instructgpt" id="toc-kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</a></li>
  <li><a href="#sec-oai-kl-reward-src" id="toc-sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</a></li>
  <li><a href="#kl-reward-最早的出处" id="toc-kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-rl-kl-optim-formulation" id="toc-sec-rl-kl-optim-formulation"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</a>
  <ul>
  <li><a href="#rl-中的-kl-散度通常定义在轨迹分布上" id="toc-rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</a></li>
  <li><a href="#将轨迹展开为状态-动作序列" id="toc-将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</a></li>
  <li><a href="#markov-决策过程中的-kl-散度" id="toc-markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</a></li>
  <li><a href="#sec-lm-as-dp" id="toc-sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</a></li>
  <li><a href="#估计-kl-散度" id="toc-估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</a>
  <ul>
  <li><a href="#几乎不可能直接计算-kl-散度的真实值" id="toc-几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</a></li>
  <li><a href="#通常使用-monte-carlo-方法估计-kl-散度" id="toc-通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</a></li>
  <li><a href="#不同的-kl-估计量" id="toc-不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#流行-on-policy-kl-优化实现的数学形式化" id="toc-流行-on-policy-kl-优化实现的数学形式化"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</a>
  <ul>
  <li><a href="#sec-kl-loss-impl" id="toc-sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</a>
  <ul>
  <li><a href="#不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" id="toc-不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</a></li>
  <li><a href="#k_1-导出的梯度期望为-0" id="toc-k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <span class="math inline">\(k_1\)</span> 导出的梯度：期望为 0</a></li>
  <li><a href="#k_2-导出的梯度" id="toc-k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <span class="math inline">\(k_2\)</span> 导出的梯度</a></li>
  <li><a href="#k_3-导出的梯度" id="toc-k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <span class="math inline">\(k_3\)</span> 导出的梯度</a></li>
  <li><a href="#小结流行的-kl-loss-项-实现并不合理" id="toc-小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</a></li>
  </ul></li>
  <li><a href="#分析流行的-kl-reward-项-实现" id="toc-分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</a>
  <ul>
  <li><a href="#sec-analogy-pg-kl" id="toc-sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</a></li>
  <li><a href="#不同-kl-估计量导出的-reward-项的作用" id="toc-不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</a></li>
  <li><a href="#小结在-on-policy-设置下修正-grpo-目标的-kl-项" id="toc-小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-derive-kld-grad" id="toc-sec-derive-kld-grad"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</a>
  <ul>
  <li><a href="#在已知环境中简化-kl-梯度估计" id="toc-在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</a></li>
  <li><a href="#简写为-contextual-bandit" id="toc-简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</a></li>
  <li><a href="#还原为已知环境决策过程" id="toc-还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</a></li>
  <li><a href="#利用因果性技巧化简-kl-梯度估计" id="toc-利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计</a></li>
  <li><a href="#sec-kl-grad-as-kl-reward" id="toc-sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</a></li>
  </ul></li>
  <li><a href="#off-policy-设置下如何估计-kl-散度的梯度" id="toc-off-policy-设置下如何估计-kl-散度的梯度"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</a>
  <ul>
  <li><a href="#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" id="toc-流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</a>
  <ul>
  <li><a href="#trl" id="toc-trl"><span class="header-section-number">6.1.1</span> TRL</a></li>
  <li><a href="#openrlhf-1" id="toc-openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</a></li>
  <li><a href="#verl-1" id="toc-verl-1"><span class="header-section-number">6.1.3</span> verl</a></li>
  </ul></li>
  <li><a href="#利用重要性采样处理-off-policy-设置" id="toc-利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</a></li>
  </ul></li>
  <li><a href="#结论如何正确地在-rl-中优化-kl-散度" id="toc-结论如何正确地在-rl-中优化-kl-散度"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</a>
  <ul>
  <li><a href="#修正-grpo-公式中的-kl-项" id="toc-修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</a></li>
  <li><a href="#修正流行-llm-rl-框架中的-kl-优化实现" id="toc-修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</a></li>
  </ul></li>
  <li><a href="#讨论" id="toc-讨论"><span class="header-section-number">8</span> 讨论</a>
  <ul>
  <li><a href="#对于-kl-梯度更好的估计样本量" id="toc-对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</a></li>
  <li><a href="#kl-regularized-rl-的理论优势" id="toc-kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</a></li>
  </ul></li>
  <li><a href="#附录" id="toc-附录"><span class="header-section-number">9</span> 附录</a>
  <ul>
  <li><a href="#相关工作" id="toc-相关工作"><span class="header-section-number">9.1</span> 相关工作</a></li>
  <li><a href="#写作契机trpoppo-与-grpo-中的-kl-为什么不一样" id="toc-写作契机trpoppo-与-grpo-中的-kl-为什么不一样"><span class="header-section-number">9.2</span> 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”</a></li>
  <li><a href="#致谢" id="toc-致谢"><span class="header-section-number">9.3</span> 致谢</a></li>
  <li><a href="#引用" id="toc-引用"><span class="header-section-number">9.4</span> 引用</a></li>
  </ul></li>
  </ul>
</nav>
<section id="sec-grpo-kl-misunderstanding" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</h1>
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(<a href="#ref-shao2024deepseekmath" role="doc-biblioref">Shao et al. 2024</a>)</span> 的优化目标公式为：</p>
<p><span id="eq-grpo-obj"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left\{\min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]-\beta \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\right\}
\end{aligned}
\tag{1}\]</span></span></p>
<p>其中</p>
<p><span id="eq-grpo-obj-kl-term"><span class="math display">\[
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]=\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1
\tag{2}\]</span></span></p>
<p>首先，<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a> 中出现了 <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>，这意味着其考虑了 off-policy 设置，但 <a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a> 中却没有相应的处理，只适用于 <span class="math inline">\(o_i \sim \pi_{\theta}\)</span>，无法正确处理 <span class="math inline">\(o_i \sim \pi_{\theta_\text{old}}\)</span>。</p>
<p>其次，<a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a> 将估计样本量 <span class="math inline">\(\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1\)</span> 写成 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]\)</span> 也并不十分恰当，因为 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]\)</span> 通常表示 KL 散度的真实值。</p>
<p>而目前流行的 LLM RL 框架，在实现 KL 优化时，通常也忽略了 off-policy 问题，同时还存在其他一系列问题：</p>
<ol type="1">
<li>误认为前向传播估计出 KL 散度，再反向传播就能得到其梯度（但实际上通常并非如此）；</li>
<li>忽略了先对动作动作对数条件似然应用 KL 估计样本量再求和并非良好定义的行为，导致梯度错误；</li>
<li>忽略了同一轨迹上 KL 对数概率必须求和以获得轨迹联合概率，而不能求平均；</li>
<li>错误地计算了平均操作。</li>
</ol>
<p>由于 on-policy 设置更加简单，但也已经暴露了上述大部分问题，我们可以先从 on-policy 设置开始讨论，后续再考虑 off-policy 设置。</p>
<aside id="footnotes" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol>
<li id="fn1"><p>http://joschu.net/blog/kl-approx.html<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="sec-popular-llm-rl-kl-optim" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</h1>
<p>我们可以先回顾目前流行的 LLM RL 框架中对于 KL 优化的实现。以下我们以</p>
<ol type="1">
<li>TRL<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>，</li>
<li>OpenRLHF<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="citation" data-cites="hu2024openrlhf">(<a href="#ref-hu2024openrlhf" role="doc-biblioref">Hu et al. 2024</a>)</span></li>
<li>verl<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="citation" data-cites="sheng2024hybridflow">(<a href="#ref-sheng2024hybridflow" role="doc-biblioref">Sheng et al. 2024</a>)</span></li>
</ol>
<p>为例。</p>
<p>熟悉这些框架的读者可以跳过本节，直接从 <a href="#sec-rl-kl-optim-formulation" class="quarto-xref">Section&nbsp;3</a> 开始阅读。</p>
<section id="trlkl-reward-项" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</h2>
<p>TRL 计算 KL 定义中的样本值 <span class="math inline">\(\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}\)</span>，并将其从 reward 中减去。对应代码可见 <a href="#lst-trl-kl-reward" class="quarto-xref">Listing&nbsp;1</a>。</p>
<div id="lst-trl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: TRL 计算 KL 样本值 <span class="math inline">\(\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}\)</span> 并从 reward 中减去<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>
</figcaption>
<div aria-describedby="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-trl-kl-reward"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-trl-kl-reward-1"><a href="#lst-trl-kl-reward-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. compute rewards</span></span>
<span id="lst-trl-kl-reward-2"><a href="#lst-trl-kl-reward-2" aria-hidden="true" tabindex="-1"></a>kl <span class="op">=</span> logprobs <span class="op">-</span> ref_logprobs</span>
<span id="lst-trl-kl-reward-3"><a href="#lst-trl-kl-reward-3" aria-hidden="true" tabindex="-1"></a>non_score_reward <span class="op">=</span> <span class="op">-</span>args.kl_coef <span class="op">*</span> kl</span>
<span id="lst-trl-kl-reward-4"><a href="#lst-trl-kl-reward-4" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> non_score_reward.clone()</span>
<span id="lst-trl-kl-reward-5"><a href="#lst-trl-kl-reward-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="lst-trl-kl-reward-6"><a href="#lst-trl-kl-reward-6" aria-hidden="true" tabindex="-1"></a>rewards[[actual_start, actual_end]] <span class="op">+=</span> scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 <a href="#sec-why-kl-reward" class="quarto-xref">Section&nbsp;2.4</a>。</p>
<aside id="footnotes-2" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="2">
<li id="fn2"><p>https://github.com/huggingface/trl<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://github.com/OpenRLHF/OpenRLHF<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://github.com/volcengine/verl<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="openrlhf" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</h2>
<section id="sec-openrlhf-kl-reward" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</h3>
<p>与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 <a href="#lst-openrlhf-calc-kl-estimator" class="quarto-xref">Listing&nbsp;2</a>。</p>
<div id="lst-openrlhf-calc-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-openrlhf-calc-kl-estimator"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-openrlhf-calc-kl-estimator-1"><a href="#lst-openrlhf-calc-kl-estimator-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_approx_kl(</span>
<span id="lst-openrlhf-calc-kl-estimator-2"><a href="#lst-openrlhf-calc-kl-estimator-2" aria-hidden="true" tabindex="-1"></a>    log_probs: torch.Tensor,</span>
<span id="lst-openrlhf-calc-kl-estimator-3"><a href="#lst-openrlhf-calc-kl-estimator-3" aria-hidden="true" tabindex="-1"></a>    log_probs_base: torch.Tensor,</span>
<span id="lst-openrlhf-calc-kl-estimator-4"><a href="#lst-openrlhf-calc-kl-estimator-4" aria-hidden="true" tabindex="-1"></a>    action_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="lst-openrlhf-calc-kl-estimator-5"><a href="#lst-openrlhf-calc-kl-estimator-5" aria-hidden="true" tabindex="-1"></a>    kl_estimator: <span class="bu">str</span> <span class="op">=</span> <span class="st">"k1"</span>,</span>
<span id="lst-openrlhf-calc-kl-estimator-6"><a href="#lst-openrlhf-calc-kl-estimator-6" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="lst-openrlhf-calc-kl-estimator-7"><a href="#lst-openrlhf-calc-kl-estimator-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="lst-openrlhf-calc-kl-estimator-8"><a href="#lst-openrlhf-calc-kl-estimator-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the approximate KL divergence between two distributions.</span></span>
<span id="lst-openrlhf-calc-kl-estimator-9"><a href="#lst-openrlhf-calc-kl-estimator-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Schulman blog: http://joschu.net/blog/kl-approx.html</span></span>
<span id="lst-openrlhf-calc-kl-estimator-10"><a href="#lst-openrlhf-calc-kl-estimator-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-11"><a href="#lst-openrlhf-calc-kl-estimator-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="lst-openrlhf-calc-kl-estimator-12"><a href="#lst-openrlhf-calc-kl-estimator-12" aria-hidden="true" tabindex="-1"></a><span class="co">        log_probs: Log probabilities of the new distribution.</span></span>
<span id="lst-openrlhf-calc-kl-estimator-13"><a href="#lst-openrlhf-calc-kl-estimator-13" aria-hidden="true" tabindex="-1"></a><span class="co">        log_probs_base: Log probabilities of the base distribution.</span></span>
<span id="lst-openrlhf-calc-kl-estimator-14"><a href="#lst-openrlhf-calc-kl-estimator-14" aria-hidden="true" tabindex="-1"></a><span class="co">        action_mask: Mask for actions.</span></span>
<span id="lst-openrlhf-calc-kl-estimator-15"><a href="#lst-openrlhf-calc-kl-estimator-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="lst-openrlhf-calc-kl-estimator-16"><a href="#lst-openrlhf-calc-kl-estimator-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-17"><a href="#lst-openrlhf-calc-kl-estimator-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_estimator <span class="op">==</span> <span class="st">"k1"</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-18"><a href="#lst-openrlhf-calc-kl-estimator-18" aria-hidden="true" tabindex="-1"></a>        log_ratio <span class="op">=</span> log_probs.<span class="bu">float</span>() <span class="op">-</span> log_probs_base.<span class="bu">float</span>()</span>
<span id="lst-openrlhf-calc-kl-estimator-19"><a href="#lst-openrlhf-calc-kl-estimator-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-20"><a href="#lst-openrlhf-calc-kl-estimator-20" aria-hidden="true" tabindex="-1"></a>            log_ratio <span class="op">=</span> log_ratio <span class="op">*</span> action_mask</span>
<span id="lst-openrlhf-calc-kl-estimator-21"><a href="#lst-openrlhf-calc-kl-estimator-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-22"><a href="#lst-openrlhf-calc-kl-estimator-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The $k_2$ estimator is the non negative kl approximation in</span></span>
<span id="lst-openrlhf-calc-kl-estimator-23"><a href="#lst-openrlhf-calc-kl-estimator-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="lst-openrlhf-calc-kl-estimator-24"><a href="#lst-openrlhf-calc-kl-estimator-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The k2_loss is approximately equivalent to the</span></span>
<span id="lst-openrlhf-calc-kl-estimator-25"><a href="#lst-openrlhf-calc-kl-estimator-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># one-step KL divergence penalty with the $k_1$ estimator</span></span>
<span id="lst-openrlhf-calc-kl-estimator-26"><a href="#lst-openrlhf-calc-kl-estimator-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># used in https://arxiv.org/abs/2310.10505.</span></span>
<span id="lst-openrlhf-calc-kl-estimator-27"><a href="#lst-openrlhf-calc-kl-estimator-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_estimator <span class="op">==</span> <span class="st">"k2"</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-28"><a href="#lst-openrlhf-calc-kl-estimator-28" aria-hidden="true" tabindex="-1"></a>        log_ratio <span class="op">=</span> log_probs.<span class="bu">float</span>() <span class="op">-</span> log_probs_base.<span class="bu">float</span>()</span>
<span id="lst-openrlhf-calc-kl-estimator-29"><a href="#lst-openrlhf-calc-kl-estimator-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-30"><a href="#lst-openrlhf-calc-kl-estimator-30" aria-hidden="true" tabindex="-1"></a>            log_ratio <span class="op">=</span> log_ratio <span class="op">*</span> action_mask</span>
<span id="lst-openrlhf-calc-kl-estimator-31"><a href="#lst-openrlhf-calc-kl-estimator-31" aria-hidden="true" tabindex="-1"></a>        log_ratio <span class="op">=</span> log_ratio<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="lst-openrlhf-calc-kl-estimator-32"><a href="#lst-openrlhf-calc-kl-estimator-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-33"><a href="#lst-openrlhf-calc-kl-estimator-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The $k_3$ estimator is the non negative kl approximation in</span></span>
<span id="lst-openrlhf-calc-kl-estimator-34"><a href="#lst-openrlhf-calc-kl-estimator-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="lst-openrlhf-calc-kl-estimator-35"><a href="#lst-openrlhf-calc-kl-estimator-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_estimator <span class="op">==</span> <span class="st">"k3"</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-36"><a href="#lst-openrlhf-calc-kl-estimator-36" aria-hidden="true" tabindex="-1"></a>        log_ratio <span class="op">=</span> log_probs.<span class="bu">float</span>() <span class="op">-</span> log_probs_base.<span class="bu">float</span>()</span>
<span id="lst-openrlhf-calc-kl-estimator-37"><a href="#lst-openrlhf-calc-kl-estimator-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-38"><a href="#lst-openrlhf-calc-kl-estimator-38" aria-hidden="true" tabindex="-1"></a>            log_ratio <span class="op">=</span> log_ratio <span class="op">*</span> action_mask</span>
<span id="lst-openrlhf-calc-kl-estimator-39"><a href="#lst-openrlhf-calc-kl-estimator-39" aria-hidden="true" tabindex="-1"></a>        log_ratio <span class="op">=</span> <span class="op">-</span>log_ratio</span>
<span id="lst-openrlhf-calc-kl-estimator-40"><a href="#lst-openrlhf-calc-kl-estimator-40" aria-hidden="true" tabindex="-1"></a>        log_ratio <span class="op">=</span> log_ratio.exp() <span class="op">-</span> <span class="dv">1</span> <span class="op">-</span> log_ratio</span>
<span id="lst-openrlhf-calc-kl-estimator-41"><a href="#lst-openrlhf-calc-kl-estimator-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-42"><a href="#lst-openrlhf-calc-kl-estimator-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_ratio</span>
<span id="lst-openrlhf-calc-kl-estimator-43"><a href="#lst-openrlhf-calc-kl-estimator-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-44"><a href="#lst-openrlhf-calc-kl-estimator-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-45"><a href="#lst-openrlhf-calc-kl-estimator-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_reward(</span>
<span id="lst-openrlhf-calc-kl-estimator-46"><a href="#lst-openrlhf-calc-kl-estimator-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-estimator-47"><a href="#lst-openrlhf-calc-kl-estimator-47" aria-hidden="true" tabindex="-1"></a>    kl_coef: <span class="bu">float</span>,</span>
<span id="lst-openrlhf-calc-kl-estimator-48"><a href="#lst-openrlhf-calc-kl-estimator-48" aria-hidden="true" tabindex="-1"></a>    kl: Union[torch.Tensor, <span class="bu">list</span>[torch.Tensor]],</span>
<span id="lst-openrlhf-calc-kl-estimator-49"><a href="#lst-openrlhf-calc-kl-estimator-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-estimator-50"><a href="#lst-openrlhf-calc-kl-estimator-50" aria-hidden="true" tabindex="-1"></a>    num_actions: Optional[Union[<span class="bu">int</span>, <span class="bu">list</span>[<span class="bu">int</span>]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="lst-openrlhf-calc-kl-estimator-51"><a href="#lst-openrlhf-calc-kl-estimator-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-estimator-52"><a href="#lst-openrlhf-calc-kl-estimator-52" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Union[torch.Tensor, <span class="bu">list</span>[torch.Tensor]]:</span>
<span id="lst-openrlhf-calc-kl-estimator-53"><a href="#lst-openrlhf-calc-kl-estimator-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-estimator-54"><a href="#lst-openrlhf-calc-kl-estimator-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-55"><a href="#lst-openrlhf-calc-kl-estimator-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-estimator-56"><a href="#lst-openrlhf-calc-kl-estimator-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="lst-openrlhf-calc-kl-estimator-57"><a href="#lst-openrlhf-calc-kl-estimator-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-estimator-58"><a href="#lst-openrlhf-calc-kl-estimator-58" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> []</span>
<span id="lst-openrlhf-calc-kl-estimator-59"><a href="#lst-openrlhf-calc-kl-estimator-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (kl_seg, action_len) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(kl, num_actions)):</span>
<span id="lst-openrlhf-calc-kl-estimator-60"><a href="#lst-openrlhf-calc-kl-estimator-60" aria-hidden="true" tabindex="-1"></a>            kl_reward <span class="op">=</span> <span class="op">-</span>kl_coef <span class="op">*</span> kl_seg</span>
<span id="lst-openrlhf-calc-kl-estimator-61"><a href="#lst-openrlhf-calc-kl-estimator-61" aria-hidden="true" tabindex="-1"></a>            kl_reward[action_len <span class="op">-</span> <span class="dv">1</span>] <span class="op">+=</span> r[i]</span>
<span id="lst-openrlhf-calc-kl-estimator-62"><a href="#lst-openrlhf-calc-kl-estimator-62" aria-hidden="true" tabindex="-1"></a>            reward.append(kl_reward)</span>
<span id="lst-openrlhf-calc-kl-estimator-63"><a href="#lst-openrlhf-calc-kl-estimator-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-estimator-64"><a href="#lst-openrlhf-calc-kl-estimator-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-3" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="6">
<li id="fn6"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</h3>
<p>此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 <a href="#lst-openrlhf-calc-kl-loss" class="quarto-xref">Listing&nbsp;3</a>。</p>
<div id="lst-openrlhf-calc-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-openrlhf-calc-kl-loss"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-openrlhf-calc-kl-loss-1"><a href="#lst-openrlhf-calc-kl-loss-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_step_actor(<span class="va">self</span>, experience: Experience) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, <span class="bu">float</span>]:</span>
<span id="lst-openrlhf-calc-kl-loss-2"><a href="#lst-openrlhf-calc-kl-loss-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.actor.train()</span>
<span id="lst-openrlhf-calc-kl-loss-3"><a href="#lst-openrlhf-calc-kl-loss-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-4"><a href="#lst-openrlhf-calc-kl-loss-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(experience.sequences, <span class="bu">list</span>):</span>
<span id="lst-openrlhf-calc-kl-loss-5"><a href="#lst-openrlhf-calc-kl-loss-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-6"><a href="#lst-openrlhf-calc-kl-loss-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="lst-openrlhf-calc-kl-loss-7"><a href="#lst-openrlhf-calc-kl-loss-7" aria-hidden="true" tabindex="-1"></a>        sequences <span class="op">=</span> experience.sequences</span>
<span id="lst-openrlhf-calc-kl-loss-8"><a href="#lst-openrlhf-calc-kl-loss-8" aria-hidden="true" tabindex="-1"></a>        old_action_log_probs <span class="op">=</span> experience.action_log_probs</span>
<span id="lst-openrlhf-calc-kl-loss-9"><a href="#lst-openrlhf-calc-kl-loss-9" aria-hidden="true" tabindex="-1"></a>        advantages <span class="op">=</span> experience.advantages</span>
<span id="lst-openrlhf-calc-kl-loss-10"><a href="#lst-openrlhf-calc-kl-loss-10" aria-hidden="true" tabindex="-1"></a>        num_actions <span class="op">=</span> experience.action_mask.size(<span class="dv">1</span>)</span>
<span id="lst-openrlhf-calc-kl-loss-11"><a href="#lst-openrlhf-calc-kl-loss-11" aria-hidden="true" tabindex="-1"></a>        packed_seq_lens <span class="op">=</span> <span class="va">None</span></span>
<span id="lst-openrlhf-calc-kl-loss-12"><a href="#lst-openrlhf-calc-kl-loss-12" aria-hidden="true" tabindex="-1"></a>        attention_mask <span class="op">=</span> experience.attention_mask</span>
<span id="lst-openrlhf-calc-kl-loss-13"><a href="#lst-openrlhf-calc-kl-loss-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.args.use_kl_loss <span class="kw">and</span> experience.base_action_log_probs <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="lst-openrlhf-calc-kl-loss-14"><a href="#lst-openrlhf-calc-kl-loss-14" aria-hidden="true" tabindex="-1"></a>            base_action_log_probs <span class="op">=</span> experience.base_action_log_probs</span>
<span id="lst-openrlhf-calc-kl-loss-15"><a href="#lst-openrlhf-calc-kl-loss-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-loss-16"><a href="#lst-openrlhf-calc-kl-loss-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># actor loss</span></span>
<span id="lst-openrlhf-calc-kl-loss-17"><a href="#lst-openrlhf-calc-kl-loss-17" aria-hidden="true" tabindex="-1"></a>    action_log_probs, output <span class="op">=</span> <span class="va">self</span>.actor(</span>
<span id="lst-openrlhf-calc-kl-loss-18"><a href="#lst-openrlhf-calc-kl-loss-18" aria-hidden="true" tabindex="-1"></a>        sequences,</span>
<span id="lst-openrlhf-calc-kl-loss-19"><a href="#lst-openrlhf-calc-kl-loss-19" aria-hidden="true" tabindex="-1"></a>        num_actions,</span>
<span id="lst-openrlhf-calc-kl-loss-20"><a href="#lst-openrlhf-calc-kl-loss-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-21"><a href="#lst-openrlhf-calc-kl-loss-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="lst-openrlhf-calc-kl-loss-22"><a href="#lst-openrlhf-calc-kl-loss-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-23"><a href="#lst-openrlhf-calc-kl-loss-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss function</span></span>
<span id="lst-openrlhf-calc-kl-loss-24"><a href="#lst-openrlhf-calc-kl-loss-24" aria-hidden="true" tabindex="-1"></a>    actor_loss <span class="op">=</span> <span class="va">self</span>.actor_loss_fn(</span>
<span id="lst-openrlhf-calc-kl-loss-25"><a href="#lst-openrlhf-calc-kl-loss-25" aria-hidden="true" tabindex="-1"></a>        action_log_probs,</span>
<span id="lst-openrlhf-calc-kl-loss-26"><a href="#lst-openrlhf-calc-kl-loss-26" aria-hidden="true" tabindex="-1"></a>        old_action_log_probs,</span>
<span id="lst-openrlhf-calc-kl-loss-27"><a href="#lst-openrlhf-calc-kl-loss-27" aria-hidden="true" tabindex="-1"></a>        advantages,</span>
<span id="lst-openrlhf-calc-kl-loss-28"><a href="#lst-openrlhf-calc-kl-loss-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-29"><a href="#lst-openrlhf-calc-kl-loss-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="lst-openrlhf-calc-kl-loss-30"><a href="#lst-openrlhf-calc-kl-loss-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-loss-31"><a href="#lst-openrlhf-calc-kl-loss-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.args.use_kl_loss:</span>
<span id="lst-openrlhf-calc-kl-loss-32"><a href="#lst-openrlhf-calc-kl-loss-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.initial_model <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="lst-openrlhf-calc-kl-loss-33"><a href="#lst-openrlhf-calc-kl-loss-33" aria-hidden="true" tabindex="-1"></a>            kl <span class="op">=</span> compute_approx_kl(</span>
<span id="lst-openrlhf-calc-kl-loss-34"><a href="#lst-openrlhf-calc-kl-loss-34" aria-hidden="true" tabindex="-1"></a>                action_log_probs,</span>
<span id="lst-openrlhf-calc-kl-loss-35"><a href="#lst-openrlhf-calc-kl-loss-35" aria-hidden="true" tabindex="-1"></a>                base_action_log_probs,</span>
<span id="lst-openrlhf-calc-kl-loss-36"><a href="#lst-openrlhf-calc-kl-loss-36" aria-hidden="true" tabindex="-1"></a>                <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-37"><a href="#lst-openrlhf-calc-kl-loss-37" aria-hidden="true" tabindex="-1"></a>                kl_estimator<span class="op">=</span><span class="va">self</span>.args.kl_estimator,</span>
<span id="lst-openrlhf-calc-kl-loss-38"><a href="#lst-openrlhf-calc-kl-loss-38" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="lst-openrlhf-calc-kl-loss-39"><a href="#lst-openrlhf-calc-kl-loss-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="lst-openrlhf-calc-kl-loss-40"><a href="#lst-openrlhf-calc-kl-loss-40" aria-hidden="true" tabindex="-1"></a>            kl <span class="op">=</span> torch.zeros_like(action_log_probs, dtype<span class="op">=</span>action_log_probs.dtype, device<span class="op">=</span>action_log_probs.device)</span>
<span id="lst-openrlhf-calc-kl-loss-41"><a href="#lst-openrlhf-calc-kl-loss-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-loss-42"><a href="#lst-openrlhf-calc-kl-loss-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.args.packing_samples:</span>
<span id="lst-openrlhf-calc-kl-loss-43"><a href="#lst-openrlhf-calc-kl-loss-43" aria-hidden="true" tabindex="-1"></a>            kl_mean <span class="op">=</span> masked_mean(kl, experience.action_mask, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="lst-openrlhf-calc-kl-loss-44"><a href="#lst-openrlhf-calc-kl-loss-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="lst-openrlhf-calc-kl-loss-45"><a href="#lst-openrlhf-calc-kl-loss-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-46"><a href="#lst-openrlhf-calc-kl-loss-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-openrlhf-calc-kl-loss-47"><a href="#lst-openrlhf-calc-kl-loss-47" aria-hidden="true" tabindex="-1"></a>        kl_loss <span class="op">=</span> kl_mean.mean()</span>
<span id="lst-openrlhf-calc-kl-loss-48"><a href="#lst-openrlhf-calc-kl-loss-48" aria-hidden="true" tabindex="-1"></a>        experience.info[<span class="st">"kl"</span>] <span class="op">=</span> kl_loss.item()</span>
<span id="lst-openrlhf-calc-kl-loss-49"><a href="#lst-openrlhf-calc-kl-loss-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="lst-openrlhf-calc-kl-loss-50"><a href="#lst-openrlhf-calc-kl-loss-50" aria-hidden="true" tabindex="-1"></a>        kl_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="lst-openrlhf-calc-kl-loss-51"><a href="#lst-openrlhf-calc-kl-loss-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-calc-kl-loss-52"><a href="#lst-openrlhf-calc-kl-loss-52" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.strategy.optimizer_step(<span class="va">self</span>.actor_optim, <span class="va">self</span>.actor, <span class="va">self</span>.actor_scheduler, name<span class="op">=</span><span class="st">"actor"</span>)</span>
<span id="lst-openrlhf-calc-kl-loss-53"><a href="#lst-openrlhf-calc-kl-loss-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-4" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="7">
<li id="fn7"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="verl" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="verl"><span class="header-section-number">2.3</span> verl</h2>
<section id="kl-reward-项" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</h3>
<p>verl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 <a href="#lst-verl-kl-reward" class="quarto-xref">Listing&nbsp;4</a>。</p>
<div id="lst-verl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: verl 将 KL 估计样本值从 reward 中减去 <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>
</figcaption>
<div aria-describedby="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-verl-kl-reward"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-verl-kl-reward-1"><a href="#lst-verl-kl-reward-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty<span class="op">=</span><span class="st">'kl'</span>):</span>
<span id="lst-verl-kl-reward-2"><a href="#lst-verl-kl-reward-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-verl-kl-reward-3"><a href="#lst-verl-kl-reward-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute kl between ref_policy and current policy</span></span>
<span id="lst-verl-kl-reward-4"><a href="#lst-verl-kl-reward-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'ref_log_prob'</span> <span class="kw">in</span> data.batch.keys():</span>
<span id="lst-verl-kl-reward-5"><a href="#lst-verl-kl-reward-5" aria-hidden="true" tabindex="-1"></a>        kld <span class="op">=</span> core_algos.kl_penalty(data.batch[<span class="st">'old_log_probs'</span>], data.batch[<span class="st">'ref_log_prob'</span>],</span>
<span id="lst-verl-kl-reward-6"><a href="#lst-verl-kl-reward-6" aria-hidden="true" tabindex="-1"></a>                                    kl_penalty<span class="op">=</span>kl_penalty)  <span class="co"># (batch_size, response_length)</span></span>
<span id="lst-verl-kl-reward-7"><a href="#lst-verl-kl-reward-7" aria-hidden="true" tabindex="-1"></a>        kld <span class="op">=</span> kld <span class="op">*</span> response_mask</span>
<span id="lst-verl-kl-reward-8"><a href="#lst-verl-kl-reward-8" aria-hidden="true" tabindex="-1"></a>        beta <span class="op">=</span> kl_ctrl.value</span>
<span id="lst-verl-kl-reward-9"><a href="#lst-verl-kl-reward-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="lst-verl-kl-reward-10"><a href="#lst-verl-kl-reward-10" aria-hidden="true" tabindex="-1"></a>        beta <span class="op">=</span> <span class="dv">0</span></span>
<span id="lst-verl-kl-reward-11"><a href="#lst-verl-kl-reward-11" aria-hidden="true" tabindex="-1"></a>        kld <span class="op">=</span> torch.zeros_like(response_mask, dtype<span class="op">=</span>torch.float32)</span>
<span id="lst-verl-kl-reward-12"><a href="#lst-verl-kl-reward-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-reward-13"><a href="#lst-verl-kl-reward-13" aria-hidden="true" tabindex="-1"></a>    token_level_rewards <span class="op">=</span> token_level_scores <span class="op">-</span> beta <span class="op">*</span> kld</span>
<span id="lst-verl-kl-reward-14"><a href="#lst-verl-kl-reward-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-5" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="8">
<li id="fn8"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项-1" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</h3>
<p>verl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 <a href="#lst-verl-kl-loss" class="quarto-xref">Listing&nbsp;5</a>。</p>
<div id="lst-verl-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>
</figcaption>
<div aria-describedby="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-verl-kl-loss"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-verl-kl-loss-1"><a href="#lst-verl-kl-loss-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_policy(<span class="va">self</span>, data: DataProto):</span>
<span id="lst-verl-kl-loss-2"><a href="#lst-verl-kl-loss-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make sure we are in training mode</span></span>
<span id="lst-verl-kl-loss-3"><a href="#lst-verl-kl-loss-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.actor_module.train()</span>
<span id="lst-verl-kl-loss-4"><a href="#lst-verl-kl-loss-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-5"><a href="#lst-verl-kl-loss-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.config.ppo_epochs):</span>
<span id="lst-verl-kl-loss-6"><a href="#lst-verl-kl-loss-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, data <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="lst-verl-kl-loss-7"><a href="#lst-verl-kl-loss-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-8"><a href="#lst-verl-kl-loss-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.actor_optimizer.zero_grad()</span>
<span id="lst-verl-kl-loss-9"><a href="#lst-verl-kl-loss-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-loss-10"><a href="#lst-verl-kl-loss-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> data <span class="kw">in</span> micro_batches:</span>
<span id="lst-verl-kl-loss-11"><a href="#lst-verl-kl-loss-11" aria-hidden="true" tabindex="-1"></a>                <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-12"><a href="#lst-verl-kl-loss-12" aria-hidden="true" tabindex="-1"></a>                responses <span class="op">=</span> data[<span class="st">'responses'</span>]</span>
<span id="lst-verl-kl-loss-13"><a href="#lst-verl-kl-loss-13" aria-hidden="true" tabindex="-1"></a>                <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-14"><a href="#lst-verl-kl-loss-14" aria-hidden="true" tabindex="-1"></a>                old_log_prob <span class="op">=</span> data[<span class="st">'old_log_probs'</span>]</span>
<span id="lst-verl-kl-loss-15"><a href="#lst-verl-kl-loss-15" aria-hidden="true" tabindex="-1"></a>                <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-16"><a href="#lst-verl-kl-loss-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-loss-17"><a href="#lst-verl-kl-loss-17" aria-hidden="true" tabindex="-1"></a>                <span class="co"># all return: (bsz, response_length)</span></span>
<span id="lst-verl-kl-loss-18"><a href="#lst-verl-kl-loss-18" aria-hidden="true" tabindex="-1"></a>                entropy, log_prob <span class="op">=</span> <span class="va">self</span>._forward_micro_batch(micro_batch<span class="op">=</span>data, temperature<span class="op">=</span>temperature)</span>
<span id="lst-verl-kl-loss-19"><a href="#lst-verl-kl-loss-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-loss-20"><a href="#lst-verl-kl-loss-20" aria-hidden="true" tabindex="-1"></a>                pg_loss, pg_clipfrac, ppo_kl <span class="op">=</span> core_algos.compute_policy_loss(old_log_prob<span class="op">=</span>old_log_prob,</span>
<span id="lst-verl-kl-loss-21"><a href="#lst-verl-kl-loss-21" aria-hidden="true" tabindex="-1"></a>                                                                                log_prob<span class="op">=</span>log_prob,</span>
<span id="lst-verl-kl-loss-22"><a href="#lst-verl-kl-loss-22" aria-hidden="true" tabindex="-1"></a>                                                                                <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-23"><a href="#lst-verl-kl-loss-23" aria-hidden="true" tabindex="-1"></a>                                                                                )</span>
<span id="lst-verl-kl-loss-24"><a href="#lst-verl-kl-loss-24" aria-hidden="true" tabindex="-1"></a>                <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-25"><a href="#lst-verl-kl-loss-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-loss-26"><a href="#lst-verl-kl-loss-26" aria-hidden="true" tabindex="-1"></a>                <span class="co"># compute policy loss</span></span>
<span id="lst-verl-kl-loss-27"><a href="#lst-verl-kl-loss-27" aria-hidden="true" tabindex="-1"></a>                policy_loss <span class="op">=</span> pg_loss <span class="op">-</span> entropy_loss <span class="op">*</span> entropy_coeff</span>
<span id="lst-verl-kl-loss-28"><a href="#lst-verl-kl-loss-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-loss-29"><a href="#lst-verl-kl-loss-29" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.config.use_kl_loss:</span>
<span id="lst-verl-kl-loss-30"><a href="#lst-verl-kl-loss-30" aria-hidden="true" tabindex="-1"></a>                    ref_log_prob <span class="op">=</span> data[<span class="st">'ref_log_prob'</span>]</span>
<span id="lst-verl-kl-loss-31"><a href="#lst-verl-kl-loss-31" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># compute kl loss</span></span>
<span id="lst-verl-kl-loss-32"><a href="#lst-verl-kl-loss-32" aria-hidden="true" tabindex="-1"></a>                    kld <span class="op">=</span> core_algos.kl_penalty(logprob<span class="op">=</span>log_prob,</span>
<span id="lst-verl-kl-loss-33"><a href="#lst-verl-kl-loss-33" aria-hidden="true" tabindex="-1"></a>                                                ref_logprob<span class="op">=</span>ref_log_prob,</span>
<span id="lst-verl-kl-loss-34"><a href="#lst-verl-kl-loss-34" aria-hidden="true" tabindex="-1"></a>                                                kl_penalty<span class="op">=</span><span class="va">self</span>.config.kl_loss_type)</span>
<span id="lst-verl-kl-loss-35"><a href="#lst-verl-kl-loss-35" aria-hidden="true" tabindex="-1"></a>                    kl_loss <span class="op">=</span> masked_mean(kld, response_mask)</span>
<span id="lst-verl-kl-loss-36"><a href="#lst-verl-kl-loss-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-loss-37"><a href="#lst-verl-kl-loss-37" aria-hidden="true" tabindex="-1"></a>                    policy_loss <span class="op">=</span> policy_loss <span class="op">+</span> kl_loss <span class="op">*</span> <span class="va">self</span>.config.kl_loss_coef</span>
<span id="lst-verl-kl-loss-38"><a href="#lst-verl-kl-loss-38" aria-hidden="true" tabindex="-1"></a>                <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-39"><a href="#lst-verl-kl-loss-39" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="lst-verl-kl-loss-40"><a href="#lst-verl-kl-loss-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-41"><a href="#lst-verl-kl-loss-41" aria-hidden="true" tabindex="-1"></a>            grad_norm <span class="op">=</span> <span class="va">self</span>._optimizer_step()</span>
<span id="lst-verl-kl-loss-42"><a href="#lst-verl-kl-loss-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-verl-kl-loss-43"><a href="#lst-verl-kl-loss-43" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.actor_optimizer.zero_grad()</span>
<span id="lst-verl-kl-loss-44"><a href="#lst-verl-kl-loss-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-6" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="9">
<li id="fn9"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="sec-why-kl-reward" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</h2>
<p>将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT <span class="citation" data-cites="ouyang2022instructgpt">(<a href="#ref-ouyang2022instructgpt" role="doc-biblioref">Ouyang et al. 2022</a>)</span>。</p>
<section id="kl-reward-的流行应当源自-rlhf-与-instructgpt" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</h3>
<p>InstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。</p>
<blockquote class="blockquote">
<p>… In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models “PPO.”</p>
<p>…</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
\text { objective }(\phi)= &amp; E_{(x, y) \sim D_\pi^{\mathrm{RL}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
&amp; \gamma E_{x \sim D_{\text {remin }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
\]</span></p>
<blockquote class="blockquote">
<p>where <span class="math inline">\(\pi_\phi^{\mathrm{RL}}\)</span>is the learned RL policy,<span class="math inline">\(\pi^{\mathrm{SFT}}\)</span> is the supervised trained model, and<span class="math inline">\(D_{\text {pretrain }}\)</span>is the pretraining distribution. The KL reward coefficient, <span class="math inline">\(\beta\)</span>, and the pretraining loss coefficient, <span class="math inline">\(\gamma\)</span>, control the strength of the KL penalty and pretraining gradients respectively. For “PPO” models, <span class="math inline">\(\gamma\)</span> is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</p>
</blockquote>
</section>
<section id="sec-oai-kl-reward-src" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</h3>
<p>然而，在OpenAI 早期的一篇论文 “Learning to summarize from human feedback” <span class="citation" data-cites="stiennon2020summarize">(<a href="#ref-stiennon2020summarize" role="doc-biblioref">Stiennon et al. 2020</a>)</span> 中，他们就已经采用了 KL reward，并提及了出处：</p>
<blockquote class="blockquote">
<p>… <strong>Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy <span class="math inline">\(\pi_\phi^{\mathrm{RL}}\)</span> with parameters <span class="math inline">\(\phi\)</span> and this original supervised model <span class="math inline">\(\pi^{\mathrm{SFT}}\)</span>, as previously done in [25].</strong> The full reward <span class="math inline">\(R\)</span> can be written as:</p>
</blockquote>
<p><span class="math display">\[
R(x, y)=r_\theta(x, y)-\beta \log \left[\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right]
\]</span></p>
<blockquote class="blockquote">
<p>This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn’t learn to produce outputs that are too different from those that the reward model has seen during training.</p>
</blockquote>
</section>
<section id="kl-reward-最早的出处" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</h3>
<p><a href="#sec-oai-kl-reward-src" class="quarto-xref">Section&nbsp;2.4.2</a> 中 OpenAI 引用的 KL reward 出处 [25] 是 “Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog” <span class="citation" data-cites="jaques2019wayoffpolicy">(<a href="#ref-jaques2019wayoffpolicy" role="doc-biblioref">Jaques et al. 2019</a>)</span>。</p>
<p>实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：</p>
<blockquote class="blockquote">
<p>Rather than simply sample from the prior, we would like the <span class="math inline">\(Q\)</span>-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior <span class="math inline">\(p(y \mid x)\)</span>, and the <span class="math inline">\(Q\)</span>-network policy <span class="math inline">\(\pi_\theta\)</span>, while still maximizing reward. Given a trajectory of actions, <span class="math inline">\(\tau=\left\{a_1, a_2, \ldots a_{t-1}\right\}\)</span>, let <span class="math inline">\(q(\tau)=\prod_{t=1}^T \pi_\theta\left(a_t, s_t\right)\)</span>be the policy of our<span class="math inline">\(Q\)</span>-learning algorithm at the trajectory level. Similarly, let <span class="math inline">\(p(\tau)=\prod_{t=1}^T p\left(a_t \mid s_t\right)\)</span>be the prior distribution over the trajectory, and<span class="math inline">\(r(\tau)\)</span> be the rewards. We seek to maximize the following KL-regularized objective:</p>
</blockquote>
<p><span class="math display">\[
L(q)=\mathbb{E}_{q(\tau)}[r(\tau)] / c-D_{\text{KL}}[q(\tau) \mid p(\tau)]
\]</span></p>
<blockquote class="blockquote">
<p>Since <span class="math inline">\(D_{\text{KL}}[q \mid p]=\sum_x q(x)(\log q(x)-\log p(x))\)</span>, we can see that this is equivalent to maximizing the following expected value function of the policy <span class="math inline">\(\pi_\theta\)</span> at the action level:</p>
</blockquote>
<p><span class="math display">\[
Q^\pi\left(s_t, a_t\right)=\mathbb{E}_\pi\left[\sum^T r\left(s_{t^{\prime}}, a_{t^{\prime}}\right) / c+\log p\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)-\log \pi\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)\right]
\]</span></p>
<blockquote class="blockquote">

</blockquote>
</section>
</section>
</section>
<section id="sec-rl-kl-optim-formulation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</h1>
<p>为了进一步分析这些 LLM RL 框架中的实现是否正确，我们需要先形式化 LLM RL 中 KL 散度的优化。</p>
<section id="rl-中的-kl-散度通常定义在轨迹分布上" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</h2>
<p>GRPO 公式 (<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a>) 中的 KL 项可以定义为：</p>
<p><span id="eq-def-kl-theta-ref"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right]
\end{aligned}
\tag{3}\]</span></span></p>
<p>其中 <span class="math inline">\(\mathbf{\tau}\)</span> 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 <span class="math inline">\(p_{\theta}\)</span> 与参考策略整体分布 <span class="math inline">\(p_{\text{ref}}\)</span> 的 KL 散度。</p>
</section>
<section id="将轨迹展开为状态-动作序列" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</h2>
<p>RL 文献中通常会将轨迹 <span class="math inline">\(\mathbf{\tau}\)</span> 展开为状态-动作序列 <span class="math inline">\(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\)</span>：<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p><span id="eq-def-kl-theta-ref-state-action-ag"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|},\right) \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|},, \mathbf{a}_{|\mathbf{\tau}|},\right)}{p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\log \frac{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] \\
\end{aligned}
\tag{4}\]</span></span></p>
<p>其中 <span class="math inline">\(|\mathbf{\tau}|\)</span> 为轨迹动作数的随机变量。</p>
<p>此处利用了联合概率的展开，以 <span class="math inline">\(p_{\theta}\)</span> 为例：</p>
<p><span id="eq-dp-expansion"><span class="math display">\[
p_{\theta}(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)
\tag{5}\]</span></span></p>
<p>注意区分整体概率分布 <span class="math inline">\(p_{\theta}\)</span>、策略（条件）概率分布 <span class="math inline">\(\pi_{\theta}\)</span> 与状态转移概率分布 <span class="math inline">\(p\)</span>。</p>
<aside id="footnotes-7" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="10">
<li id="fn10"><p>这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，<span class="math inline">\(\mathbf{q}\)</span> 对应于 <span class="math inline">\(\mathbf{s}_1\)</span>，而 <span class="math inline">\({\mathbf{o}}\)</span> 对应于 <span class="math inline">\(\mathbf{\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T}\)</span>。<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="markov-决策过程中的-kl-散度" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</h2>
<p>实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>。</p>
<p>Markov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 <span class="math inline">\(n\)</span> 个历史状态和动作，而非全部的历史信息，对应的过程称为 <span class="math inline">\(n\)</span> 阶 Markov 过程。以 <span class="math inline">\(n=1\)</span> 为例：</p>
<p><span id="eq-def-markov-prop"><span class="math display">\[
\begin{aligned}
\pi(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) &amp; = \pi(\mathbf{a}_t \mid \mathbf{s}_t) \\
p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) &amp; = p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) \\
\end{aligned}
\tag{6}\]</span></span></p>
<p>则 <a href="#eq-dp-expansion" class="quarto-xref">Equation&nbsp;5</a> 中的联合概率可以进一步简化为：</p>
<p><span id="eq-dp-expansion-markov-1"><span class="math display">\[
p(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(s_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t)
\tag{7}\]</span></span></p>
<p>如果考虑一阶 Markov 过程，则 <a href="#eq-def-kl-theta-ref-state-action-ag" class="quarto-xref">Equation&nbsp;4</a> 中的 KL 可以进一步简化为：</p>
<p><span id="eq-def-kl-theta-ref-state-action-markov-1"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] = &amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}\right] \\
\end{aligned}
\tag{8}\]</span></span></p>
<aside id="footnotes-8" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="11">
<li id="fn11"><p>https://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="sec-lm-as-dp" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</h2>
<p>目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。</p>
<p>尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 <span class="math inline">\(s_1\)</span> 表示 prompt 中的所有 token，对于 <span class="math inline">\(t &gt;1\)</span>，如果令 <span class="math inline">\(s_t\)</span> 表示第 <span class="math inline">\(t\)</span> 个动作 token 前的所有 token，则自回归模型满足 Markov 性质，否则不一定。</p>
<p>接下来，我们先令 <span class="math inline">\(s_t\)</span> 表示前 <span class="math inline">\(t\)</span> 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。</p>
</section>
<section id="估计-kl-散度" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</h2>
<section id="几乎不可能直接计算-kl-散度的真实值" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</h3>
<p>实际实现中，我们几乎不可能直接计算出 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\)</span>，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 <span class="math inline">\(\left|\mathcal{T}\right|\)</span> 与轨迹最大长度 <span class="math inline">\(T = \max_{\mathbf{\tau} \in \mathcal{T}} |\mathbf{\tau}|\)</span> 成指数关系： <span id="eq-def-rl-kl-avg-over-traj"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] \\
&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) \\
\end{aligned}
\tag{9}\]</span></span></p>
</section>
<section id="通常使用-monte-carlo-方法估计-kl-散度" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</h3>
<p>所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>来估计 RL 中的 KL 散度，例如：</p>
<p><span id="eq-def-rl-kl-mc-k1"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) \\
&amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{|\mathbf{\tau_{i }}|} \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)
\end{aligned}
\tag{10}\]</span></span></p>
<p>其中，<span class="math inline">\(\mathbf{\tau_{i}} = \left(\mathbf{s}_{i,1}, \mathbf{a}_{i,1}, \cdots, \mathbf{s}_{i,|\mathbf{\tau_{i}}|}, \mathbf{a}_{i,|\mathbf{\tau_{i}}|}\right) \sim p_{\theta}\)</span>，<span class="math inline">\(N\)</span> 为估计使用的轨迹样本数量。</p>
<aside id="footnotes-9" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="12">
<li id="fn12"><p>https://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="不同的-kl-估计量" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</h3>
<p>实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。</p>
<p>设 KL 估计量为 <span class="math inline">\(k\)</span>，则对应的 KL 估计值为</p>
<p><span id="eq-def-rl-kl-mc-general"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} k(\tau_i)
\end{aligned}
\tag{11}\]</span></span></p>
<p>例如 <a href="#sec-openrlhf-kl-reward" class="quarto-xref">Section&nbsp;2.2.1</a> 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 <code>k1</code>, <code>k2</code>, <code>k3</code>，这应该是主要参考了 John Schulman 的博客 “Approximating KL Divergence”。</p>
<p>verl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>，但目前还没有实现。对应代码可见 <a href="#lst-verl-kl-estimator" class="quarto-xref">Listing&nbsp;6</a>。</p>
<div id="lst-verl-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: verl 的 KL 散度 Monte Carlo 估计样本值<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>
</figcaption>
<div aria-describedby="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-verl-kl-estimator"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-verl-kl-estimator-1"><a href="#lst-verl-kl-estimator-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) <span class="op">-&gt;</span> torch.FloatTensor:</span>
<span id="lst-verl-kl-estimator-2"><a href="#lst-verl-kl-estimator-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-verl-kl-estimator-3"><a href="#lst-verl-kl-estimator-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"kl"</span>:</span>
<span id="lst-verl-kl-estimator-4"><a href="#lst-verl-kl-estimator-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logprob <span class="op">-</span> ref_logprob</span>
<span id="lst-verl-kl-estimator-5"><a href="#lst-verl-kl-estimator-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-estimator-6"><a href="#lst-verl-kl-estimator-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"abs"</span>:</span>
<span id="lst-verl-kl-estimator-7"><a href="#lst-verl-kl-estimator-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (logprob <span class="op">-</span> ref_logprob).<span class="bu">abs</span>()</span>
<span id="lst-verl-kl-estimator-8"><a href="#lst-verl-kl-estimator-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-estimator-9"><a href="#lst-verl-kl-estimator-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"mse"</span>:</span>
<span id="lst-verl-kl-estimator-10"><a href="#lst-verl-kl-estimator-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (logprob <span class="op">-</span> ref_logprob).square()</span>
<span id="lst-verl-kl-estimator-11"><a href="#lst-verl-kl-estimator-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-estimator-12"><a href="#lst-verl-kl-estimator-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># J. Schulman. Approximating kl divergence, 2020.</span></span>
<span id="lst-verl-kl-estimator-13"><a href="#lst-verl-kl-estimator-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># # URL http://joschu.net/blog/kl-approx.html.</span></span>
<span id="lst-verl-kl-estimator-14"><a href="#lst-verl-kl-estimator-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">'low_var_kl'</span>:</span>
<span id="lst-verl-kl-estimator-15"><a href="#lst-verl-kl-estimator-15" aria-hidden="true" tabindex="-1"></a>        kl <span class="op">=</span> ref_logprob <span class="op">-</span> logprob</span>
<span id="lst-verl-kl-estimator-16"><a href="#lst-verl-kl-estimator-16" aria-hidden="true" tabindex="-1"></a>        ratio <span class="op">=</span> torch.exp(kl)</span>
<span id="lst-verl-kl-estimator-17"><a href="#lst-verl-kl-estimator-17" aria-hidden="true" tabindex="-1"></a>        kld <span class="op">=</span> (ratio <span class="op">-</span> kl <span class="op">-</span> <span class="dv">1</span>).contiguous()</span>
<span id="lst-verl-kl-estimator-18"><a href="#lst-verl-kl-estimator-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.clamp(kld, <span class="bu">min</span><span class="op">=-</span><span class="dv">10</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">10</span>)</span>
<span id="lst-verl-kl-estimator-19"><a href="#lst-verl-kl-estimator-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-estimator-20"><a href="#lst-verl-kl-estimator-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"full"</span>:</span>
<span id="lst-verl-kl-estimator-21"><a href="#lst-verl-kl-estimator-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so, here logprob and ref_logprob should contain the logits for every token in vocabulary</span></span>
<span id="lst-verl-kl-estimator-22"><a href="#lst-verl-kl-estimator-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="lst-verl-kl-estimator-23"><a href="#lst-verl-kl-estimator-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-verl-kl-estimator-24"><a href="#lst-verl-kl-estimator-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>由于 <span class="math inline">\(k_1\)</span>、<span class="math inline">\(k_2\)</span>、<span class="math inline">\(k_3\)</span> 三种估计量最为流行，我们将以这三种估计量为例展开分析。</p>
<p>考虑 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} k_j(\tau_i)\)</span>，其中 <span class="math inline">\(\tau_i \sim p_{\theta}\)</span>，令 <span class="math inline">\(r = \frac{\pi_{\text{ref}}(\tau_i)}{\pi_{\theta}(\tau_i)}\)</span>，注意，此处 <span class="math inline">\(r\)</span> 并非 KL 定义中的样本量，而是其倒数，则：</p>
<p><span id="eq-def-kl-estimators"><span class="math display">\[
\begin{aligned}
k_{1} &amp; = - \log r \\
k_{2} &amp; = \frac{1}{2} (\log r)^2 \\
k_{3} &amp; = (r - 1) - \log r
\end{aligned}
\tag{12}\]</span></span></p>
<aside id="footnotes-10" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="13">
<li id="fn13"><p>这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
</section>
<section id="流行-on-policy-kl-优化实现的数学形式化" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</h1>
<p>神经网络模型普遍使用梯度法优化，因此，我们主要关注这些 KL 优化实现导出的梯度。</p>
<p>而由于 reward 项优化的实现涉及到基线（Baseline）、折扣（Discounting）、GAE <span class="citation" data-cites="schulman2018gae">(<a href="#ref-schulman2018gae" role="doc-biblioref">Schulman et al. 2018</a>)</span> 等内容，较为复杂，我们可以先分析 KL loss 项实现。</p>
<section id="sec-kl-loss-impl" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</h2>
<p>上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。</p>
<p>然而，如 <a href="#sec-grpo-kl-misunderstanding" class="quarto-xref">Section&nbsp;1</a> 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。</p>
<section id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</h3>
<p>观察 <a href="#lst-openrlhf-calc-kl-loss" class="quarto-xref">Listing&nbsp;3</a> 计算 “KL loss” 项的部分。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># ...</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>kl <span class="op">=</span> compute_approx_kl(</span>
<span id="cb1-3"><a href="#cb1-3"></a>    action_log_probs,</span>
<span id="cb1-4"><a href="#cb1-4"></a>    base_action_log_probs,</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="co"># ...</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>    kl_estimator<span class="op">=</span><span class="va">self</span>.args.kl_estimator,</span>
<span id="cb1-7"><a href="#cb1-7"></a>)</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># ...</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>kl_mean <span class="op">=</span> masked_mean(kl, experience.action_mask, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co"># ...</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>kl_loss <span class="op">=</span> kl_mean.mean()</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>这些代码：</p>
<ol type="1">
<li>计算了 <code>kl</code>，对应对每个动作 token <span class="math inline">\(a_{i,t}\)</span> 计算 “KL 估计量” <span class="math inline">\(k\)</span>。</li>
<li>计算了 <code>kl_mean</code>，对应对每个轨迹 <span class="math inline">\(\tau_i\)</span> 计算均值 <span class="math inline">\(\frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k\)</span>。</li>
<li>计算了 <code>kl_loss</code>，对应对所有轨迹样本计算均值 <span class="math inline">\(\frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k\)</span>。</li>
</ol>
<p>由于其没有去除任何梯度，因此其导出的梯度估计值为</p>
<p><span id="eq-def-kl-loss-grad-estim-openrlhf"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \left( \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \frac{1}{|\tau_i|} k \right) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k
\end{aligned}
\tag{13}\]</span></span></p>
<p><a href="#lst-verl-kl-loss" class="quarto-xref">Listing&nbsp;5</a> 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：</p>
<p><span id="eq-def-kl-loss-grad-estim-verl"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \left( \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} k \right) = \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} \nabla_{\theta} k
\end{aligned}
\tag{14}\]</span></span></p>
<p>我们将平均操作一般化为权重 <span class="math inline">\(w_{\mathbf{\tau}}\)</span> 与 <span class="math inline">\(w_{t}\)</span>，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：</p>
<p><span id="eq-def-kl-loss-grad-estim-general"><span class="math display">\[
\begin{aligned}
\sum_{i=1}^{N} w_{\mathbf{\tau}_i} \sum_{t=1}^{|\tau_i|} w_{t} \nabla_{\theta} k \\
\end{aligned}
\tag{15}\]</span></span></p>
<p>则</p>
<ul>
<li>OpenRLHF 对应 <span class="math inline">\(w_{\mathbf{\tau}} = \frac{1}{N}, w_{t} = \frac{1}{|\tau|}\)</span>；</li>
<li>verl 对应 <span class="math inline">\(w_{\mathbf{\tau}} = \frac{1}{\sum_{i=1}^{N} |\tau_i|}, w_{t} = 1\)</span>。</li>
</ul>
<p>此处，我们先以 OpenRLHF 的梯度估计 (<a href="#eq-def-kl-loss-grad-estim-openrlhf" class="quarto-xref">Equation&nbsp;13</a>) 为例，分析不同 KL 估计量导出的梯度估计，其满足：</p>
<p><span id="eq-def-kl-loss-grad-expect-openrlhf"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau}_i \sim p_{\theta}} \left[ \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k \right] = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} k \right]
\tag{16}\]</span></span></p>
<p>我们会在 <a href="#sec-derive-kld-grad" class="quarto-xref">Section&nbsp;5</a> 中推导正确的 KL 梯度估计。</p>
</section>
<section id="k_1-导出的梯度期望为-0" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <span class="math inline">\(k_1\)</span> 导出的梯度：期望为 0</h3>
<p>向 <a href="#eq-def-kl-loss-grad-expect-openrlhf" class="quarto-xref">Equation&nbsp;16</a> 代入 <span class="math inline">\(k = k_1 = - \log r = \log \frac{1}{r} = \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\)</span>，导出的梯度估计为</p>
<p><span id="eq-kl-loss-grad-sample-k1"><span class="math display">\[
\begin{aligned}
&amp; \frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k \\
=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \\
=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}\log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \left( \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) + \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) + \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \right) \right) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) \right) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_\theta(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\tau)
\end{aligned}
\tag{17}\]</span></span></p>
<p>则其导出的梯度期望满足：</p>
<p><span id="eq-kl-loss-grad-expect-k1"><span class="math display">\[
\begin{aligned}
\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})\right]
&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} \nabla_{\theta} \log p_{\theta}(\tau) \\
&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \\
&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} \nabla_{\theta} p_{\theta}(\tau) \\
&amp; = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} \\
&amp; = \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \right]
\end{aligned}
\tag{18}\]</span></span></p>
<p>此处利用了 <span class="math inline">\(p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \frac{1}{p_{\theta}(\tau)} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \nabla_{\theta} p_{\theta}(\tau)\)</span>。</p>
<p>所以 <span class="math inline">\(k_1\)</span> loss 项优化的量是 <span class="math inline">\(\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \right]\)</span>。这意味着该优化过程会降低采样轨迹的长度。</p>
<p>特别地，当不对同一轨迹中的 “<span class="math inline">\(k_1\)</span> 估计量”求均值，而是求和时，可以直接将 <span class="math inline">\(\frac{1}{|\tau|}\)</span> 这一项替换为 <span class="math inline">\(1\)</span>，得到 <span id="eq-kl-loss-grad-expect-k1-no-intra-traj-mean"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) = \sum_{\tau \in \mathcal{T}} \nabla_{\theta} p_{\theta} = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta} = \nabla_{\theta} 1 = 0
\tag{19}\]</span></span><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<p>这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。</p>
<p>无论哪种情况，<span class="math inline">\(k_1\)</span> 导出的优化量都非常奇怪，不太可能出于实现者的本意。</p>
<p>同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。接下来，我们将忽略这一操作，即将 <span class="math inline">\(\frac{1}{|\tau|}\)</span> 一项替换为 <span class="math inline">\(1\)</span>。</p>
<aside id="footnotes-11" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="15">
<li id="fn15"><p>此处对数似然的梯度的期望值为 0，是一个著名的性质，会在接下来频繁用到。<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="k_2-导出的梯度" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <span class="math inline">\(k_2\)</span> 导出的梯度</h3>
<p>向 <a href="#eq-def-kl-loss-grad-expect-openrlhf" class="quarto-xref">Equation&nbsp;16</a> 代入 <span class="math inline">\(k = k_2 = \frac{1}{2} (\log r)^2 = \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right)^2\)</span>，导出的单条轨迹 <span class="math inline">\(\mathbf{\tau} \sim p_{\theta}\)</span> 的梯度为 <span id="eq-kl-loss-grad-sample-k2"><span class="math display">\[
\begin{aligned}
&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k\\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}  \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)^2 \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \\
\end{aligned}
\tag{20}\]</span></span></p>
<p>显然，</p>
<p><span id="eq-kl-loss-grad-sample-k2-wrong"><span class="math display">\[
\begin{aligned}
&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \\
\neq &amp; \left( \sum_{t=1}^{|\mathbf{\tau}|}  \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \left( \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \right) \\
=&amp; \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})
\end{aligned}
\tag{21}\]</span></span></p>
<p>然而，</p>
<p><span id="eq-kl-loss-grad-expect-k2-wrong"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] \\
=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} \log p_{\theta}(\tau) \\
=&amp; \sum_{\tau \in \mathcal{T}} \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} p_{\theta}(\tau) \\
=&amp; \sum_{\tau \in \mathcal{T}} \left[ \left( \log p_{\theta}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) - \left( \log p_{\text{ref}}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) \right] \\
=&amp; \sum_{\tau \in \mathcal{T}}  \left[ \nabla_{\theta} (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) -  \nabla_{\theta} \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right] \\
=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  \left[ (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) - \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right] \\
=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  p_{\theta} \left[ \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} - 1 \right) \right] \\
=&amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} - 1 \right) \right] \\
= &amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right] \\
= &amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]
\end{aligned}
\tag{22}\]</span></span></p>
<p>此处利用了 <span class="math inline">\(\log p(x) \nabla_{\theta} p(x) = \nabla_{\theta} (\log p(x) - 1) p(x)\)</span></p>
<p>因此，最小化 <span class="math inline">\(k_2\)</span> loss 项 (<a href="#eq-kl-loss-grad-sample-k2-wrong" class="quarto-xref">Equation&nbsp;21</a>) ，并非在优化 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\)</span>。</p>
</section>
<section id="k_3-导出的梯度" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <span class="math inline">\(k_3\)</span> 导出的梯度</h3>
<p>向 <a href="#eq-def-kl-loss-grad-expect-openrlhf" class="quarto-xref">Equation&nbsp;16</a> 代入 <span class="math inline">\(k = k_3 = (r - 1) - \log r = (\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1) - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\)</span>，导出的单条轨迹 <span class="math inline">\(\mathbf{\tau} \sim p_{\theta}\)</span> 的梯度为 <span id="eq-kl-loss-grad-sample-k3"><span class="math display">\[
\begin{aligned}
&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \left(\frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1 - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right) \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} - \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} \\
=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} \\
=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) + \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \\
\end{aligned}
\tag{23}\]</span></span></p>
<p>其中，根据 <a href="#eq-kl-loss-grad-expect-k1-no-intra-traj-mean" class="quarto-xref">Equation&nbsp;19</a>，<span class="math inline">\(\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = 0\)</span>，不妨直接省略。</p>
<p>而剩余部分似乎很难通过消去 <span class="math inline">\(\pi_{\theta}(\mathbf{\tau})\)</span> 来提出 <span class="math inline">\(\nabla_{\theta}\)</span> 并准确分析。但显然也并非在优化 KL 散度。</p>
</section>
<section id="小结流行的-kl-loss-项-实现并不合理" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</h3>
<p>综上所述，对于 OpenRLHF 实现的 “KL loss 项”，</p>
<ol type="1">
<li>对同一轨迹内的 “KL 估计量” 求均值这一操作很可能是错误的，正确操作应当为求和，对应于根据对数条件概率求对数联合概率。</li>
<li><span class="math inline">\(k_1\)</span> 导出的梯度
<ol type="1">
<li>若对同一轨迹内的 “KL 估计量” 求均值，则会导致输出长度减小，</li>
<li>而如果修正为求和，则其期望为 0，在平均意义上不改分布。</li>
</ol></li>
<li><span class="math inline">\(k_2\)</span>，<span class="math inline">\(k_3\)</span> 导出的梯度则十分复杂，难以分析，但都并非在优化 KL 散度，这可能是因为其错误地将 KL 估计样本量应用于动作对数条件似然并求和。回顾 KL 估计量公式 (<a href="#eq-def-kl-estimators" class="quarto-xref">Equation&nbsp;12</a>) ，应当注意到这些估计量是直接作用于似然 <span class="math inline">\(p_{\theta}(\mathbf{\tau})\)</span>，而没有保证作用于概率后求积/对数和仍然有意义。</li>
</ol>
</section>
</section>
<section id="分析流行的-kl-reward-项-实现" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</h2>
<section id="sec-analogy-pg-kl" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</h3>
<p>由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是： <span id="eq-pg-est-adv"><span class="math display">\[
\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[r(\mathbf{\tau})\right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \hat{A}_t \right]
\tag{24}\]</span></span></p>
<p>其中 <span class="math inline">\(\hat{A}_t\)</span> 为优势（Advantage）的估计量。</p>
<p>为了方便观察 KL reward 项发挥的作用，我们将 <span class="math inline">\(r_{\mathbf{\tau}}\)</span> 展开，并不妨考虑一个更简单的估计，例如：</p>
<p><span id="eq-pg-est-ret"><span class="math display">\[
\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{|\mathbf{\tau}|} r(\mathbf{s}_t, \mathbf{a}_t) \right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \sum_{t'=1}^{|\tau|} r(s_{t'}, a_{t'}) \right]
\tag{25}\]</span></span></p>
<p>简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 “Policy Gradient” 一讲<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>。</p>
<p>类比 <span class="math inline">\(r_{t'}\)</span> 导出的梯度期望，将负的 KL 样本量 <span class="math inline">\(- \log \frac{\pi_\theta\left(a_t \mid s_t \right)}{\pi_{\text{ref}}\left(a_t \mid s_t \right)}\)</span> 加入 reward <span class="math inline">\(r_{t'}\)</span> 代入其中，导出的梯度期望为：</p>
<p><span id="eq-kl-grad-est-markov-1"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|}  \left( \nabla_\theta \log \pi_\theta\left(a_t \mid s_t \right) \right) \sum_{t'=1}^{|\tau|} - \log \frac{\pi_\theta\left(a_{t'} \mid s_{t'} \right)}{\pi_{\text{ref}}\left(a_{t'} \mid s_{t'} \right)} \right] = \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}\right]
\tag{26}\]</span></span></p>
<p>注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (<a href="#eq-def-markov-prop" class="quarto-xref">Equation&nbsp;6</a>)。</p>
<p>实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：</p>
<p><span id="eq-kl-grad-est-dp"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta}- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)} \right] \\
\to&amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] \\
= &amp; \nabla_{\theta} -  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{\prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{ \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] \\
= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) }{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) } \right] \\
= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p_\theta\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)}{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)} \right] \\
= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta} \left[ \log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)} \right] \\
= &amp; \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \\
\end{aligned}
\tag{27}\]</span></span></p>
<p>可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。</p>
<aside id="footnotes-12" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="16">
<li id="fn16"><p>https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="不同-kl-估计量导出的-reward-项的作用" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</h3>
<p>不难注意到，<a href="#sec-analogy-pg-kl" class="quarto-xref">Section&nbsp;4.2.1</a> 中的 KL 样本量对应于 <span class="math inline">\(k_1\)</span> 估计量。</p>
<p>一个自然的问题是，如果对动作条件似然使用 <span class="math inline">\(k_2\)</span> 或 <span class="math inline">\(k_3\)</span> 等其他估计量，会得到什么结果？</p>
<p><span class="math inline">\(k_2\)</span> 或 <span class="math inline">\(k_3\)</span> 等其他估计量导致的一个问题，求和时通常无法得到联合概率。具体来说，其他估计量分别在优化</p>
<ul>
<li><span class="math inline">\(k_2\)</span>: <span class="math inline">\(- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \frac{1}{2} \left( \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right)^{2} \right]\)</span></li>
<li><span class="math inline">\(k_3\)</span>: <span class="math inline">\(- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} (\frac{\pi_{\text{ref}} \left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} - 1 - \log \frac{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}) \right]\)</span></li>
</ul>
<p>显然，这里的求和无法得到联合概率，也就无法实现类似 <a href="#eq-kl-grad-est-dp" class="quarto-xref">Equation&nbsp;27</a> 中的效果了。</p>
</section>
<section id="小结在-on-policy-设置下修正-grpo-目标的-kl-项" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</h3>
<p>若对动作对数条件似然计算 KL 估计样本量，则由于涉及到求和，<span class="math inline">\(k_1\)</span> 之外的估计量通常没有良好定义。</p>
<p>但是若放弃对动作条件似然计算 KL 估计样本量，而是对求和之后的对数（条件）似然进行计算，则只需满足</p>
<p><span id="eq-kl-reward-grad-expect-general"><span class="math display">\[
\nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right) \right]
\approx \nabla_{\theta} - \frac{1}{N} k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right)
\approx \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]
\tag{28}\]</span></span></p>
<p>暂时不考虑 off-policy 问题，根据 <a href="#eq-kl-reward-grad-expect-general" class="quarto-xref">Equation&nbsp;28</a>, GRPO 公式 (<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a>, <a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a>) 应当修正 KL 项如下：</p>
<p><span id="eq-grpo-obj-kl-fixed"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \left\{ \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|} \min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]  \right\}  -\beta k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)
\end{aligned}
\tag{29}\]</span></span></p>
</section>
</section>
</section>
<section id="sec-derive-kld-grad" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</h1>
<p>前文中，我们分析了流行的 LLM RL 框架中对 KL 散度优化的实现，并得出了结论。另一种思路是直接推导出 KL 散度的梯度估计表达式，并据此实现代码。</p>
<p>由于我们使用的是梯度法，为了优化 KL 散度，我们需要准确估计的是 KL 散度的梯度而非其本身。类似地，在 PG 中，我们需要最大化 <span class="math inline">\(\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]\)</span>，估计的是其梯度 <span class="math inline">\(\nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]=\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau}) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})]\)</span>而不是<span class="math inline">\(r(\mathbf{\tau})\)</span> 本身。</p>
<p>同时，如 <a href="#sec-kl-loss-impl" class="quarto-xref">Section&nbsp;4.1</a> 所述，先前向传播估计 KL 散度，再直接反向传播，通常是无法直接得到 KL 散度的梯度的。所以，我们需要直接估计 KL 散度的梯度。</p>
<p>首先，展开 KL 散度的表达式：</p>
<p><span id="eq-kl-expansion-sum"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] \\
&amp; \propto \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right)
\end{aligned}
\tag{30}\]</span></span></p>
<p>再计算其梯度：</p>
<p><span id="eq-kl-grad-expansion"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \propto \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\prod_{t=1}^{|\tau|-1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right)  \\
&amp; \cdot \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) \\
&amp; = \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau| - 1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right) \\
&amp; \cdot \nabla_{\theta} \left(\left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) \right)
\end{aligned}
\tag{31}\]</span></span></p>
<p><a href="#eq-kl-grad-expansion" class="quarto-xref">Equation&nbsp;31</a> 中的梯度相当复杂，难以直接计算。接下来，我们将引入一系列合理的假设来简化它。</p>
<section id="在已知环境中简化-kl-梯度估计" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</h2>
<p>实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。</p>
<p>当状态转移概率分布已知时，<span class="math inline">\(\forall t, p_{\theta}(a_1, \cdots, s_t, a_t \mid s_1)\)</span> 都是可以计算的，则 KL 散度可以直接写成：</p>
<p><span id="eq-kl-grad-expansion-known-transition"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\mathbf{\tau} \in \mathcal{T}} p(\mathbf{s}_1) p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1) \log \frac{p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}{p_{\text{ref}}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}  \\
\end{aligned}
\tag{32}\]</span></span></p>
</section>
<section id="简写为-contextual-bandit" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</h2>
<p>为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 <span class="math inline">\(\mathbf{s}_1 = \mathbf{x} \in \mathcal{P}, (\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T) = \mathbf{y} \in \mathcal{R}\)</span>，其中 <span class="math inline">\(\mathcal{P}, \mathcal{R}\)</span> 分别表示 prompt / response 空间，则 KL 散度变为：</p>
<p><span id="eq-def-kl-cb"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}}\left[\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right] \\
&amp; = \sum_{(x, y) \in \mathcal{T}} p_{\theta}(x, y) \left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \\
&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)
\end{aligned}
\tag{33}\]</span></span></p>
<p>其梯度变为：</p>
<p><span id="eq-def-kl-grad-cb"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \nabla_{\theta} \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \\
&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right)
\end{aligned}
\tag{34}\]</span></span></p>
<p>其中梯度项可以进一步展开为：</p>
<p><span id="eq-def-kl-grad-cb-grad-term"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right) \\
=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \nabla_{\theta} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \\
=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \frac{1}{\pi_\theta(y \mid x)} \nabla_{\theta} \pi_{\theta}(y \mid x) \\
=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \nabla_{\theta} \pi_{\theta}(y \mid x) \\
=&amp; \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x)
\end{aligned}
\tag{35}\]</span></span></p>
<p>代入回 KL 梯度表达式：</p>
<p><span id="eq-def-kl-grad-cb-expect"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \\
=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x) \\
=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \\
=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x) \\
=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] \\
=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] + \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] \\
=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right]
\end{aligned}
\tag{36}\]</span></span></p>
<p>这里为了重新获得期望形式，引入了 <span class="math inline">\(1 = \pi_{\theta}(y \mid x) / \pi_{\theta}(y \mid x)\)</span>，并利用了 <span class="math inline">\(\nabla_{\theta} \log \pi_{\theta}(y \mid x) = \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)}\)</span> 和 <span class="math inline">\(\mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] = 0\)</span>。</p>
<p>进行 Monte Carlo 估计：</p>
<p><span id="eq-def-kl-grad-cb-mc"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\log \frac{\pi_{\theta}(y_i \mid x_i)}{\pi_{\text{ref}}(y_i \mid x_i)}\right) \nabla_{\theta} \log \pi_{\theta}(y_i \mid x_i)
\end{aligned}
\tag{37}\]</span></span></p>
<p>其中 <span class="math inline">\((\mathbf{x}_i, \mathbf{y}_i) \sim p_{\theta}\)</span>。</p>
</section>
<section id="还原为已知环境决策过程" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</h2>
<p>将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：</p>
<p><span id="eq-def-kl-grad-kt-mc"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\\
=&amp; \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{y} \mid \mathbf{x})\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T}) \sim p_{\theta}} \left[\left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)\right)\right]
\end{aligned}
\tag{38}\]</span></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T}\log \frac{\pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}{\pi_{\text{ref}}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})\right)
\end{aligned}
\tag{39}\]</span></span></p>
</section>
<section id="利用因果性技巧化简-kl-梯度估计" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></h2>
<p>因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。</p>
<p>对于任何 <span class="math inline">\(0 \leq t \leq |\tau|\)</span>，我们有 <span id="eq-score-expect-zero"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) }\left[\nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \\
=&amp; \sum_{a_t \in \mathcal{A}} \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \nabla_\theta \log \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \\
=&amp; \sum_{a_j \in \mathcal{A}} \pi_\theta(a_j \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_j) \cdot 0 \\
=&amp; 0
\end{aligned}
\tag{40}\]</span></span></p>
<p>更进一步，如果 <span class="math inline">\(\mathbf{\Psi}_{t'}\)</span> 是一个与 <span class="math inline">\(\mathbf{a}_t, \mathbf{s}_{t+1}, \mathbf{a}_{t+1}, \ldots\)</span> 独立的随机变量，那么 <span id="eq-score-indep-mul-expect-zero"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{(\mathbf{a}_t, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]
\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \mathbb{E}_{
    (\mathbf{s}_{t+1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t})} \left[\mathbf{\Psi}_{t'} \right] \right]
\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]
\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[
            \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right]
        \right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbf{\Psi}_{t'} \cdot 0 \right] \\
=&amp; 0
\end{aligned}
\tag{41}\]</span></span></p>
<p>其中，为了利用 <a href="#eq-score-expect-zero" class="quarto-xref">Equation&nbsp;40</a> 的结论，我们利用了全期望定律，即</p>
<p><span id="eq-law-of-total-expectation"><span class="math display">\[
\mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p} \left[\mathbf{x}\right] = \mathbb{E}_{\mathbf{y} \sim p} \left[\mathbb{E}_{\mathbf{x} \sim p(\cdot \mid \mathbf{y})} [\mathbf{x}] \right]
\tag{42}\]</span></span></p>
<p>来引入我们想要的期望。</p>
<p><span id="eq-score-indep-mul-expect-zero"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_i \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \\
=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \\
=&amp; \sum_{\tau \in \mathcal{T}} p_\theta(s_1, a_1, \cdots, s_t) \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) p_\theta(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \\
=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t)  \sum_{(a_{t}, s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})} \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \Psi_{t'} \nabla_\theta p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right)  \\
=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t) \sum_{a_t \in \mathcal{A}}  \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \sum_{(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})}  p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} \\
\end{aligned}
\tag{43}\]</span></span></p>
<p>考虑 Monte Carlo 估计式 <a href="#eq-def-kl-grad-kt-mc-loss" class="quarto-xref">Equation&nbsp;39</a> 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss-estimator-one-grad"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[
\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})
\right]
\tag{44}\]</span></span></p>
<p>由于序列决策过程满足因果性，即 <span class="math inline">\(\forall t' &lt; t\)</span>，<span class="math inline">\(\mathbf{s}_{t'}, \mathbf{a}_{t'}\)</span> 独立于 <span class="math inline">\(\mathbf{s}_{t}, \mathbf{a}_{t}\)</span>，则可令 <span class="math inline">\(\mathbf{\Psi}_{t'} = \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}\)</span>，其独立于 <span class="math inline">\(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}, \ldots\)</span>，利用 <a href="#eq-score-indep-mul-expect-zero" class="quarto-xref">Equation&nbsp;43</a> 的性质，则有 <span id="eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero"><span class="math display">\[
\forall t' &lt; t, \mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[
\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})
\right] = 0
\tag{45}\]</span></span></p>
<p>将 <a href="#eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero" class="quarto-xref">Equation&nbsp;45</a> 代入 KL 梯度表达式 (<a href="#eq-def-kl-grad-kt-mc" class="quarto-xref">Equation&nbsp;38</a>) ，即可简化得到：</p>
<p><span id="eq-def-kl-grad-kt-reduce"><span class="math display">\[
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{t}) \right]
\tag{46}\]</span></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc"><span class="math display">\[
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \left(\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})
\tag{47}\]</span></span></p>
<p>同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc-loss"><span class="math display">\[
\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \text{nograd}\left (\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})
\tag{48}\]</span></span></p>
<p>这里也可以看到，KL loss 项正确的实现要求：</p>
<ol type="1">
<li>在序列内 token 间，对对数条件似然先求和，得到 KL 样本值，</li>
<li>再在序列间求均值。</li>
</ol>
<p>因此 OpenRLHF (<a href="#eq-def-kl-loss-grad-estim-openrlhf" class="quarto-xref">Equation&nbsp;13</a>) 与 verl (<a href="#eq-def-kl-loss-grad-estim-verl" class="quarto-xref">Equation&nbsp;14</a>) 的权重都是错误的。</p>
<aside id="footnotes-13" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="17">
<li id="fn17"><p>https://www.wikiwand.com/en/articles/Policy_gradient_method<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="sec-kl-grad-as-kl-reward" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</h2>
<p>在 <a href="#eq-def-kl-grad-kt-reduce" class="quarto-xref">Equation&nbsp;46</a> 中，令 <span class="math inline">\(k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) = \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}\)</span>，则有： <span id="eq-def-kl-grad-kt-reduce-k"><span class="math display">\[
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t-1}, \mathbf{s}_{t}) \right]
\tag{49}\]</span></span></p>
<p>不难注意到 <a href="#eq-def-kl-grad-kt-reduce-k" class="quarto-xref">Equation&nbsp;49</a> 中 <span class="math inline">\(k\)</span> 与 <a href="#eq-pg-est-ret" class="quarto-xref">Equation&nbsp;25</a> 中 reward <span class="math inline">\(r\)</span> 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。</p>
<p>类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS285<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> 等材料。</p>
<aside id="footnotes-14" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="18">
<li id="fn18"><p>https://rail.eecs.berkeley.edu/deeprlcourse/<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="off-policy-设置下如何估计-kl-散度的梯度" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</h1>
<p>上面的推导中，我们假设了 RL 是 on-policy 设置，即采样策略即为最新策略 <span class="math inline">\(\pi_\theta\)</span>。</p>
<p>在这一节，我们进一步考虑 off-policy 设置，即一次采样获得样本会用于多次更新，除了第一次更新，采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 与最新策略 <span class="math inline">\(\pi_\theta\)</span> 都会不同。off-policy 设置给 KL 散度优化带来的问题在于，我们需要优化最新策略 <span class="math inline">\(\pi_\theta\)</span> 的 KL 散度，但却没有来自 <span class="math inline">\(p_{\theta}\)</span> 的样本，这意味着我们无法直接使用梯度估计式 <a href="#eq-def-kl-grad-kt-reduce-mc" class="quarto-xref">Equation&nbsp;47</a>。</p>
<section id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</h2>
<p>遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 <span class="math inline">\(\pi_\theta \neq \pi_{\theta_{\text{old}}}\)</span>，尽管没有来自最新策略 <span class="math inline">\(p_{\theta}\)</span> 的样本，却仍然在使用基于 on-policy 设置的优化方式。</p>
<section id="trl" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="trl"><span class="header-section-number">6.1.1</span> TRL</h3>
<p>TRL 在 <a href="#lst-trl-kl-reward" class="quarto-xref">Listing&nbsp;1</a> 中计算 KL 样本值使用的 <code>logprobs</code> 及其对应的轨迹样本均来自采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span>。对应代码可见 <a href="#lst-trl-sample-and-calc-old-logprob" class="quarto-xref">Listing&nbsp;7</a>。</p>
<div id="lst-trl-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: TRL 使用采样样本并使用 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 计算对数似然<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>
</figcaption>
<div aria-describedby="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-trl-sample-and-calc-old-logprob"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-trl-sample-and-calc-old-logprob-1"><a href="#lst-trl-sample-and-calc-old-logprob-1" aria-hidden="true" tabindex="-1"></a>queries <span class="op">=</span> data[<span class="st">"input_ids"</span>].to(device)</span>
<span id="lst-trl-sample-and-calc-old-logprob-2"><a href="#lst-trl-sample-and-calc-old-logprob-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="lst-trl-sample-and-calc-old-logprob-3"><a href="#lst-trl-sample-and-calc-old-logprob-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-trl-sample-and-calc-old-logprob-4"><a href="#lst-trl-sample-and-calc-old-logprob-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> unwrap_model_for_generation(</span>
<span id="lst-trl-sample-and-calc-old-logprob-5"><a href="#lst-trl-sample-and-calc-old-logprob-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.model, <span class="co">#...</span></span>
<span id="lst-trl-sample-and-calc-old-logprob-6"><a href="#lst-trl-sample-and-calc-old-logprob-6" aria-hidden="true" tabindex="-1"></a>) <span class="im">as</span> unwrapped_model:</span>
<span id="lst-trl-sample-and-calc-old-logprob-7"><a href="#lst-trl-sample-and-calc-old-logprob-7" aria-hidden="true" tabindex="-1"></a>    query_responses, logitss <span class="op">=</span> batch_generation(</span>
<span id="lst-trl-sample-and-calc-old-logprob-8"><a href="#lst-trl-sample-and-calc-old-logprob-8" aria-hidden="true" tabindex="-1"></a>        unwrapped_model.policy,</span>
<span id="lst-trl-sample-and-calc-old-logprob-9"><a href="#lst-trl-sample-and-calc-old-logprob-9" aria-hidden="true" tabindex="-1"></a>        queries,</span>
<span id="lst-trl-sample-and-calc-old-logprob-10"><a href="#lst-trl-sample-and-calc-old-logprob-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-trl-sample-and-calc-old-logprob-11"><a href="#lst-trl-sample-and-calc-old-logprob-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="lst-trl-sample-and-calc-old-logprob-12"><a href="#lst-trl-sample-and-calc-old-logprob-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-trl-sample-and-calc-old-logprob-13"><a href="#lst-trl-sample-and-calc-old-logprob-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-trl-sample-and-calc-old-logprob-14"><a href="#lst-trl-sample-and-calc-old-logprob-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, queries.shape[<span class="dv">0</span>], args.local_rollout_forward_batch_size):</span>
<span id="lst-trl-sample-and-calc-old-logprob-15"><a href="#lst-trl-sample-and-calc-old-logprob-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-trl-sample-and-calc-old-logprob-16"><a href="#lst-trl-sample-and-calc-old-logprob-16" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> logitss[i : i <span class="op">+</span> args.local_rollout_forward_batch_size]</span>
<span id="lst-trl-sample-and-calc-old-logprob-17"><a href="#lst-trl-sample-and-calc-old-logprob-17" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> selective_log_softmax(logits, response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>注意，基于 <span class="math inline">\(\mathbf{\tau} \sim \pi_{\theta_{\text{old}}}\)</span> 计算的 KL 样本值可以用于估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_{\theta_{\text{old}}} \mid \pi_{\text{ref}}\right]\)</span>，在第一次更新时，由于 <span class="math inline">\(\pi_\theta = \pi_{\theta_{\text{old}}}\)</span>，所以也可以用于估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。但问题在于，从第二次更新开始，<span class="math inline">\(\pi_\theta \neq \pi_{\theta_{\text{old}}}\)</span>，而我们仍然希望估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。</p>
<p>随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 <span class="math inline">\(\pi_{\theta}\)</span> 重新估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。对应代码可见 <a href="#lst-trl-ppo-update" class="quarto-xref">Listing&nbsp;8</a>。</p>
<div id="lst-trl-ppo-update" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: TRL PPO 多轮更新
</figcaption>
<div aria-describedby="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-trl-ppo-update"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-trl-ppo-update-1"><a href="#lst-trl-ppo-update-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span>
<span id="lst-trl-ppo-update-2"><a href="#lst-trl-ppo-update-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ppo_epoch_idx <span class="kw">in</span> <span class="bu">range</span>(args.num_ppo_epochs):</span>
<span id="lst-trl-ppo-update-3"><a href="#lst-trl-ppo-update-3" aria-hidden="true" tabindex="-1"></a>    b_inds <span class="op">=</span> np.random.permutation(args.local_batch_size)</span>
<span id="lst-trl-ppo-update-4"><a href="#lst-trl-ppo-update-4" aria-hidden="true" tabindex="-1"></a>    minibatch_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="lst-trl-ppo-update-5"><a href="#lst-trl-ppo-update-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> mini_batch_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, args.local_batch_size, args.local_mini_batch_size):</span>
<span id="lst-trl-ppo-update-6"><a href="#lst-trl-ppo-update-6" aria-hidden="true" tabindex="-1"></a>        mini_batch_end <span class="op">=</span> mini_batch_start <span class="op">+</span> args.local_mini_batch_size</span>
<span id="lst-trl-ppo-update-7"><a href="#lst-trl-ppo-update-7" aria-hidden="true" tabindex="-1"></a>        mini_batch_inds <span class="op">=</span> b_inds[mini_batch_start:mini_batch_end]</span>
<span id="lst-trl-ppo-update-8"><a href="#lst-trl-ppo-update-8" aria-hidden="true" tabindex="-1"></a>        gradient_accumulation_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="lst-trl-ppo-update-9"><a href="#lst-trl-ppo-update-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> micro_batch_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, args.local_mini_batch_size, args.per_device_train_batch_size):</span>
<span id="lst-trl-ppo-update-10"><a href="#lst-trl-ppo-update-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> accelerator.accumulate(model):</span>
<span id="lst-trl-ppo-update-11"><a href="#lst-trl-ppo-update-11" aria-hidden="true" tabindex="-1"></a>                micro_batch_end <span class="op">=</span> micro_batch_start <span class="op">+</span> args.per_device_train_batch_size</span>
<span id="lst-trl-ppo-update-12"><a href="#lst-trl-ppo-update-12" aria-hidden="true" tabindex="-1"></a>                micro_batch_inds <span class="op">=</span> mini_batch_inds[micro_batch_start:micro_batch_end]</span>
<span id="lst-trl-ppo-update-13"><a href="#lst-trl-ppo-update-13" aria-hidden="true" tabindex="-1"></a>                mb_advantage <span class="op">=</span> advantages[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-14"><a href="#lst-trl-ppo-update-14" aria-hidden="true" tabindex="-1"></a>                mb_responses <span class="op">=</span> responses[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-15"><a href="#lst-trl-ppo-update-15" aria-hidden="true" tabindex="-1"></a>                mb_query_responses <span class="op">=</span> query_responses[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-16"><a href="#lst-trl-ppo-update-16" aria-hidden="true" tabindex="-1"></a>                mb_logprobs <span class="op">=</span> logprobs[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-17"><a href="#lst-trl-ppo-update-17" aria-hidden="true" tabindex="-1"></a>                mb_return <span class="op">=</span> returns[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-18"><a href="#lst-trl-ppo-update-18" aria-hidden="true" tabindex="-1"></a>                mb_values <span class="op">=</span> values[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-19"><a href="#lst-trl-ppo-update-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-trl-ppo-update-20"><a href="#lst-trl-ppo-update-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-trl-ppo-update-21"><a href="#lst-trl-ppo-update-21" aria-hidden="true" tabindex="-1"></a>                output, vpred_temp <span class="op">=</span> forward(model, mb_query_responses, processing_class.pad_token_id)</span>
<span id="lst-trl-ppo-update-22"><a href="#lst-trl-ppo-update-22" aria-hidden="true" tabindex="-1"></a>                logits <span class="op">=</span> output.logits[:, context_length <span class="op">-</span> <span class="dv">1</span> : <span class="op">-</span><span class="dv">1</span>]</span>
<span id="lst-trl-ppo-update-23"><a href="#lst-trl-ppo-update-23" aria-hidden="true" tabindex="-1"></a>                logits <span class="op">/=</span> args.temperature <span class="op">+</span> <span class="fl">1e-7</span></span>
<span id="lst-trl-ppo-update-24"><a href="#lst-trl-ppo-update-24" aria-hidden="true" tabindex="-1"></a>                new_logprobs <span class="op">=</span> selective_log_softmax(logits, mb_responses)</span>
<span id="lst-trl-ppo-update-25"><a href="#lst-trl-ppo-update-25" aria-hidden="true" tabindex="-1"></a>                new_logprobs <span class="op">=</span> torch.masked_fill(</span>
<span id="lst-trl-ppo-update-26"><a href="#lst-trl-ppo-update-26" aria-hidden="true" tabindex="-1"></a>                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB</span>
<span id="lst-trl-ppo-update-27"><a href="#lst-trl-ppo-update-27" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="lst-trl-ppo-update-28"><a href="#lst-trl-ppo-update-28" aria-hidden="true" tabindex="-1"></a>                vpred <span class="op">=</span> vpred_temp[:, context_length <span class="op">-</span> <span class="dv">1</span> : <span class="op">-</span><span class="dv">1</span>].squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="lst-trl-ppo-update-29"><a href="#lst-trl-ppo-update-29" aria-hidden="true" tabindex="-1"></a>                vpred <span class="op">=</span> torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], <span class="dv">0</span>)</span>
<span id="lst-trl-ppo-update-30"><a href="#lst-trl-ppo-update-30" aria-hidden="true" tabindex="-1"></a>                vpredclipped <span class="op">=</span> torch.clamp(</span>
<span id="lst-trl-ppo-update-31"><a href="#lst-trl-ppo-update-31" aria-hidden="true" tabindex="-1"></a>                    vpred,</span>
<span id="lst-trl-ppo-update-32"><a href="#lst-trl-ppo-update-32" aria-hidden="true" tabindex="-1"></a>                    mb_values <span class="op">-</span> args.cliprange_value,</span>
<span id="lst-trl-ppo-update-33"><a href="#lst-trl-ppo-update-33" aria-hidden="true" tabindex="-1"></a>                    mb_values <span class="op">+</span> args.cliprange_value,</span>
<span id="lst-trl-ppo-update-34"><a href="#lst-trl-ppo-update-34" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="lst-trl-ppo-update-35"><a href="#lst-trl-ppo-update-35" aria-hidden="true" tabindex="-1"></a>                vf_losses1 <span class="op">=</span> torch.square(vpred <span class="op">-</span> mb_return)</span>
<span id="lst-trl-ppo-update-36"><a href="#lst-trl-ppo-update-36" aria-hidden="true" tabindex="-1"></a>                vf_losses2 <span class="op">=</span> torch.square(vpredclipped <span class="op">-</span> mb_return)</span>
<span id="lst-trl-ppo-update-37"><a href="#lst-trl-ppo-update-37" aria-hidden="true" tabindex="-1"></a>                vf_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(vf_losses1, vf_losses2)</span>
<span id="lst-trl-ppo-update-38"><a href="#lst-trl-ppo-update-38" aria-hidden="true" tabindex="-1"></a>                vf_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> masked_mean(vf_loss_max, <span class="op">~</span>padding_mask_p1[micro_batch_inds])</span>
<span id="lst-trl-ppo-update-39"><a href="#lst-trl-ppo-update-39" aria-hidden="true" tabindex="-1"></a>                vf_clipfrac <span class="op">=</span> masked_mean(</span>
<span id="lst-trl-ppo-update-40"><a href="#lst-trl-ppo-update-40" aria-hidden="true" tabindex="-1"></a>                    (vf_losses2 <span class="op">&gt;</span> vf_losses1).<span class="bu">float</span>(), <span class="op">~</span>padding_mask_p1[micro_batch_inds]</span>
<span id="lst-trl-ppo-update-41"><a href="#lst-trl-ppo-update-41" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="lst-trl-ppo-update-42"><a href="#lst-trl-ppo-update-42" aria-hidden="true" tabindex="-1"></a>                logprobs_diff <span class="op">=</span> new_logprobs <span class="op">-</span> mb_logprobs</span>
<span id="lst-trl-ppo-update-43"><a href="#lst-trl-ppo-update-43" aria-hidden="true" tabindex="-1"></a>                ratio <span class="op">=</span> torch.exp(logprobs_diff)</span>
<span id="lst-trl-ppo-update-44"><a href="#lst-trl-ppo-update-44" aria-hidden="true" tabindex="-1"></a>                pg_losses <span class="op">=</span> <span class="op">-</span>mb_advantage <span class="op">*</span> ratio</span>
<span id="lst-trl-ppo-update-45"><a href="#lst-trl-ppo-update-45" aria-hidden="true" tabindex="-1"></a>                pg_losses2 <span class="op">=</span> <span class="op">-</span>mb_advantage <span class="op">*</span> torch.clamp(ratio, <span class="fl">1.0</span> <span class="op">-</span> args.cliprange, <span class="fl">1.0</span> <span class="op">+</span> args.cliprange)</span>
<span id="lst-trl-ppo-update-46"><a href="#lst-trl-ppo-update-46" aria-hidden="true" tabindex="-1"></a>                pg_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(pg_losses, pg_losses2)</span>
<span id="lst-trl-ppo-update-47"><a href="#lst-trl-ppo-update-47" aria-hidden="true" tabindex="-1"></a>                pg_loss <span class="op">=</span> masked_mean(pg_loss_max, <span class="op">~</span>padding_mask[micro_batch_inds])</span>
<span id="lst-trl-ppo-update-48"><a href="#lst-trl-ppo-update-48" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> pg_loss <span class="op">+</span> args.vf_coef <span class="op">*</span> vf_loss</span>
<span id="lst-trl-ppo-update-49"><a href="#lst-trl-ppo-update-49" aria-hidden="true" tabindex="-1"></a>                accelerator.backward(loss)</span>
<span id="lst-trl-ppo-update-50"><a href="#lst-trl-ppo-update-50" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="lst-trl-ppo-update-51"><a href="#lst-trl-ppo-update-51" aria-hidden="true" tabindex="-1"></a>                optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<aside id="footnotes-15" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="19">
<li id="fn19"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="openrlhf-1" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</h3>
<p>类似地，OpenRLHF 在 <a href="#lst-openrlhf-calc-kl-estimator" class="quarto-xref">Listing&nbsp;2</a> 中计算 KL 样本值使用的 <code>log_probs</code> 在 <code>make_experience</code> 时被计算，和对应的样本 <code>sequences</code> 都来自采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span>，而非当前策略 <span class="math inline">\(\pi_{\theta}\)</span>。对应代码可见 <a href="#lst-openrlhf-sample-and-calc-old-logprob" class="quarto-xref">Listing&nbsp;9</a>。</p>
<div id="lst-openrlhf-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;9: OpenRLHF 采样样本并使用 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 计算对数似然
</figcaption>
<div aria-describedby="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="lst-openrlhf-sample-and-calc-old-logprob"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-openrlhf-sample-and-calc-old-logprob-1"><a href="#lst-openrlhf-sample-and-calc-old-logprob-1" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-2"><a href="#lst-openrlhf-sample-and-calc-old-logprob-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_experience(<span class="va">self</span>, samples: Samples) <span class="op">-&gt;</span> Experience:</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-3"><a href="#lst-openrlhf-sample-and-calc-old-logprob-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-4"><a href="#lst-openrlhf-sample-and-calc-old-logprob-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-5"><a href="#lst-openrlhf-sample-and-calc-old-logprob-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-6"><a href="#lst-openrlhf-sample-and-calc-old-logprob-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-7"><a href="#lst-openrlhf-sample-and-calc-old-logprob-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-8"><a href="#lst-openrlhf-sample-and-calc-old-logprob-8" aria-hidden="true" tabindex="-1"></a>    action_log_probs <span class="op">=</span> <span class="va">self</span>.actor(</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-9"><a href="#lst-openrlhf-sample-and-calc-old-logprob-9" aria-hidden="true" tabindex="-1"></a>        sequences,</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-10"><a href="#lst-openrlhf-sample-and-calc-old-logprob-10" aria-hidden="true" tabindex="-1"></a>        num_actions,</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-11"><a href="#lst-openrlhf-sample-and-calc-old-logprob-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-12"><a href="#lst-openrlhf-sample-and-calc-old-logprob-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-13"><a href="#lst-openrlhf-sample-and-calc-old-logprob-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-14"><a href="#lst-openrlhf-sample-and-calc-old-logprob-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-15"><a href="#lst-openrlhf-sample-and-calc-old-logprob-15" aria-hidden="true" tabindex="-1"></a>    kl <span class="op">=</span> compute_approx_kl(</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-16"><a href="#lst-openrlhf-sample-and-calc-old-logprob-16" aria-hidden="true" tabindex="-1"></a>        action_log_probs,</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-17"><a href="#lst-openrlhf-sample-and-calc-old-logprob-17" aria-hidden="true" tabindex="-1"></a>        base_action_log_probs,</span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-18"><a href="#lst-openrlhf-sample-and-calc-old-logprob-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="lst-openrlhf-sample-and-calc-old-logprob-19"><a href="#lst-openrlhf-sample-and-calc-old-logprob-19" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>从 <a href="#lst-openrlhf-calc-kl-loss" class="quarto-xref">Listing&nbsp;3</a> 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 <span class="math inline">\(\pi_{\theta}\)</span> 计算的对数似然，但如 <a href="#sec-kl-loss-impl" class="quarto-xref">Section&nbsp;4.1</a> 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。</p>
</section>
<section id="verl-1" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="verl-1"><span class="header-section-number">6.1.3</span> verl</h3>
<p>从 <a href="#lst-verl-kl-reward" class="quarto-xref">Listing&nbsp;4</a> 可见，verl 同样使用 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 计算 KL 样本值。</p>
<p>从 <a href="#lst-verl-kl-loss" class="quarto-xref">Listing&nbsp;5</a> 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 的 KL 样本值。</p>
</section>
</section>
<section id="利用重要性采样处理-off-policy-设置" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</h2>
<p>off-policy 设置下，我们没有来自最新策略 <span class="math inline">\(\pi_{\theta}\)</span> 的样本，而只能使用来自采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 的样本，但我们仍然希望估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。</p>
<p>熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即</p>
<p><span id="eq-is-off-policy-kl"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[f(\mathbf{\tau})\right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) f(\tau)  = \sum_{\tau \in \mathcal{T}} p_{\theta_{\text{old}}}(\tau) \frac{p_{\theta}(\tau)}{p_{\theta_{\text{old}}}(\tau)} f(\tau) = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}} \left[\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} f(\mathbf{\tau})\right]
\tag{50}\]</span></span></p>
<p>此处，重要性采样系数 <span class="math inline">\(\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})}\)</span> 可以仿照 <a href="#eq-dp-expansion" class="quarto-xref">Equation&nbsp;5</a> 展开为：</p>
<p><span id="eq-is-coef-expansion"><span class="math display">\[
\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} = \prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{\pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}
\tag{51}\]</span></span> <a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p>利用重要性采样 (<a href="#eq-is-off-policy-kl" class="quarto-xref">Equation&nbsp;50</a>, <a href="#eq-is-coef-expansion" class="quarto-xref">Equation&nbsp;51</a>) ，KL 梯度表达式 <a href="#eq-def-kl-grad-kt-reduce" class="quarto-xref">Equation&nbsp;46</a> 可以转化为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right] \\
=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right] \\
=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \frac{p_{\theta}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}{p_{\theta_{\text{old}}}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}  \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})  \right] \\
=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \left(\prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}\right) \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right]
\end{aligned}
\tag{52}\]</span></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \\
\approx&amp; \frac{1}{N} \sum_{i=1}^{N} \left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) \\
=&amp; \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})
\end{aligned}
\tag{53}\]</span></span></p>
<p>对应的 loss 函数为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc-loss"><span class="math display">\[
\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_{i}|} \text{nograd}\left(\left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right)\sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})
\tag{54}\]</span></span></p>
<p>类似 <a href="#eq-def-kl-grad-kt-reduce-k" class="quarto-xref">Equation&nbsp;49</a>，我们可以令</p>
<p><span id="eq-def-kl-reward-is"><span class="math display">\[
k(\mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t}) = \left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}
\tag{55}\]</span></span></p>
<p>注意，<a href="#eq-def-kl-reward-is" class="quarto-xref">Equation&nbsp;55</a> 中的 <span class="math inline">\(k\)</span> 需要对于每个新的 <span class="math inline">\(\pi_{\theta}\)</span> 重新计算。</p>
<aside id="footnotes-16" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="20">
<li id="fn20"><p>实际计算中，<a href="#eq-is-coef-expansion" class="quarto-xref">Equation&nbsp;51</a> 由于涉及到 <span class="math inline">\(|\mathbf{\tau}|\)</span> 次连乘，方差大且数值稳定性差，需要利用因果性、近似等技术来化简。本文目前省略该部分，后续将会更新相关内容。<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="结论如何正确地在-rl-中优化-kl-散度" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</h1>
<section id="修正-grpo-公式中的-kl-项" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</h2>
<p>GRPO 公式 (<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a>, <a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a>) 对于 KL 优化主要存在两个错误：</p>
<ol type="1">
<li>忽略了 KL 优化的 off-policy 问题</li>
<li>先将 <span class="math inline">\(k_{3}\)</span> 估计样本量应用于动作条件似然再求和，导致得到异常的梯度</li>
</ol>
<p>对于这两个问题，在 <a href="#eq-grpo-obj-kl-fixed" class="quarto-xref">Equation&nbsp;29</a> 的基础上，仿照 <a href="#eq-def-kl-reward-is" class="quarto-xref">Equation&nbsp;55</a>，我们可以按如下方式修正：</p>
<p><span id="eq-grpo-obj-kl-fixed-is"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \left\{ \sum_{t=1}^{\left|o_i\right|} \min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old}}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right] \right\} -\beta \left(\prod_{t=1}^{|o_{i}|}\frac{\pi_{\theta}(o_{i, t} | q, o_{i,\lt t})}{ \pi_{\theta_{\text{old}}}(o_{i, t} | q, o_{i,\lt t})}\right) k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)
\end{aligned}
\tag{56}\]</span></span></p>
</section>
<section id="修正流行-llm-rl-框架中的-kl-优化实现" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</h2>
<p>目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：</p>
<ol type="1">
<li>实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）</li>
<li>错误地实现了平均操作</li>
</ol>
<p>对于这些问题，可以按照如下思路修正：</p>
<ol type="1">
<li>为 KL 项添加重要性采样，这需要从第二轮更新开始，每次基于新的 <span class="math inline">\(\pi_\theta\)</span> 重新计算 KL loss / reward 项，包括重要性采样系数</li>
<li>应用 KL 估计样本量时，先对于序列内 token 间的对数条件似然求和，得到轨迹联合概率，再代入公式</li>
<li>如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 <a href="#eq-def-kl-reward-is" class="quarto-xref">Equation&nbsp;55</a> 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）</li>
<li>如果不希望应用 reward 优化的其他技术，可以按 <a href="#eq-def-kl-grad-kt-reduce-is-mc-loss" class="quarto-xref">Equation&nbsp;54</a> 实现为 KL loss 项</li>
</ol>
</section>
</section>
<section id="讨论" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> 讨论</h1>
<section id="对于-kl-梯度更好的估计样本量" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</h2>
<p>如 <a href="#sec-kl-grad-as-kl-reward" class="quarto-xref">Section&nbsp;5.5</a> 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？</p>
<p>此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？</p>
</section>
<section id="kl-regularized-rl-的理论优势" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</h2>
<p>最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。</p>
<p>然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 <span class="citation" data-cites="zhao2025logregretkl">Zhao et al. (<a href="#ref-zhao2025logregretkl" role="doc-biblioref">2025</a>)</span> 证明了 KL-regularized RL 的 regret 只有 <span class="math inline">\(\mathcal{O}(\log T)\)</span>，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 <span class="math inline">\(\mathcal{O}(\sqrt{T})\)</span>。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="附录" class="level1 appendix" data-number="9"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9</span> 附录</h2><div class="quarto-appendix-contents">

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>本文的作者（童雨轩）仍在寻求北美的 Ph.D.&nbsp;或 RA 机会。如果你觉得本文对你有帮助，欢迎浏览其主页<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>来获取进一步了解。</p>
</div>
</div>




</div></section><section id="相关工作" class="level2 appendix" data-number="9.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.1</span> 相关工作</h2><div class="quarto-appendix-contents">

<p>与本文同期也有许多精彩的讨论，由于笔者还没能通读全文，此处仅提供链接，不作概括，欢迎感兴趣的读者自行阅读：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/28440962040">GRPO 中的 KL Loss 实现细节问题 - Hongyu Zang @ 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28735759256">k2 loss就是比k3 loss好！以及GRPO_off-policy - Yiming Liu @ 知乎</a></li>
</ul>
<aside id="footnotes-17" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="21">
<li id="fn21"><p>https://tongyx361.github.io<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</div></section><section id="写作契机trpoppo-与-grpo-中的-kl-为什么不一样" class="level2 appendix" data-number="9.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.2</span> 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”</h2><div class="quarto-appendix-contents">

<p>笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>：</p>
<blockquote class="blockquote">
<p>A small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?</p>
<p>TRPO <span class="citation" data-cites="schulman2015trpo">(<a href="#ref-schulman2015trpo" role="doc-biblioref">Schulman et al. 2015</a>)</span></p>
</blockquote>
<p><span id="eq-trpo"><span class="math display">\[
\begin{aligned}
&amp; \underset{\theta}{\text{maximize}}~L_{\theta_{\text {old }}}(\theta) \\
&amp; \text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta
\end{aligned}
\tag{57}\]</span></span></p>
<blockquote class="blockquote">
<p>PPO <span class="citation" data-cites="schulman2017ppo">(<a href="#ref-schulman2017ppo" role="doc-biblioref">Schulman et al. 2017</a>)</span></p>
</blockquote>
<p><span id="eq-ppo-klpen"><span class="math display">\[
L^{K L P E N}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(\mathbf{y}_t \mid \mathbf{x}_t\right)}{\pi_{\theta_{\text {old }}}\left(\mathbf{y}_t \mid \mathbf{x}_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid \mathbf{x}_t\right), \pi_\theta\left(\cdot \mid \mathbf{x}_t\right)\right]\right]
\tag{58}\]</span></span></p>
<blockquote class="blockquote">
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(<a href="#ref-shao2024deepseekmath" role="doc-biblioref">Shao et al. 2024</a>)</span></p>
</blockquote>
<p><span id="eq-grpo"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left\{\min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]-\beta \mathbb{D}_{K L}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\right\}
\end{aligned}
\tag{59}\]</span></span></p>
<p>这个问题本身的答案是非常简单的。</p>
<p>首先，这个问题混淆了两种不同的 KL 惩罚项：</p>
<ol type="1">
<li><span class="math inline">\(\text{KL}[\pi_{\theta_{\text{old}}},\pi_{\theta}]\)</span>，其作用是约束最新策略 <span class="math inline">\(\pi_{\theta}\)</span>不要离采样策略<span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。</li>
<li><span class="math inline">\(\text{KL}[\pi_{\theta},\pi_{\theta_{\text{ref}}}]\)</span>，其作用是约束最新策略 <span class="math inline">\(\pi_{\theta}\)</span>不要离参考策略<span class="math inline">\(\pi_{\theta_{\text{ref}}}\)</span> 太远，从而更充分地利用参考策略中的先验。</li>
</ol>
<p>另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 <span class="math inline">\(\text{KL}[\pi_{\theta_{\text{old}}},\pi_{\theta}]\)</span>。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：</p>
<blockquote class="blockquote">
<p>Let <span class="math inline">\(r_t(\theta)\)</span> denote the probability ratio <span class="math inline">\(r_{t}(\theta)=\frac{\pi_{\theta}\left(a_t \mid s_t\right)}{\left(\pi_{\theta_{\text {old }}}\left|a_t\right| s_t\right)}\)</span>, so <span class="math inline">\(r\left(\theta_{\text{old}}\right)=1\)</span>. TRPO maximizes a “surrogate” objective</p>
</blockquote>
<p><span class="math display">\[
L^{\text{CPI}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t\right]=\hat{\mathbb{E}}_t\left[r_t(\theta) \hat{A}_t\right] .
\]</span></p>
<blockquote class="blockquote">
<p>…</p>
<p>The main objective we propose is the following:</p>
</blockquote>
<p><span class="math display">\[
L^{\text{CLIP}}(\theta)=\hat{\mathbb{E}}_t\left[\min \left(r_t(\theta) \hat{A}_t, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right]
\]</span></p>
<blockquote class="blockquote">
<p>where epsilon is a hyperparameter, say, <span class="math inline">\(\epsilon=0.2\)</span>. The motivation for this objective is as follows. The first term inside the <span class="math inline">\(\min\)</span> is <span class="math inline">\(L^{\text{CPI}}\)</span>. The second term, <span class="math inline">\(\text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\)</span>, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving <span class="math inline">\(r_t\)</span> outside of the interval <span class="math inline">\([1-\epsilon, 1+\epsilon]\)</span>.</p>
<p>…</p>
<p><strong>Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence</strong>, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence <span class="math inline">\(d_{\text{targ}}\)</span> each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve included it here because it’s an important baseline.</p>
<p>In the simplest instantiation of this algorithm, we perform the following steps in each policy update:</p>
<ul>
<li>Using several epochs of minibatch SGD, optimize the KL-penalized objective</li>
</ul>
</blockquote>
<p><span class="math display">\[
L^{\text{KLPEN}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid s_t\right)\right]\right]
\]</span></p>
<blockquote class="blockquote">

</blockquote>
<p>顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 <span class="math inline">\(r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}\)</span>就是<span class="math inline">\(K L\left[\pi_{\theta_{d d}}, \pi_\theta\right]=\mathbb{E}_{a_t \sim \pi_{\theta_{d t}}\left(\cdot \mid s_t\right)}\left[\log \frac{\pi_{\theta_{d t}}\left(a_t \mid s_t\right)}{\pi_\theta\left(a_t \mid s_t\right)}\right]\)</span> 中对单个样本 <span class="math inline">\((s_t, a_t)\)</span> 的值中 <span class="math inline">\(\log\)</span> 的真数。</p>
<aside id="footnotes-18" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="22">
<li id="fn22"><p>https://x.com/pufanyi/status/1888845956684370202<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</div></section><section id="致谢" class="level2 appendix" data-number="9.3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.3</span> 致谢</h2><div class="quarto-appendix-contents">

<p>感谢王浩然、YuMS 对本文提供的重要反馈。</p>
<p>感谢生广明、Wei Xiong、刘仁彪、刘威、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论以及对于本文的有益反馈。</p>
<p>感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。</p>
</div></section><section id="引用" class="level2 appendix" data-number="9.4"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.4</span> 引用</h2><div class="quarto-appendix-contents">

<p>BibTeX:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource bibtex number-lines code-with-copy"><code class="sourceCode bibtex"><span id="cb2-1"><a href="#cb2-1"></a><span class="va">@online</span>{<span class="ot">tong2025kl</span>,</span>
<span id="cb2-2"><a href="#cb2-2"></a>  <span class="dt">author</span> = {童雨轩},</span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="dt">title</span> = {重新思考 {RL} 中的 {KL} 梯度优化},</span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="dt">year</span> = {2025},</span>
<span id="cb2-5"><a href="#cb2-5"></a>  <span class="dt">url</span> = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},</span>
<span id="cb2-6"><a href="#cb2-6"></a>  <span class="dt">urldate</span> = {2025-03-09},</span>
<span id="cb2-7"><a href="#cb2-7"></a>  <span class="dt">language</span> = {Chinese},</span>
<span id="cb2-8"><a href="#cb2-8"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>文本：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb3-1"><a href="#cb3-1"></a>童雨轩. 2025. “重新思考 RL 中的 KL 梯度优化.” https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->


</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-hu2024openrlhf" class="csl-entry" role="listitem">
Hu, Jian, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. <span>“OpenRLHF: An Easy-to-Use, Scalable and High-Performance RLHF Framework.”</span> <em>arXiv Preprint arXiv:2405.11143</em>.
</div>
<div id="ref-jaques2019wayoffpolicy" class="csl-entry" role="listitem">
Jaques, Natasha, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. <span>“Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.”</span> <a href="https://arxiv.org/abs/1907.00456">https://arxiv.org/abs/1907.00456</a>.
</div>
<div id="ref-ouyang2022instructgpt" class="csl-entry" role="listitem">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. <a href="https://openreview.net/forum?id=TG8KACxEON">https://openreview.net/forum?id=TG8KACxEON</a>.
</div>
<div id="ref-schulman2015trpo" class="csl-entry" role="listitem">
Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 1889–97. PMLR.
</div>
<div id="ref-schulman2018gae" class="csl-entry" role="listitem">
Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2018. <span>“High-Dimensional Continuous Control Using Generalized Advantage Estimation.”</span> <a href="https://arxiv.org/abs/1506.02438">https://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-schulman2017ppo" class="csl-entry" role="listitem">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv:1707.06347</em>.
</div>
<div id="ref-shao2024deepseekmath" class="csl-entry" role="listitem">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, et al. 2024. <span>“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.”</span> <em>arXiv Preprint arXiv:2402.03300</em>.
</div>
<div id="ref-sheng2024hybridflow" class="csl-entry" role="listitem">
Sheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. <span>“HybridFlow: A Flexible and Efficient RLHF Framework.”</span> <em>arXiv Preprint arXiv: 2409.19256</em>.
</div>
<div id="ref-stiennon2020summarize" class="csl-entry" role="listitem">
Stiennon, Nisan, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020. <span>“Learning to Summarize with Human Feedback.”</span> In <em>NeurIPS</em>. <a href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html</a>.
</div>
<div id="ref-zhao2025logregretkl" class="csl-entry" role="listitem">
Zhao, Heyang, Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. 2025. <span>“Logarithmic Regret for Online KL-Regularized Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2502.07460">https://arxiv.org/abs/2502.07460</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tongyx361\.github\.io\/blogs");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="tongyx361/blogs" data-repo-id="R_kgDOOFf8xw" data-category="General" data-category-id="DIC_kwDOOFf8x84CoCU7" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="co"># title</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="an">title:</span><span class="co"> "重新思考 RL 中的 KL 梯度优化"</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="an">subtitle:</span><span class="co"> "修正 GRPO 公式与流行 LLM RL 框架"</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="an">date:</span><span class="co"> "2025-03-09"</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co">  name: "童雨轩"</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co">  email: "tongyuxuan361@gmail.com"</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="an">copyright:</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="co">  holder: "童雨轩"</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co">  year: 2025</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="co"># abstract</span></span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="an">abstract-title:</span><span class="co"> Takeways</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co">    对于 LLM RL 中相对于参考策略的 KL 优化，GRPO 公式</span></span>
<span id="cb4-16"><a href="#cb4-16"></a></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co">    1) 没有处理 KL 项的 off-policy 问题，这可以通过在多轮更新时重新计算 KL 项并添加重要性采样系数解决</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">    2) 先将 KL 估计样本量应用于动作对数条件似然再求和，而非先求和得到概率再应用估计样本量，与 John Schulman "Approximating KL Divergence"^[http://joschu.net/blog/kl-approx.html] 分析不符（对应导出的梯度也可能因此而错误）</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">    </span></span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="co">    目前流行的 LLM RL 框架（TRL，OpenRLHF，verl）也没有避免上述问题，且存在其他问题：</span></span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a><span class="co">    3) 在计算 KL loss 项时默认不去除任何梯度，实际得到的梯度通常不是在优化 KL 散度</span></span>
<span id="cb4-23"><a href="#cb4-23"></a><span class="co">    4) KL loss 项的平均操作存在错误。</span></span>
<span id="cb4-24"><a href="#cb4-24"></a><span class="co">  </span></span>
<span id="cb4-25"><a href="#cb4-25"></a><span class="co">    本文基于序列决策过程（而非 bandit）建模分析了上述问题，并提供了正确的 KL loss / reward 项实现的数学推导与上述问题的修正思路。</span></span>
<span id="cb4-26"><a href="#cb4-26"></a><span class="an">toc-expand:</span><span class="co"> false</span></span>
<span id="cb4-27"><a href="#cb4-27"></a><span class="co"># citation</span></span>
<span id="cb4-28"><a href="#cb4-28"></a><span class="an">citation:</span><span class="co"> false # Manual citation</span></span>
<span id="cb4-29"><a href="#cb4-29"></a><span class="co"># language</span></span>
<span id="cb4-30"><a href="#cb4-30"></a><span class="an">toc-title:</span><span class="co"> 目录</span></span>
<span id="cb4-31"><a href="#cb4-31"></a><span class="an">categories:</span></span>
<span id="cb4-32"><a href="#cb4-32"></a><span class="co">    - Chinese 中文</span></span>
<span id="cb4-33"><a href="#cb4-33"></a><span class="co">    - Technical 技术</span></span>
<span id="cb4-34"><a href="#cb4-34"></a><span class="co">---</span></span>
<span id="cb4-35"><a href="#cb4-35"></a></span>
<span id="cb4-36"><a href="#cb4-36"></a><span class="fu"># 引言：GRPO 公式的“错误”{#sec-grpo-kl-misunderstanding}</span></span>
<span id="cb4-37"><a href="#cb4-37"></a></span>
<span id="cb4-38"><a href="#cb4-38"></a>GRPO <span class="co">[</span><span class="ot">@shao2024deepseekmath</span><span class="co">]</span> 的优化目标公式为：</span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a>$$</span>
<span id="cb4-41"><a href="#cb4-41"></a>\begin{aligned}</span>
<span id="cb4-42"><a href="#cb4-42"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb4-43"><a href="#cb4-43"></a>&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left<span class="sc">\{</span>\min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span>-\beta \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\right<span class="sc">\}</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>\end{aligned}</span>
<span id="cb4-45"><a href="#cb4-45"></a>$$ {#eq-grpo-obj}</span>
<span id="cb4-46"><a href="#cb4-46"></a></span>
<span id="cb4-47"><a href="#cb4-47"></a>其中</span>
<span id="cb4-48"><a href="#cb4-48"></a></span>
<span id="cb4-49"><a href="#cb4-49"></a>$$</span>
<span id="cb4-50"><a href="#cb4-50"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]=\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1</span>
<span id="cb4-51"><a href="#cb4-51"></a>$$ {#eq-grpo-obj-kl-term}</span>
<span id="cb4-52"><a href="#cb4-52"></a></span>
<span id="cb4-53"><a href="#cb4-53"></a></span>
<span id="cb4-54"><a href="#cb4-54"></a>首先，<span class="co">[</span><span class="ot">@eq-grpo-obj</span><span class="co">]</span> 中出现了 $\pi_{\theta_\text{old}}$，这意味着其考虑了 off-policy 设置，但 <span class="co">[</span><span class="ot">@eq-grpo-obj-kl-term</span><span class="co">]</span> 中却没有相应的处理，只适用于 $o_i \sim \pi_{\theta}$，无法正确处理 $o_i \sim \pi_{\theta_\text{old}}$。</span>
<span id="cb4-55"><a href="#cb4-55"></a></span>
<span id="cb4-56"><a href="#cb4-56"></a>其次，<span class="co">[</span><span class="ot">@eq-grpo-obj-kl-term</span><span class="co">]</span> 将估计样本量 $\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1$ 写成 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]$ 也并不十分恰当，因为 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]$ 通常表示 KL 散度的真实值。</span>
<span id="cb4-57"><a href="#cb4-57"></a></span>
<span id="cb4-58"><a href="#cb4-58"></a>而目前流行的 LLM RL 框架，在实现 KL 优化时，通常也忽略了 off-policy 问题，同时还存在其他一系列问题：</span>
<span id="cb4-59"><a href="#cb4-59"></a></span>
<span id="cb4-60"><a href="#cb4-60"></a><span class="ss">1. </span>误认为前向传播估计出 KL 散度，再反向传播就能得到其梯度（但实际上通常并非如此）；</span>
<span id="cb4-61"><a href="#cb4-61"></a><span class="ss">2. </span>忽略了先对动作动作对数条件似然应用 KL 估计样本量再求和并非良好定义的行为，导致梯度错误；</span>
<span id="cb4-62"><a href="#cb4-62"></a><span class="ss">3. </span>忽略了同一轨迹上 KL 对数概率必须求和以获得轨迹联合概率，而不能求平均；</span>
<span id="cb4-63"><a href="#cb4-63"></a><span class="ss">4. </span>错误地计算了平均操作。</span>
<span id="cb4-64"><a href="#cb4-64"></a></span>
<span id="cb4-65"><a href="#cb4-65"></a>由于 on-policy 设置更加简单，但也已经暴露了上述大部分问题，我们可以先从 on-policy 设置开始讨论，后续再考虑 off-policy 设置。</span>
<span id="cb4-66"><a href="#cb4-66"></a></span>
<span id="cb4-67"><a href="#cb4-67"></a><span class="fu"># 流行 LLM RL 框架中 on-policy KL 优化的实现 {#sec-popular-llm-rl-kl-optim}</span></span>
<span id="cb4-68"><a href="#cb4-68"></a></span>
<span id="cb4-69"><a href="#cb4-69"></a>我们可以先回顾目前流行的 LLM RL 框架中对于 KL 优化的实现。以下我们以</span>
<span id="cb4-70"><a href="#cb4-70"></a></span>
<span id="cb4-71"><a href="#cb4-71"></a><span class="ss">1. </span>TRL^<span class="co">[</span><span class="ot">https://github.com/huggingface/trl</span><span class="co">]</span>，</span>
<span id="cb4-72"><a href="#cb4-72"></a><span class="ss">2. </span>OpenRLHF^<span class="co">[</span><span class="ot">https://github.com/OpenRLHF/OpenRLHF</span><span class="co">] [@hu2024openrlhf]</span></span>
<span id="cb4-73"><a href="#cb4-73"></a><span class="ss">3. </span>verl^<span class="co">[</span><span class="ot">https://github.com/volcengine/verl</span><span class="co">] [@sheng2024hybridflow]</span></span>
<span id="cb4-74"><a href="#cb4-74"></a></span>
<span id="cb4-75"><a href="#cb4-75"></a>为例。</span>
<span id="cb4-76"><a href="#cb4-76"></a></span>
<span id="cb4-77"><a href="#cb4-77"></a>熟悉这些框架的读者可以跳过本节，直接从 <span class="co">[</span><span class="ot">@sec-rl-kl-optim-formulation</span><span class="co">]</span> 开始阅读。</span>
<span id="cb4-78"><a href="#cb4-78"></a></span>
<span id="cb4-79"><a href="#cb4-79"></a><span class="fu">## TRL：KL reward 项</span></span>
<span id="cb4-80"><a href="#cb4-80"></a></span>
<span id="cb4-81"><a href="#cb4-81"></a>TRL 计算 KL 定义中的样本值 $\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}$，并将其从 reward 中减去。对应代码可见 <span class="co">[</span><span class="ot">@lst-trl-kl-reward</span><span class="co">]</span>。</span>
<span id="cb4-82"><a href="#cb4-82"></a></span>
<span id="cb4-83"><a href="#cb4-83"></a><span class="in">```{#lst-trl-kl-reward .python lst-cap="TRL 计算 KL 样本值 $\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}$ 并从 reward 中减去^[https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506]"}</span></span>
<span id="cb4-84"><a href="#cb4-84"></a><span class="in"># 4. compute rewards</span></span>
<span id="cb4-85"><a href="#cb4-85"></a><span class="in">kl = logprobs - ref_logprobs</span></span>
<span id="cb4-86"><a href="#cb4-86"></a><span class="in">non_score_reward = -args.kl_coef * kl</span></span>
<span id="cb4-87"><a href="#cb4-87"></a><span class="in">rewards = non_score_reward.clone()</span></span>
<span id="cb4-88"><a href="#cb4-88"></a><span class="in"># ...</span></span>
<span id="cb4-89"><a href="#cb4-89"></a><span class="in">rewards[[actual_start, actual_end]] += scores</span></span>
<span id="cb4-90"><a href="#cb4-90"></a><span class="in">```</span></span>
<span id="cb4-91"><a href="#cb4-91"></a></span>
<span id="cb4-92"><a href="#cb4-92"></a>这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 @sec-why-kl-reward。</span>
<span id="cb4-93"><a href="#cb4-93"></a></span>
<span id="cb4-94"><a href="#cb4-94"></a><span class="fu">## OpenRLHF</span></span>
<span id="cb4-95"><a href="#cb4-95"></a></span>
<span id="cb4-96"><a href="#cb4-96"></a><span class="fu">### KL reward 项 {#sec-openrlhf-kl-reward}</span></span>
<span id="cb4-97"><a href="#cb4-97"></a></span>
<span id="cb4-98"><a href="#cb4-98"></a>与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-estimator</span><span class="co">]</span>。</span>
<span id="cb4-99"><a href="#cb4-99"></a></span>
<span id="cb4-100"><a href="#cb4-100"></a><span class="in">```{#lst-openrlhf-calc-kl-estimator .python lst-cap="OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 ^[https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88]"}</span></span>
<span id="cb4-101"><a href="#cb4-101"></a><span class="in">def compute_approx_kl(</span></span>
<span id="cb4-102"><a href="#cb4-102"></a><span class="in">    log_probs: torch.Tensor,</span></span>
<span id="cb4-103"><a href="#cb4-103"></a><span class="in">    log_probs_base: torch.Tensor,</span></span>
<span id="cb4-104"><a href="#cb4-104"></a><span class="in">    action_mask: Optional[torch.Tensor] = None,</span></span>
<span id="cb4-105"><a href="#cb4-105"></a><span class="in">    kl_estimator: str = "k1",</span></span>
<span id="cb4-106"><a href="#cb4-106"></a><span class="in">) -&gt; torch.Tensor:</span></span>
<span id="cb4-107"><a href="#cb4-107"></a><span class="in">    """</span></span>
<span id="cb4-108"><a href="#cb4-108"></a><span class="in">    Compute the approximate KL divergence between two distributions.</span></span>
<span id="cb4-109"><a href="#cb4-109"></a><span class="in">    Schulman blog: http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb4-110"><a href="#cb4-110"></a></span>
<span id="cb4-111"><a href="#cb4-111"></a><span class="in">    Args:</span></span>
<span id="cb4-112"><a href="#cb4-112"></a><span class="in">        log_probs: Log probabilities of the new distribution.</span></span>
<span id="cb4-113"><a href="#cb4-113"></a><span class="in">        log_probs_base: Log probabilities of the base distribution.</span></span>
<span id="cb4-114"><a href="#cb4-114"></a><span class="in">        action_mask: Mask for actions.</span></span>
<span id="cb4-115"><a href="#cb4-115"></a><span class="in">    """</span></span>
<span id="cb4-116"><a href="#cb4-116"></a></span>
<span id="cb4-117"><a href="#cb4-117"></a><span class="in">    if kl_estimator == "k1":</span></span>
<span id="cb4-118"><a href="#cb4-118"></a><span class="in">        log_ratio = log_probs.float() - log_probs_base.float()</span></span>
<span id="cb4-119"><a href="#cb4-119"></a><span class="in">        if action_mask is not None:</span></span>
<span id="cb4-120"><a href="#cb4-120"></a><span class="in">            log_ratio = log_ratio * action_mask</span></span>
<span id="cb4-121"><a href="#cb4-121"></a></span>
<span id="cb4-122"><a href="#cb4-122"></a><span class="in">    # The $k_2$ estimator is the non negative kl approximation in</span></span>
<span id="cb4-123"><a href="#cb4-123"></a><span class="in">    # http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb4-124"><a href="#cb4-124"></a><span class="in">    # The k2_loss is approximately equivalent to the</span></span>
<span id="cb4-125"><a href="#cb4-125"></a><span class="in">    # one-step KL divergence penalty with the $k_1$ estimator</span></span>
<span id="cb4-126"><a href="#cb4-126"></a><span class="in">    # used in https://arxiv.org/abs/2310.10505.</span></span>
<span id="cb4-127"><a href="#cb4-127"></a><span class="in">    if kl_estimator == "k2":</span></span>
<span id="cb4-128"><a href="#cb4-128"></a><span class="in">        log_ratio = log_probs.float() - log_probs_base.float()</span></span>
<span id="cb4-129"><a href="#cb4-129"></a><span class="in">        if action_mask is not None:</span></span>
<span id="cb4-130"><a href="#cb4-130"></a><span class="in">            log_ratio = log_ratio * action_mask</span></span>
<span id="cb4-131"><a href="#cb4-131"></a><span class="in">        log_ratio = log_ratio**2 / 2.0</span></span>
<span id="cb4-132"><a href="#cb4-132"></a></span>
<span id="cb4-133"><a href="#cb4-133"></a><span class="in">    # The $k_3$ estimator is the non negative kl approximation in</span></span>
<span id="cb4-134"><a href="#cb4-134"></a><span class="in">    # http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb4-135"><a href="#cb4-135"></a><span class="in">    if kl_estimator == "k3":</span></span>
<span id="cb4-136"><a href="#cb4-136"></a><span class="in">        log_ratio = log_probs.float() - log_probs_base.float()</span></span>
<span id="cb4-137"><a href="#cb4-137"></a><span class="in">        if action_mask is not None:</span></span>
<span id="cb4-138"><a href="#cb4-138"></a><span class="in">            log_ratio = log_ratio * action_mask</span></span>
<span id="cb4-139"><a href="#cb4-139"></a><span class="in">        log_ratio = -log_ratio</span></span>
<span id="cb4-140"><a href="#cb4-140"></a><span class="in">        log_ratio = log_ratio.exp() - 1 - log_ratio</span></span>
<span id="cb4-141"><a href="#cb4-141"></a></span>
<span id="cb4-142"><a href="#cb4-142"></a><span class="in">    return log_ratio</span></span>
<span id="cb4-143"><a href="#cb4-143"></a></span>
<span id="cb4-144"><a href="#cb4-144"></a></span>
<span id="cb4-145"><a href="#cb4-145"></a><span class="in">def compute_reward(</span></span>
<span id="cb4-146"><a href="#cb4-146"></a><span class="in">    # ...</span></span>
<span id="cb4-147"><a href="#cb4-147"></a><span class="in">    kl_coef: float,</span></span>
<span id="cb4-148"><a href="#cb4-148"></a><span class="in">    kl: Union[torch.Tensor, list[torch.Tensor]],</span></span>
<span id="cb4-149"><a href="#cb4-149"></a><span class="in">    # ...</span></span>
<span id="cb4-150"><a href="#cb4-150"></a><span class="in">    num_actions: Optional[Union[int, list[int]]] = None,</span></span>
<span id="cb4-151"><a href="#cb4-151"></a><span class="in">    # ...</span></span>
<span id="cb4-152"><a href="#cb4-152"></a><span class="in">) -&gt; Union[torch.Tensor, list[torch.Tensor]]:</span></span>
<span id="cb4-153"><a href="#cb4-153"></a><span class="in">    # ...</span></span>
<span id="cb4-154"><a href="#cb4-154"></a><span class="in">    if action_mask is not None:</span></span>
<span id="cb4-155"><a href="#cb4-155"></a><span class="in">        # ...</span></span>
<span id="cb4-156"><a href="#cb4-156"></a><span class="in">    else:</span></span>
<span id="cb4-157"><a href="#cb4-157"></a><span class="in">        # ...</span></span>
<span id="cb4-158"><a href="#cb4-158"></a><span class="in">        reward = []</span></span>
<span id="cb4-159"><a href="#cb4-159"></a><span class="in">        for i, (kl_seg, action_len) in enumerate(zip(kl, num_actions)):</span></span>
<span id="cb4-160"><a href="#cb4-160"></a><span class="in">            kl_reward = -kl_coef * kl_seg</span></span>
<span id="cb4-161"><a href="#cb4-161"></a><span class="in">            kl_reward[action_len - 1] += r[i]</span></span>
<span id="cb4-162"><a href="#cb4-162"></a><span class="in">            reward.append(kl_reward)</span></span>
<span id="cb4-163"><a href="#cb4-163"></a></span>
<span id="cb4-164"><a href="#cb4-164"></a><span class="in">    return reward</span></span>
<span id="cb4-165"><a href="#cb4-165"></a><span class="in">```</span></span>
<span id="cb4-166"><a href="#cb4-166"></a></span>
<span id="cb4-167"><a href="#cb4-167"></a><span class="fu">### KL loss 项</span></span>
<span id="cb4-168"><a href="#cb4-168"></a></span>
<span id="cb4-169"><a href="#cb4-169"></a>此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-loss</span><span class="co">]</span>。</span>
<span id="cb4-170"><a href="#cb4-170"></a></span>
<span id="cb4-171"><a href="#cb4-171"></a><span class="in">```{#lst-openrlhf-calc-kl-loss .python lst-cap="OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 ^[https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470]"}</span></span>
<span id="cb4-172"><a href="#cb4-172"></a><span class="in">def training_step_actor(self, experience: Experience) -&gt; Dict[str, float]:</span></span>
<span id="cb4-173"><a href="#cb4-173"></a><span class="in">    self.actor.train()</span></span>
<span id="cb4-174"><a href="#cb4-174"></a><span class="in">    # ...</span></span>
<span id="cb4-175"><a href="#cb4-175"></a><span class="in">    if isinstance(experience.sequences, list):</span></span>
<span id="cb4-176"><a href="#cb4-176"></a><span class="in">        # ...</span></span>
<span id="cb4-177"><a href="#cb4-177"></a><span class="in">    else:</span></span>
<span id="cb4-178"><a href="#cb4-178"></a><span class="in">        sequences = experience.sequences</span></span>
<span id="cb4-179"><a href="#cb4-179"></a><span class="in">        old_action_log_probs = experience.action_log_probs</span></span>
<span id="cb4-180"><a href="#cb4-180"></a><span class="in">        advantages = experience.advantages</span></span>
<span id="cb4-181"><a href="#cb4-181"></a><span class="in">        num_actions = experience.action_mask.size(1)</span></span>
<span id="cb4-182"><a href="#cb4-182"></a><span class="in">        packed_seq_lens = None</span></span>
<span id="cb4-183"><a href="#cb4-183"></a><span class="in">        attention_mask = experience.attention_mask</span></span>
<span id="cb4-184"><a href="#cb4-184"></a><span class="in">        if self.args.use_kl_loss and experience.base_action_log_probs is not None:</span></span>
<span id="cb4-185"><a href="#cb4-185"></a><span class="in">            base_action_log_probs = experience.base_action_log_probs</span></span>
<span id="cb4-186"><a href="#cb4-186"></a></span>
<span id="cb4-187"><a href="#cb4-187"></a><span class="in">    # actor loss</span></span>
<span id="cb4-188"><a href="#cb4-188"></a><span class="in">    action_log_probs, output = self.actor(</span></span>
<span id="cb4-189"><a href="#cb4-189"></a><span class="in">        sequences,</span></span>
<span id="cb4-190"><a href="#cb4-190"></a><span class="in">        num_actions,</span></span>
<span id="cb4-191"><a href="#cb4-191"></a><span class="in">        # ...</span></span>
<span id="cb4-192"><a href="#cb4-192"></a><span class="in">    )</span></span>
<span id="cb4-193"><a href="#cb4-193"></a><span class="in">    # ...</span></span>
<span id="cb4-194"><a href="#cb4-194"></a><span class="in">    # loss function</span></span>
<span id="cb4-195"><a href="#cb4-195"></a><span class="in">    actor_loss = self.actor_loss_fn(</span></span>
<span id="cb4-196"><a href="#cb4-196"></a><span class="in">        action_log_probs,</span></span>
<span id="cb4-197"><a href="#cb4-197"></a><span class="in">        old_action_log_probs,</span></span>
<span id="cb4-198"><a href="#cb4-198"></a><span class="in">        advantages,</span></span>
<span id="cb4-199"><a href="#cb4-199"></a><span class="in">        # ...</span></span>
<span id="cb4-200"><a href="#cb4-200"></a><span class="in">    )</span></span>
<span id="cb4-201"><a href="#cb4-201"></a></span>
<span id="cb4-202"><a href="#cb4-202"></a><span class="in">    if self.args.use_kl_loss:</span></span>
<span id="cb4-203"><a href="#cb4-203"></a><span class="in">        if self.initial_model is not None:</span></span>
<span id="cb4-204"><a href="#cb4-204"></a><span class="in">            kl = compute_approx_kl(</span></span>
<span id="cb4-205"><a href="#cb4-205"></a><span class="in">                action_log_probs,</span></span>
<span id="cb4-206"><a href="#cb4-206"></a><span class="in">                base_action_log_probs,</span></span>
<span id="cb4-207"><a href="#cb4-207"></a><span class="in">                # ...</span></span>
<span id="cb4-208"><a href="#cb4-208"></a><span class="in">                kl_estimator=self.args.kl_estimator,</span></span>
<span id="cb4-209"><a href="#cb4-209"></a><span class="in">            )</span></span>
<span id="cb4-210"><a href="#cb4-210"></a><span class="in">        else:</span></span>
<span id="cb4-211"><a href="#cb4-211"></a><span class="in">            kl = torch.zeros_like(action_log_probs, dtype=action_log_probs.dtype, device=action_log_probs.device)</span></span>
<span id="cb4-212"><a href="#cb4-212"></a></span>
<span id="cb4-213"><a href="#cb4-213"></a><span class="in">        if not self.args.packing_samples:</span></span>
<span id="cb4-214"><a href="#cb4-214"></a><span class="in">            kl_mean = masked_mean(kl, experience.action_mask, dim=-1)</span></span>
<span id="cb4-215"><a href="#cb4-215"></a><span class="in">        else:</span></span>
<span id="cb4-216"><a href="#cb4-216"></a><span class="in">            # ...</span></span>
<span id="cb4-217"><a href="#cb4-217"></a></span>
<span id="cb4-218"><a href="#cb4-218"></a><span class="in">        kl_loss = kl_mean.mean()</span></span>
<span id="cb4-219"><a href="#cb4-219"></a><span class="in">        experience.info["kl"] = kl_loss.item()</span></span>
<span id="cb4-220"><a href="#cb4-220"></a><span class="in">    else:</span></span>
<span id="cb4-221"><a href="#cb4-221"></a><span class="in">        kl_loss = 0</span></span>
<span id="cb4-222"><a href="#cb4-222"></a><span class="in">    # ...</span></span>
<span id="cb4-223"><a href="#cb4-223"></a><span class="in">    self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name="actor")</span></span>
<span id="cb4-224"><a href="#cb4-224"></a><span class="in">    # ...</span></span>
<span id="cb4-225"><a href="#cb4-225"></a><span class="in">```</span></span>
<span id="cb4-226"><a href="#cb4-226"></a></span>
<span id="cb4-227"><a href="#cb4-227"></a><span class="fu">## verl</span></span>
<span id="cb4-228"><a href="#cb4-228"></a></span>
<span id="cb4-229"><a href="#cb4-229"></a><span class="fu">### KL reward 项</span></span>
<span id="cb4-230"><a href="#cb4-230"></a></span>
<span id="cb4-231"><a href="#cb4-231"></a>verl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 <span class="co">[</span><span class="ot">@lst-verl-kl-reward</span><span class="co">]</span>。</span>
<span id="cb4-232"><a href="#cb4-232"></a></span>
<span id="cb4-233"><a href="#cb4-233"></a><span class="in">```{#lst-verl-kl-reward .python lst-cap="verl 将 KL 估计样本值从 reward 中减去 ^[https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160]"}</span></span>
<span id="cb4-234"><a href="#cb4-234"></a><span class="in">def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):</span></span>
<span id="cb4-235"><a href="#cb4-235"></a><span class="in">    # ...</span></span>
<span id="cb4-236"><a href="#cb4-236"></a><span class="in">    # compute kl between ref_policy and current policy</span></span>
<span id="cb4-237"><a href="#cb4-237"></a><span class="in">    if 'ref_log_prob' in data.batch.keys():</span></span>
<span id="cb4-238"><a href="#cb4-238"></a><span class="in">        kld = core_algos.kl_penalty(data.batch['old_log_probs'], data.batch['ref_log_prob'],</span></span>
<span id="cb4-239"><a href="#cb4-239"></a><span class="in">                                    kl_penalty=kl_penalty)  # (batch_size, response_length)</span></span>
<span id="cb4-240"><a href="#cb4-240"></a><span class="in">        kld = kld * response_mask</span></span>
<span id="cb4-241"><a href="#cb4-241"></a><span class="in">        beta = kl_ctrl.value</span></span>
<span id="cb4-242"><a href="#cb4-242"></a><span class="in">    else:</span></span>
<span id="cb4-243"><a href="#cb4-243"></a><span class="in">        beta = 0</span></span>
<span id="cb4-244"><a href="#cb4-244"></a><span class="in">        kld = torch.zeros_like(response_mask, dtype=torch.float32)</span></span>
<span id="cb4-245"><a href="#cb4-245"></a></span>
<span id="cb4-246"><a href="#cb4-246"></a><span class="in">    token_level_rewards = token_level_scores - beta * kld</span></span>
<span id="cb4-247"><a href="#cb4-247"></a><span class="in">    # ...</span></span>
<span id="cb4-248"><a href="#cb4-248"></a><span class="in">```</span></span>
<span id="cb4-249"><a href="#cb4-249"></a></span>
<span id="cb4-250"><a href="#cb4-250"></a><span class="fu">### KL loss 项</span></span>
<span id="cb4-251"><a href="#cb4-251"></a></span>
<span id="cb4-252"><a href="#cb4-252"></a>verl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 <span class="co">[</span><span class="ot">@lst-verl-kl-loss</span><span class="co">]</span>。</span>
<span id="cb4-253"><a href="#cb4-253"></a></span>
<span id="cb4-254"><a href="#cb4-254"></a><span class="in">```{#lst-verl-kl-loss .python lst-cap="verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 ^[https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327]"}</span></span>
<span id="cb4-255"><a href="#cb4-255"></a><span class="in">def update_policy(self, data: DataProto):</span></span>
<span id="cb4-256"><a href="#cb4-256"></a><span class="in">    # make sure we are in training mode</span></span>
<span id="cb4-257"><a href="#cb4-257"></a><span class="in">    self.actor_module.train()</span></span>
<span id="cb4-258"><a href="#cb4-258"></a><span class="in">    # ...</span></span>
<span id="cb4-259"><a href="#cb4-259"></a><span class="in">    for epoch in range(self.config.ppo_epochs):</span></span>
<span id="cb4-260"><a href="#cb4-260"></a><span class="in">        for batch_idx, data in enumerate(dataloader):</span></span>
<span id="cb4-261"><a href="#cb4-261"></a><span class="in">            # ...</span></span>
<span id="cb4-262"><a href="#cb4-262"></a><span class="in">            self.actor_optimizer.zero_grad()</span></span>
<span id="cb4-263"><a href="#cb4-263"></a></span>
<span id="cb4-264"><a href="#cb4-264"></a><span class="in">            for data in micro_batches:</span></span>
<span id="cb4-265"><a href="#cb4-265"></a><span class="in">                # ...</span></span>
<span id="cb4-266"><a href="#cb4-266"></a><span class="in">                responses = data['responses']</span></span>
<span id="cb4-267"><a href="#cb4-267"></a><span class="in">                # ...</span></span>
<span id="cb4-268"><a href="#cb4-268"></a><span class="in">                old_log_prob = data['old_log_probs']</span></span>
<span id="cb4-269"><a href="#cb4-269"></a><span class="in">                # ...</span></span>
<span id="cb4-270"><a href="#cb4-270"></a></span>
<span id="cb4-271"><a href="#cb4-271"></a><span class="in">                # all return: (bsz, response_length)</span></span>
<span id="cb4-272"><a href="#cb4-272"></a><span class="in">                entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature)</span></span>
<span id="cb4-273"><a href="#cb4-273"></a></span>
<span id="cb4-274"><a href="#cb4-274"></a><span class="in">                pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=old_log_prob,</span></span>
<span id="cb4-275"><a href="#cb4-275"></a><span class="in">                                                                                log_prob=log_prob,</span></span>
<span id="cb4-276"><a href="#cb4-276"></a><span class="in">                                                                                # ...</span></span>
<span id="cb4-277"><a href="#cb4-277"></a><span class="in">                                                                                )</span></span>
<span id="cb4-278"><a href="#cb4-278"></a><span class="in">                # ...</span></span>
<span id="cb4-279"><a href="#cb4-279"></a></span>
<span id="cb4-280"><a href="#cb4-280"></a><span class="in">                # compute policy loss</span></span>
<span id="cb4-281"><a href="#cb4-281"></a><span class="in">                policy_loss = pg_loss - entropy_loss * entropy_coeff</span></span>
<span id="cb4-282"><a href="#cb4-282"></a></span>
<span id="cb4-283"><a href="#cb4-283"></a><span class="in">                if self.config.use_kl_loss:</span></span>
<span id="cb4-284"><a href="#cb4-284"></a><span class="in">                    ref_log_prob = data['ref_log_prob']</span></span>
<span id="cb4-285"><a href="#cb4-285"></a><span class="in">                    # compute kl loss</span></span>
<span id="cb4-286"><a href="#cb4-286"></a><span class="in">                    kld = core_algos.kl_penalty(logprob=log_prob,</span></span>
<span id="cb4-287"><a href="#cb4-287"></a><span class="in">                                                ref_logprob=ref_log_prob,</span></span>
<span id="cb4-288"><a href="#cb4-288"></a><span class="in">                                                kl_penalty=self.config.kl_loss_type)</span></span>
<span id="cb4-289"><a href="#cb4-289"></a><span class="in">                    kl_loss = masked_mean(kld, response_mask)</span></span>
<span id="cb4-290"><a href="#cb4-290"></a></span>
<span id="cb4-291"><a href="#cb4-291"></a><span class="in">                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef</span></span>
<span id="cb4-292"><a href="#cb4-292"></a><span class="in">                # ...</span></span>
<span id="cb4-293"><a href="#cb4-293"></a><span class="in">                loss.backward()</span></span>
<span id="cb4-294"><a href="#cb4-294"></a><span class="in">            # ...</span></span>
<span id="cb4-295"><a href="#cb4-295"></a><span class="in">            grad_norm = self._optimizer_step()</span></span>
<span id="cb4-296"><a href="#cb4-296"></a><span class="in">    # ...</span></span>
<span id="cb4-297"><a href="#cb4-297"></a><span class="in">    self.actor_optimizer.zero_grad()</span></span>
<span id="cb4-298"><a href="#cb4-298"></a><span class="in">    # ...</span></span>
<span id="cb4-299"><a href="#cb4-299"></a><span class="in">```</span></span>
<span id="cb4-300"><a href="#cb4-300"></a></span>
<span id="cb4-301"><a href="#cb4-301"></a></span>
<span id="cb4-302"><a href="#cb4-302"></a><span class="fu">## 为什么要将 KL 从 reward 中减去 {#sec-why-kl-reward}</span></span>
<span id="cb4-303"><a href="#cb4-303"></a></span>
<span id="cb4-304"><a href="#cb4-304"></a>将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT <span class="co">[</span><span class="ot">@ouyang2022instructgpt</span><span class="co">]</span>。</span>
<span id="cb4-305"><a href="#cb4-305"></a></span>
<span id="cb4-306"><a href="#cb4-306"></a><span class="fu">### KL reward 的流行应当源自 RLHF 与 InstructGPT</span></span>
<span id="cb4-307"><a href="#cb4-307"></a></span>
<span id="cb4-308"><a href="#cb4-308"></a>InstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。</span>
<span id="cb4-309"><a href="#cb4-309"></a></span>
<span id="cb4-310"><a href="#cb4-310"></a><span class="at">&gt; ... In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models "PPO."</span></span>
<span id="cb4-311"><a href="#cb4-311"></a><span class="at">&gt;</span></span>
<span id="cb4-312"><a href="#cb4-312"></a><span class="at">&gt; ...</span></span>
<span id="cb4-313"><a href="#cb4-313"></a></span>
<span id="cb4-314"><a href="#cb4-314"></a>$$</span>
<span id="cb4-315"><a href="#cb4-315"></a>\begin{aligned}</span>
<span id="cb4-316"><a href="#cb4-316"></a>\text { objective }(\phi)= &amp; E_{(x, y) \sim D_\pi^{\mathrm{RL}}}\left<span class="co">[</span><span class="ot">r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right</span><span class="co">]</span>+ <span class="sc">\\</span></span>
<span id="cb4-317"><a href="#cb4-317"></a>&amp; \gamma E_{x \sim D_{\text {remin }}}\left<span class="co">[</span><span class="ot">\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right</span><span class="co">]</span></span>
<span id="cb4-318"><a href="#cb4-318"></a>\end{aligned}</span>
<span id="cb4-319"><a href="#cb4-319"></a>$$</span>
<span id="cb4-320"><a href="#cb4-320"></a></span>
<span id="cb4-321"><a href="#cb4-321"></a><span class="at">&gt; where $\pi_\phi^{\mathrm{RL}}$is the learned RL policy,$\pi^{\mathrm{SFT}}$ is the supervised trained model, and$D_{\text {pretrain }}$is the pretraining distribution. The KL reward coefficient, $\beta$, and the pretraining loss coefficient, $\gamma$, control the strength of the KL penalty and pretraining gradients respectively. For "PPO" models, $\gamma$ is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</span></span>
<span id="cb4-322"><a href="#cb4-322"></a></span>
<span id="cb4-323"><a href="#cb4-323"></a><span class="fu">### OpenAI 论文中 KL reward 的出处 {#sec-oai-kl-reward-src}</span></span>
<span id="cb4-324"><a href="#cb4-324"></a></span>
<span id="cb4-325"><a href="#cb4-325"></a>然而，在OpenAI 早期的一篇论文 "Learning to summarize from human feedback" <span class="co">[</span><span class="ot">@stiennon2020summarize</span><span class="co">]</span> 中，他们就已经采用了 KL reward，并提及了出处：</span>
<span id="cb4-326"><a href="#cb4-326"></a></span>
<span id="cb4-327"><a href="#cb4-327"></a><span class="at">&gt; ... **Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy $\pi_\phi^{\mathrm{RL}}$ with parameters $\phi$ and this original supervised model $\pi^{\mathrm{SFT}}$, as previously done in [25].** The full reward $R$ can be written as:</span></span>
<span id="cb4-328"><a href="#cb4-328"></a></span>
<span id="cb4-329"><a href="#cb4-329"></a>$$</span>
<span id="cb4-330"><a href="#cb4-330"></a>R(x, y)=r_\theta(x, y)-\beta \log \left<span class="co">[</span><span class="ot">\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right</span><span class="co">]</span></span>
<span id="cb4-331"><a href="#cb4-331"></a>$$</span>
<span id="cb4-332"><a href="#cb4-332"></a></span>
<span id="cb4-333"><a href="#cb4-333"></a><span class="at">&gt; This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn't learn to produce outputs that are too different from those that the reward model has seen during training.</span></span>
<span id="cb4-334"><a href="#cb4-334"></a></span>
<span id="cb4-335"><a href="#cb4-335"></a><span class="fu">### KL reward 最早的出处</span></span>
<span id="cb4-336"><a href="#cb4-336"></a></span>
<span id="cb4-337"><a href="#cb4-337"></a><span class="co">[</span><span class="ot">@sec-oai-kl-reward-src</span><span class="co">]</span> 中 OpenAI 引用的 KL reward 出处 <span class="co">[</span><span class="ot">25</span><span class="co">]</span> 是 "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog" <span class="co">[</span><span class="ot">@jaques2019wayoffpolicy</span><span class="co">]</span>。</span>
<span id="cb4-338"><a href="#cb4-338"></a></span>
<span id="cb4-339"><a href="#cb4-339"></a>实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：</span>
<span id="cb4-340"><a href="#cb4-340"></a></span>
<span id="cb4-341"><a href="#cb4-341"></a><span class="at">&gt; Rather than simply sample from the prior, we would like the $Q$-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior $p(y \mid x)$, and the $Q$-network policy $\pi_\theta$, while still maximizing reward. Given a trajectory of actions, $\tau=\left\{a_1, a_2, \ldots a_{t-1}\right\}$, let $q(\tau)=\prod_{t=1}^T \pi_\theta\left(a_t, s_t\right)$be the policy of our$Q$-learning algorithm at the trajectory level. Similarly, let $p(\tau)=\prod_{t=1}^T p\left(a_t \mid s_t\right)$be the prior distribution over the trajectory, and$r(\tau)$ be the rewards. We seek to maximize the following KL-regularized objective:</span></span>
<span id="cb4-342"><a href="#cb4-342"></a></span>
<span id="cb4-343"><a href="#cb4-343"></a>$$</span>
<span id="cb4-344"><a href="#cb4-344"></a>L(q)=\mathbb{E}_{q(\tau)}[r(\tau)] / c-D_{\text{KL}}<span class="co">[</span><span class="ot">q(\tau) \mid p(\tau)</span><span class="co">]</span></span>
<span id="cb4-345"><a href="#cb4-345"></a>$$</span>
<span id="cb4-346"><a href="#cb4-346"></a></span>
<span id="cb4-347"><a href="#cb4-347"></a><span class="at">&gt; Since $D_{\text{KL}}[q \mid p]=\sum_x q(x)(\log q(x)-\log p(x))$, we can see that this is equivalent to maximizing the following expected value function of the policy $\pi_\theta$ at the action level:</span></span>
<span id="cb4-348"><a href="#cb4-348"></a></span>
<span id="cb4-349"><a href="#cb4-349"></a>$$</span>
<span id="cb4-350"><a href="#cb4-350"></a>Q^\pi\left(s_t, a_t\right)=\mathbb{E}_\pi\left[\sum^T r\left(s_{t^{\prime}}, a_{t^{\prime}}\right) / c+\log p\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)-\log \pi\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)\right]</span>
<span id="cb4-351"><a href="#cb4-351"></a>$$</span>
<span id="cb4-352"><a href="#cb4-352"></a></span>
<span id="cb4-353"><a href="#cb4-353"></a><span class="at">&gt; </span></span>
<span id="cb4-354"><a href="#cb4-354"></a></span>
<span id="cb4-355"><a href="#cb4-355"></a><span class="fu"># LLM RL 中 KL 优化的数学形式化 {#sec-rl-kl-optim-formulation}</span></span>
<span id="cb4-356"><a href="#cb4-356"></a></span>
<span id="cb4-357"><a href="#cb4-357"></a>为了进一步分析这些 LLM RL 框架中的实现是否正确，我们需要先形式化 LLM RL 中 KL 散度的优化。</span>
<span id="cb4-358"><a href="#cb4-358"></a></span>
<span id="cb4-359"><a href="#cb4-359"></a><span class="fu">## RL 中的 KL 散度通常定义在轨迹分布上</span></span>
<span id="cb4-360"><a href="#cb4-360"></a></span>
<span id="cb4-361"><a href="#cb4-361"></a>GRPO 公式 (@eq-grpo-obj) 中的 KL 项可以定义为：</span>
<span id="cb4-362"><a href="#cb4-362"></a></span>
<span id="cb4-363"><a href="#cb4-363"></a>$$</span>
<span id="cb4-364"><a href="#cb4-364"></a>\begin{aligned}</span>
<span id="cb4-365"><a href="#cb4-365"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right]</span>
<span id="cb4-366"><a href="#cb4-366"></a>\end{aligned}</span>
<span id="cb4-367"><a href="#cb4-367"></a>$$ {#eq-def-kl-theta-ref}</span>
<span id="cb4-368"><a href="#cb4-368"></a></span>
<span id="cb4-369"><a href="#cb4-369"></a>其中 $\mathbf{\tau}$ 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 $p_{\theta}$ 与参考策略整体分布 $p_{\text{ref}}$ 的 KL 散度。</span>
<span id="cb4-370"><a href="#cb4-370"></a></span>
<span id="cb4-371"><a href="#cb4-371"></a><span class="fu">## 将轨迹展开为状态-动作序列</span></span>
<span id="cb4-372"><a href="#cb4-372"></a></span>
<span id="cb4-373"><a href="#cb4-373"></a>RL 文献中通常会将轨迹 $\mathbf{\tau}$ 展开为状态-动作序列 $\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}$：^<span class="co">[</span><span class="ot">这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，$\mathbf{q}$ 对应于 $\mathbf{s}_1$，而 ${\mathbf{o}}$ 对应于 $\mathbf{\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T}$。</span><span class="co">]</span></span>
<span id="cb4-374"><a href="#cb4-374"></a></span>
<span id="cb4-375"><a href="#cb4-375"></a>$$</span>
<span id="cb4-376"><a href="#cb4-376"></a>\begin{aligned}</span>
<span id="cb4-377"><a href="#cb4-377"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right] <span class="sc">\\</span></span>
<span id="cb4-378"><a href="#cb4-378"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|},\right) \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|},, \mathbf{a}_{|\mathbf{\tau}|},\right)}{p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right)}\right] <span class="sc">\\</span></span>
<span id="cb4-379"><a href="#cb4-379"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\log \frac{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}\right] <span class="sc">\\</span></span>
<span id="cb4-380"><a href="#cb4-380"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb4-381"><a href="#cb4-381"></a>\end{aligned}</span>
<span id="cb4-382"><a href="#cb4-382"></a>$$ {#eq-def-kl-theta-ref-state-action-ag}</span>
<span id="cb4-383"><a href="#cb4-383"></a></span>
<span id="cb4-384"><a href="#cb4-384"></a>其中 $|\mathbf{\tau}|$ 为轨迹动作数的随机变量。</span>
<span id="cb4-385"><a href="#cb4-385"></a></span>
<span id="cb4-386"><a href="#cb4-386"></a>此处利用了联合概率的展开，以 $p_{\theta}$ 为例：</span>
<span id="cb4-387"><a href="#cb4-387"></a></span>
<span id="cb4-388"><a href="#cb4-388"></a>$$</span>
<span id="cb4-389"><a href="#cb4-389"></a>p_{\theta}(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)</span>
<span id="cb4-390"><a href="#cb4-390"></a>$$ {#eq-dp-expansion}</span>
<span id="cb4-391"><a href="#cb4-391"></a></span>
<span id="cb4-392"><a href="#cb4-392"></a>注意区分整体概率分布 $p_{\theta}$、策略（条件）概率分布 $\pi_{\theta}$ 与状态转移概率分布 $p$。</span>
<span id="cb4-393"><a href="#cb4-393"></a></span>
<span id="cb4-394"><a href="#cb4-394"></a><span class="fu">## Markov 决策过程中的 KL 散度</span></span>
<span id="cb4-395"><a href="#cb4-395"></a></span>
<span id="cb4-396"><a href="#cb4-396"></a>实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP^<span class="co">[</span><span class="ot">https://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B</span><span class="co">]</span>。</span>
<span id="cb4-397"><a href="#cb4-397"></a></span>
<span id="cb4-398"><a href="#cb4-398"></a>Markov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 $n$ 个历史状态和动作，而非全部的历史信息，对应的过程称为 $n$ 阶 Markov 过程。以 $n=1$ 为例：</span>
<span id="cb4-399"><a href="#cb4-399"></a></span>
<span id="cb4-400"><a href="#cb4-400"></a>$$</span>
<span id="cb4-401"><a href="#cb4-401"></a>\begin{aligned}</span>
<span id="cb4-402"><a href="#cb4-402"></a>\pi(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) &amp; = \pi(\mathbf{a}_t \mid \mathbf{s}_t) <span class="sc">\\</span></span>
<span id="cb4-403"><a href="#cb4-403"></a>p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) &amp; = p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) <span class="sc">\\</span></span>
<span id="cb4-404"><a href="#cb4-404"></a>\end{aligned}</span>
<span id="cb4-405"><a href="#cb4-405"></a>$$ {#eq-def-markov-prop}</span>
<span id="cb4-406"><a href="#cb4-406"></a></span>
<span id="cb4-407"><a href="#cb4-407"></a>则 <span class="co">[</span><span class="ot">@eq-dp-expansion</span><span class="co">]</span> 中的联合概率可以进一步简化为：</span>
<span id="cb4-408"><a href="#cb4-408"></a></span>
<span id="cb4-409"><a href="#cb4-409"></a>$$</span>
<span id="cb4-410"><a href="#cb4-410"></a>p(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(s_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t)</span>
<span id="cb4-411"><a href="#cb4-411"></a>$$ {#eq-dp-expansion-markov-1}</span>
<span id="cb4-412"><a href="#cb4-412"></a></span>
<span id="cb4-413"><a href="#cb4-413"></a>如果考虑一阶 Markov 过程，则 <span class="co">[</span><span class="ot">@eq-def-kl-theta-ref-state-action-ag</span><span class="co">]</span> 中的 KL 可以进一步简化为：</span>
<span id="cb4-414"><a href="#cb4-414"></a></span>
<span id="cb4-415"><a href="#cb4-415"></a>$$</span>
<span id="cb4-416"><a href="#cb4-416"></a>\begin{aligned}</span>
<span id="cb4-417"><a href="#cb4-417"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] = &amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb4-418"><a href="#cb4-418"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb4-419"><a href="#cb4-419"></a>\end{aligned}</span>
<span id="cb4-420"><a href="#cb4-420"></a>$$ {#eq-def-kl-theta-ref-state-action-markov-1}</span>
<span id="cb4-421"><a href="#cb4-421"></a></span>
<span id="cb4-422"><a href="#cb4-422"></a><span class="fu">## 语言模型作为序列决策过程 {#sec-lm-as-dp}</span></span>
<span id="cb4-423"><a href="#cb4-423"></a></span>
<span id="cb4-424"><a href="#cb4-424"></a>目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。</span>
<span id="cb4-425"><a href="#cb4-425"></a></span>
<span id="cb4-426"><a href="#cb4-426"></a>尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 $s_1$ 表示 prompt 中的所有 token，对于 $t &gt;1$，如果令 $s_t$ 表示第 $t$ 个动作 token 前的所有 token，则自回归模型满足 Markov 性质，否则不一定。</span>
<span id="cb4-427"><a href="#cb4-427"></a></span>
<span id="cb4-428"><a href="#cb4-428"></a>接下来，我们先令 $s_t$ 表示前 $t$ 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。</span>
<span id="cb4-429"><a href="#cb4-429"></a></span>
<span id="cb4-430"><a href="#cb4-430"></a><span class="fu">## 估计 KL 散度</span></span>
<span id="cb4-431"><a href="#cb4-431"></a></span>
<span id="cb4-432"><a href="#cb4-432"></a><span class="fu">### 几乎不可能直接计算 KL 散度的真实值</span></span>
<span id="cb4-433"><a href="#cb4-433"></a></span>
<span id="cb4-434"><a href="#cb4-434"></a>实际实现中，我们几乎不可能直接计算出 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]$，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 $\left|\mathcal{T}\right|$ 与轨迹最大长度 $T = \max_{\mathbf{\tau} \in \mathcal{T}} |\mathbf{\tau}|$ 成指数关系：</span>
<span id="cb4-435"><a href="#cb4-435"></a>$$</span>
<span id="cb4-436"><a href="#cb4-436"></a>\begin{aligned}</span>
<span id="cb4-437"><a href="#cb4-437"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb4-438"><a href="#cb4-438"></a>&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) <span class="sc">\\</span></span>
<span id="cb4-439"><a href="#cb4-439"></a>\end{aligned}</span>
<span id="cb4-440"><a href="#cb4-440"></a>$$ {#eq-def-rl-kl-avg-over-traj}</span>
<span id="cb4-441"><a href="#cb4-441"></a></span>
<span id="cb4-442"><a href="#cb4-442"></a><span class="fu">### 通常使用 Monte Carlo 方法估计 KL 散度</span></span>
<span id="cb4-443"><a href="#cb4-443"></a></span>
<span id="cb4-444"><a href="#cb4-444"></a>所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法^<span class="co">[</span><span class="ot">https://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95</span><span class="co">]</span>来估计 RL 中的 KL 散度，例如：</span>
<span id="cb4-445"><a href="#cb4-445"></a></span>
<span id="cb4-446"><a href="#cb4-446"></a>$$</span>
<span id="cb4-447"><a href="#cb4-447"></a>\begin{aligned}</span>
<span id="cb4-448"><a href="#cb4-448"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) <span class="sc">\\</span></span>
<span id="cb4-449"><a href="#cb4-449"></a>&amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{|\mathbf{\tau_{i }}|} \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)</span>
<span id="cb4-450"><a href="#cb4-450"></a>\end{aligned}</span>
<span id="cb4-451"><a href="#cb4-451"></a>$$ {#eq-def-rl-kl-mc-k1}</span>
<span id="cb4-452"><a href="#cb4-452"></a></span>
<span id="cb4-453"><a href="#cb4-453"></a>其中，$\mathbf{\tau_{i}} = \left(\mathbf{s}_{i,1}, \mathbf{a}_{i,1}, \cdots, \mathbf{s}_{i,|\mathbf{\tau_{i}}|}, \mathbf{a}_{i,|\mathbf{\tau_{i}}|}\right) \sim p_{\theta}$，$N$ 为估计使用的轨迹样本数量。</span>
<span id="cb4-454"><a href="#cb4-454"></a></span>
<span id="cb4-455"><a href="#cb4-455"></a><span class="fu">### 不同的 KL 估计量</span></span>
<span id="cb4-456"><a href="#cb4-456"></a></span>
<span id="cb4-457"><a href="#cb4-457"></a>实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。</span>
<span id="cb4-458"><a href="#cb4-458"></a></span>
<span id="cb4-459"><a href="#cb4-459"></a>设 KL 估计量为 $k$，则对应的 KL 估计值为</span>
<span id="cb4-460"><a href="#cb4-460"></a></span>
<span id="cb4-461"><a href="#cb4-461"></a>$$</span>
<span id="cb4-462"><a href="#cb4-462"></a>\begin{aligned}</span>
<span id="cb4-463"><a href="#cb4-463"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} k(\tau_i)</span>
<span id="cb4-464"><a href="#cb4-464"></a>\end{aligned}</span>
<span id="cb4-465"><a href="#cb4-465"></a>$$ {#eq-def-rl-kl-mc-general}</span>
<span id="cb4-466"><a href="#cb4-466"></a></span>
<span id="cb4-467"><a href="#cb4-467"></a>例如 <span class="co">[</span><span class="ot">@sec-openrlhf-kl-reward</span><span class="co">]</span> 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 <span class="in">`k1`</span>, <span class="in">`k2`</span>, <span class="in">`k3`</span>，这应该是主要参考了 John Schulman 的博客 "Approximating KL Divergence"。</span>
<span id="cb4-468"><a href="#cb4-468"></a></span>
<span id="cb4-469"><a href="#cb4-469"></a>verl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度^<span class="co">[</span><span class="ot">这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。</span><span class="co">]</span>，但目前还没有实现。对应代码可见 <span class="co">[</span><span class="ot">@lst-verl-kl-estimator</span><span class="co">]</span>。</span>
<span id="cb4-470"><a href="#cb4-470"></a></span>
<span id="cb4-471"><a href="#cb4-471"></a><span class="in">```{#lst-verl-kl-estimator .python lst-cap="verl 的 KL 散度 Monte Carlo 估计样本值^[https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383]"}</span></span>
<span id="cb4-472"><a href="#cb4-472"></a><span class="in">def kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) -&gt; torch.FloatTensor:</span></span>
<span id="cb4-473"><a href="#cb4-473"></a><span class="in">    # ...</span></span>
<span id="cb4-474"><a href="#cb4-474"></a><span class="in">    if kl_penalty == "kl":</span></span>
<span id="cb4-475"><a href="#cb4-475"></a><span class="in">        return logprob - ref_logprob</span></span>
<span id="cb4-476"><a href="#cb4-476"></a></span>
<span id="cb4-477"><a href="#cb4-477"></a><span class="in">    if kl_penalty == "abs":</span></span>
<span id="cb4-478"><a href="#cb4-478"></a><span class="in">        return (logprob - ref_logprob).abs()</span></span>
<span id="cb4-479"><a href="#cb4-479"></a></span>
<span id="cb4-480"><a href="#cb4-480"></a><span class="in">    if kl_penalty == "mse":</span></span>
<span id="cb4-481"><a href="#cb4-481"></a><span class="in">        return 0.5 * (logprob - ref_logprob).square()</span></span>
<span id="cb4-482"><a href="#cb4-482"></a></span>
<span id="cb4-483"><a href="#cb4-483"></a><span class="in">    # J. Schulman. Approximating kl divergence, 2020.</span></span>
<span id="cb4-484"><a href="#cb4-484"></a><span class="in">    # # URL http://joschu.net/blog/kl-approx.html.</span></span>
<span id="cb4-485"><a href="#cb4-485"></a><span class="in">    if kl_penalty == 'low_var_kl':</span></span>
<span id="cb4-486"><a href="#cb4-486"></a><span class="in">        kl = ref_logprob - logprob</span></span>
<span id="cb4-487"><a href="#cb4-487"></a><span class="in">        ratio = torch.exp(kl)</span></span>
<span id="cb4-488"><a href="#cb4-488"></a><span class="in">        kld = (ratio - kl - 1).contiguous()</span></span>
<span id="cb4-489"><a href="#cb4-489"></a><span class="in">        return torch.clamp(kld, min=-10, max=10)</span></span>
<span id="cb4-490"><a href="#cb4-490"></a></span>
<span id="cb4-491"><a href="#cb4-491"></a><span class="in">    if kl_penalty == "full":</span></span>
<span id="cb4-492"><a href="#cb4-492"></a><span class="in">        # so, here logprob and ref_logprob should contain the logits for every token in vocabulary</span></span>
<span id="cb4-493"><a href="#cb4-493"></a><span class="in">        raise NotImplementedError</span></span>
<span id="cb4-494"><a href="#cb4-494"></a></span>
<span id="cb4-495"><a href="#cb4-495"></a><span class="in">    raise NotImplementedError</span></span>
<span id="cb4-496"><a href="#cb4-496"></a><span class="in">```</span></span>
<span id="cb4-497"><a href="#cb4-497"></a></span>
<span id="cb4-498"><a href="#cb4-498"></a>由于 $k_1$、$k_2$、$k_3$ 三种估计量最为流行，我们将以这三种估计量为例展开分析。</span>
<span id="cb4-499"><a href="#cb4-499"></a></span>
<span id="cb4-500"><a href="#cb4-500"></a>考虑 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} k_j(\tau_i)$，其中 $\tau_i \sim p_{\theta}$，令 $r = \frac{\pi_{\text{ref}}(\tau_i)}{\pi_{\theta}(\tau_i)}$，注意，此处 $r$ 并非 KL 定义中的样本量，而是其倒数，则：</span>
<span id="cb4-501"><a href="#cb4-501"></a></span>
<span id="cb4-502"><a href="#cb4-502"></a>$$</span>
<span id="cb4-503"><a href="#cb4-503"></a>\begin{aligned}</span>
<span id="cb4-504"><a href="#cb4-504"></a>k_{1} &amp; = - \log r <span class="sc">\\</span></span>
<span id="cb4-505"><a href="#cb4-505"></a>k_{2} &amp; = \frac{1}{2} (\log r)^2 <span class="sc">\\</span></span>
<span id="cb4-506"><a href="#cb4-506"></a>k_{3} &amp; = (r - 1) - \log r</span>
<span id="cb4-507"><a href="#cb4-507"></a>\end{aligned}</span>
<span id="cb4-508"><a href="#cb4-508"></a>$$ {#eq-def-kl-estimators}</span>
<span id="cb4-509"><a href="#cb4-509"></a></span>
<span id="cb4-510"><a href="#cb4-510"></a><span class="fu"># 流行 on-policy KL 优化实现的数学形式化</span></span>
<span id="cb4-511"><a href="#cb4-511"></a></span>
<span id="cb4-512"><a href="#cb4-512"></a>神经网络模型普遍使用梯度法优化，因此，我们主要关注这些 KL 优化实现导出的梯度。</span>
<span id="cb4-513"><a href="#cb4-513"></a></span>
<span id="cb4-514"><a href="#cb4-514"></a>而由于 reward 项优化的实现涉及到基线（Baseline）、折扣（Discounting）、GAE <span class="co">[</span><span class="ot">@schulman2018gae</span><span class="co">]</span> 等内容，较为复杂，我们可以先分析 KL loss 项实现。</span>
<span id="cb4-515"><a href="#cb4-515"></a></span>
<span id="cb4-516"><a href="#cb4-516"></a><span class="fu">## 分析流行的 “KL loss 项” 实现 {#sec-kl-loss-impl}</span></span>
<span id="cb4-517"><a href="#cb4-517"></a></span>
<span id="cb4-518"><a href="#cb4-518"></a>上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。</span>
<span id="cb4-519"><a href="#cb4-519"></a></span>
<span id="cb4-520"><a href="#cb4-520"></a>然而，如 <span class="co">[</span><span class="ot">@sec-grpo-kl-misunderstanding</span><span class="co">]</span> 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。</span>
<span id="cb4-521"><a href="#cb4-521"></a></span>
<span id="cb4-522"><a href="#cb4-522"></a><span class="fu">### 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</span></span>
<span id="cb4-523"><a href="#cb4-523"></a></span>
<span id="cb4-524"><a href="#cb4-524"></a>观察 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-loss</span><span class="co">]</span> 计算 “KL loss” 项的部分。</span>
<span id="cb4-525"><a href="#cb4-525"></a></span>
<span id="cb4-526"><a href="#cb4-526"></a><span class="in">```python</span></span>
<span id="cb4-527"><a href="#cb4-527"></a><span class="co"># ...</span></span>
<span id="cb4-528"><a href="#cb4-528"></a>kl <span class="op">=</span> compute_approx_kl(</span>
<span id="cb4-529"><a href="#cb4-529"></a>    action_log_probs,</span>
<span id="cb4-530"><a href="#cb4-530"></a>    base_action_log_probs,</span>
<span id="cb4-531"><a href="#cb4-531"></a>    <span class="co"># ...</span></span>
<span id="cb4-532"><a href="#cb4-532"></a>    kl_estimator<span class="op">=</span><span class="va">self</span>.args.kl_estimator,</span>
<span id="cb4-533"><a href="#cb4-533"></a>)</span>
<span id="cb4-534"><a href="#cb4-534"></a><span class="co"># ...</span></span>
<span id="cb4-535"><a href="#cb4-535"></a>kl_mean <span class="op">=</span> masked_mean(kl, experience.action_mask, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-536"><a href="#cb4-536"></a><span class="co"># ...</span></span>
<span id="cb4-537"><a href="#cb4-537"></a>kl_loss <span class="op">=</span> kl_mean.mean()</span>
<span id="cb4-538"><a href="#cb4-538"></a><span class="co"># ...</span></span>
<span id="cb4-539"><a href="#cb4-539"></a><span class="in">```</span></span>
<span id="cb4-540"><a href="#cb4-540"></a></span>
<span id="cb4-541"><a href="#cb4-541"></a>这些代码：</span>
<span id="cb4-542"><a href="#cb4-542"></a></span>
<span id="cb4-543"><a href="#cb4-543"></a><span class="ss">1. </span>计算了 <span class="in">`kl`</span>，对应对每个动作 token $a_{i,t}$ 计算 “KL 估计量” $k$。</span>
<span id="cb4-544"><a href="#cb4-544"></a><span class="ss">2. </span>计算了 <span class="in">`kl_mean`</span>，对应对每个轨迹 $\tau_i$ 计算均值 $\frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k$。</span>
<span id="cb4-545"><a href="#cb4-545"></a><span class="ss">3. </span>计算了 <span class="in">`kl_loss`</span>，对应对所有轨迹样本计算均值 $\frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k$。</span>
<span id="cb4-546"><a href="#cb4-546"></a></span>
<span id="cb4-547"><a href="#cb4-547"></a>由于其没有去除任何梯度，因此其导出的梯度估计值为</span>
<span id="cb4-548"><a href="#cb4-548"></a></span>
<span id="cb4-549"><a href="#cb4-549"></a>$$</span>
<span id="cb4-550"><a href="#cb4-550"></a>\begin{aligned}</span>
<span id="cb4-551"><a href="#cb4-551"></a>\nabla_{\theta} \left( \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \frac{1}{|\tau_i|} k \right) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k</span>
<span id="cb4-552"><a href="#cb4-552"></a>\end{aligned}</span>
<span id="cb4-553"><a href="#cb4-553"></a>$$ {#eq-def-kl-loss-grad-estim-openrlhf}</span>
<span id="cb4-554"><a href="#cb4-554"></a></span>
<span id="cb4-555"><a href="#cb4-555"></a><span class="co">[</span><span class="ot">@lst-verl-kl-loss</span><span class="co">]</span> 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：</span>
<span id="cb4-556"><a href="#cb4-556"></a></span>
<span id="cb4-557"><a href="#cb4-557"></a>$$</span>
<span id="cb4-558"><a href="#cb4-558"></a>\begin{aligned}</span>
<span id="cb4-559"><a href="#cb4-559"></a>\nabla_{\theta} \left( \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} k \right) = \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} \nabla_{\theta} k</span>
<span id="cb4-560"><a href="#cb4-560"></a>\end{aligned}</span>
<span id="cb4-561"><a href="#cb4-561"></a>$$ {#eq-def-kl-loss-grad-estim-verl}</span>
<span id="cb4-562"><a href="#cb4-562"></a></span>
<span id="cb4-563"><a href="#cb4-563"></a>我们将平均操作一般化为权重 $w_{\mathbf{\tau}}$ 与 $w_{t}$，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：</span>
<span id="cb4-564"><a href="#cb4-564"></a></span>
<span id="cb4-565"><a href="#cb4-565"></a>$$</span>
<span id="cb4-566"><a href="#cb4-566"></a>\begin{aligned}</span>
<span id="cb4-567"><a href="#cb4-567"></a>\sum_{i=1}^{N} w_{\mathbf{\tau}_i} \sum_{t=1}^{|\tau_i|} w_{t} \nabla_{\theta} k <span class="sc">\\</span></span>
<span id="cb4-568"><a href="#cb4-568"></a>\end{aligned}</span>
<span id="cb4-569"><a href="#cb4-569"></a>$$ {#eq-def-kl-loss-grad-estim-general}</span>
<span id="cb4-570"><a href="#cb4-570"></a></span>
<span id="cb4-571"><a href="#cb4-571"></a>则</span>
<span id="cb4-572"><a href="#cb4-572"></a></span>
<span id="cb4-573"><a href="#cb4-573"></a><span class="ss">- </span>OpenRLHF 对应 $w_{\mathbf{\tau}} = \frac{1}{N}, w_{t} = \frac{1}{|\tau|}$；</span>
<span id="cb4-574"><a href="#cb4-574"></a><span class="ss">- </span>verl 对应 $w_{\mathbf{\tau}} = \frac{1}{\sum_{i=1}^{N} |\tau_i|}, w_{t} = 1$。</span>
<span id="cb4-575"><a href="#cb4-575"></a></span>
<span id="cb4-576"><a href="#cb4-576"></a>此处，我们先以 OpenRLHF 的梯度估计 (@eq-def-kl-loss-grad-estim-openrlhf) 为例，分析不同 KL 估计量导出的梯度估计，其满足：</span>
<span id="cb4-577"><a href="#cb4-577"></a></span>
<span id="cb4-578"><a href="#cb4-578"></a>$$</span>
<span id="cb4-579"><a href="#cb4-579"></a>\mathbb{E}_{\mathbf{\tau}_i \sim p_{\theta}} \left[ \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k \right] = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} k \right]</span>
<span id="cb4-580"><a href="#cb4-580"></a>$$ {#eq-def-kl-loss-grad-expect-openrlhf}</span>
<span id="cb4-581"><a href="#cb4-581"></a></span>
<span id="cb4-582"><a href="#cb4-582"></a>我们会在 <span class="co">[</span><span class="ot">@sec-derive-kld-grad</span><span class="co">]</span> 中推导正确的 KL 梯度估计。</span>
<span id="cb4-583"><a href="#cb4-583"></a></span>
<span id="cb4-584"><a href="#cb4-584"></a><span class="fu">### $k_1$ 导出的梯度：期望为 0</span></span>
<span id="cb4-585"><a href="#cb4-585"></a></span>
<span id="cb4-586"><a href="#cb4-586"></a>向 <span class="co">[</span><span class="ot">@eq-def-kl-loss-grad-expect-openrlhf</span><span class="co">]</span> 代入 $k = k_1 = - \log r = \log \frac{1}{r} = \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}$，导出的梯度估计为</span>
<span id="cb4-587"><a href="#cb4-587"></a></span>
<span id="cb4-588"><a href="#cb4-588"></a>$$</span>
<span id="cb4-589"><a href="#cb4-589"></a>\begin{aligned}</span>
<span id="cb4-590"><a href="#cb4-590"></a>&amp; \frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k <span class="sc">\\</span></span>
<span id="cb4-591"><a href="#cb4-591"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} <span class="sc">\\</span></span>
<span id="cb4-592"><a href="#cb4-592"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}\log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) <span class="sc">\\</span></span>
<span id="cb4-593"><a href="#cb4-593"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) <span class="sc">\\</span></span>
<span id="cb4-594"><a href="#cb4-594"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \left( \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) + \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) + \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \right) \right) <span class="sc">\\</span></span>
<span id="cb4-595"><a href="#cb4-595"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) \right) <span class="sc">\\</span></span>
<span id="cb4-596"><a href="#cb4-596"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_\theta(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) <span class="sc">\\</span></span>
<span id="cb4-597"><a href="#cb4-597"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\tau)</span>
<span id="cb4-598"><a href="#cb4-598"></a>\end{aligned}</span>
<span id="cb4-599"><a href="#cb4-599"></a>$$ {#eq-kl-loss-grad-sample-k1}</span>
<span id="cb4-600"><a href="#cb4-600"></a></span>
<span id="cb4-601"><a href="#cb4-601"></a>则其导出的梯度期望满足：</span>
<span id="cb4-602"><a href="#cb4-602"></a></span>
<span id="cb4-603"><a href="#cb4-603"></a>$$</span>
<span id="cb4-604"><a href="#cb4-604"></a>\begin{aligned}</span>
<span id="cb4-605"><a href="#cb4-605"></a>\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})\right] </span>
<span id="cb4-606"><a href="#cb4-606"></a>&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} \nabla_{\theta} \log p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb4-607"><a href="#cb4-607"></a>&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb4-608"><a href="#cb4-608"></a>&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} \nabla_{\theta} p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb4-609"><a href="#cb4-609"></a>&amp; = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} <span class="sc">\\</span></span>
<span id="cb4-610"><a href="#cb4-610"></a>&amp; = \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left<span class="co">[</span><span class="ot"> \frac{1}{|\mathbf{\tau}|} \right</span><span class="co">]</span></span>
<span id="cb4-611"><a href="#cb4-611"></a>\end{aligned}</span>
<span id="cb4-612"><a href="#cb4-612"></a>$$ {#eq-kl-loss-grad-expect-k1}</span>
<span id="cb4-613"><a href="#cb4-613"></a></span>
<span id="cb4-614"><a href="#cb4-614"></a>此处利用了 $p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \frac{1}{p_{\theta}(\tau)} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \nabla_{\theta} p_{\theta}(\tau)$。</span>
<span id="cb4-615"><a href="#cb4-615"></a></span>
<span id="cb4-616"><a href="#cb4-616"></a>所以 $k_1$ loss 项优化的量是 $\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left<span class="co">[</span><span class="ot"> \frac{1}{|\mathbf{\tau}|} \right</span><span class="co">]</span>$。这意味着该优化过程会降低采样轨迹的长度。</span>
<span id="cb4-617"><a href="#cb4-617"></a></span>
<span id="cb4-618"><a href="#cb4-618"></a>特别地，当不对同一轨迹中的 “$k_1$ 估计量”求均值，而是求和时，可以直接将 $\frac{1}{|\tau|}$ 这一项替换为 $1$，得到</span>
<span id="cb4-619"><a href="#cb4-619"></a>$$</span>
<span id="cb4-620"><a href="#cb4-620"></a>\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) = \sum_{\tau \in \mathcal{T}} \nabla_{\theta} p_{\theta} = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta} = \nabla_{\theta} 1 = 0</span>
<span id="cb4-621"><a href="#cb4-621"></a>$$ {#eq-kl-loss-grad-expect-k1-no-intra-traj-mean}^<span class="co">[</span><span class="ot">此处对数似然的梯度的期望值为 0，是一个著名的性质，会在接下来频繁用到。</span><span class="co">]</span></span>
<span id="cb4-622"><a href="#cb4-622"></a></span>
<span id="cb4-623"><a href="#cb4-623"></a>这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。</span>
<span id="cb4-624"><a href="#cb4-624"></a></span>
<span id="cb4-625"><a href="#cb4-625"></a>无论哪种情况，$k_1$ 导出的优化量都非常奇怪，不太可能出于实现者的本意。</span>
<span id="cb4-626"><a href="#cb4-626"></a></span>
<span id="cb4-627"><a href="#cb4-627"></a>同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。接下来，我们将忽略这一操作，即将 $\frac{1}{|\tau|}$ 一项替换为 $1$。</span>
<span id="cb4-628"><a href="#cb4-628"></a></span>
<span id="cb4-629"><a href="#cb4-629"></a><span class="fu">### $k_2$ 导出的梯度</span></span>
<span id="cb4-630"><a href="#cb4-630"></a></span>
<span id="cb4-631"><a href="#cb4-631"></a>向 <span class="co">[</span><span class="ot">@eq-def-kl-loss-grad-expect-openrlhf</span><span class="co">]</span> 代入 $k = k_2 = \frac{1}{2} (\log r)^2 = \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right)^2$，导出的单条轨迹 $\mathbf{\tau} \sim p_{\theta}$ 的梯度为</span>
<span id="cb4-632"><a href="#cb4-632"></a>$$</span>
<span id="cb4-633"><a href="#cb4-633"></a>\begin{aligned}</span>
<span id="cb4-634"><a href="#cb4-634"></a>&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k<span class="sc">\\</span></span>
<span id="cb4-635"><a href="#cb4-635"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}  \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)^2 <span class="sc">\\</span></span>
<span id="cb4-636"><a href="#cb4-636"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} <span class="sc">\\</span></span>
<span id="cb4-637"><a href="#cb4-637"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) <span class="sc">\\</span></span>
<span id="cb4-638"><a href="#cb4-638"></a>\end{aligned}</span>
<span id="cb4-639"><a href="#cb4-639"></a>$$ {#eq-kl-loss-grad-sample-k2}</span>
<span id="cb4-640"><a href="#cb4-640"></a></span>
<span id="cb4-641"><a href="#cb4-641"></a>显然，</span>
<span id="cb4-642"><a href="#cb4-642"></a></span>
<span id="cb4-643"><a href="#cb4-643"></a>$$</span>
<span id="cb4-644"><a href="#cb4-644"></a>\begin{aligned}</span>
<span id="cb4-645"><a href="#cb4-645"></a>&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) <span class="sc">\\</span></span>
<span id="cb4-646"><a href="#cb4-646"></a>\neq &amp; \left( \sum_{t=1}^{|\mathbf{\tau}|}  \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \left( \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \right) <span class="sc">\\</span></span>
<span id="cb4-647"><a href="#cb4-647"></a>=&amp; \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})</span>
<span id="cb4-648"><a href="#cb4-648"></a>\end{aligned}</span>
<span id="cb4-649"><a href="#cb4-649"></a>$$ {#eq-kl-loss-grad-sample-k2-wrong}</span>
<span id="cb4-650"><a href="#cb4-650"></a></span>
<span id="cb4-651"><a href="#cb4-651"></a>然而，</span>
<span id="cb4-652"><a href="#cb4-652"></a></span>
<span id="cb4-653"><a href="#cb4-653"></a>$$</span>
<span id="cb4-654"><a href="#cb4-654"></a>\begin{aligned}</span>
<span id="cb4-655"><a href="#cb4-655"></a>&amp; \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] <span class="sc">\\</span></span>
<span id="cb4-656"><a href="#cb4-656"></a>=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} \log p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb4-657"><a href="#cb4-657"></a>=&amp; \sum_{\tau \in \mathcal{T}} \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb4-658"><a href="#cb4-658"></a>=&amp; \sum_{\tau \in \mathcal{T}} \left<span class="co">[</span><span class="ot"> \left( \log p_{\theta}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) - \left( \log p_{\text{ref}}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb4-659"><a href="#cb4-659"></a>=&amp; \sum_{\tau \in \mathcal{T}}  \left<span class="co">[</span><span class="ot"> \nabla_{\theta} (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) -  \nabla_{\theta} \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb4-660"><a href="#cb4-660"></a>=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  \left<span class="co">[</span><span class="ot"> (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) - \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb4-661"><a href="#cb4-661"></a>=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  p_{\theta} \left<span class="co">[</span><span class="ot"> \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} - 1 \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb4-662"><a href="#cb4-662"></a>=&amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} - 1 \right) \right] <span class="sc">\\</span></span>
<span id="cb4-663"><a href="#cb4-663"></a>= &amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right] <span class="sc">\\</span></span>
<span id="cb4-664"><a href="#cb4-664"></a>= &amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]</span>
<span id="cb4-665"><a href="#cb4-665"></a>\end{aligned}</span>
<span id="cb4-666"><a href="#cb4-666"></a>$$ {#eq-kl-loss-grad-expect-k2-wrong}</span>
<span id="cb4-667"><a href="#cb4-667"></a></span>
<span id="cb4-668"><a href="#cb4-668"></a>此处利用了 $\log p(x) \nabla_{\theta} p(x) = \nabla_{\theta} (\log p(x) - 1) p(x)$</span>
<span id="cb4-669"><a href="#cb4-669"></a></span>
<span id="cb4-670"><a href="#cb4-670"></a>因此，最小化 $k_2$ loss 项 (@eq-kl-loss-grad-sample-k2-wrong) ，并非在优化 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]$。</span>
<span id="cb4-671"><a href="#cb4-671"></a></span>
<span id="cb4-672"><a href="#cb4-672"></a><span class="fu">### $k_3$ 导出的梯度</span></span>
<span id="cb4-673"><a href="#cb4-673"></a></span>
<span id="cb4-674"><a href="#cb4-674"></a>向 <span class="co">[</span><span class="ot">@eq-def-kl-loss-grad-expect-openrlhf</span><span class="co">]</span> 代入 $k = k_3 = (r - 1) - \log r = (\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1) - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}$，导出的单条轨迹 $\mathbf{\tau} \sim p_{\theta}$ 的梯度为</span>
<span id="cb4-675"><a href="#cb4-675"></a>$$</span>
<span id="cb4-676"><a href="#cb4-676"></a>\begin{aligned}</span>
<span id="cb4-677"><a href="#cb4-677"></a>&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k <span class="sc">\\</span></span>
<span id="cb4-678"><a href="#cb4-678"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \left(\frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1 - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right) <span class="sc">\\</span></span>
<span id="cb4-679"><a href="#cb4-679"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} - \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} <span class="sc">\\</span></span>
<span id="cb4-680"><a href="#cb4-680"></a>=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} <span class="sc">\\</span></span>
<span id="cb4-681"><a href="#cb4-681"></a>=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) + \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) <span class="sc">\\</span></span>
<span id="cb4-682"><a href="#cb4-682"></a>\end{aligned}</span>
<span id="cb4-683"><a href="#cb4-683"></a>$$ {#eq-kl-loss-grad-sample-k3}</span>
<span id="cb4-684"><a href="#cb4-684"></a></span>
<span id="cb4-685"><a href="#cb4-685"></a>其中，根据 <span class="co">[</span><span class="ot">@eq-kl-loss-grad-expect-k1-no-intra-traj-mean</span><span class="co">]</span>，$\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = 0$，不妨直接省略。</span>
<span id="cb4-686"><a href="#cb4-686"></a></span>
<span id="cb4-687"><a href="#cb4-687"></a>而剩余部分似乎很难通过消去 $\pi_{\theta}(\mathbf{\tau})$ 来提出 $\nabla_{\theta}$ 并准确分析。但显然也并非在优化 KL 散度。</span>
<span id="cb4-688"><a href="#cb4-688"></a></span>
<span id="cb4-689"><a href="#cb4-689"></a><span class="fu">### 小结：流行的 ”KL loss 项“ 实现并不合理</span></span>
<span id="cb4-690"><a href="#cb4-690"></a></span>
<span id="cb4-691"><a href="#cb4-691"></a>综上所述，对于 OpenRLHF 实现的 “KL loss 项”，</span>
<span id="cb4-692"><a href="#cb4-692"></a></span>
<span id="cb4-693"><a href="#cb4-693"></a></span>
<span id="cb4-694"><a href="#cb4-694"></a><span class="ss">1. </span>对同一轨迹内的 “KL 估计量” 求均值这一操作很可能是错误的，正确操作应当为求和，对应于根据对数条件概率求对数联合概率。</span>
<span id="cb4-695"><a href="#cb4-695"></a><span class="ss">2. </span>$k_1$ 导出的梯度</span>
<span id="cb4-696"><a href="#cb4-696"></a><span class="ss">   1. </span>若对同一轨迹内的 “KL 估计量” 求均值，则会导致输出长度减小，</span>
<span id="cb4-697"><a href="#cb4-697"></a><span class="ss">   2. </span>而如果修正为求和，则其期望为 0，在平均意义上不改分布。</span>
<span id="cb4-698"><a href="#cb4-698"></a><span class="ss">3. </span>$k_2$，$k_3$ 导出的梯度则十分复杂，难以分析，但都并非在优化 KL 散度，这可能是因为其错误地将 KL 估计样本量应用于动作对数条件似然并求和。回顾 KL 估计量公式 (@eq-def-kl-estimators) ，应当注意到这些估计量是直接作用于似然 $p_{\theta}(\mathbf{\tau})$，而没有保证作用于概率后求积/对数和仍然有意义。</span>
<span id="cb4-699"><a href="#cb4-699"></a></span>
<span id="cb4-700"><a href="#cb4-700"></a><span class="fu">## 分析流行的 “KL reward 项“ 实现</span></span>
<span id="cb4-701"><a href="#cb4-701"></a></span>
<span id="cb4-702"><a href="#cb4-702"></a><span class="fu">### 类比 PG 优化 reward 来分析 KL reward 的作用 {#sec-analogy-pg-kl}</span></span>
<span id="cb4-703"><a href="#cb4-703"></a></span>
<span id="cb4-704"><a href="#cb4-704"></a>由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是：</span>
<span id="cb4-705"><a href="#cb4-705"></a>$$</span>
<span id="cb4-706"><a href="#cb4-706"></a>\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[r(\mathbf{\tau})\right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \hat{A}_t \right]</span>
<span id="cb4-707"><a href="#cb4-707"></a>$$ {#eq-pg-est-adv}</span>
<span id="cb4-708"><a href="#cb4-708"></a></span>
<span id="cb4-709"><a href="#cb4-709"></a>其中 $\hat{A}_t$ 为优势（Advantage）的估计量。</span>
<span id="cb4-710"><a href="#cb4-710"></a></span>
<span id="cb4-711"><a href="#cb4-711"></a>为了方便观察 KL reward 项发挥的作用，我们将 $r_{\mathbf{\tau}}$ 展开，并不妨考虑一个更简单的估计，例如：</span>
<span id="cb4-712"><a href="#cb4-712"></a></span>
<span id="cb4-713"><a href="#cb4-713"></a>$$</span>
<span id="cb4-714"><a href="#cb4-714"></a>\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{|\mathbf{\tau}|} r(\mathbf{s}_t, \mathbf{a}_t) \right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \sum_{t'=1}^{|\tau|} r(s_{t'}, a_{t'}) \right]</span>
<span id="cb4-715"><a href="#cb4-715"></a>$$ {#eq-pg-est-ret}</span>
<span id="cb4-716"><a href="#cb4-716"></a></span>
<span id="cb4-717"><a href="#cb4-717"></a>简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 "Policy Gradient" 一讲^<span class="co">[</span><span class="ot">https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf</span><span class="co">]</span>。</span>
<span id="cb4-718"><a href="#cb4-718"></a></span>
<span id="cb4-719"><a href="#cb4-719"></a>类比 $r_{t'}$ 导出的梯度期望，将负的 KL 样本量 $- \log \frac{\pi_\theta\left(a_t \mid s_t \right)}{\pi_{\text{ref}}\left(a_t \mid s_t \right)}$ 加入 reward $r_{t'}$ 代入其中，导出的梯度期望为：</span>
<span id="cb4-720"><a href="#cb4-720"></a></span>
<span id="cb4-721"><a href="#cb4-721"></a>$$</span>
<span id="cb4-722"><a href="#cb4-722"></a>\mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|}  \left( \nabla_\theta \log \pi_\theta\left(a_t \mid s_t \right) \right) \sum_{t'=1}^{|\tau|} - \log \frac{\pi_\theta\left(a_{t'} \mid s_{t'} \right)}{\pi_{\text{ref}}\left(a_{t'} \mid s_{t'} \right)} \right] = \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}\right]</span>
<span id="cb4-723"><a href="#cb4-723"></a>$$ {#eq-kl-grad-est-markov-1}</span>
<span id="cb4-724"><a href="#cb4-724"></a></span>
<span id="cb4-725"><a href="#cb4-725"></a>注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (@eq-def-markov-prop)。</span>
<span id="cb4-726"><a href="#cb4-726"></a></span>
<span id="cb4-727"><a href="#cb4-727"></a>实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：</span>
<span id="cb4-728"><a href="#cb4-728"></a></span>
<span id="cb4-729"><a href="#cb4-729"></a>$$</span>
<span id="cb4-730"><a href="#cb4-730"></a>\begin{aligned}</span>
<span id="cb4-731"><a href="#cb4-731"></a>&amp; \nabla_{\theta}- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)} \right] <span class="sc">\\</span></span>
<span id="cb4-732"><a href="#cb4-732"></a>\to&amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] <span class="sc">\\</span></span>
<span id="cb4-733"><a href="#cb4-733"></a>= &amp; \nabla_{\theta} -  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{\prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{ \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] <span class="sc">\\</span></span>
<span id="cb4-734"><a href="#cb4-734"></a>= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) }{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) } \right] <span class="sc">\\</span></span>
<span id="cb4-735"><a href="#cb4-735"></a>= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p_\theta\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)}{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)} \right] <span class="sc">\\</span></span>
<span id="cb4-736"><a href="#cb4-736"></a>= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta} \left[ \log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)} \right] <span class="sc">\\</span></span>
<span id="cb4-737"><a href="#cb4-737"></a>= &amp; \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb4-738"><a href="#cb4-738"></a>\end{aligned}</span>
<span id="cb4-739"><a href="#cb4-739"></a>$$ {#eq-kl-grad-est-dp}</span>
<span id="cb4-740"><a href="#cb4-740"></a></span>
<span id="cb4-741"><a href="#cb4-741"></a>可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。</span>
<span id="cb4-742"><a href="#cb4-742"></a></span>
<span id="cb4-743"><a href="#cb4-743"></a><span class="fu">### 不同 KL 估计量导出的 reward 项的作用</span></span>
<span id="cb4-744"><a href="#cb4-744"></a></span>
<span id="cb4-745"><a href="#cb4-745"></a>不难注意到，<span class="co">[</span><span class="ot">@sec-analogy-pg-kl</span><span class="co">]</span> 中的 KL 样本量对应于 $k_1$ 估计量。</span>
<span id="cb4-746"><a href="#cb4-746"></a></span>
<span id="cb4-747"><a href="#cb4-747"></a>一个自然的问题是，如果对动作条件似然使用 $k_2$ 或 $k_3$ 等其他估计量，会得到什么结果？</span>
<span id="cb4-748"><a href="#cb4-748"></a></span>
<span id="cb4-749"><a href="#cb4-749"></a>$k_2$ 或 $k_3$ 等其他估计量导致的一个问题，求和时通常无法得到联合概率。具体来说，其他估计量分别在优化</span>
<span id="cb4-750"><a href="#cb4-750"></a></span>
<span id="cb4-751"><a href="#cb4-751"></a><span class="ss">- </span>$k_2$: $- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \frac{1}{2} \left( \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right)^{2} \right]$</span>
<span id="cb4-752"><a href="#cb4-752"></a><span class="ss">- </span>$k_3$: $- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} (\frac{\pi_{\text{ref}} \left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} - 1 - \log \frac{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}) \right]$</span>
<span id="cb4-753"><a href="#cb4-753"></a></span>
<span id="cb4-754"><a href="#cb4-754"></a>显然，这里的求和无法得到联合概率，也就无法实现类似 <span class="co">[</span><span class="ot">@eq-kl-grad-est-dp</span><span class="co">]</span> 中的效果了。</span>
<span id="cb4-755"><a href="#cb4-755"></a></span>
<span id="cb4-756"><a href="#cb4-756"></a><span class="fu">### 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</span></span>
<span id="cb4-757"><a href="#cb4-757"></a></span>
<span id="cb4-758"><a href="#cb4-758"></a>若对动作对数条件似然计算 KL 估计样本量，则由于涉及到求和，$k_1$ 之外的估计量通常没有良好定义。</span>
<span id="cb4-759"><a href="#cb4-759"></a></span>
<span id="cb4-760"><a href="#cb4-760"></a>但是若放弃对动作条件似然计算 KL 估计样本量，而是对求和之后的对数（条件）似然进行计算，则只需满足</span>
<span id="cb4-761"><a href="#cb4-761"></a></span>
<span id="cb4-762"><a href="#cb4-762"></a>$$</span>
<span id="cb4-763"><a href="#cb4-763"></a>\nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right) \right] </span>
<span id="cb4-764"><a href="#cb4-764"></a>\approx \nabla_{\theta} - \frac{1}{N} k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right)</span>
<span id="cb4-765"><a href="#cb4-765"></a>\approx \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]</span>
<span id="cb4-766"><a href="#cb4-766"></a>$$ {#eq-kl-reward-grad-expect-general}</span>
<span id="cb4-767"><a href="#cb4-767"></a></span>
<span id="cb4-768"><a href="#cb4-768"></a>暂时不考虑 off-policy 问题，根据 <span class="co">[</span><span class="ot">@eq-kl-reward-grad-expect-general</span><span class="co">]</span>, GRPO 公式 (@eq-grpo-obj, @eq-grpo-obj-kl-term) 应当修正 KL 项如下：</span>
<span id="cb4-769"><a href="#cb4-769"></a></span>
<span id="cb4-770"><a href="#cb4-770"></a>$$</span>
<span id="cb4-771"><a href="#cb4-771"></a>\begin{aligned}</span>
<span id="cb4-772"><a href="#cb4-772"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb4-773"><a href="#cb4-773"></a>&amp; \frac{1}{G} \sum_{i=1}^G \left<span class="sc">\{</span> \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|} \min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span>  \right<span class="sc">\}</span>  -\beta k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)</span>
<span id="cb4-774"><a href="#cb4-774"></a>\end{aligned}</span>
<span id="cb4-775"><a href="#cb4-775"></a>$$ {#eq-grpo-obj-kl-fixed}</span>
<span id="cb4-776"><a href="#cb4-776"></a></span>
<span id="cb4-777"><a href="#cb4-777"></a><span class="fu"># 推导 on-policy 设置下 KL 散度的梯度估计 {#sec-derive-kld-grad}</span></span>
<span id="cb4-778"><a href="#cb4-778"></a></span>
<span id="cb4-779"><a href="#cb4-779"></a>前文中，我们分析了流行的 LLM RL 框架中对 KL 散度优化的实现，并得出了结论。另一种思路是直接推导出 KL 散度的梯度估计表达式，并据此实现代码。</span>
<span id="cb4-780"><a href="#cb4-780"></a></span>
<span id="cb4-781"><a href="#cb4-781"></a>由于我们使用的是梯度法，为了优化 KL 散度，我们需要准确估计的是 KL 散度的梯度而非其本身。类似地，在 PG 中，我们需要最大化 $\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]$，估计的是其梯度 $\nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]=\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau}) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})]$而不是$r(\mathbf{\tau})$ 本身。</span>
<span id="cb4-782"><a href="#cb4-782"></a></span>
<span id="cb4-783"><a href="#cb4-783"></a>同时，如 <span class="co">[</span><span class="ot">@sec-kl-loss-impl</span><span class="co">]</span> 所述，先前向传播估计 KL 散度，再直接反向传播，通常是无法直接得到 KL 散度的梯度的。所以，我们需要直接估计 KL 散度的梯度。</span>
<span id="cb4-784"><a href="#cb4-784"></a></span>
<span id="cb4-785"><a href="#cb4-785"></a>首先，展开 KL 散度的表达式：</span>
<span id="cb4-786"><a href="#cb4-786"></a></span>
<span id="cb4-787"><a href="#cb4-787"></a>$$</span>
<span id="cb4-788"><a href="#cb4-788"></a>\begin{aligned}</span>
<span id="cb4-789"><a href="#cb4-789"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb4-790"><a href="#cb4-790"></a>&amp; \propto \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right)</span>
<span id="cb4-791"><a href="#cb4-791"></a>\end{aligned}</span>
<span id="cb4-792"><a href="#cb4-792"></a>$$ {#eq-kl-expansion-sum}</span>
<span id="cb4-793"><a href="#cb4-793"></a></span>
<span id="cb4-794"><a href="#cb4-794"></a>再计算其梯度：</span>
<span id="cb4-795"><a href="#cb4-795"></a></span>
<span id="cb4-796"><a href="#cb4-796"></a>$$</span>
<span id="cb4-797"><a href="#cb4-797"></a>\begin{aligned}</span>
<span id="cb4-798"><a href="#cb4-798"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \propto \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\prod_{t=1}^{|\tau|-1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right)  <span class="sc">\\</span></span>
<span id="cb4-799"><a href="#cb4-799"></a>&amp; \cdot \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) <span class="sc">\\</span></span>
<span id="cb4-800"><a href="#cb4-800"></a>&amp; = \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau| - 1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right) <span class="sc">\\</span></span>
<span id="cb4-801"><a href="#cb4-801"></a>&amp; \cdot \nabla_{\theta} \left(\left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) \right)</span>
<span id="cb4-802"><a href="#cb4-802"></a>\end{aligned}</span>
<span id="cb4-803"><a href="#cb4-803"></a>$$ {#eq-kl-grad-expansion}</span>
<span id="cb4-804"><a href="#cb4-804"></a></span>
<span id="cb4-805"><a href="#cb4-805"></a><span class="co">[</span><span class="ot">@eq-kl-grad-expansion</span><span class="co">]</span> 中的梯度相当复杂，难以直接计算。接下来，我们将引入一系列合理的假设来简化它。</span>
<span id="cb4-806"><a href="#cb4-806"></a></span>
<span id="cb4-807"><a href="#cb4-807"></a><span class="fu">## 在已知环境中简化 KL 梯度估计</span></span>
<span id="cb4-808"><a href="#cb4-808"></a></span>
<span id="cb4-809"><a href="#cb4-809"></a>实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。</span>
<span id="cb4-810"><a href="#cb4-810"></a></span>
<span id="cb4-811"><a href="#cb4-811"></a>当状态转移概率分布已知时，$\forall t, p_{\theta}(a_1, \cdots, s_t, a_t \mid s_1)$ 都是可以计算的，则 KL 散度可以直接写成：</span>
<span id="cb4-812"><a href="#cb4-812"></a></span>
<span id="cb4-813"><a href="#cb4-813"></a>$$</span>
<span id="cb4-814"><a href="#cb4-814"></a>\begin{aligned}</span>
<span id="cb4-815"><a href="#cb4-815"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\mathbf{\tau} \in \mathcal{T}} p(\mathbf{s}_1) p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1) \log \frac{p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}{p_{\text{ref}}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}  <span class="sc">\\</span></span>
<span id="cb4-816"><a href="#cb4-816"></a>\end{aligned}</span>
<span id="cb4-817"><a href="#cb4-817"></a>$$ {#eq-kl-grad-expansion-known-transition}</span>
<span id="cb4-818"><a href="#cb4-818"></a></span>
<span id="cb4-819"><a href="#cb4-819"></a><span class="fu">## 简写为 Contextual Bandit</span></span>
<span id="cb4-820"><a href="#cb4-820"></a></span>
<span id="cb4-821"><a href="#cb4-821"></a>为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 $\mathbf{s}_1 = \mathbf{x} \in \mathcal{P}, (\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T) = \mathbf{y} \in \mathcal{R}$，其中 $\mathcal{P}, \mathcal{R}$ 分别表示 prompt / response 空间，则 KL 散度变为：</span>
<span id="cb4-822"><a href="#cb4-822"></a></span>
<span id="cb4-823"><a href="#cb4-823"></a>$$</span>
<span id="cb4-824"><a href="#cb4-824"></a>\begin{aligned}</span>
<span id="cb4-825"><a href="#cb4-825"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}}\left[\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right] <span class="sc">\\</span></span>
<span id="cb4-826"><a href="#cb4-826"></a>&amp; = \sum_{(x, y) \in \mathcal{T}} p_{\theta}(x, y) \left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) <span class="sc">\\</span></span>
<span id="cb4-827"><a href="#cb4-827"></a>&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)</span>
<span id="cb4-828"><a href="#cb4-828"></a>\end{aligned}</span>
<span id="cb4-829"><a href="#cb4-829"></a>$$ {#eq-def-kl-cb}</span>
<span id="cb4-830"><a href="#cb4-830"></a></span>
<span id="cb4-831"><a href="#cb4-831"></a>其梯度变为：</span>
<span id="cb4-832"><a href="#cb4-832"></a></span>
<span id="cb4-833"><a href="#cb4-833"></a>$$</span>
<span id="cb4-834"><a href="#cb4-834"></a>\begin{aligned}</span>
<span id="cb4-835"><a href="#cb4-835"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \nabla_{\theta} \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) <span class="sc">\\</span></span>
<span id="cb4-836"><a href="#cb4-836"></a>&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right)</span>
<span id="cb4-837"><a href="#cb4-837"></a>\end{aligned}</span>
<span id="cb4-838"><a href="#cb4-838"></a>$$ {#eq-def-kl-grad-cb}</span>
<span id="cb4-839"><a href="#cb4-839"></a></span>
<span id="cb4-840"><a href="#cb4-840"></a>其中梯度项可以进一步展开为：</span>
<span id="cb4-841"><a href="#cb4-841"></a></span>
<span id="cb4-842"><a href="#cb4-842"></a>$$</span>
<span id="cb4-843"><a href="#cb4-843"></a>\begin{aligned}</span>
<span id="cb4-844"><a href="#cb4-844"></a>&amp; \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right) <span class="sc">\\</span></span>
<span id="cb4-845"><a href="#cb4-845"></a>=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \nabla_{\theta} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) <span class="sc">\\</span></span>
<span id="cb4-846"><a href="#cb4-846"></a>=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \frac{1}{\pi_\theta(y \mid x)} \nabla_{\theta} \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb4-847"><a href="#cb4-847"></a>=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \nabla_{\theta} \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb4-848"><a href="#cb4-848"></a>=&amp; \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x)</span>
<span id="cb4-849"><a href="#cb4-849"></a>\end{aligned}</span>
<span id="cb4-850"><a href="#cb4-850"></a>$$ {#eq-def-kl-grad-cb-grad-term}</span>
<span id="cb4-851"><a href="#cb4-851"></a></span>
<span id="cb4-852"><a href="#cb4-852"></a>代入回 KL 梯度表达式：</span>
<span id="cb4-853"><a href="#cb4-853"></a></span>
<span id="cb4-854"><a href="#cb4-854"></a>$$</span>
<span id="cb4-855"><a href="#cb4-855"></a>\begin{aligned}</span>
<span id="cb4-856"><a href="#cb4-856"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb4-857"><a href="#cb4-857"></a>=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb4-858"><a href="#cb4-858"></a>=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) <span class="sc">\\</span></span>
<span id="cb4-859"><a href="#cb4-859"></a>=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb4-860"><a href="#cb4-860"></a>=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] <span class="sc">\\</span></span>
<span id="cb4-861"><a href="#cb4-861"></a>=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] + \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] <span class="sc">\\</span></span>
<span id="cb4-862"><a href="#cb4-862"></a>=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right]</span>
<span id="cb4-863"><a href="#cb4-863"></a>\end{aligned}</span>
<span id="cb4-864"><a href="#cb4-864"></a>$$ {#eq-def-kl-grad-cb-expect}</span>
<span id="cb4-865"><a href="#cb4-865"></a></span>
<span id="cb4-866"><a href="#cb4-866"></a>这里为了重新获得期望形式，引入了 $1 = \pi_{\theta}(y \mid x) / \pi_{\theta}(y \mid x)$，并利用了 $\nabla_{\theta} \log \pi_{\theta}(y \mid x) = \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)}$ 和 $\mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] = 0$。</span>
<span id="cb4-867"><a href="#cb4-867"></a></span>
<span id="cb4-868"><a href="#cb4-868"></a>进行 Monte Carlo 估计：</span>
<span id="cb4-869"><a href="#cb4-869"></a></span>
<span id="cb4-870"><a href="#cb4-870"></a>$$</span>
<span id="cb4-871"><a href="#cb4-871"></a>\begin{aligned}</span>
<span id="cb4-872"><a href="#cb4-872"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\log \frac{\pi_{\theta}(y_i \mid x_i)}{\pi_{\text{ref}}(y_i \mid x_i)}\right) \nabla_{\theta} \log \pi_{\theta}(y_i \mid x_i)</span>
<span id="cb4-873"><a href="#cb4-873"></a>\end{aligned}</span>
<span id="cb4-874"><a href="#cb4-874"></a>$$ {#eq-def-kl-grad-cb-mc}</span>
<span id="cb4-875"><a href="#cb4-875"></a></span>
<span id="cb4-876"><a href="#cb4-876"></a>其中 $(\mathbf{x}_i, \mathbf{y}_i) \sim p_{\theta}$。</span>
<span id="cb4-877"><a href="#cb4-877"></a></span>
<span id="cb4-878"><a href="#cb4-878"></a><span class="fu">## 还原为已知环境决策过程</span></span>
<span id="cb4-879"><a href="#cb4-879"></a></span>
<span id="cb4-880"><a href="#cb4-880"></a>将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：</span>
<span id="cb4-881"><a href="#cb4-881"></a></span>
<span id="cb4-882"><a href="#cb4-882"></a>$$</span>
<span id="cb4-883"><a href="#cb4-883"></a>\begin{aligned}</span>
<span id="cb4-884"><a href="#cb4-884"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]<span class="sc">\\</span></span>
<span id="cb4-885"><a href="#cb4-885"></a>=&amp; \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{y} \mid \mathbf{x})\right] <span class="sc">\\</span></span>
<span id="cb4-886"><a href="#cb4-886"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T}) \sim p_{\theta}} \left[\left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)\right)\right]</span>
<span id="cb4-887"><a href="#cb4-887"></a>\end{aligned}</span>
<span id="cb4-888"><a href="#cb4-888"></a>$$ {#eq-def-kl-grad-kt-mc}</span>
<span id="cb4-889"><a href="#cb4-889"></a></span>
<span id="cb4-890"><a href="#cb4-890"></a>对应的 Monte Carlo 估计式为：</span>
<span id="cb4-891"><a href="#cb4-891"></a></span>
<span id="cb4-892"><a href="#cb4-892"></a>$$</span>
<span id="cb4-893"><a href="#cb4-893"></a>\begin{aligned}</span>
<span id="cb4-894"><a href="#cb4-894"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T}\log \frac{\pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}{\pi_{\text{ref}}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})\right)</span>
<span id="cb4-895"><a href="#cb4-895"></a>\end{aligned}</span>
<span id="cb4-896"><a href="#cb4-896"></a>$$ {#eq-def-kl-grad-kt-mc-loss}</span>
<span id="cb4-897"><a href="#cb4-897"></a></span>
<span id="cb4-898"><a href="#cb4-898"></a><span class="fu">## 利用因果性技巧化简 KL 梯度估计^[https://www.wikiwand.com/en/articles/Policy_gradient_method]</span></span>
<span id="cb4-899"><a href="#cb4-899"></a></span>
<span id="cb4-900"><a href="#cb4-900"></a>因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。</span>
<span id="cb4-901"><a href="#cb4-901"></a></span>
<span id="cb4-902"><a href="#cb4-902"></a>对于任何 $0 \leq t \leq |\tau|$，我们有</span>
<span id="cb4-903"><a href="#cb4-903"></a>$$</span>
<span id="cb4-904"><a href="#cb4-904"></a>\begin{aligned}</span>
<span id="cb4-905"><a href="#cb4-905"></a>&amp; \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) }\left[\nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] <span class="sc">\\</span></span>
<span id="cb4-906"><a href="#cb4-906"></a>=&amp; \sum_{a_t \in \mathcal{A}} \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \nabla_\theta \log \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) <span class="sc">\\</span></span>
<span id="cb4-907"><a href="#cb4-907"></a>=&amp; \sum_{a_j \in \mathcal{A}} \pi_\theta(a_j \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_j) \cdot 0 <span class="sc">\\</span></span>
<span id="cb4-908"><a href="#cb4-908"></a>=&amp; 0</span>
<span id="cb4-909"><a href="#cb4-909"></a>\end{aligned}</span>
<span id="cb4-910"><a href="#cb4-910"></a>$$ {#eq-score-expect-zero}</span>
<span id="cb4-911"><a href="#cb4-911"></a></span>
<span id="cb4-912"><a href="#cb4-912"></a>更进一步，如果 $\mathbf{\Psi}_{t'}$ 是一个与 $\mathbf{a}_t, \mathbf{s}_{t+1}, \mathbf{a}_{t+1}, \ldots$ 独立的随机变量，那么</span>
<span id="cb4-913"><a href="#cb4-913"></a>$$</span>
<span id="cb4-914"><a href="#cb4-914"></a>\begin{aligned}</span>
<span id="cb4-915"><a href="#cb4-915"></a>&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] <span class="sc">\\</span></span>
<span id="cb4-916"><a href="#cb4-916"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{(\mathbf{a}_t, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]</span>
<span id="cb4-917"><a href="#cb4-917"></a> \right] <span class="sc">\\</span></span>
<span id="cb4-918"><a href="#cb4-918"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \mathbb{E}_{</span>
<span id="cb4-919"><a href="#cb4-919"></a>    (\mathbf{s}_{t+1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t})} \left[\mathbf{\Psi}_{t'} \right] \right]</span>
<span id="cb4-920"><a href="#cb4-920"></a> \right] <span class="sc">\\</span></span>
<span id="cb4-921"><a href="#cb4-921"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]</span>
<span id="cb4-922"><a href="#cb4-922"></a> \right] <span class="sc">\\</span></span>
<span id="cb4-923"><a href="#cb4-923"></a>=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[</span>
<span id="cb4-924"><a href="#cb4-924"></a>            \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] </span>
<span id="cb4-925"><a href="#cb4-925"></a>        \right] <span class="sc">\\</span></span>
<span id="cb4-926"><a href="#cb4-926"></a>=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \right] <span class="sc">\\</span></span>
<span id="cb4-927"><a href="#cb4-927"></a>=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbf{\Psi}_{t'} \cdot 0 \right] <span class="sc">\\</span></span>
<span id="cb4-928"><a href="#cb4-928"></a>=&amp; 0</span>
<span id="cb4-929"><a href="#cb4-929"></a>\end{aligned}</span>
<span id="cb4-930"><a href="#cb4-930"></a>$$ {#eq-score-indep-mul-expect-zero}</span>
<span id="cb4-931"><a href="#cb4-931"></a></span>
<span id="cb4-932"><a href="#cb4-932"></a>其中，为了利用 <span class="co">[</span><span class="ot">@eq-score-expect-zero</span><span class="co">]</span> 的结论，我们利用了全期望定律，即</span>
<span id="cb4-933"><a href="#cb4-933"></a></span>
<span id="cb4-934"><a href="#cb4-934"></a>$$</span>
<span id="cb4-935"><a href="#cb4-935"></a>\mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p} \left[\mathbf{x}\right] = \mathbb{E}_{\mathbf{y} \sim p} \left[\mathbb{E}_{\mathbf{x} \sim p(\cdot \mid \mathbf{y})} <span class="co">[</span><span class="ot">\mathbf{x}</span><span class="co">]</span> \right]</span>
<span id="cb4-936"><a href="#cb4-936"></a>$$ {#eq-law-of-total-expectation}</span>
<span id="cb4-937"><a href="#cb4-937"></a></span>
<span id="cb4-938"><a href="#cb4-938"></a>来引入我们想要的期望。</span>
<span id="cb4-939"><a href="#cb4-939"></a></span>
<span id="cb4-940"><a href="#cb4-940"></a>$$</span>
<span id="cb4-941"><a href="#cb4-941"></a>\begin{aligned}</span>
<span id="cb4-942"><a href="#cb4-942"></a>&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_i \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] <span class="sc">\\</span></span>
<span id="cb4-943"><a href="#cb4-943"></a>=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) <span class="sc">\\</span></span>
<span id="cb4-944"><a href="#cb4-944"></a>=&amp; \sum_{\tau \in \mathcal{T}} p_\theta(s_1, a_1, \cdots, s_t) \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) p_\theta(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) <span class="sc">\\</span></span>
<span id="cb4-945"><a href="#cb4-945"></a>=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t)  \sum_{(a_{t}, s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})} \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \Psi_{t'} \nabla_\theta p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right)  <span class="sc">\\</span></span>
<span id="cb4-946"><a href="#cb4-946"></a>=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t) \sum_{a_t \in \mathcal{A}}  \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \sum_{(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})}  p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} <span class="sc">\\</span></span>
<span id="cb4-947"><a href="#cb4-947"></a>\end{aligned}</span>
<span id="cb4-948"><a href="#cb4-948"></a>$$ {#eq-score-indep-mul-expect-zero}</span>
<span id="cb4-949"><a href="#cb4-949"></a></span>
<span id="cb4-950"><a href="#cb4-950"></a></span>
<span id="cb4-951"><a href="#cb4-951"></a>考虑 Monte Carlo 估计式 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-mc-loss</span><span class="co">]</span> 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：</span>
<span id="cb4-952"><a href="#cb4-952"></a></span>
<span id="cb4-953"><a href="#cb4-953"></a>$$</span>
<span id="cb4-954"><a href="#cb4-954"></a>\mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[</span>
<span id="cb4-955"><a href="#cb4-955"></a>\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})</span>
<span id="cb4-956"><a href="#cb4-956"></a>\right]</span>
<span id="cb4-957"><a href="#cb4-957"></a>$$ {#eq-def-kl-grad-kt-mc-loss-estimator-one-grad}</span>
<span id="cb4-958"><a href="#cb4-958"></a></span>
<span id="cb4-959"><a href="#cb4-959"></a></span>
<span id="cb4-960"><a href="#cb4-960"></a>由于序列决策过程满足因果性，即 $\forall t' &lt; t$，$\mathbf{s}_{t'}, \mathbf{a}_{t'}$ 独立于 $\mathbf{s}_{t}, \mathbf{a}_{t}$，则可令 $\mathbf{\Psi}_{t'} = \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}$，其独立于 $\mathbf{s}_{i, t}, \mathbf{a}_{i, t}, \ldots$，利用 <span class="co">[</span><span class="ot">@eq-score-indep-mul-expect-zero</span><span class="co">]</span> 的性质，则有</span>
<span id="cb4-961"><a href="#cb4-961"></a>$$</span>
<span id="cb4-962"><a href="#cb4-962"></a>\forall t' &lt; t, \mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[</span>
<span id="cb4-963"><a href="#cb4-963"></a>\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})</span>
<span id="cb4-964"><a href="#cb4-964"></a>\right] = 0</span>
<span id="cb4-965"><a href="#cb4-965"></a>$$ {#eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero}</span>
<span id="cb4-966"><a href="#cb4-966"></a></span>
<span id="cb4-967"><a href="#cb4-967"></a></span>
<span id="cb4-968"><a href="#cb4-968"></a>将 <span class="co">[</span><span class="ot">@eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero</span><span class="co">]</span> 代入 KL 梯度表达式 (@eq-def-kl-grad-kt-mc) ，即可简化得到：</span>
<span id="cb4-969"><a href="#cb4-969"></a></span>
<span id="cb4-970"><a href="#cb4-970"></a>$$</span>
<span id="cb4-971"><a href="#cb4-971"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{t}) \right]</span>
<span id="cb4-972"><a href="#cb4-972"></a>$$ {#eq-def-kl-grad-kt-reduce}</span>
<span id="cb4-973"><a href="#cb4-973"></a></span>
<span id="cb4-974"><a href="#cb4-974"></a>对应的 Monte Carlo 估计式为：</span>
<span id="cb4-975"><a href="#cb4-975"></a></span>
<span id="cb4-976"><a href="#cb4-976"></a>$$</span>
<span id="cb4-977"><a href="#cb4-977"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \left(\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})</span>
<span id="cb4-978"><a href="#cb4-978"></a>$$ {#eq-def-kl-grad-kt-reduce-mc}</span>
<span id="cb4-979"><a href="#cb4-979"></a></span>
<span id="cb4-980"><a href="#cb4-980"></a>同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：</span>
<span id="cb4-981"><a href="#cb4-981"></a></span>
<span id="cb4-982"><a href="#cb4-982"></a>$$</span>
<span id="cb4-983"><a href="#cb4-983"></a>\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \text{nograd}\left (\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})</span>
<span id="cb4-984"><a href="#cb4-984"></a>$$ {#eq-def-kl-grad-kt-reduce-mc-loss}</span>
<span id="cb4-985"><a href="#cb4-985"></a></span>
<span id="cb4-986"><a href="#cb4-986"></a>这里也可以看到，KL loss 项正确的实现要求：</span>
<span id="cb4-987"><a href="#cb4-987"></a></span>
<span id="cb4-988"><a href="#cb4-988"></a><span class="ss">1. </span>在序列内 token 间，对对数条件似然先求和，得到 KL 样本值，</span>
<span id="cb4-989"><a href="#cb4-989"></a><span class="ss">2. </span>再在序列间求均值。</span>
<span id="cb4-990"><a href="#cb4-990"></a></span>
<span id="cb4-991"><a href="#cb4-991"></a>因此 OpenRLHF (@eq-def-kl-loss-grad-estim-openrlhf) 与 verl (@eq-def-kl-loss-grad-estim-verl) 的权重都是错误的。</span>
<span id="cb4-992"><a href="#cb4-992"></a></span>
<span id="cb4-993"><a href="#cb4-993"></a><span class="fu">## KL 梯度优化可以实现为 KL 样本值 reward {#sec-kl-grad-as-kl-reward}</span></span>
<span id="cb4-994"><a href="#cb4-994"></a></span>
<span id="cb4-995"><a href="#cb4-995"></a>在 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce</span><span class="co">]</span> 中，令 $k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) = \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}$，则有：</span>
<span id="cb4-996"><a href="#cb4-996"></a>$$</span>
<span id="cb4-997"><a href="#cb4-997"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t-1}, \mathbf{s}_{t}) \right]</span>
<span id="cb4-998"><a href="#cb4-998"></a>$$ {#eq-def-kl-grad-kt-reduce-k}</span>
<span id="cb4-999"><a href="#cb4-999"></a></span>
<span id="cb4-1000"><a href="#cb4-1000"></a>不难注意到 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-k</span><span class="co">]</span> 中 $k$ 与 <span class="co">[</span><span class="ot">@eq-pg-est-ret</span><span class="co">]</span> 中 reward $r$ 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。</span>
<span id="cb4-1001"><a href="#cb4-1001"></a></span>
<span id="cb4-1002"><a href="#cb4-1002"></a>类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS285^<span class="co">[</span><span class="ot">https://rail.eecs.berkeley.edu/deeprlcourse/</span><span class="co">]</span> 等材料。</span>
<span id="cb4-1003"><a href="#cb4-1003"></a></span>
<span id="cb4-1004"><a href="#cb4-1004"></a><span class="fu"># off-policy 设置下如何估计 KL 散度的梯度</span></span>
<span id="cb4-1005"><a href="#cb4-1005"></a></span>
<span id="cb4-1006"><a href="#cb4-1006"></a>上面的推导中，我们假设了 RL 是 on-policy 设置，即采样策略即为最新策略 $\pi_\theta$。</span>
<span id="cb4-1007"><a href="#cb4-1007"></a></span>
<span id="cb4-1008"><a href="#cb4-1008"></a>在这一节，我们进一步考虑 off-policy 设置，即一次采样获得样本会用于多次更新，除了第一次更新，采样策略 $\pi_{\theta_{\text{old}}}$ 与最新策略 $\pi_\theta$ 都会不同。off-policy 设置给 KL 散度优化带来的问题在于，我们需要优化最新策略 $\pi_\theta$ 的 KL 散度，但却没有来自 $p_{\theta}$ 的样本，这意味着我们无法直接使用梯度估计式 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-mc</span><span class="co">]</span>。</span>
<span id="cb4-1009"><a href="#cb4-1009"></a></span>
<span id="cb4-1010"><a href="#cb4-1010"></a><span class="fu">## 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</span></span>
<span id="cb4-1011"><a href="#cb4-1011"></a></span>
<span id="cb4-1012"><a href="#cb4-1012"></a>遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 $\pi_\theta \neq \pi_{\theta_{\text{old}}}$，尽管没有来自最新策略 $p_{\theta}$ 的样本，却仍然在使用基于 on-policy 设置的优化方式。</span>
<span id="cb4-1013"><a href="#cb4-1013"></a></span>
<span id="cb4-1014"><a href="#cb4-1014"></a><span class="fu">### TRL</span></span>
<span id="cb4-1015"><a href="#cb4-1015"></a></span>
<span id="cb4-1016"><a href="#cb4-1016"></a>TRL 在 <span class="co">[</span><span class="ot">@lst-trl-kl-reward</span><span class="co">]</span> 中计算 KL 样本值使用的 <span class="in">`logprobs`</span> 及其对应的轨迹样本均来自采样策略 $\pi_{\theta_{\text{old}}}$。对应代码可见 <span class="co">[</span><span class="ot">@lst-trl-sample-and-calc-old-logprob</span><span class="co">]</span>。</span>
<span id="cb4-1017"><a href="#cb4-1017"></a></span>
<span id="cb4-1018"><a href="#cb4-1018"></a><span class="in">```{#lst-trl-sample-and-calc-old-logprob .python lst-cap="TRL 使用采样样本并使用 $\pi_{\theta_{\text{old}}}$ 计算对数似然^[https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432]"}</span></span>
<span id="cb4-1019"><a href="#cb4-1019"></a><span class="in">queries = data["input_ids"].to(device)</span></span>
<span id="cb4-1020"><a href="#cb4-1020"></a><span class="in"># ...</span></span>
<span id="cb4-1021"><a href="#cb4-1021"></a></span>
<span id="cb4-1022"><a href="#cb4-1022"></a><span class="in">with unwrap_model_for_generation(</span></span>
<span id="cb4-1023"><a href="#cb4-1023"></a><span class="in">    self.model, #...</span></span>
<span id="cb4-1024"><a href="#cb4-1024"></a><span class="in">) as unwrapped_model:</span></span>
<span id="cb4-1025"><a href="#cb4-1025"></a><span class="in">    query_responses, logitss = batch_generation(</span></span>
<span id="cb4-1026"><a href="#cb4-1026"></a><span class="in">        unwrapped_model.policy,</span></span>
<span id="cb4-1027"><a href="#cb4-1027"></a><span class="in">        queries,</span></span>
<span id="cb4-1028"><a href="#cb4-1028"></a><span class="in">        # ...</span></span>
<span id="cb4-1029"><a href="#cb4-1029"></a><span class="in">    )</span></span>
<span id="cb4-1030"><a href="#cb4-1030"></a></span>
<span id="cb4-1031"><a href="#cb4-1031"></a></span>
<span id="cb4-1032"><a href="#cb4-1032"></a><span class="in">for i in range(0, queries.shape[0], args.local_rollout_forward_batch_size):</span></span>
<span id="cb4-1033"><a href="#cb4-1033"></a><span class="in">    # ...</span></span>
<span id="cb4-1034"><a href="#cb4-1034"></a><span class="in">    logits = logitss[i : i + args.local_rollout_forward_batch_size]</span></span>
<span id="cb4-1035"><a href="#cb4-1035"></a><span class="in">    logprob = selective_log_softmax(logits, response)</span></span>
<span id="cb4-1036"><a href="#cb4-1036"></a><span class="in">```</span></span>
<span id="cb4-1037"><a href="#cb4-1037"></a></span>
<span id="cb4-1038"><a href="#cb4-1038"></a>注意，基于 $\mathbf{\tau} \sim \pi_{\theta_{\text{old}}}$ 计算的 KL 样本值可以用于估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_{\theta_{\text{old}}} \mid \pi_{\text{ref}}\right]$，在第一次更新时，由于 $\pi_\theta = \pi_{\theta_{\text{old}}}$，所以也可以用于估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]$。但问题在于，从第二次更新开始，$\pi_\theta \neq \pi_{\theta_{\text{old}}}$，而我们仍然希望估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]$。</span>
<span id="cb4-1039"><a href="#cb4-1039"></a></span>
<span id="cb4-1040"><a href="#cb4-1040"></a>随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 $\pi_{\theta}$ 重新估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]$。对应代码可见 <span class="co">[</span><span class="ot">@lst-trl-ppo-update</span><span class="co">]</span>。</span>
<span id="cb4-1041"><a href="#cb4-1041"></a></span>
<span id="cb4-1042"><a href="#cb4-1042"></a><span class="in">```{#lst-trl-ppo-update .python lst-cap="TRL PPO 多轮更新"}</span></span>
<span id="cb4-1043"><a href="#cb4-1043"></a><span class="in"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span>
<span id="cb4-1044"><a href="#cb4-1044"></a><span class="in">for ppo_epoch_idx in range(args.num_ppo_epochs):</span></span>
<span id="cb4-1045"><a href="#cb4-1045"></a><span class="in">    b_inds = np.random.permutation(args.local_batch_size)</span></span>
<span id="cb4-1046"><a href="#cb4-1046"></a><span class="in">    minibatch_idx = 0</span></span>
<span id="cb4-1047"><a href="#cb4-1047"></a><span class="in">    for mini_batch_start in range(0, args.local_batch_size, args.local_mini_batch_size):</span></span>
<span id="cb4-1048"><a href="#cb4-1048"></a><span class="in">        mini_batch_end = mini_batch_start + args.local_mini_batch_size</span></span>
<span id="cb4-1049"><a href="#cb4-1049"></a><span class="in">        mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]</span></span>
<span id="cb4-1050"><a href="#cb4-1050"></a><span class="in">        gradient_accumulation_idx = 0</span></span>
<span id="cb4-1051"><a href="#cb4-1051"></a><span class="in">        for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):</span></span>
<span id="cb4-1052"><a href="#cb4-1052"></a><span class="in">            with accelerator.accumulate(model):</span></span>
<span id="cb4-1053"><a href="#cb4-1053"></a><span class="in">                micro_batch_end = micro_batch_start + args.per_device_train_batch_size</span></span>
<span id="cb4-1054"><a href="#cb4-1054"></a><span class="in">                micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]</span></span>
<span id="cb4-1055"><a href="#cb4-1055"></a><span class="in">                mb_advantage = advantages[micro_batch_inds]</span></span>
<span id="cb4-1056"><a href="#cb4-1056"></a><span class="in">                mb_responses = responses[micro_batch_inds]</span></span>
<span id="cb4-1057"><a href="#cb4-1057"></a><span class="in">                mb_query_responses = query_responses[micro_batch_inds]</span></span>
<span id="cb4-1058"><a href="#cb4-1058"></a><span class="in">                mb_logprobs = logprobs[micro_batch_inds]</span></span>
<span id="cb4-1059"><a href="#cb4-1059"></a><span class="in">                mb_return = returns[micro_batch_inds]</span></span>
<span id="cb4-1060"><a href="#cb4-1060"></a><span class="in">                mb_values = values[micro_batch_inds]</span></span>
<span id="cb4-1061"><a href="#cb4-1061"></a></span>
<span id="cb4-1062"><a href="#cb4-1062"></a></span>
<span id="cb4-1063"><a href="#cb4-1063"></a><span class="in">                output, vpred_temp = forward(model, mb_query_responses, processing_class.pad_token_id)</span></span>
<span id="cb4-1064"><a href="#cb4-1064"></a><span class="in">                logits = output.logits[:, context_length - 1 : -1]</span></span>
<span id="cb4-1065"><a href="#cb4-1065"></a><span class="in">                logits /= args.temperature + 1e-7</span></span>
<span id="cb4-1066"><a href="#cb4-1066"></a><span class="in">                new_logprobs = selective_log_softmax(logits, mb_responses)</span></span>
<span id="cb4-1067"><a href="#cb4-1067"></a><span class="in">                new_logprobs = torch.masked_fill(</span></span>
<span id="cb4-1068"><a href="#cb4-1068"></a><span class="in">                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB</span></span>
<span id="cb4-1069"><a href="#cb4-1069"></a><span class="in">                )</span></span>
<span id="cb4-1070"><a href="#cb4-1070"></a><span class="in">                vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)</span></span>
<span id="cb4-1071"><a href="#cb4-1071"></a><span class="in">                vpred = torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], 0)</span></span>
<span id="cb4-1072"><a href="#cb4-1072"></a><span class="in">                vpredclipped = torch.clamp(</span></span>
<span id="cb4-1073"><a href="#cb4-1073"></a><span class="in">                    vpred,</span></span>
<span id="cb4-1074"><a href="#cb4-1074"></a><span class="in">                    mb_values - args.cliprange_value,</span></span>
<span id="cb4-1075"><a href="#cb4-1075"></a><span class="in">                    mb_values + args.cliprange_value,</span></span>
<span id="cb4-1076"><a href="#cb4-1076"></a><span class="in">                )</span></span>
<span id="cb4-1077"><a href="#cb4-1077"></a><span class="in">                vf_losses1 = torch.square(vpred - mb_return)</span></span>
<span id="cb4-1078"><a href="#cb4-1078"></a><span class="in">                vf_losses2 = torch.square(vpredclipped - mb_return)</span></span>
<span id="cb4-1079"><a href="#cb4-1079"></a><span class="in">                vf_loss_max = torch.max(vf_losses1, vf_losses2)</span></span>
<span id="cb4-1080"><a href="#cb4-1080"></a><span class="in">                vf_loss = 0.5 * masked_mean(vf_loss_max, ~padding_mask_p1[micro_batch_inds])</span></span>
<span id="cb4-1081"><a href="#cb4-1081"></a><span class="in">                vf_clipfrac = masked_mean(</span></span>
<span id="cb4-1082"><a href="#cb4-1082"></a><span class="in">                    (vf_losses2 &gt; vf_losses1).float(), ~padding_mask_p1[micro_batch_inds]</span></span>
<span id="cb4-1083"><a href="#cb4-1083"></a><span class="in">                )</span></span>
<span id="cb4-1084"><a href="#cb4-1084"></a><span class="in">                logprobs_diff = new_logprobs - mb_logprobs</span></span>
<span id="cb4-1085"><a href="#cb4-1085"></a><span class="in">                ratio = torch.exp(logprobs_diff)</span></span>
<span id="cb4-1086"><a href="#cb4-1086"></a><span class="in">                pg_losses = -mb_advantage * ratio</span></span>
<span id="cb4-1087"><a href="#cb4-1087"></a><span class="in">                pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)</span></span>
<span id="cb4-1088"><a href="#cb4-1088"></a><span class="in">                pg_loss_max = torch.max(pg_losses, pg_losses2)</span></span>
<span id="cb4-1089"><a href="#cb4-1089"></a><span class="in">                pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])</span></span>
<span id="cb4-1090"><a href="#cb4-1090"></a><span class="in">                loss = pg_loss + args.vf_coef * vf_loss</span></span>
<span id="cb4-1091"><a href="#cb4-1091"></a><span class="in">                accelerator.backward(loss)</span></span>
<span id="cb4-1092"><a href="#cb4-1092"></a><span class="in">                optimizer.step()</span></span>
<span id="cb4-1093"><a href="#cb4-1093"></a><span class="in">                optimizer.zero_grad()</span></span>
<span id="cb4-1094"><a href="#cb4-1094"></a><span class="in">```</span></span>
<span id="cb4-1095"><a href="#cb4-1095"></a></span>
<span id="cb4-1096"><a href="#cb4-1096"></a><span class="fu">### OpenRLHF</span></span>
<span id="cb4-1097"><a href="#cb4-1097"></a></span>
<span id="cb4-1098"><a href="#cb4-1098"></a>类似地，OpenRLHF 在 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-estimator</span><span class="co">]</span> 中计算 KL 样本值使用的 <span class="in">`log_probs`</span> 在 <span class="in">`make_experience`</span> 时被计算，和对应的样本 <span class="in">`sequences`</span> 都来自采样策略 $\pi_{\theta_{\text{old}}}$，而非当前策略 $\pi_{\theta}$。对应代码可见 <span class="co">[</span><span class="ot">@lst-openrlhf-sample-and-calc-old-logprob</span><span class="co">]</span>。</span>
<span id="cb4-1099"><a href="#cb4-1099"></a></span>
<span id="cb4-1100"><a href="#cb4-1100"></a><span class="in">```{#lst-openrlhf-sample-and-calc-old-logprob .python lst-cap="OpenRLHF 采样样本并使用 $\pi_{\theta_{\text{old}}}$ 计算对数似然"}</span></span>
<span id="cb4-1101"><a href="#cb4-1101"></a><span class="in"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595</span></span>
<span id="cb4-1102"><a href="#cb4-1102"></a><span class="in">def make_experience(self, samples: Samples) -&gt; Experience:</span></span>
<span id="cb4-1103"><a href="#cb4-1103"></a><span class="in">    """</span></span>
<span id="cb4-1104"><a href="#cb4-1104"></a><span class="in">    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.</span></span>
<span id="cb4-1105"><a href="#cb4-1105"></a><span class="in">    """</span></span>
<span id="cb4-1106"><a href="#cb4-1106"></a><span class="in">    # ...</span></span>
<span id="cb4-1107"><a href="#cb4-1107"></a><span class="in">    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680</span></span>
<span id="cb4-1108"><a href="#cb4-1108"></a><span class="in">    action_log_probs = self.actor(</span></span>
<span id="cb4-1109"><a href="#cb4-1109"></a><span class="in">        sequences,</span></span>
<span id="cb4-1110"><a href="#cb4-1110"></a><span class="in">        num_actions,</span></span>
<span id="cb4-1111"><a href="#cb4-1111"></a><span class="in">        # ...</span></span>
<span id="cb4-1112"><a href="#cb4-1112"></a><span class="in">    )</span></span>
<span id="cb4-1113"><a href="#cb4-1113"></a><span class="in">    # ...</span></span>
<span id="cb4-1114"><a href="#cb4-1114"></a><span class="in">    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709</span></span>
<span id="cb4-1115"><a href="#cb4-1115"></a><span class="in">    kl = compute_approx_kl(</span></span>
<span id="cb4-1116"><a href="#cb4-1116"></a><span class="in">        action_log_probs,</span></span>
<span id="cb4-1117"><a href="#cb4-1117"></a><span class="in">        base_action_log_probs,</span></span>
<span id="cb4-1118"><a href="#cb4-1118"></a><span class="in">        # ...</span></span>
<span id="cb4-1119"><a href="#cb4-1119"></a><span class="in">    )</span></span>
<span id="cb4-1120"><a href="#cb4-1120"></a><span class="in">```</span></span>
<span id="cb4-1121"><a href="#cb4-1121"></a></span>
<span id="cb4-1122"><a href="#cb4-1122"></a>从 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-loss</span><span class="co">]</span> 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 $\pi_{\theta_{\text{old}}}$ 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 $\pi_{\theta}$ 计算的对数似然，但如 <span class="co">[</span><span class="ot">@sec-kl-loss-impl</span><span class="co">]</span> 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。</span>
<span id="cb4-1123"><a href="#cb4-1123"></a></span>
<span id="cb4-1124"><a href="#cb4-1124"></a></span>
<span id="cb4-1125"><a href="#cb4-1125"></a><span class="fu">### verl</span></span>
<span id="cb4-1126"><a href="#cb4-1126"></a></span>
<span id="cb4-1127"><a href="#cb4-1127"></a>从 <span class="co">[</span><span class="ot">@lst-verl-kl-reward</span><span class="co">]</span> 可见，verl 同样使用 $\pi_{\theta_{\text{old}}}$ 计算 KL 样本值。</span>
<span id="cb4-1128"><a href="#cb4-1128"></a></span>
<span id="cb4-1129"><a href="#cb4-1129"></a>从 <span class="co">[</span><span class="ot">@lst-verl-kl-loss</span><span class="co">]</span> 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 $\pi_{\theta_{\text{old}}}$ 的 KL 样本值。</span>
<span id="cb4-1130"><a href="#cb4-1130"></a></span>
<span id="cb4-1131"><a href="#cb4-1131"></a><span class="fu">## 利用重要性采样处理 off-policy 设置</span></span>
<span id="cb4-1132"><a href="#cb4-1132"></a></span>
<span id="cb4-1133"><a href="#cb4-1133"></a>off-policy 设置下，我们没有来自最新策略 $\pi_{\theta}$ 的样本，而只能使用来自采样策略 $\pi_{\theta_{\text{old}}}$ 的样本，但我们仍然希望估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right]$。</span>
<span id="cb4-1134"><a href="#cb4-1134"></a></span>
<span id="cb4-1135"><a href="#cb4-1135"></a>熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即</span>
<span id="cb4-1136"><a href="#cb4-1136"></a></span>
<span id="cb4-1137"><a href="#cb4-1137"></a>$$</span>
<span id="cb4-1138"><a href="#cb4-1138"></a>\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[f(\mathbf{\tau})\right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) f(\tau)  = \sum_{\tau \in \mathcal{T}} p_{\theta_{\text{old}}}(\tau) \frac{p_{\theta}(\tau)}{p_{\theta_{\text{old}}}(\tau)} f(\tau) = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}} \left[\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} f(\mathbf{\tau})\right]</span>
<span id="cb4-1139"><a href="#cb4-1139"></a>$$ {#eq-is-off-policy-kl}</span>
<span id="cb4-1140"><a href="#cb4-1140"></a></span>
<span id="cb4-1141"><a href="#cb4-1141"></a>此处，重要性采样系数 $\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})}$ 可以仿照 <span class="co">[</span><span class="ot">@eq-dp-expansion</span><span class="co">]</span> 展开为：</span>
<span id="cb4-1142"><a href="#cb4-1142"></a></span>
<span id="cb4-1143"><a href="#cb4-1143"></a>$$</span>
<span id="cb4-1144"><a href="#cb4-1144"></a>\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} = \prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{\pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}</span>
<span id="cb4-1145"><a href="#cb4-1145"></a>$$ {#eq-is-coef-expansion}</span>
<span id="cb4-1146"><a href="#cb4-1146"></a>^<span class="co">[</span><span class="ot">实际计算中，[@eq-is-coef-expansion] 由于涉及到 $|\mathbf{\tau}|$ 次连乘，方差大且数值稳定性差，需要利用因果性、近似等技术来化简。本文目前省略该部分，后续将会更新相关内容。</span><span class="co">]</span></span>
<span id="cb4-1147"><a href="#cb4-1147"></a></span>
<span id="cb4-1148"><a href="#cb4-1148"></a>利用重要性采样 (@eq-is-off-policy-kl, @eq-is-coef-expansion) ，KL 梯度表达式 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce</span><span class="co">]</span> 可以转化为：</span>
<span id="cb4-1149"><a href="#cb4-1149"></a></span>
<span id="cb4-1150"><a href="#cb4-1150"></a>$$</span>
<span id="cb4-1151"><a href="#cb4-1151"></a>\begin{aligned}</span>
<span id="cb4-1152"><a href="#cb4-1152"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb4-1153"><a href="#cb4-1153"></a>=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right] <span class="sc">\\</span></span>
<span id="cb4-1154"><a href="#cb4-1154"></a>=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \frac{p_{\theta}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}{p_{\theta_{\text{old}}}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}  \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})  \right] <span class="sc">\\</span></span>
<span id="cb4-1155"><a href="#cb4-1155"></a>=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \left(\prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}\right) \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right]</span>
<span id="cb4-1156"><a href="#cb4-1156"></a>\end{aligned}</span>
<span id="cb4-1157"><a href="#cb4-1157"></a>$$ {#eq-def-kl-grad-kt-reduce-is}</span>
<span id="cb4-1158"><a href="#cb4-1158"></a></span>
<span id="cb4-1159"><a href="#cb4-1159"></a></span>
<span id="cb4-1160"><a href="#cb4-1160"></a>对应的 Monte Carlo 估计式为：</span>
<span id="cb4-1161"><a href="#cb4-1161"></a></span>
<span id="cb4-1162"><a href="#cb4-1162"></a>$$</span>
<span id="cb4-1163"><a href="#cb4-1163"></a>\begin{aligned}</span>
<span id="cb4-1164"><a href="#cb4-1164"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb4-1165"><a href="#cb4-1165"></a>\approx&amp; \frac{1}{N} \sum_{i=1}^{N} \left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) <span class="sc">\\</span></span>
<span id="cb4-1166"><a href="#cb4-1166"></a>=&amp; \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})</span>
<span id="cb4-1167"><a href="#cb4-1167"></a>\end{aligned}</span>
<span id="cb4-1168"><a href="#cb4-1168"></a>$$ {#eq-def-kl-grad-kt-reduce-is-mc}</span>
<span id="cb4-1169"><a href="#cb4-1169"></a></span>
<span id="cb4-1170"><a href="#cb4-1170"></a>对应的 loss 函数为：</span>
<span id="cb4-1171"><a href="#cb4-1171"></a></span>
<span id="cb4-1172"><a href="#cb4-1172"></a>$$</span>
<span id="cb4-1173"><a href="#cb4-1173"></a>\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_{i}|} \text{nograd}\left(\left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right)\sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})</span>
<span id="cb4-1174"><a href="#cb4-1174"></a>$$ {#eq-def-kl-grad-kt-reduce-is-mc-loss}</span>
<span id="cb4-1175"><a href="#cb4-1175"></a></span>
<span id="cb4-1176"><a href="#cb4-1176"></a></span>
<span id="cb4-1177"><a href="#cb4-1177"></a>类似 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-k</span><span class="co">]</span>，我们可以令 </span>
<span id="cb4-1178"><a href="#cb4-1178"></a></span>
<span id="cb4-1179"><a href="#cb4-1179"></a>$$</span>
<span id="cb4-1180"><a href="#cb4-1180"></a>k(\mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t}) = \left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}</span>
<span id="cb4-1181"><a href="#cb4-1181"></a>$$ {#eq-def-kl-reward-is}</span>
<span id="cb4-1182"><a href="#cb4-1182"></a></span>
<span id="cb4-1183"><a href="#cb4-1183"></a>注意，<span class="co">[</span><span class="ot">@eq-def-kl-reward-is</span><span class="co">]</span> 中的 $k$ 需要对于每个新的 $\pi_{\theta}$ 重新计算。</span>
<span id="cb4-1184"><a href="#cb4-1184"></a></span>
<span id="cb4-1185"><a href="#cb4-1185"></a><span class="fu"># 结论：如何正确地在 RL 中优化 KL 散度</span></span>
<span id="cb4-1186"><a href="#cb4-1186"></a></span>
<span id="cb4-1187"><a href="#cb4-1187"></a><span class="fu">## 修正 GRPO 公式中的 KL 项</span></span>
<span id="cb4-1188"><a href="#cb4-1188"></a></span>
<span id="cb4-1189"><a href="#cb4-1189"></a>GRPO 公式 (@eq-grpo-obj, @eq-grpo-obj-kl-term) 对于 KL 优化主要存在两个错误：</span>
<span id="cb4-1190"><a href="#cb4-1190"></a></span>
<span id="cb4-1191"><a href="#cb4-1191"></a><span class="ss">1. </span>忽略了 KL 优化的 off-policy 问题</span>
<span id="cb4-1192"><a href="#cb4-1192"></a><span class="ss">2. </span>先将 $k_{3}$ 估计样本量应用于动作条件似然再求和，导致得到异常的梯度</span>
<span id="cb4-1193"><a href="#cb4-1193"></a></span>
<span id="cb4-1194"><a href="#cb4-1194"></a>对于这两个问题，在 <span class="co">[</span><span class="ot">@eq-grpo-obj-kl-fixed</span><span class="co">]</span> 的基础上，仿照 <span class="co">[</span><span class="ot">@eq-def-kl-reward-is</span><span class="co">]</span>，我们可以按如下方式修正：</span>
<span id="cb4-1195"><a href="#cb4-1195"></a></span>
<span id="cb4-1196"><a href="#cb4-1196"></a>$$</span>
<span id="cb4-1197"><a href="#cb4-1197"></a>\begin{aligned}</span>
<span id="cb4-1198"><a href="#cb4-1198"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb4-1199"><a href="#cb4-1199"></a>&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \left<span class="sc">\{</span> \sum_{t=1}^{\left|o_i\right|} \min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old}}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span> \right<span class="sc">\}</span> -\beta \left(\prod_{t=1}^{|o_{i}|}\frac{\pi_{\theta}(o_{i, t} | q, o_{i,\lt t})}{ \pi_{\theta_{\text{old}}}(o_{i, t} | q, o_{i,\lt t})}\right) k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)</span>
<span id="cb4-1200"><a href="#cb4-1200"></a>\end{aligned}</span>
<span id="cb4-1201"><a href="#cb4-1201"></a>$$ {#eq-grpo-obj-kl-fixed-is}</span>
<span id="cb4-1202"><a href="#cb4-1202"></a></span>
<span id="cb4-1203"><a href="#cb4-1203"></a><span class="fu">## 修正流行 LLM RL 框架中的 KL 优化实现</span></span>
<span id="cb4-1204"><a href="#cb4-1204"></a></span>
<span id="cb4-1205"><a href="#cb4-1205"></a>目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：</span>
<span id="cb4-1206"><a href="#cb4-1206"></a></span>
<span id="cb4-1207"><a href="#cb4-1207"></a><span class="ss">1. </span>实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）</span>
<span id="cb4-1208"><a href="#cb4-1208"></a><span class="ss">2. </span>错误地实现了平均操作</span>
<span id="cb4-1209"><a href="#cb4-1209"></a></span>
<span id="cb4-1210"><a href="#cb4-1210"></a>对于这些问题，可以按照如下思路修正：</span>
<span id="cb4-1211"><a href="#cb4-1211"></a></span>
<span id="cb4-1212"><a href="#cb4-1212"></a><span class="ss">1. </span>为 KL 项添加重要性采样，这需要从第二轮更新开始，每次基于新的 $\pi_\theta$ 重新计算 KL loss / reward 项，包括重要性采样系数</span>
<span id="cb4-1213"><a href="#cb4-1213"></a><span class="ss">2. </span>应用 KL 估计样本量时，先对于序列内 token 间的对数条件似然求和，得到轨迹联合概率，再代入公式</span>
<span id="cb4-1214"><a href="#cb4-1214"></a><span class="ss">3. </span>如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 <span class="co">[</span><span class="ot">@eq-def-kl-reward-is</span><span class="co">]</span> 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）</span>
<span id="cb4-1215"><a href="#cb4-1215"></a><span class="ss">4. </span>如果不希望应用 reward 优化的其他技术，可以按 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-is-mc-loss</span><span class="co">]</span> 实现为 KL loss 项</span>
<span id="cb4-1216"><a href="#cb4-1216"></a></span>
<span id="cb4-1217"><a href="#cb4-1217"></a><span class="fu"># 讨论</span></span>
<span id="cb4-1218"><a href="#cb4-1218"></a></span>
<span id="cb4-1219"><a href="#cb4-1219"></a><span class="fu">## 对于 KL 梯度更好的估计样本量</span></span>
<span id="cb4-1220"><a href="#cb4-1220"></a></span>
<span id="cb4-1221"><a href="#cb4-1221"></a>如 <span class="co">[</span><span class="ot">@sec-kl-grad-as-kl-reward</span><span class="co">]</span> 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？</span>
<span id="cb4-1222"><a href="#cb4-1222"></a></span>
<span id="cb4-1223"><a href="#cb4-1223"></a>此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？</span>
<span id="cb4-1224"><a href="#cb4-1224"></a></span>
<span id="cb4-1225"><a href="#cb4-1225"></a><span class="fu">## KL-Regularized RL 的理论优势</span></span>
<span id="cb4-1226"><a href="#cb4-1226"></a></span>
<span id="cb4-1227"><a href="#cb4-1227"></a>最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。</span>
<span id="cb4-1228"><a href="#cb4-1228"></a></span>
<span id="cb4-1229"><a href="#cb4-1229"></a>然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 @zhao2025logregretkl 证明了 KL-regularized RL 的 regret 只有 $\mathcal{O}(\log T)$，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 $\mathcal{O}(\sqrt{T})$。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。</span>
<span id="cb4-1230"><a href="#cb4-1230"></a></span>
<span id="cb4-1231"><a href="#cb4-1231"></a><span class="fu"># 附录 {.appendix}</span></span>
<span id="cb4-1232"><a href="#cb4-1232"></a></span>
<span id="cb4-1233"><a href="#cb4-1233"></a>::: {.callout-tip}</span>
<span id="cb4-1234"><a href="#cb4-1234"></a></span>
<span id="cb4-1235"><a href="#cb4-1235"></a>本文的作者（童雨轩）仍在寻求北美的 Ph.D. 或 RA 机会。如果你觉得本文对你有帮助，欢迎浏览其主页^<span class="co">[</span><span class="ot">https://tongyx361.github.io</span><span class="co">]</span>来获取进一步了解。</span>
<span id="cb4-1236"><a href="#cb4-1236"></a></span>
<span id="cb4-1237"><a href="#cb4-1237"></a>:::</span>
<span id="cb4-1238"><a href="#cb4-1238"></a></span>
<span id="cb4-1239"><a href="#cb4-1239"></a><span class="fu">## 相关工作 {.appendix}</span></span>
<span id="cb4-1240"><a href="#cb4-1240"></a></span>
<span id="cb4-1241"><a href="#cb4-1241"></a>与本文同期也有许多精彩的讨论，由于笔者还没能通读全文，此处仅提供链接，不作概括，欢迎感兴趣的读者自行阅读：</span>
<span id="cb4-1242"><a href="#cb4-1242"></a></span>
<span id="cb4-1243"><a href="#cb4-1243"></a><span class="ss">- </span><span class="co">[</span><span class="ot">GRPO 中的 KL Loss 实现细节问题 - Hongyu Zang @ 知乎</span><span class="co">](https://zhuanlan.zhihu.com/p/28440962040)</span></span>
<span id="cb4-1244"><a href="#cb4-1244"></a><span class="ss">- </span><span class="co">[</span><span class="ot">k2 loss就是比k3 loss好！以及GRPO_off-policy - Yiming Liu @ 知乎</span><span class="co">](https://zhuanlan.zhihu.com/p/28735759256)</span></span>
<span id="cb4-1245"><a href="#cb4-1245"></a></span>
<span id="cb4-1246"><a href="#cb4-1246"></a><span class="fu">## 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？” {.appendix}</span></span>
<span id="cb4-1247"><a href="#cb4-1247"></a></span>
<span id="cb4-1248"><a href="#cb4-1248"></a>笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题^<span class="co">[</span><span class="ot">https://x.com/pufanyi/status/1888845956684370202</span><span class="co">]</span>：</span>
<span id="cb4-1249"><a href="#cb4-1249"></a></span>
<span id="cb4-1250"><a href="#cb4-1250"></a><span class="at">&gt; A small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?</span></span>
<span id="cb4-1251"><a href="#cb4-1251"></a><span class="at">&gt;</span></span>
<span id="cb4-1252"><a href="#cb4-1252"></a><span class="at">&gt; TRPO </span><span class="co">[</span><span class="ot">@schulman2015trpo</span><span class="co">]</span></span>
<span id="cb4-1253"><a href="#cb4-1253"></a></span>
<span id="cb4-1254"><a href="#cb4-1254"></a>$$</span>
<span id="cb4-1255"><a href="#cb4-1255"></a>\begin{aligned}</span>
<span id="cb4-1256"><a href="#cb4-1256"></a>&amp; \underset{\theta}{\text{maximize}}~L_{\theta_{\text {old }}}(\theta) <span class="sc">\\</span></span>
<span id="cb4-1257"><a href="#cb4-1257"></a>&amp; \text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta</span>
<span id="cb4-1258"><a href="#cb4-1258"></a>\end{aligned}</span>
<span id="cb4-1259"><a href="#cb4-1259"></a>$$ {#eq-trpo}</span>
<span id="cb4-1260"><a href="#cb4-1260"></a></span>
<span id="cb4-1261"><a href="#cb4-1261"></a><span class="at">&gt; PPO </span><span class="co">[</span><span class="ot">@schulman2017ppo</span><span class="co">]</span></span>
<span id="cb4-1262"><a href="#cb4-1262"></a></span>
<span id="cb4-1263"><a href="#cb4-1263"></a>$$ </span>
<span id="cb4-1264"><a href="#cb4-1264"></a>L^{K L P E N}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(\mathbf{y}_t \mid \mathbf{x}_t\right)}{\pi_{\theta_{\text {old }}}\left(\mathbf{y}_t \mid \mathbf{x}_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid \mathbf{x}_t\right), \pi_\theta\left(\cdot \mid \mathbf{x}_t\right)\right]\right]</span>
<span id="cb4-1265"><a href="#cb4-1265"></a>$$ {#eq-ppo-klpen}</span>
<span id="cb4-1266"><a href="#cb4-1266"></a></span>
<span id="cb4-1267"><a href="#cb4-1267"></a><span class="at">&gt; GRPO </span><span class="co">[</span><span class="ot">@shao2024deepseekmath</span><span class="co">]</span></span>
<span id="cb4-1268"><a href="#cb4-1268"></a></span>
<span id="cb4-1269"><a href="#cb4-1269"></a>$$</span>
<span id="cb4-1270"><a href="#cb4-1270"></a>\begin{aligned}</span>
<span id="cb4-1271"><a href="#cb4-1271"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb4-1272"><a href="#cb4-1272"></a>&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left<span class="sc">\{</span>\min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span>-\beta \mathbb{D}_{K L}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\right<span class="sc">\}</span></span>
<span id="cb4-1273"><a href="#cb4-1273"></a>\end{aligned}</span>
<span id="cb4-1274"><a href="#cb4-1274"></a>$$ {#eq-grpo}</span>
<span id="cb4-1275"><a href="#cb4-1275"></a></span>
<span id="cb4-1276"><a href="#cb4-1276"></a>这个问题本身的答案是非常简单的。</span>
<span id="cb4-1277"><a href="#cb4-1277"></a></span>
<span id="cb4-1278"><a href="#cb4-1278"></a>首先，这个问题混淆了两种不同的 KL 惩罚项：</span>
<span id="cb4-1279"><a href="#cb4-1279"></a></span>
<span id="cb4-1280"><a href="#cb4-1280"></a><span class="ss">1. </span>$\text{KL}<span class="co">[</span><span class="ot">\pi_{\theta_{\text{old}}},\pi_{\theta}</span><span class="co">]</span>$，其作用是约束最新策略 $\pi_{\theta}$不要离采样策略$\pi_{\theta_{\text{old}}}$ 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。</span>
<span id="cb4-1281"><a href="#cb4-1281"></a><span class="ss">2. </span>$\text{KL}<span class="co">[</span><span class="ot">\pi_{\theta},\pi_{\theta_{\text{ref}}}</span><span class="co">]</span>$，其作用是约束最新策略 $\pi_{\theta}$不要离参考策略$\pi_{\theta_{\text{ref}}}$ 太远，从而更充分地利用参考策略中的先验。</span>
<span id="cb4-1282"><a href="#cb4-1282"></a></span>
<span id="cb4-1283"><a href="#cb4-1283"></a>另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 $\text{KL}<span class="co">[</span><span class="ot">\pi_{\theta_{\text{old}}},\pi_{\theta}</span><span class="co">]</span>$。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：</span>
<span id="cb4-1284"><a href="#cb4-1284"></a></span>
<span id="cb4-1285"><a href="#cb4-1285"></a><span class="at">&gt; Let $r_t(\theta)$ denote the probability ratio $r_{t}(\theta)=\frac{\pi_{\theta}\left(a_t \mid s_t\right)}{\left(\pi_{\theta_{\text {old }}}\left|a_t\right| s_t\right)}$, so $r\left(\theta_{\text{old}}\right)=1$. TRPO maximizes a "surrogate" objective</span></span>
<span id="cb4-1286"><a href="#cb4-1286"></a></span>
<span id="cb4-1287"><a href="#cb4-1287"></a>$$</span>
<span id="cb4-1288"><a href="#cb4-1288"></a>L^{\text{CPI}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t\right]=\hat{\mathbb{E}}_t\left<span class="co">[</span><span class="ot">r_t(\theta) \hat{A}_t\right</span><span class="co">]</span> .</span>
<span id="cb4-1289"><a href="#cb4-1289"></a>$$</span>
<span id="cb4-1290"><a href="#cb4-1290"></a></span>
<span id="cb4-1291"><a href="#cb4-1291"></a><span class="at">&gt; ...</span></span>
<span id="cb4-1292"><a href="#cb4-1292"></a><span class="at">&gt;</span></span>
<span id="cb4-1293"><a href="#cb4-1293"></a><span class="at">&gt; The main objective we propose is the following:</span></span>
<span id="cb4-1294"><a href="#cb4-1294"></a></span>
<span id="cb4-1295"><a href="#cb4-1295"></a>$$</span>
<span id="cb4-1296"><a href="#cb4-1296"></a>L^{\text{CLIP}}(\theta)=\hat{\mathbb{E}}_t\left<span class="co">[</span><span class="ot">\min \left(r_t(\theta) \hat{A}_t, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right</span><span class="co">]</span></span>
<span id="cb4-1297"><a href="#cb4-1297"></a>$$</span>
<span id="cb4-1298"><a href="#cb4-1298"></a></span>
<span id="cb4-1299"><a href="#cb4-1299"></a><span class="at">&gt; where epsilon is a hyperparameter, say, $\epsilon=0.2$. The motivation for this objective is as follows. The first term inside the $\min$ is $L^{\text{CPI}}$. The second term, $\text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t$, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving $r_t$ outside of the interval $</span><span class="co">[</span><span class="ot">1-\epsilon, 1+\epsilon</span><span class="co">]</span><span class="at">$.</span></span>
<span id="cb4-1300"><a href="#cb4-1300"></a><span class="at">&gt;</span></span>
<span id="cb4-1301"><a href="#cb4-1301"></a><span class="at">&gt; ...</span></span>
<span id="cb4-1302"><a href="#cb4-1302"></a><span class="at">&gt;</span></span>
<span id="cb4-1303"><a href="#cb4-1303"></a><span class="at">&gt; **Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence**, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence $d_{\text{targ}}$ each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we've included it here because it's an important baseline.</span></span>
<span id="cb4-1304"><a href="#cb4-1304"></a><span class="at">&gt;</span></span>
<span id="cb4-1305"><a href="#cb4-1305"></a><span class="at">&gt; In the simplest instantiation of this algorithm, we perform the following steps in each policy update:</span></span>
<span id="cb4-1306"><a href="#cb4-1306"></a><span class="at">&gt;</span></span>
<span id="cb4-1307"><a href="#cb4-1307"></a><span class="at">&gt; - Using several epochs of minibatch SGD, optimize the KL-penalized objective</span></span>
<span id="cb4-1308"><a href="#cb4-1308"></a></span>
<span id="cb4-1309"><a href="#cb4-1309"></a>$$</span>
<span id="cb4-1310"><a href="#cb4-1310"></a>L^{\text{KLPEN}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid s_t\right)\right]\right]</span>
<span id="cb4-1311"><a href="#cb4-1311"></a>$$</span>
<span id="cb4-1312"><a href="#cb4-1312"></a></span>
<span id="cb4-1313"><a href="#cb4-1313"></a><span class="at">&gt; </span></span>
<span id="cb4-1314"><a href="#cb4-1314"></a></span>
<span id="cb4-1315"><a href="#cb4-1315"></a>顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 $r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}$就是$K L\left<span class="co">[</span><span class="ot">\pi_{\theta_{d d}}, \pi_\theta\right</span><span class="co">]</span>=\mathbb{E}_{a_t \sim \pi_{\theta_{d t}}\left(\cdot \mid s_t\right)}\left[\log \frac{\pi_{\theta_{d t}}\left(a_t \mid s_t\right)}{\pi_\theta\left(a_t \mid s_t\right)}\right]$ 中对单个样本 $(s_t, a_t)$ 的值中 $\log$ 的真数。</span>
<span id="cb4-1316"><a href="#cb4-1316"></a></span>
<span id="cb4-1317"><a href="#cb4-1317"></a><span class="fu">## 致谢 {.appendix}</span></span>
<span id="cb4-1318"><a href="#cb4-1318"></a></span>
<span id="cb4-1319"><a href="#cb4-1319"></a>感谢王浩然、YuMS 对本文提供的重要反馈。</span>
<span id="cb4-1320"><a href="#cb4-1320"></a></span>
<span id="cb4-1321"><a href="#cb4-1321"></a>感谢生广明、Wei Xiong、刘仁彪、刘威、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论以及对于本文的有益反馈。</span>
<span id="cb4-1322"><a href="#cb4-1322"></a></span>
<span id="cb4-1323"><a href="#cb4-1323"></a>感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。</span>
<span id="cb4-1324"><a href="#cb4-1324"></a></span>
<span id="cb4-1325"><a href="#cb4-1325"></a><span class="fu">## 引用 {.appendix}</span></span>
<span id="cb4-1326"><a href="#cb4-1326"></a></span>
<span id="cb4-1327"><a href="#cb4-1327"></a>BibTeX:</span>
<span id="cb4-1328"><a href="#cb4-1328"></a></span>
<span id="cb4-1329"><a href="#cb4-1329"></a><span class="in">```BibTeX</span></span>
<span id="cb4-1330"><a href="#cb4-1330"></a><span class="in">@online{tong2025kl,</span></span>
<span id="cb4-1331"><a href="#cb4-1331"></a><span class="in">  author = {童雨轩},</span></span>
<span id="cb4-1332"><a href="#cb4-1332"></a><span class="in">  title = {重新思考 {RL} 中的 {KL} 梯度优化},</span></span>
<span id="cb4-1333"><a href="#cb4-1333"></a><span class="in">  year = {2025},</span></span>
<span id="cb4-1334"><a href="#cb4-1334"></a><span class="in">  url = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},</span></span>
<span id="cb4-1335"><a href="#cb4-1335"></a><span class="in">  urldate = {2025-03-09},</span></span>
<span id="cb4-1336"><a href="#cb4-1336"></a><span class="in">  language = {Chinese},</span></span>
<span id="cb4-1337"><a href="#cb4-1337"></a><span class="in">}</span></span>
<span id="cb4-1338"><a href="#cb4-1338"></a><span class="in">```</span></span>
<span id="cb4-1339"><a href="#cb4-1339"></a></span>
<span id="cb4-1340"><a href="#cb4-1340"></a>文本：</span>
<span id="cb4-1341"><a href="#cb4-1341"></a></span>
<span id="cb4-1342"><a href="#cb4-1342"></a><span class="in">```text</span></span>
<span id="cb4-1343"><a href="#cb4-1343"></a><span class="in">童雨轩. 2025. “重新思考 RL 中的 KL 梯度优化.” https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh.</span></span>
<span id="cb4-1344"><a href="#cb4-1344"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>