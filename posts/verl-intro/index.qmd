---
title: "verl: Flexible and Efficient RL for LLMs"
author:
  name: "Yuxuan Tong (童雨轩)"
  affiliation: "ByteDance Seed & Tsinghua University"
  email: tongyuxuan.361@bytedance.com
date: "2025-05-24"
format:
  revealjs:
    logo: ./assets/logo-bytedance-seed.png
    footer: "verl: Flexible and Efficient RL for LLMs"
    incremental: true
    embed-resources: true
    code-line-numbers: true
toc: false
---

# Background

## Modelling RL as Dataflow Graph {.smaller}

![](./assets/rl-dataflow-examples.png){fig-align="center" height=300 .lightbox}

Reinforcement Learning (RL) for Large Language Models (LLMs) can typically be modeled as a **dataflow graph**, consisting of:

1. **multiple models**: actor, critic, reference, reward model, etc.
2. **multiple stages**: generating, preparing experiences, training
3. **multiple workloads**: generation, inference, training

## Implementing Dataflow Graph as Execution Pattern

In practice, we should implement the dataflow graph as **execution pattern on a GPU cluster** with restrictions.

![](./assets/implement-ppo-w-device-placement.png){fig-align="center" height=400 .lightbox}

::: {.notes}

Specifically, we:

1. design the parallelism strategy and model placement to optimize the throughput
2. while restricted by the temporal dependencies and device resources

:::

# Paradigm: HybridFlow

## Background: Single-Controller vs. Multi-Controller {.smaller}

- Single-Controller (MPMD): A centralized controller manages all the workers.
- Multi-Controller (SPMD): Each worker computes on its own and communicates with each other.

. . .

::: {#fig-controller-paradigm layout-ncol=2}

![Single-Controller (MPMD)](./assets/single-controller-mpmd-pathways.png){height=200px .lightbox #fig-single-controller-mpmd}

![Multi-Controller (SPMD)](./assets/multi-controller-spmd-pathways.png){height=200px .lightbox #fig-multi-controller-spmd}

Single-Controller (Multi-Program-Multi-Data, MPMD) vs. Multi-Controller (Single-Program-Multi-Data, SPMD)

:::

---

- Single-Controller (MPMD) is flexible but suffers from communication overhead.
- Multi-Controller (SPMD) is efficient but is complex for programming

| Paradigm                 | Pro       | Con                   |
|--------------------------|-----------|-----------------------|
| Single-Controller | Flexible  | Commnucation Overhead |
| Multi-Controller  | Efficient | Complex Programming   |

## Paradigm: Hybrid-Controller

verl introduces hybrid-controller, where a single-controller manages multiple multi-controllers.

![](./assets/hybrid-controller.png){fig-align="center" height=450px .lightbox}

::: {.notes}

| Paradigm                 | Pro       | Con                   | Task             | Example Class |
|--------------------------|-----------|-----------------------|-----------------|----------------|
| Single-Controller (MPMD) | Flexible  | Commnucation Overhead | RL control flow | `RayPPOTrainer` |
| Multi-Controller (SPMD)  | Efficient | Complex Programming   | Distributed computation | `ActorRolloutWorker`, `CriticWorker` |

: Hybrid-controller utilizes different paradigms for different tasks.

"multi-controller", i.e., SPMD (Single Program Multiple Data), is the most popular programming paradigm in distributed computing, e.g., PyTorch DDP, FSDP, DeepSpeed, Megatron, etc.

- It can be understood as multiple processes run with `torchrun`, each of which runs the same program but processes different data, reducing the communication overhead.

:::

## Flexibility: Single-Controller {.smaller}

Single-controller enables verl to implement various RL algorithms by only modifying **a few lines**, usually only in **the `fit` function**.

. . .

:::: {.columns}

::: {.column width="49%"}
```{#lst-verl-fit-example-code-ppo .python lst-cap="PPO example code." code-line-numbers="7-8"}
for prompts in dataloader:
    # Stage 1: Sampling Trajectories
    batch = actor.generate_sequences(prompts)
    # Stage 2: Preparing Experiences
    batch = reward.compute_reward(batch)
    batch = reference.compute_log_prob(batch)
    batch = critic.compute_values(batch)
    batch = compute_advantage(batch, "gae")
    # Stage 3: Training
    critic.update_critic(batch)
    actor.update_actor(batch)
```
:::

::: {.column width="49%"}
```{#lst-verl-fit-example-code-grpo .python lst-cap="GRPO example code." code-line-numbers="7"}
for prompts in dataloader:
    # Stage 1: Sampling Trajectories
    batch = actor.generate_sequences(prompts)
    # Stage 2: Preparing Experiences
    batch = reward.compute_reward(batch)
    batch = reference.compute_log_prob(batch)
    batch = compute_advantage(batch, "grpo")
    # Stage 3: Training
    critic.update_critic(batch)
    actor.update_actor(batch)
```
:::

::::

. . .

With such flexibility, verl has supported diverse RL algorithms including [PPO](https://github.com/volcengine/verl/blob/main/examples/ppo_trainer), [GRPO](https://github.com/volcengine/verl/blob/main/examples/grpo_trainer), [RLOO](https://github.com/volcengine/verl/blob/main/examples/rloo_trainer), [ReMax](https://github.com/volcengine/verl/blob/main/examples/remax_trainer), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [PRIME](https://github.com/volcengine/verl/blob/main/recipe/prime), [DAPO](https://github.com/volcengine/verl/blob/main/recipe/dapo), [Dr. GRPO](https://github.com/volcengine/verl/blob/main/recipe/drgrpo), etc.

## Efficiency: Mutli-Controller {.center}

---

### Inter-Stage: Hybrid Engine

The optimal execution pattern for different workloads, e.g., training, generation, are usually different.

- Instead of **splitting the devices** to deploy different engines separately for different workloads, causing many bubbles,

- verl implements a **hybrid engine** that can switch between the different workloads **on the same cluster**, fully utilizing all the GPUs.

---

### Intra-Stage: Diverse Parallelisms

Hybrid engine allows verl to flexibly switch between parallelism strategies to optimize the performance.

:::: {.columns}

::: {.column width="60%"}

::: {.nonincremental}

Parallelism Algorithms:

- Data Parallelism
- Tensor Parallelism
- Pipeline Parallelism
- Context / Sequence Parallelism

:::

:::

::: {.column width="40%"}

Training Backend:

::: {.nonincremental}

- FSDP
- Megatron

:::

Generation Backend:

::: {.nonincremental}

- vLLM
- SGLang

:::

:::

::::

# Features / Optimizations in verl

## Sequence Packing {.smaller}

1. Remove padding tokens and packs multiple data sequences into a row
2. Tweak the attention mask & position IDs to avoid cross-contamination

. . .

![](https://cdn-uploads.huggingface.co/production/uploads/6041ff7ff84ebe399f1c85ea/wfZM7YcPyvS0qYnwhdqic.png){fig-align="center" height=300px .lightbox}

. . .

To enable this, use `use_remove_padding`.

## DP Balancing

---

### Load Imbalance in DP

- Parallelism usually needs **synchronization** between different ranks.
- **Data Parallelism (DP)** like ZeRO is the most commonly used parallelism strategy.
- However, DP performance might be damaged by **load imbalance**, which is especially severe in long-context training.

---

### Balancing across DP Ranks

![](./assets/dp-balancing-across-ranks.png){fig-align="center" height=250px .lightbox}

::: {.nonincremental}
- balance the valid tokens dispatched to each rank
- by reordering the samples in each batch
:::

To enable this, use `balance_batch`.

---

### Balancing across Micro Batches

However, in gradient accumulation,

::: {.nonincremental}
- it's not enough to only balance valid tokens in a batch, 
- since DP syncs in the unit of micro batch.
:::

To resolve this, verl supports to

::: {.nonincremental}
- balance the valid tokens across micro batches
- by evenly deviding the data sequences in the batch before packing into micro batches
:::

To enable this, use `use_dynamic_bsz`.

## Async Engine for Multi-Turn Generation

Multi-turn generation might have different implementations:

- Naive: wrap the batch generation in a for-loop, **synchronizing for each turn**, thus causing many bubbles.
- Efficient: utilize the async engine, **managing generation at the request level** instead of the batch level.

---

Specifically, verl integrates:

- SGLang's `Engine.async_generate` (contributed by the SGLang RL team)
- vLLM-V1's `AsyncLLM` (contributed by Xibin Wu from ByteDance)

## Other Features

1. Multi-Model LLMs' RL
2. Full support for [RL with AMD (ROCm Kernel) hardwares](https://github.com/volcengine/verl?tab=readme-ov-file#hardware-support-amd-rocm-kernel)
3. Gradient Checkpointing (`enable_gradient_checkpointing`)
4. Torch Compile (`use_torch_compile`)
5. Liger Kernel (`use_liger`)
6. ...

# Roadmap

For the most timely updates of important features, please keep an eye on [verl's README](https://github.com/volcengine/verl?tab=readme-ov-file#upcoming-features).

## Efficient RL with Huge MoE like DeepSeek-V3-671B (ETA: Late May'25)

verl is working on supporting efficient RL training for huge MoE like DeepSeek-V3-671B, based on the following features:

1. MoE models with `GPTModel` class for actor and critic
2. Multi-node inference
3. Parameter sharding manager for Megatron-Core V0.12 + latest version of inference engines

. . .

For more details, please check [our tracker #708](https://github.com/volcengine/verl/pull/708).

## Agentic RL with Diverse Environments & Tools (Planned)

1. [Our ongoing RFC](https://github.com/volcengine/verl/issues/1172)
2. Integrating MDP
3. Integrating existing implementations, e.g. the Atropos library from Nous Research

## Other Plans

1. Partial Rollout
2. Multi-Token-Prediction (MTP)
3. ...

. . .

Welcome to join the verl community to [discuss](https://github.com/volcengine/verl?tab=readme-ov-file#verl-volcano-engine-reinforcement-learning-for-llms) and [contribute](https://github.com/volcengine/verl?tab=readme-ov-file#contribution-guide)!

# Thanks for Listening! {.unnumbered}

Repo: [https://github.com/volcengine/verl](https://github.com/volcengine/verl)

Contact:

::: {.nonincremental}
- Email: [tongyuxuan361@gmail.com](mailto:tongyuxuan361@gmail.com)
- WeChat / [X](https://x.com/tongyx361): tongyx361
:::

::: {layout-ncol=2}

![](./assets/qrcode-github-repo-verl.png){fig-align="center" width=250px height=250px .lightbox}

![](./assets/qrcode-wechat-tongyx361.png){fig-align="center" width=250px height=250px .lightbox}

:::

# Programming Guide

## Customizing the Dataset {.smaller}

A canonical RL dataset in verl has the following fields:

- `prompt`: a list of messages `{"role": "...", "content": "..."}`
- `data_source`: used to choose the reward function
- `reward_model`: a dict containing
  -  `"ground_truth"`
  -  `"style"` like `"model"` or `"rule"`
- (Optional) `extra_info`: a dict containing extra information

. . .

For VLM RL, verl expects fields `"images"` and/or `"videos"`

. . .

For examples, please check the `examples/data_preprocess`.

---

You could also customize the field names via config. Please check the `data` section in config files like `ppo_trainer.yaml` for more details.

. . .

For further customization, verl provides the `data.custom_cls` config,

```{#lst-verl-custom-cls .yaml lst-cap="Config for custom dataset class."}
data:
  custom_cls:
    path: null # path to the `.py` file containing the `class` definition
    name: null # the `class` name
```

. . .

An example CLI config could be:

```{#lst-verl-custom-cls-cli .bash lst-cap="Example config for custom dataset class."}
--data.custom_cls.path=./examples/dataset/custom_dataset.py \
--data.custom_cls.name=CustomDataset
```

---

The custom dataset class defined in the `.py` file is required to accept the following initialization parameters:

```{#lst-verl-custom-cls-init .python lst-cap="Custom dataset class initialization."}
class CustomDataset: # You could also inherit from `RLHFDataset`
  def __init__(
      self,
      data_files: Union[str, List[str]],
      tokenizer: PreTrainedTokenizer,
      config: DictConfig,
      processor: Optional[ProcessorMixin] = None,
  ):
      ...
```

## Customizing the Reward

verl allows to define custom reward function via the `custom_reward_function` config:

```{#lst-verl-custom-reward-function .yaml lst-cap="Config for custom reward function."}
custom_reward_function:
  path: null # path to the `.py` file containing the function definition
  name: compute_score # the function name after `def`
reward_model:
  reward_manager: naive
```

. . .

An example CLI config could be:

```{#lst-verl-custom-reward-function-cli .bash lst-cap="Example config for custom reward function."}
--custom_reward_function.path=./examples/reward_fn/custom_reward_fn.py \
--custom_reward_function.name=compute_score \
--reward_model.reward_manager=naive
```

---

The function defined in `.py` should accept the parameters passed from **the reward manager `__call__` method**. Taking `NaiveRewardManager` as an example:

```{#lst-verl-custom-reward-function-call .python lst-cap="How a reward function is called in NaiveRewardManager."}
class NaiveRewardManager:
    def __call__(self, data: DataProto, return_dict: bool=False):
        # Preprocessing for the input data
        score = self.compute_score(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )
        # Other processing for the final `reward`
```

. . .

For more complex features, you can also add a new reward manager like `PRIMERewardManager` or `DAPORewardManager`.

## Customizing the Loss Function

To modify the loss function, the most convenient way is to

1. search for the `.backward()` call
2. modify functions like `compute_policy_loss`
3. or add loss terms like `entropy_loss`

---

For example, the `DataParallelPPOActor.update_policy` method defines the loss function as follows:

```{#lst-verl-actor-loss .python lst-cap="Simplified loss function definition in DataParallelPPOActor." code-line-numbers="3-6,8,13-14"}
class DataParallelPPOActor(BasePPOActor):
    def update_policy(self, data: DataProto):
        pg_loss = compute_policy_loss(
            old_log_prob=old_log_prob, log_prob=log_prob,
            advantages=advantages, # ...
        )
        entropy_loss = agg_loss(loss_mat=entropy)
        policy_loss = pg_loss - entropy_loss * entropy_coeff
        kld = kl_penalty(
            logprob=log_prob, ref_logprob=ref_log_prob, # ...
        )
        kl_loss = agg_loss(loss_mat=kld)
        policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
        loss.backward()
```

## Customizing the Training Logic

As mentioned above, the main training logic is concentrated in the `fit` function of the trainer classes like `RayPPOTrainer`.

For example, the `DAPORayTrainer` class overrides the `fit` function to implement the "dynamic sampling" feature:

(See the next slide for the code ➡️)

---

```{#lst-verl-dapo-trainer-fit .python lst-cap="Simplified fit function in DAPORayTrainer, with dynamic sampling highlighted." code-line-numbers="4,6,8,10,12-23"}
class RayDAPOTrainer(RayPPOTrainer):
  def fit(self):
    for epoch in range(self.config.trainer.total_epochs):
      batch = None
      for batch_dict in self.train_dataloader:
        new_batch = DataProto.from_single_dict(batch_dict)
        num_gen_batches += 1
        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
        new_batch = new_batch.union(gen_batch_output)
        if not self.config.algorithm.filter_groups.enable:
          batch = new_batch
        else:
          # Getting `kept_traj_idxs` ...
          new_batch = new_batch[kept_traj_idxs]
          batch = new_batch if batch is None else DataProto.concat([batch, new_batch])
          prompt_bsz = self.config.data.train_batch_size
          if num_prompt_in_batch < prompt_bsz:
            max_num_gen_batches = self.config.algorithm.filter_groups.max_num_gen_batches
            if max_num_gen_batches <= 0 or num_gen_batches < max_num_gen_batches:
                continue
          else:
            traj_bsz = self.config.data.train_batch_size * self.config.actor_rollout_ref.rollout.n
            batch = batch[:traj_bsz]
        # ...
```