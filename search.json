[
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "",
    "text": "1 引言：GRPO 公式的“错误”\n  2 流行 LLM RL 框架中 on-policy KL 优化的实现\n  \n  2.1 TRL：KL reward 项\n  2.2 OpenRLHF\n  \n  2.2.1 KL reward 项\n  2.2.2 KL loss 项\n  \n  2.3 verl\n  \n  2.3.1 KL reward 项\n  2.3.2 KL loss 项\n  \n  2.4 为什么要将 KL 从 reward 中减去\n  \n  2.4.1 KL reward 的流行应当源自 RLHF 与 InstructGPT\n  2.4.2 OpenAI 论文中 KL reward 的出处\n  2.4.3 KL reward 最早的出处\n  \n  \n  3 LLM RL 中 KL 优化的数学形式化\n  \n  3.1 RL 中的 KL 散度通常定义在轨迹分布上\n  3.2 将轨迹展开为状态-动作序列\n  3.3 Markov 决策过程中的 KL 散度\n  3.4 语言模型作为序列决策过程\n  3.5 估计 KL 散度\n  \n  3.5.1 几乎不可能直接计算 KL 散度的真实值\n  3.5.2 通常使用 Monte Carlo 方法估计 KL 散度\n  3.5.3 不同的 KL 估计量\n  \n  \n  4 流行 on-policy KL 优化实现的数学形式化\n  \n  4.1 分析流行的 “KL loss 项” 实现\n  \n  4.1.1 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式\n  4.1.2 \\(k_1\\) 导出的梯度：期望为 0\n  4.1.3 \\(k_2\\) 导出的梯度\n  4.1.4 为什么不应该对同一轨迹中的 “KL 估计量” 求均值\n  4.1.5 \\(k_3\\) 导出的梯度\n  4.1.6 小结：”KL loss 项“中，只有基于 \\(k_2\\) 相对合理\n  \n  4.2 分析流行的 “KL reward 项“ 实现\n  \n  4.2.1 类比 PG 优化 reward 来分析 KL reward 的作用\n  4.2.2 不同 KL 估计量导出的 reward 项的作用\n  4.2.3 小结：KL reward 中，需要先计算出联合概率，再应用估计器\n  \n  \n  5 推导 on-policy 设置下 KL 散度的梯度估计方法\n  \n  5.1 在已知环境中简化 KL 梯度估计\n  5.2 简写为 Contextual Bandit\n  5.3 还原为已知环境决策过程\n  5.4 利用因果性技巧化简 KL 梯度估计\n  5.5 KL 梯度优化可以实现为 KL 样本值 reward\n  \n  6 off-policy 设置下如何估计 KL 散度的梯度\n  \n  6.1 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题\n  \n  6.1.1 TRL\n  6.1.2 OpenRLHF\n  6.1.3 verl\n  \n  6.2 利用重要性采样处理 off-policy 设置\n  \n  7 结论：如何正确地在 RL 中优化 KL 散度\n  \n  7.1 修正 GRPO 公式中的 KL 项\n  7.2 修正流行 LLM RL 框架中的 KL 优化实现\n  \n  8 讨论\n  \n  8.1 对于 KL 梯度更好的估计样本量\n  8.2 KL-Regularized RL 的理论优势\n  \n  9 附录\n  \n  9.1 致谢\n  9.2 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#trlkl-reward-项",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#trlkl-reward-项",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "2.1 TRL：KL reward 项",
    "text": "2.1 TRL：KL reward 项\nTRL 计算 KL 定义中的样本值 \\(\\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,t})}{\\pi_{\\theta_{\\text{ref}}}(a_{i,t} \\mid s_{i,t})}\\)，并将其从 reward 中减去。对应代码可见 列表 1。\n\n\n\n列表 1: TRL 计算 KL 样本值 \\(\\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,t})}{\\pi_{\\theta_{\\text{ref}}}(a_{i,t} \\mid s_{i,t})}\\) 并从 reward 中减去4\n\n\n# 4. compute rewards\nkl = logprobs - ref_logprobs\nnon_score_reward = -args.kl_coef * kl\nrewards = non_score_reward.clone()\n# ...\nrewards[[actual_start, actual_end]] += scores\n\n\n\n这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 小节 2.4。\n\n\n\nhttps://github.com/huggingface/trl↩︎\nhttps://github.com/OpenRLHF/OpenRLHF↩︎\nhttps://github.com/volcengine/verl↩︎\nhttps://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#openrlhf",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#openrlhf",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "2.2 OpenRLHF",
    "text": "2.2 OpenRLHF\n\n2.2.1 KL reward 项\n与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 列表 2。\n\n\n\n列表 2: OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 5\n\n\ndef compute_approx_kl(\n    log_probs: torch.Tensor,\n    log_probs_base: torch.Tensor,\n    action_mask: Optional[torch.Tensor] = None,\n    kl_estimator: str = \"k1\",\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the approximate KL divergence between two distributions.\n    Schulman blog: http://joschu.net/blog/kl-approx.html\n\n    Args:\n        log_probs: Log probabilities of the new distribution.\n        log_probs_base: Log probabilities of the base distribution.\n        action_mask: Mask for actions.\n    \"\"\"\n\n    if kl_estimator == \"k1\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n\n    # The $k_2$ estimator is the non negative kl approximation in\n    # http://joschu.net/blog/kl-approx.html\n    # The k2_loss is approximately equivalent to the\n    # one-step KL divergence penalty with the $k_1$ estimator\n    # used in https://arxiv.org/abs/2310.10505.\n    if kl_estimator == \"k2\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n        log_ratio = log_ratio**2 / 2.0\n\n    # The $k_3$ estimator is the non negative kl approximation in\n    # http://joschu.net/blog/kl-approx.html\n    if kl_estimator == \"k3\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n        log_ratio = -log_ratio\n        log_ratio = log_ratio.exp() - 1 - log_ratio\n\n    return log_ratio\n\n\ndef compute_reward(\n    # ...\n    kl_coef: float,\n    kl: Union[torch.Tensor, list[torch.Tensor]],\n    # ...\n    num_actions: Optional[Union[int, list[int]]] = None,\n    # ...\n) -&gt; Union[torch.Tensor, list[torch.Tensor]]:\n    # ...\n    if action_mask is not None:\n        # ...\n    else:\n        # ...\n        reward = []\n        for i, (kl_seg, action_len) in enumerate(zip(kl, num_actions)):\n            kl_reward = -kl_coef * kl_seg\n            kl_reward[action_len - 1] += r[i]\n            reward.append(kl_reward)\n\n    return reward\n\n\n\n\n\n\nhttps://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88↩︎\n\n\n\n\n2.2.2 KL loss 项\n此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 列表 3。\n\n\n\n列表 3: OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 6\n\n\ndef training_step_actor(self, experience: Experience) -&gt; Dict[str, float]:\n    self.actor.train()\n    # ...\n    if isinstance(experience.sequences, list):\n        # ...\n    else:\n        sequences = experience.sequences\n        old_action_log_probs = experience.action_log_probs\n        advantages = experience.advantages\n        num_actions = experience.action_mask.size(1)\n        packed_seq_lens = None\n        attention_mask = experience.attention_mask\n        if self.args.use_kl_loss and experience.base_action_log_probs is not None:\n            base_action_log_probs = experience.base_action_log_probs\n\n    # actor loss\n    action_log_probs, output = self.actor(\n        sequences,\n        num_actions,\n        # ...\n    )\n    # ...\n    # loss function\n    actor_loss = self.actor_loss_fn(\n        action_log_probs,\n        old_action_log_probs,\n        advantages,\n        # ...\n    )\n\n    if self.args.use_kl_loss:\n        if self.initial_model is not None:\n            kl = compute_approx_kl(\n                action_log_probs,\n                base_action_log_probs,\n                # ...\n                kl_estimator=self.args.kl_estimator,\n            )\n        else:\n            kl = torch.zeros_like(action_log_probs, dtype=action_log_probs.dtype, device=action_log_probs.device)\n\n        if not self.args.packing_samples:\n            kl_mean = masked_mean(kl, experience.action_mask, dim=-1)\n        else:\n            # ...\n\n        kl_loss = kl_mean.mean()\n        experience.info[\"kl\"] = kl_loss.item()\n    else:\n        kl_loss = 0\n    # ...\n    self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name=\"actor\")\n    # ...\n\n\n\n\n\n\nhttps://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#verl",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#verl",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "2.3 verl",
    "text": "2.3 verl\n\n2.3.1 KL reward 项\nverl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 列表 4。\n\n\n\n列表 4: verl 将 KL 估计样本值从 reward 中减去 7\n\n\ndef apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):\n    # ...\n    # compute kl between ref_policy and current policy\n    if 'ref_log_prob' in data.batch.keys():\n        kld = core_algos.kl_penalty(data.batch['old_log_probs'], data.batch['ref_log_prob'],\n                                    kl_penalty=kl_penalty)  # (batch_size, response_length)\n        kld = kld * response_mask\n        beta = kl_ctrl.value\n    else:\n        beta = 0\n        kld = torch.zeros_like(response_mask, dtype=torch.float32)\n\n    token_level_rewards = token_level_scores - beta * kld\n    # ...\n\n\n\n\n\n\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160↩︎\n\n\n\n\n2.3.2 KL loss 项\nverl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 列表 5。\n\n\n\n列表 5: verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 8\n\n\ndef update_policy(self, data: DataProto):\n    # make sure we are in training mode\n    self.actor_module.train()\n    # ...\n    for epoch in range(self.config.ppo_epochs):\n        for batch_idx, data in enumerate(dataloader):\n            # ...\n            self.actor_optimizer.zero_grad()\n\n            for data in micro_batches:\n                # ...\n                responses = data['responses']\n                # ...\n                old_log_prob = data['old_log_probs']\n                # ...\n\n                # all return: (bsz, response_length)\n                entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature)\n\n                pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=old_log_prob,\n                                                                                log_prob=log_prob,\n                                                                                # ...\n                                                                                )\n                # ...\n\n                # compute policy loss\n                policy_loss = pg_loss - entropy_loss * entropy_coeff\n\n                if self.config.use_kl_loss:\n                    ref_log_prob = data['ref_log_prob']\n                    # compute kl loss\n                    kld = core_algos.kl_penalty(logprob=log_prob,\n                                                ref_logprob=ref_log_prob,\n                                                kl_penalty=self.config.kl_loss_type)\n                    kl_loss = masked_mean(kld, response_mask)\n\n                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef\n                # ...\n                loss.backward()\n            # ...\n            grad_norm = self._optimizer_step()\n    # ...\n    self.actor_optimizer.zero_grad()\n    # ...\n\n\n\n\n\n\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-why-kl-reward",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-why-kl-reward",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "2.4 为什么要将 KL 从 reward 中减去",
    "text": "2.4 为什么要将 KL 从 reward 中减去\n将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT (Ouyang 等 2022)。\n\n2.4.1 KL reward 的流行应当源自 RLHF 与 InstructGPT\nInstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。\n\n… In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models “PPO.”\n…\n\n\\[\n\\begin{aligned}\n\\text { objective }(\\phi)= & E_{(x, y) \\sim D_\\pi^{\\mathrm{RL}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {remin }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}\n\\]\n\nwhere \\(\\pi_\\phi^{\\mathrm{RL}}\\)is the learned RL policy,\\(\\pi^{\\mathrm{SFT}}\\) is the supervised trained model, and\\(D_{\\text {pretrain }}\\)is the pretraining distribution. The KL reward coefficient, \\(\\beta\\), and the pretraining loss coefficient, \\(\\gamma\\), control the strength of the KL penalty and pretraining gradients respectively. For “PPO” models, \\(\\gamma\\) is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.\n\n\n\n2.4.2 OpenAI 论文中 KL reward 的出处\n然而，在OpenAI 早期的一篇论文 “Learning to summarize from human feedback” (Stiennon 等 2020) 中，他们就已经采用了 KL reward，并提及了出处：\n\n… Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy \\(\\pi_\\phi^{\\mathrm{RL}}\\) with parameters \\(\\phi\\) and this original supervised model \\(\\pi^{\\mathrm{SFT}}\\), as previously done in [25]. The full reward \\(R\\) can be written as:\n\n\\[\nR(x, y)=r_\\theta(x, y)-\\beta \\log \\left[\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right]\n\\]\n\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn’t learn to produce outputs that are too different from those that the reward model has seen during training.\n\n\n\n2.4.3 KL reward 最早的出处\n小节 2.4.2 中 OpenAI 引用的 KL reward 出处 [25] 是 “Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog” (Jaques 等 2019)。\n实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：\n\nRather than simply sample from the prior, we would like the \\(Q\\)-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior \\(p(y \\mid x)\\), and the \\(Q\\)-network policy \\(\\pi_\\theta\\), while still maximizing reward. Given a trajectory of actions, \\(\\tau=\\left\\{a_1, a_2, \\ldots a_{t-1}\\right\\}\\), let \\(q(\\tau)=\\prod_{t=1}^T \\pi_\\theta\\left(a_t, s_t\\right)\\)be the policy of our\\(Q\\)-learning algorithm at the trajectory level. Similarly, let \\(p(\\tau)=\\prod_{t=1}^T p\\left(a_t \\mid s_t\\right)\\)be the prior distribution over the trajectory, and\\(r(\\tau)\\) be the rewards. We seek to maximize the following KL-regularized objective:\n\n\\[\nL(q)=\\mathbb{E}_{q(\\tau)}[r(\\tau)] / c-D_{\\text{KL}}[q(\\tau) \\mid p(\\tau)]\n\\]\n\nSince \\(D_{\\text{KL}}[q \\mid p]=\\sum_x q(x)(\\log q(x)-\\log p(x))\\), we can see that this is equivalent to maximizing the following expected value function of the policy \\(\\pi_\\theta\\) at the action level:\n\n\\[\nQ^\\pi\\left(s_t, a_t\\right)=\\mathbb{E}_\\pi\\left[\\sum^T r\\left(s_{t^{\\prime}}, a_{t^{\\prime}}\\right) / c+\\log p\\left(a_{t^{\\prime}} \\mid s_{t^{\\prime}}\\right)-\\log \\pi\\left(a_{t^{\\prime}} \\mid s_{t^{\\prime}}\\right)\\right]\n\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#rl-中的-kl-散度通常定义在轨迹分布上",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#rl-中的-kl-散度通常定义在轨迹分布上",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "3.1 RL 中的 KL 散度通常定义在轨迹分布上",
    "text": "3.1 RL 中的 KL 散度通常定义在轨迹分布上\nGRPO 公式 (式 1) 中的 KL 项可以定义为：\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & =\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)}\\right]\n\\end{aligned}\n\\tag{3}\\]\n其中 \\(\\mathbf{\\tau}\\) 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 \\(p_{\\theta}\\) 与参考策略整体分布 \\(p_{\\text{ref}}\\) 的 KL 散度。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#将轨迹展开为状态-动作序列",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#将轨迹展开为状态-动作序列",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "3.2 将轨迹展开为状态-动作序列",
    "text": "3.2 将轨迹展开为状态-动作序列\nRL 文献中通常会将轨迹 \\(\\mathbf{\\tau}\\) 展开为状态-动作序列 \\(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\)：9\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & =\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|},\\right) \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|},, \\mathbf{a}_{|\\mathbf{\\tau}|},\\right)}{p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\log \\frac{p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)}{p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\right] \\\\\n\\end{aligned}\n\\tag{4}\\]\n其中 \\(|\\mathbf{\\tau}|\\) 为轨迹动作数的随机变量。\n此处利用了联合概率的展开，以 \\(p_{\\theta}\\) 为例：\n\\[\np_{\\theta}(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) = p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)\n\\tag{5}\\]\n注意区分整体概率分布 \\(p_{\\theta}\\)、策略（条件）概率分布 \\(\\pi_{\\theta}\\) 与状态转移概率分布 \\(p\\)。\n\n\n\n这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，\\(\\mathbf{q}\\) 对应于 \\(\\mathbf{s}_1\\)，而 \\({\\mathbf{o}}\\) 对应于 \\(\\mathbf{\\mathbf{a}_1, \\cdots, \\mathbf{s}_T, \\mathbf{a}_T}\\)。↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#markov-决策过程中的-kl-散度",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#markov-决策过程中的-kl-散度",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "3.3 Markov 决策过程中的 KL 散度",
    "text": "3.3 Markov 决策过程中的 KL 散度\n实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP10。\nMarkov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 \\(n\\) 个历史状态和动作，而非全部的历史信息，对应的过程称为 \\(n\\) 阶 Markov 过程。以 \\(n=1\\) 为例：\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) & = \\pi(\\mathbf{a}_t \\mid \\mathbf{s}_t) \\\\\np(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) & = p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t) \\\\\n\\end{aligned}\n\\tag{6}\\]\n则 式 5 中的联合概率可以进一步简化为：\n\\[\np(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) = p(s_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t)\n\\tag{7}\\]\n如果考虑一阶 Markov 过程，则 式 4 中的 KL 可以进一步简化为：\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] = & = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_t)}\\right] \\\\\n\\end{aligned}\n\\tag{8}\\]\n\n\n\nhttps://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-lm-as-dp",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-lm-as-dp",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "3.4 语言模型作为序列决策过程",
    "text": "3.4 语言模型作为序列决策过程\n目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。\n尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 \\(s_1\\) 表示 prompt 中的所有 token，对于 \\(t &gt;1\\)，\n\n如果令 \\(s_t\\) 表示第 \\(t\\) 个 token 前的所有 token，则自回归模型满足 Markov 性质。\n如果令 \\(s_t\\) 仅表示第 \\(t-1\\) 个 token，则自回归模型不满足 Markov 性质。\n\n接下来，我们先令 \\(s_t\\) 表示前 \\(t\\) 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#估计-kl-散度",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#估计-kl-散度",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "3.5 估计 KL 散度",
    "text": "3.5 估计 KL 散度\n\n3.5.1 几乎不可能直接计算 KL 散度的真实值\n实际实现中，我们几乎不可能直接计算出 \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\)，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 \\(\\left|\\mathcal{T}\\right|\\) 与轨迹最大长度 \\(T = \\max_{\\mathbf{\\tau} \\in \\mathcal{T}} |\\mathbf{\\tau}|\\) 成指数关系： \\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid  \\mathbf{s}_1, \\mathbf{a}_1, \\cdots,\\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots,\\mathbf{s}_t)}\\right] \\\\\n& = \\frac{1}{\\left|\\mathcal{T}\\right|} \\sum_{(s_1, a_1, \\cdots, s_{|\\tau|}, a_{|\\tau|}) \\in \\mathcal{T}} \\left(\\sum_{t=1}^{|\\tau|} \\log \\frac{\\pi_{\\theta}(a_t \\mid  s_1, a_1, \\cdots, s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_1, a_1, \\cdots, s_t)}\\right) \\\\\n\\end{aligned}\n\\tag{9}\\]\n\n\n3.5.2 通常使用 Monte Carlo 方法估计 KL 散度\n所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法11来估计 RL 中的 KL 散度，例如：\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\frac{1}{\\left|\\mathcal{T}\\right|} \\sum_{(s_1, a_1, \\cdots, s_{|\\mathbf{\\tau}|}, a_{|\\mathbf{\\tau}|}) \\in \\mathcal{T}} \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(a_t \\mid  s_1, a_1, \\cdots, s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_1, a_1, \\cdots, s_t)} \\\\\n& \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\sum_{t=1}^{|\\mathbf{\\tau_{i }}|} \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}\\right)\n\\end{aligned}\n\\tag{10}\\]\n其中，\\(\\left(\\mathbf{s}_{i,1}, \\mathbf{a}_{i,1}, \\cdots, \\mathbf{s}_{i,|\\mathbf{\\tau_{i}}|}, \\mathbf{a}_{i,|\\mathbf{\\tau_{i}}|}\\right) \\sim p_{\\theta}\\)，\\(N\\) 为估计使用的轨迹样本数量。\n\n\n\nhttps://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95↩︎\n\n\n\n\n3.5.3 不同的 KL 估计量\n实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。\n设 KL 估计量为 \\(k\\)，则对应的 KL 估计值为\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N} k(\\tau_i)\n\\end{aligned}\n\\tag{11}\\]\n例如 小节 2.2.1 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 k1, k2, k3，这应该是主要参考了 John Schulman 的博客 “Approximating KL Divergence”12。\nverl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度13，但目前还没有实现。对应代码可见 列表 6。\n\n\n\n列表 6: verl 的 KL 散度 Monte Carlo 估计样本值14\n\n\ndef kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) -&gt; torch.FloatTensor:\n    # ...\n    if kl_penalty == \"kl\":\n        return logprob - ref_logprob\n\n    if kl_penalty == \"abs\":\n        return (logprob - ref_logprob).abs()\n\n    if kl_penalty == \"mse\":\n        return 0.5 * (logprob - ref_logprob).square()\n\n    # J. Schulman. Approximating kl divergence, 2020.\n    # # URL http://joschu.net/blog/kl-approx.html.\n    if kl_penalty == 'low_var_kl':\n        kl = ref_logprob - logprob\n        ratio = torch.exp(kl)\n        kld = (ratio - kl - 1).contiguous()\n        return torch.clamp(kld, min=-10, max=10)\n\n    if kl_penalty == \"full\":\n        # so, here logprob and ref_logprob should contain the logits for every token in vocabulary\n        raise NotImplementedError\n\n    raise NotImplementedError\n\n\n\n由于 \\(k_1\\)、\\(k_2\\)、\\(k_3\\) 三种估计量最为流行，我们将以这三种估计量为例展开分析。\n考虑 \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} k_j(\\tau_i)\\)，其中 \\(\\tau_i \\sim p_{\\theta}\\)，令 \\(r = \\frac{\\pi_{\\text{ref}}(\\tau_i)}{\\pi_{\\theta}(\\tau_i)}\\)，注意，此处 \\(r\\) 并非 KL 定义中的样本量，而是其倒数，则：\n\\[\n\\begin{aligned}\nk_{1} & = - \\log r \\\\\nk_{2} & = \\frac{1}{2} (\\log r)^2 \\\\\nk_{3} & = (r - 1) - \\log r\n\\end{aligned}\n\\tag{12}\\]\n\n\n\nhttp://joschu.net/blog/kl-approx.html↩︎\n这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。↩︎\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-loss-impl",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-loss-impl",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "4.1 分析流行的 “KL loss 项” 实现",
    "text": "4.1 分析流行的 “KL loss 项” 实现\n上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。\n然而，如 小节 1 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。\n\n4.1.1 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式\n观察 列表 3 计算 “KL loss” 项的部分。\n# ...\nkl = compute_approx_kl(\n    action_log_probs,\n    base_action_log_probs,\n    # ...\n    kl_estimator=self.args.kl_estimator,\n)\n# ...\nkl_mean = masked_mean(kl, experience.action_mask, dim=-1)\n# ...\nkl_loss = kl_mean.mean()\n# ...\n这些代码：\n\n计算了 kl，对应对每个动作 token \\(a_{i,t}\\) 计算 “KL 估计量” \\(k\\)。\n计算了 kl_mean，对应对每个轨迹 \\(\\tau_i\\) 计算均值 \\(\\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|} k\\)。\n计算了 kl_loss，对应对所有轨迹样本计算均值 \\(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|} k\\)。\n\n由于其没有去除任何梯度，因此其导出的梯度估计值为\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\frac{1}{|\\tau_i|} k \\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|}  \\nabla_{\\theta} k\n\\end{aligned}\n\\tag{13}\\]\n列表 5 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\left( \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|} \\sum_{i=1}^{N} k \\right) = \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|} \\sum_{i=1}^{N} \\nabla_{\\theta} k\n\\end{aligned}\n\\tag{14}\\]\n我们将平均操作一般化为权重 \\(w_{\\mathbf{\\tau}}\\) 与 \\(w_{t}\\)，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{N} w_{\\mathbf{\\tau}_i} \\sum_{t=1}^{|\\tau_i|} w_{t} \\nabla_{\\theta} k \\\\\n\\end{aligned}\n\\tag{15}\\]\n则\n\nOpenRLHF 对应 \\(w_{\\mathbf{\\tau}} = \\frac{1}{N}, w_{t} = \\frac{1}{|\\tau|}\\)；\nverl 对应 \\(w_{\\mathbf{\\tau}} = \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|}, w_{t} = 1\\)。\n\n此处，我们先以 OpenRLHF 的梯度估计 (式 13) 为例，分析不同 KL 估计量导出的梯度估计，其满足：\n\\[\n\\mathbb{E}_{\\mathbf{\\tau}_i \\sim p_{\\theta}} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|}  \\nabla_{\\theta} k \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} k \\right]\n\\tag{16}\\]\n我们会在 小节 5 中推导正确的 KL 梯度估计。\n\n\n4.1.2 \\(k_1\\) 导出的梯度：期望为 0\n向 式 16 代入 \\(k = k_1 = - \\log r = \\log \\frac{1}{r} = \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\)，导出的梯度估计为\n\\[\n\\begin{aligned}\n& \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta}\\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\left( \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) + \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t}) + \\nabla_{\\theta} \\log \\left( p(\\mathbf{s}_{1}) \\right) \\right) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\left( p(\\mathbf{s}_{1}) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t}) \\right) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_\\theta(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_{\\theta}(\\tau)\n\\end{aligned}\n\\tag{17}\\]\n则其导出的梯度期望满足：\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\\right]\n& = \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\frac{1}{|\\tau|} \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n& = \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n& = \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} \\nabla_{\\theta} p_{\\theta}(\\tau) \\\\\n& = \\nabla_{\\theta} \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|}  p_{\\theta}(\\tau) \\\\\n& = \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} p_{\\theta}(\\mathbf{\\tau}) \\right]\n\\end{aligned}\n\\tag{18}\\]\n此处利用了 \\(p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\frac{1}{p_{\\theta}(\\tau)} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} p_{\\theta}(\\tau)\\)。\n所以 \\(k_1\\) loss 项优化的量是 \\(\\frac{1}{|\\mathcal{T}|}\\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|}  p_{\\theta}(\\tau)\\)。 `\n注意，\\(\\forall \\theta, \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) = 1\\)。\n因此，在这一约束下最小化 \\(k_1\\) loss 项，更长的轨迹权重 \\(\\frac{1}{|\\tau|}\\) 更小，减小一定概率，对 \\(\\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|}  p_{\\theta}(\\tau)\\) 的贡献更小，这意味着该优化过程倾向于增大长轨迹的概率。\n特别地，当 \\(|\\tau|\\) 为定值，或不对同一轨迹中的 “\\(k_1\\) 估计量”求均值时，可以直接去掉 \\(\\frac{1}{|\\tau|}\\) 这一项，得到\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\\right] = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) = \\nabla_{\\theta} 1 = 0\n\\tag{19}\\]\n这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。\n无论哪种情况，\\(k_1\\) 导出的优化量都非常奇怪。同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。\n但保守起见，我们仍然保留该条件推导 \\(k_2\\) 梯度的表达式。\n\n\n4.1.3 \\(k_2\\) 导出的梯度\n向 式 16 代入 \\(k = k_2 = \\frac{1}{2} (\\log r)^2 = \\frac{1}{2} \\left(\\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\right)^2\\)，导出的单条轨迹 \\(\\mathbf{\\tau} \\sim p_{\\theta}\\) 的梯度为 \\[\n\\begin{aligned}\n& \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k\\\\\n=& \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta}  \\frac{1}{2} \\left(\\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}\\right)^2 \\\\\n=& \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\\\\n=& - \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\\\\n=& - \\frac{1}{|\\mathbf{\\tau}|} \\left( \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\\\\n=& \\frac{1}{|\\mathbf{\\tau}|} \\left( \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\n\\end{aligned}\n\\tag{20}\\]\n则其导出的梯度期望满足：\n\\[\n\\begin{aligned}\n& \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] \\\\\n=& \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\frac{1}{|\\tau|} \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n=& \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n=& \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} \\left[ \\left( \\log p_{\\theta}(\\tau) \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) - \\left( \\log p_{\\text{ref}}(\\tau) \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\right] \\\\\n=& \\nabla_{\\theta} \\frac{1}{|\\mathcal{T}|} \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|}  \\left[ (\\log p_{\\theta}(\\tau) - 1) p_{\\theta}(\\tau) - \\log p_{\\text{ref}}(\\tau) p_{\\theta}(\\tau) \\right] \\\\\n=& \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\tau|}  p_{\\theta}(\\tau) \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} - 1 \\right) \\right] \\\\\n=& \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} - 1 \\right) \\right] \\\\\n= & \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right] - \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{\\tau}) \\right]\n\\end{aligned}\n\\tag{21}\\]\n此处利用了 \\(\\log p(x) \\nabla_{\\theta} p(x) = \\nabla_{\\theta} (\\log p(x) - 1) p(x)\\)\n尽管 \\(k_2\\) 的优化量相对复杂，但注意，如果不对同一轨迹中的 “\\(k_2\\) 估计量”求均值，则其优化量将变为：\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[\\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} - 1 \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[\\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right] = \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\n\\end{aligned}\n\\tag{22}\\]\n此时，最小化 \\(k_2\\) loss 项，确实是在优化 \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\)。\n这也进一步说明了，对同一轨迹中的 “KL 估计量” 求均值，这一操作应当是错误的。\n\n\n4.1.4 为什么不应该对同一轨迹中的 “KL 估计量” 求均值\n回顾 KL 估计量公式 (式 12) ，应当注意到这些估计量使用的都是轨迹联合概率 \\(p_{\\theta}(\\mathbf{\\tau})\\)，而不是条件概率 \\(\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)\\) 或 \\(p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)\\)。\n假设我们知晓全部条件概率，则可以通过条件概率求积或对数条件概率求和的方式来求解联合概率，进而正确应用估计量。无论如何，都不应该有平均操作。额外加入平均操作，实际上添加了一个与轨迹动作数有关的权重 \\(\\frac{1}{|\\tau|}\\)，通常会使得梯度估计量出现偏差。\n此外，如果对条件概率强行计算估计量，则很难保证其积或其对数和仍然对应于联合概率。例如 \\(k_2\\)，\\(k_3\\) 的和就包含并非联合概率的奇怪结果。\n接下来我们将直接忽略这一操作，即去除 \\(\\frac{1}{|\\tau|}\\) 这一项。\n\n\n4.1.5 \\(k_3\\) 导出的梯度\n向 式 16 代入 \\(k = k_3 = (r - 1) - \\log r = (\\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} - 1) - \\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\)，导出的单条轨迹 \\(\\mathbf{\\tau} \\sim p_{\\theta}\\) 的梯度为 \\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\left(\\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} - 1 - \\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\right) \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} - \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) - \\nabla_{\\theta} \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\\\\n=& - \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\right) - \\nabla_{\\theta} \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\\\\n=& - \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\right) + \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\\\\n\\end{aligned}\n\\tag{23}\\]\n参考 式 17 的推导，\\(\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\log p_{\\theta}(\\mathbf{\\tau}) \\right] = 0\\)，不妨直接省略。\n而剩余部分似乎很难通过消去 \\(\\pi_{\\theta}(\\mathbf{\\tau})\\) 来提出 \\(\\nabla_{\\theta}\\) 并准确分析。但显然也并非在优化 KL 散度。\n\n\n4.1.6 小结：”KL loss 项“中，只有基于 \\(k_2\\) 相对合理\n综上所述，对于 OpenRLHF 实现的 “KL loss 项”，若修正去除对同一轨迹内的 “KL 估计量” 求均值这一操作，则\n\n\\(k_1\\) 导出的梯度期望为 0，在平均意义上不改变分布。\n\\(k_2\\) 导出的梯度能够正确优化 KL 散度。\n\\(k_3\\) 导出的梯度十分复杂，难以分析，但并非在优化 KL 散度。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#分析流行的-kl-reward-项-实现",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#分析流行的-kl-reward-项-实现",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "4.2 分析流行的 “KL reward 项“ 实现",
    "text": "4.2 分析流行的 “KL reward 项“ 实现\n\n4.2.1 类比 PG 优化 reward 来分析 KL reward 的作用\n由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是： \\[\n\\nabla_\\theta \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[r(\\mathbf{\\tau})\\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right) \\hat{A}_t \\right]\n\\tag{24}\\]\n其中 \\(\\hat{A}_t\\) 为优势（Advantage）的估计量。\n为了方便观察 KL reward 项发挥的作用，我们将 \\(r_{\\mathbf{\\tau}}\\) 展开，并不妨考虑一个更简单的估计，例如：\n\\[\n\\nabla_\\theta \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} r(\\mathbf{s}_t, \\mathbf{a}_t) \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right) \\sum_{t'=1}^{|\\tau|} r(s_{t'}, a_{t'}) \\right]\n\\tag{25}\\]\n简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 “Policy Gradient” 一讲15。\n类比 \\(r_{t'}\\) 导出的梯度期望，将负的 KL 样本量 \\(- \\log \\frac{\\pi_\\theta\\left(a_t \\mid s_t \\right)}{\\pi_{\\text{ref}}\\left(a_t \\mid s_t \\right)}\\) 加入 reward \\(r_{t'}\\) 代入其中，导出的梯度期望为：\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t \\right) \\sum_{t'=1}^{|\\tau|} - \\log \\frac{\\pi_\\theta\\left(a_{t'} \\mid s_{t'} \\right)}{\\pi_{\\text{ref}}\\left(a_{t'} \\mid s_{t'} \\right)} \\right] = \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}\\right]\n\\tag{26}\\]\n注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (式 6)。\n实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta}- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)} \\right] \\\\\n\\to& \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right] \\\\\n= & \\nabla_{\\theta} -  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{\\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{ \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{ p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) }{ p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) } \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{ p_\\theta\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta} \\left[ \\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n\\end{aligned}\n\\tag{27}\\]\n可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。\n\n\n\nhttps://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf↩︎\n\n\n\n\n4.2.2 不同 KL 估计量导出的 reward 项的作用\n不难注意到，小节 4.2.1 中的 KL 样本量对应于 \\(k_1\\) 估计量。\n一个自然的问题是，如果使用 \\(k_2\\) 或 \\(k_3\\) 等其他估计量，会得到什么结果？\n如 小节 4.1.4 所述，使用 \\(k_2\\) 或 \\(k_3\\) 等其他估计量，求和时通常无法得到联合概率，也就无法实现类似 式 27 中的效果了。具体来说，其他估计量分别在优化\n\nk2: \\(- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{1}{2} \\left( \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right)^{2} \\right]\\)\nk3: \\(- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} (\\frac{\\pi_{\\text{ref}} \\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\theta}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} - 1 - \\log \\frac{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\theta}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}) \\right]\\)\n\n显然，这里的求和无法得到联合概率。\n而如果先计算出联合概率，则只需考虑\n\\[\n\\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  k\\left(\\frac{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}\\right) \\right]\n\\approx \\nabla_{\\theta} \\frac{1}{N} k\\left(\\frac{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}\\right)\n\\approx \\nabla_{\\theta} - \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\n\\tag{28}\\]\n应当仍然符合条件。\n\n\n4.2.3 小结：KL reward 中，需要先计算出联合概率，再应用估计器\n暂时不考虑 off-policy 问题，GRPO 公式 (式 1, 式 2) 应当修正 KL 项如下：\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\left\\{ \\sum_{t=1}^{\\left|o_i\\right|} \\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right] -\\beta k\\left( \\frac{\\prod_{t=1}^{|o_i|} \\pi_{\\text{ref}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\prod_{t=1}^{|o_i|} \\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\right) \\right\\}\n\\end{aligned}\n\\tag{29}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#在已知环境中简化-kl-梯度估计",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#在已知环境中简化-kl-梯度估计",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "5.1 在已知环境中简化 KL 梯度估计",
    "text": "5.1 在已知环境中简化 KL 梯度估计\n实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。\n当状态转移概率分布已知时，\\(\\forall t, p_{\\theta}(a_1, \\cdots, s_t, a_t \\mid s_1)\\) 都是可以计算的，则 KL 散度可以直接写成：\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\sum_{\\mathbf{\\tau} \\in \\mathcal{T}} p(\\mathbf{s}_1) p_{\\theta}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1) \\log \\frac{p_{\\theta}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1)}{p_{\\text{ref}}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1)}  \\\\\n\\end{aligned}\n\\tag{32}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#简写为-contextual-bandit",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#简写为-contextual-bandit",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "5.2 简写为 Contextual Bandit",
    "text": "5.2 简写为 Contextual Bandit\n为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 \\(\\mathbf{s}_1 = \\mathbf{x} \\in \\mathcal{P}, (\\mathbf{a}_1, \\cdots, \\mathbf{s}_T, \\mathbf{a}_T) = \\mathbf{y} \\in \\mathcal{R}\\)，其中 \\(\\mathcal{P}, \\mathcal{R}\\) 分别表示 prompt / response 空间，则 KL 散度变为：\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p_{\\theta}}\\left[\\log \\frac{\\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})}{\\pi_{\\text{ref}}(\\mathbf{y} \\mid \\mathbf{x})}\\right] \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p_{\\theta}(x, y) \\left(\\sum_{t=1}^{T} \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\n\\end{aligned}\n\\tag{33}\\]\n其梯度变为：\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\nabla_{\\theta} \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\nabla_{\\theta} \\left(\\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\\right)\n\\end{aligned}\n\\tag{34}\\]\n其中梯度项可以进一步展开为：\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\left(\\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\\right) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\pi_{\\theta}(y \\mid x) \\nabla_{\\theta} \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\pi_{\\theta}(y \\mid x) \\frac{1}{\\pi_\\theta(y \\mid x)} \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\n\\end{aligned}\n\\tag{35}\\]\n代入回 KL 梯度表达式：\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)}{\\pi_{\\theta}(y \\mid x)} \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x) \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] + \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right]\n\\end{aligned}\n\\tag{36}\\]\n这里为了重新获得期望形式，引入了 \\(1 = \\pi_{\\theta}(y \\mid x) / \\pi_{\\theta}(y \\mid x)\\)，并利用了 \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x) = \\frac{\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)}{\\pi_{\\theta}(y \\mid x)}\\) 和 \\(\\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] = 0\\)。\n进行 Monte Carlo 估计：\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\log \\frac{\\pi_{\\theta}(y_i \\mid x_i)}{\\pi_{\\text{ref}}(y_i \\mid x_i)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y_i \\mid x_i)\n\\end{aligned}\n\\tag{37}\\]\n其中 \\((\\mathbf{x}_i, \\mathbf{y}_i) \\sim p_{\\theta}\\)。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#还原为已知环境决策过程",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#还原为已知环境决策过程",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "5.3 还原为已知环境决策过程",
    "text": "5.3 还原为已知环境决策过程\n将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\\\\n=& \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})}{\\pi_{\\text{ref}}(\\mathbf{y} \\mid \\mathbf{x})}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T}) \\sim p_{\\theta}} \\left[\\left(\\sum_{t=1}^{T} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)}\\right) \\left(\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)\\right)\\right]\n\\end{aligned}\n\\tag{38}\\]\n对应的 Monte Carlo 估计式为：\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\left(\\sum_{t=1}^{T}\\log \\frac{\\pi_{\\theta}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})}{\\pi_{\\text{ref}}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})}\\right) \\left(\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})\\right)\n\\end{aligned}\n\\tag{39}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#利用因果性技巧化简-kl-梯度估计",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#利用因果性技巧化简-kl-梯度估计",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "5.4 利用因果性技巧化简 KL 梯度估计16",
    "text": "5.4 利用因果性技巧化简 KL 梯度估计16\n因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。\n对于任何 \\(0 \\leq t \\leq |\\tau|\\)，我们有 \\[\n\\begin{aligned}\n& \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) }\\left[\\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\sum_{a_t \\in \\mathcal{A}} \\pi_\\theta(a_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\\\\n=& \\sum_{a_j \\in \\mathcal{A}} \\pi_\\theta(a_j \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_j) \\cdot 0 \\\\\n=& 0\n\\end{aligned}\n\\tag{40}\\]\n更进一步，如果 \\(\\mathbf{\\Psi}_{t'}\\) 是一个与 \\(\\mathbf{a}_t, \\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}, \\ldots\\) 独立的随机变量，那么 \\[\n\\begin{aligned}\n& \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{(\\mathbf{a}_t, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\mathbf{\\Psi}_{t'} \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\mathbb{E}_{\n    (\\mathbf{s}_{t+1}, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t})} \\left[\\mathbf{\\Psi}_{t'} \\right] \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\mathbf{\\Psi}_{t'} \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[\n            \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right]\n        \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[ \\mathbf{\\Psi}_{t'} \\cdot 0 \\right] \\\\\n=& 0\n\\end{aligned}\n\\tag{41}\\]\n其中，为了利用 式 40 的结论，我们利用了全期望定律，即\n\\[\n\\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p} \\left[\\mathbf{x}\\right] = \\mathbb{E}_{\\mathbf{y} \\sim p} \\left[\\mathbb{E}_{\\mathbf{x} \\sim p(\\cdot \\mid \\mathbf{y})} [\\mathbf{x}] \\right]\n\\tag{42}\\]\n来引入我们想要的期望。\n\\[\n\\begin{aligned}\n& \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\mathbf{\\Psi}_i \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\Psi_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_\\theta(s_1, a_1, \\cdots, s_t) \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) p_\\theta(s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\Psi_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\\\\n=& \\sum_{(s_{1}, a_{1}, \\cdots, s_{t})} p_\\theta(s_1, a_1, \\cdots, s_t)  \\sum_{(a_{t}, s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|})} \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) \\Psi_{t'} \\nabla_\\theta p_\\theta(s_{t+1}, \\cdots, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right)  \\\\\n=& \\sum_{(s_{1}, a_{1}, \\cdots, s_{t})} p_\\theta(s_1, a_1, \\cdots, s_t) \\sum_{a_t \\in \\mathcal{A}}  \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\sum_{(s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|})}  p_\\theta(s_{t+1}, \\cdots, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\Psi_{t'} \\\\\n\\end{aligned}\n\\tag{43}\\]\n考虑 Monte Carlo 估计式 式 39 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：\n\\[\n\\mathbb{E}_{\\mathbf{\\tau_{i}} \\sim p_{\\theta}} \\left[\n\\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})\n\\right]\n\\tag{44}\\]\n由于序列决策过程满足因果性，即 \\(\\forall t' &lt; t\\)，\\(\\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\) 独立于 \\(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\)，则可令 ${t’} = {} $，其独立于 \\(\\mathbf{s}_{i, t}, \\mathbf{a}_{i, t}, \\ldots\\)，利用 式 43 的性质，则有 \\[\n\\forall t' &lt; t, \\mathbb{E}_{\\mathbf{\\tau_{i}} \\sim p_{\\theta}} \\left[\n\\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})\n\\right] = 0\n\\tag{45}\\]\n将 式 45 代入 KL 梯度表达式 (式 38) ，即可简化得到：\n\\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] =  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{T} \\left(\\sum_{t'=t}^{T} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{t}) \\right]\n\\tag{46}\\]\n对应的 Monte Carlo 估计式为：\n\\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\left(\\sum_{t'=t}^{|\\tau_i|} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} \\mid s_{i, 1}, \\cdots, a_{i, t-1}, s_{i, t})\n\\tag{47}\\]\n同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：\n\\[\n\\mathcal{L}^{KL}_{\\theta} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\text{nograd}\\left (\\sum_{t'=t}^{|\\tau_i|} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\log \\pi_{\\theta}(a_{i, t} \\mid s_{i, 1}, \\cdots, a_{i, t-1}, s_{i, t})\n\\tag{48}\\]\n这里也可以看到，KL loss 项正确的实现要求：\n\n在序列内 token 间，对对数条件概率先求和，得到 KL 样本值，\n再在序列间求均值。\n\n因此 OpenRLHF (式 13) 与 verl (式 14) 的权重都是错误的。\n\n\n\nhttps://www.wikiwand.com/en/articles/Policy_gradient_method↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-grad-as-kl-reward",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-grad-as-kl-reward",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "5.5 KL 梯度优化可以实现为 KL 样本值 reward",
    "text": "5.5 KL 梯度优化可以实现为 KL 样本值 reward\n在 式 46 中，令 \\(k\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\right) = \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'-1}, \\mathbf{s}_{t'})}\\)，则有： \\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] =  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{T} \\left(\\sum_{t'=t}^{T} k\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\right) \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t-1}, \\mathbf{s}_{t}) \\right]\n\\tag{49}\\]\n不难注意到 式 49 中 \\(k\\) 与 式 25 中 reward \\(r\\) 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。\n类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS28517 等材料。\n\n\n\nhttps://rail.eecs.berkeley.edu/deeprlcourse/↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "6.1 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题",
    "text": "6.1 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题\n遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 \\(\\pi_\\theta \\neq \\pi_{\\theta_{\\text{old}}}\\)，尽管没有来自最新策略 \\(p_{\\theta}\\) 的样本，却仍然在使用基于 on-policy 设置的优化方式。\n\n6.1.1 TRL\nTRL 在 列表 1 中计算 KL 样本值使用的 logprobs 及其对应的轨迹样本均来自采样策略 \\(\\pi_{\\theta_{\\text{old}}}\\)。对应代码可见 列表 7。\n\n\n\n列表 7: TRL 使用采样样本并使用 \\(\\pi_{\\theta_{\\text{old}}}\\) 计算对数似然18\n\n\nqueries = data[\"input_ids\"].to(device)\n# ...\n\nwith unwrap_model_for_generation(\n    self.model, #...\n) as unwrapped_model:\n    query_responses, logitss = batch_generation(\n        unwrapped_model.policy,\n        queries,\n        # ...\n    )\n\n\nfor i in range(0, queries.shape[0], args.local_rollout_forward_batch_size):\n    # ...\n    logits = logitss[i : i + args.local_rollout_forward_batch_size]\n    logprob = selective_log_softmax(logits, response)\n\n\n\n注意，基于 \\(\\mathbf{\\tau} \\sim \\pi_{\\theta_{\\text{old}}}\\) 计算的 KL 样本值可以用于估计 \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_{\\theta_{\\text{old}}} \\mid \\pi_{\\text{ref}}\\right]\\)，在第一次更新时，由于 \\(\\pi_\\theta = \\pi_{\\theta_{\\text{old}}}\\)，所以也可以用于估计 \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)。但问题在于，从第二次更新开始，\\(\\pi_\\theta \\neq \\pi_{\\theta_{\\text{old}}}\\)，而我们仍然希望估计 \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)。\n随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 \\(\\pi_{\\theta}\\) 重新估计 \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)。对应代码可见 列表 8。\n\n\n\n列表 8: TRL PPO 多轮更新\n\n\n# Do multiple epochs of PPO training, with a fresh random shuffle in each epoch\nfor ppo_epoch_idx in range(args.num_ppo_epochs):\n    b_inds = np.random.permutation(args.local_batch_size)\n    minibatch_idx = 0\n    for mini_batch_start in range(0, args.local_batch_size, args.local_mini_batch_size):\n        mini_batch_end = mini_batch_start + args.local_mini_batch_size\n        mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]\n        gradient_accumulation_idx = 0\n        for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):\n            with accelerator.accumulate(model):\n                micro_batch_end = micro_batch_start + args.per_device_train_batch_size\n                micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]\n                mb_advantage = advantages[micro_batch_inds]\n                mb_responses = responses[micro_batch_inds]\n                mb_query_responses = query_responses[micro_batch_inds]\n                mb_logprobs = logprobs[micro_batch_inds]\n                mb_return = returns[micro_batch_inds]\n                mb_values = values[micro_batch_inds]\n\n\n                output, vpred_temp = forward(model, mb_query_responses, processing_class.pad_token_id)\n                logits = output.logits[:, context_length - 1 : -1]\n                logits /= args.temperature + 1e-7\n                new_logprobs = selective_log_softmax(logits, mb_responses)\n                new_logprobs = torch.masked_fill(\n                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB\n                )\n                vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)\n                vpred = torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], 0)\n                vpredclipped = torch.clamp(\n                    vpred,\n                    mb_values - args.cliprange_value,\n                    mb_values + args.cliprange_value,\n                )\n                vf_losses1 = torch.square(vpred - mb_return)\n                vf_losses2 = torch.square(vpredclipped - mb_return)\n                vf_loss_max = torch.max(vf_losses1, vf_losses2)\n                vf_loss = 0.5 * masked_mean(vf_loss_max, ~padding_mask_p1[micro_batch_inds])\n                vf_clipfrac = masked_mean(\n                    (vf_losses2 &gt; vf_losses1).float(), ~padding_mask_p1[micro_batch_inds]\n                )\n                logprobs_diff = new_logprobs - mb_logprobs\n                ratio = torch.exp(logprobs_diff)\n                pg_losses = -mb_advantage * ratio\n                pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)\n                pg_loss_max = torch.max(pg_losses, pg_losses2)\n                pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])\n                loss = pg_loss + args.vf_coef * vf_loss\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n\n\n\n\n\n\nhttps://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432↩︎\n\n\n\n\n6.1.2 OpenRLHF\n类似地，OpenRLHF 在 列表 2 中计算 KL 样本值使用的 log_probs 在 make_experience 时被计算，和对应的样本 sequences 都来自采样策略 \\(\\pi_{\\theta_{\\text{old}}}\\)，而非当前策略 \\(\\pi_{\\theta}\\)。对应代码可见 列表 9。\n\n\n\n列表 9: OpenRLHF 采样样本并使用 \\(\\pi_{\\theta_{\\text{old}}}\\) 计算对数似然\n\n\n# https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595\ndef make_experience(self, samples: Samples) -&gt; Experience:\n    \"\"\"\n    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.\n    \"\"\"\n    # ...\n    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680\n    action_log_probs = self.actor(\n        sequences,\n        num_actions,\n        # ...\n    )\n    # ...\n    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709\n    kl = compute_approx_kl(\n        action_log_probs,\n        base_action_log_probs,\n        # ...\n    )\n\n\n\n从 列表 3 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 \\(\\pi_{\\theta_{\\text{old}}}\\) 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 \\(\\pi_{\\theta}\\) 计算的对数似然，但如 小节 4.1 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。\n\n\n6.1.3 verl\n从 列表 4 可见，verl 同样使用 \\(\\pi_{\\theta_{\\text{old}}}\\) 计算 KL 样本值。\n从 列表 5 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 \\(\\pi_{\\theta_{\\text{old}}}\\) 的 KL 样本值。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#利用重要性采样处理-off-policy-设置",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#利用重要性采样处理-off-policy-设置",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "6.2 利用重要性采样处理 off-policy 设置",
    "text": "6.2 利用重要性采样处理 off-policy 设置\noff-policy 设置下，我们没有来自最新策略 \\(\\pi_{\\theta}\\) 的样本，而只能使用来自采样策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 的样本，但我们仍然希望估计 \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}} \\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)。\n熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[f(\\mathbf{\\tau})\\right] = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) f(\\tau)  = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta_{\\text{old}}}(\\tau) \\frac{p_{\\theta}(\\tau)}{p_{\\theta_{\\text{old}}}(\\tau)} f(\\tau) = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}} \\left[\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})} f(\\mathbf{\\tau})\\right]\n\\tag{50}\\]\n利用重要性采样 (式 50) ，KL 梯度表达式 式 46 可以转化为：\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}} \\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t}) \\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}}\\left[ \\frac{p_{\\theta}(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T})}{p_{\\theta_{\\text{old}}}(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T})}  \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})  \\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}}\\left[ \\left(\\prod_{t=1}^{|\\mathbf{\\tau}|} \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}\\right) \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t}) \\right]\n\\end{aligned}\n\\tag{51}\\]\n对应的 Monte Carlo 估计式为：\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n\\approx& \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\prod_{t=1}^{|\\mathbf{\\tau}_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t=1}^{|\\mathbf{\\tau}_{i}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1}) }{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t}) \\\\\n=& \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\mathbf{\\tau}_{i}|} \\left(\\left(\\prod_{t=1}^{|\\mathbf{\\tau}_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t'=t}^{|\\mathbf{\\tau}_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1}) }{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t})\n\\end{aligned}\n\\tag{52}\\]\n对应的 loss 函数为：\n\\[\n\\mathcal{L}^{KL}_{\\theta} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_{i}|} \\text{nograd}\\left(\\left(\\prod_{t=1}^{|\\tau_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right)\\sum_{t'=t}^{|\\tau_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t})\n\\tag{53}\\]\n类似 式 49，我们可以令\n\\[\nk(\\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t}) = \\left(\\prod_{t=1}^{|\\tau_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t'=t}^{|\\tau_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}\n\\tag{54}\\]\n注意，式 54 中的 \\(k\\) 需要对于每个新的 \\(\\pi_{\\theta}\\) 重新计算。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#修正-grpo-公式中的-kl-项",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#修正-grpo-公式中的-kl-项",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "7.1 修正 GRPO 公式中的 KL 项",
    "text": "7.1 修正 GRPO 公式中的 KL 项\nGRPO 公式 (式 1, 式 2) 对于 KL 优化主要存在两个错误：\n\n忽略了 KL 优化的 off-policy 问题\n错误地将 KL 估计器应用于条件概率\n\n对于这两个问题，在 式 29 的基础上，仿照 式 54，我们可以按如下方式修正：\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\left\\{ \\sum_{t=1}^{\\left|o_i\\right|} \\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old}}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right] -\\beta \\left(\\prod_{t=1}^{|o_{i}|}\\frac{\\pi_{\\theta}(o_{i, t} | q, o_{i,\\lt t})}{ \\pi_{\\theta_{\\text{old}}}(o_{i, t} | q, o_{i,\\lt t})}\\right) k\\left( \\frac{\\prod_{t=1}^{|o_i|} \\pi_{\\text{ref}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\prod_{t=1}^{|o_i|} \\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\right) \\right\\}\n\\end{aligned}\n\\tag{55}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#修正流行-llm-rl-框架中的-kl-优化实现",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#修正流行-llm-rl-框架中的-kl-优化实现",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "7.2 修正流行 LLM RL 框架中的 KL 优化实现",
    "text": "7.2 修正流行 LLM RL 框架中的 KL 优化实现\n目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：\n\n实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）\n错误地实现了平均操作\n\n对于这些问题，可以按照如下思路修正：\n\n为 KL 项添加重要性采样系数，这需要从第二轮更新开始，每次基于新的 \\(\\pi_\\theta\\) 重新计算 KL loss / reward 项\n应用 KL 估计器时，先对于序列内 token 间的对数条件概率求和，得到轨迹联合概率，再代入公式\n如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 式 54 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）\n如果不希望应用 reward 优化的其他技术，可以按 式 53 实现为 KL loss 项"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#对于-kl-梯度更好的估计样本量",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#对于-kl-梯度更好的估计样本量",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "8.1 对于 KL 梯度更好的估计样本量",
    "text": "8.1 对于 KL 梯度更好的估计样本量\n如 小节 5.5 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？\n此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#kl-regularized-rl-的理论优势",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#kl-regularized-rl-的理论优势",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "8.2 KL-Regularized RL 的理论优势",
    "text": "8.2 KL-Regularized RL 的理论优势\n最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。\n然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 Zhao 等 (2025) 证明了 KL-regularized RL 的 regret 只有 \\(\\mathcal{O}(\\log T)\\)，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 \\(\\mathcal{O}(\\sqrt{T})\\)。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#致谢",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#致谢",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "9.1 致谢",
    "text": "9.1 致谢\n\n感谢生广明、Wei Xiong、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论。\n感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。\n\n\n\nhttps://tongyx361.github.io↩︎"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#写作契机trpoppo-与-grpo-中的-kl-为什么不一样",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#写作契机trpoppo-与-grpo-中的-kl-为什么不一样",
    "title": "重新思考 RL 中的 KL 梯度优化",
    "section": "9.2 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”",
    "text": "9.2 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”\n\n笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题20：\n\nA small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?\nTRPO (Schulman 等 2015)\n\n\\[\n\\begin{aligned}\n& \\underset{\\theta}{\\text{maximize}}~L_{\\theta_{\\text {old }}}(\\theta) \\\\\n& \\text { subject to } \\bar{D}_{\\mathrm{KL}}^{\\rho_{\\theta_{\\text {old }}}}\\left(\\theta_{\\text {old }}, \\theta\\right) \\leq \\delta\n\\end{aligned}\n\\tag{56}\\]\n\nPPO (Schulman 等 2017)\n\n\\[\nL^{K L P E N}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(\\mathbf{y}_t \\mid \\mathbf{x}_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(\\mathbf{y}_t \\mid \\mathbf{x}_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid \\mathbf{x}_t\\right), \\pi_\\theta\\left(\\cdot \\mid \\mathbf{x}_t\\right)\\right]\\right]\n\\tag{57}\\]\n\nGRPO (Shao 等 2024)\n\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\right\\}\n\\end{aligned}\n\\tag{58}\\]\n这个问题本身的答案是非常简单的。\n首先，这个问题混淆了两种不同的 KL 惩罚项：\n\n\\(\\text{KL}[\\pi_{\\theta_{\\text{old}}},\\pi_{\\theta}]\\)，其作用是约束最新策略 \\(\\pi_{\\theta}\\)不要离采样策略\\(\\pi_{\\theta_{\\text{old}}}\\) 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。\n\\(\\text{KL}[\\pi_{\\theta},\\pi_{\\theta_{\\text{ref}}}]\\)，其作用是约束最新策略 \\(\\pi_{\\theta}\\)不要离参考策略\\(\\pi_{\\theta_{\\text{ref}}}\\) 太远，从而更充分地利用参考策略中的先验。\n\n另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 \\(\\text{KL}[\\pi_{\\theta_{\\text{old}}},\\pi_{\\theta}]\\)。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：\n\nLet \\(r_t(\\theta)\\)denote the probability ratio\\(r_{t}(\\theta)=\\frac{\\pi_{\\theta}\\left(a_t \\mid s_t\\right)}{\\left.\\pi_{\\theta_{\\text {old }}}\\left|a_t\\right| s_t\\right)}\\), so \\(r\\left(\\theta_{\\text{old}}\\right)=1\\). TRPO maximizes a “surrogate” objective\n\n\\[\nL^{\\text{CPI}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t\\right]=\\hat{\\mathbb{E}}_t\\left[r_t(\\theta) \\hat{A}_t\\right] .\n\\]\n\n…\nThe main objective we propose is the following:\n\n\\[\nL^{\\text{CLIP}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\n\\]\n\nwhere ePsilon is a hyperparameter, say, \\(\\epsilon=0.2\\). The motivation for this objective is as follows. The first term inside the \\(\\min\\) is \\(L^{C P I}\\). The second term, \\(\\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\), modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving \\(r_t\\)outside of the interval\\([1-\\epsilon, 1+\\epsilon]\\).\n…\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence \\(d_{\\text {targ }}\\) each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve included it here because it’s an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy update:\n\nUsing several epochs of minibatch SGD, optimize the KL-penalized objective\n\n\n\\[\nL^{\\text{KLPEN}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right]\n\\]\n\n\n\n顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 \\(r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}\\)就是\\(K L\\left[\\pi_{\\theta_{d d}}, \\pi_\\theta\\right]=\\mathbb{E}_{a_t \\sim \\pi_{\\theta_{d t}}\\left(\\cdot \\mid s_t\\right)}\\left[\\log \\frac{\\pi_{\\theta_{d t}}\\left(a_t \\mid s_t\\right)}{\\pi_\\theta\\left(a_t \\mid s_t\\right)}\\right]\\) 中对单个样本 \\((s_t, a_t)\\) 的值中 \\(\\log\\) 的真数。\n\n\n\n\n\n\n\n\nhttps://x.com/pufanyi/status/1888845956684370202↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs 博客",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n重新思考 RL 中的 KL 梯度优化\n\n\n修正 GRPO 公式与流行 LLM RL 框架\n\n\n\nChinese 中文\n\n\nTechnical 技术\n\n\n\n对于 LLM RL 中相对于参考策略的 KL 优化，\nGRPO 公式\n\n没有处理 KL 项的 off-policy 问题，这可以通过在多轮更新时重新计算并添加重要性采样系数解决\n将 KL 估计样本量应用于动作条件概率，而非轨迹联合概率，与 John Schulman 原分析不符（对应导出的梯度也可能因此而错误）\n\n目前流行的 LLM RL 框架（TRL，OpenRLHF，verl）也没有避免上述问题，存在其他问题：\n\n在计算 KL loss 项时默认不去除任何梯度，实际得到的梯度通常不是在优化 KL 散度\nKL loss 项的平均操作存在错误。\n\n本文分析了上述问题，并提供了正确的 KL loss / reward 项实现的数学推导与上述问题的修正思路。 \n\n\n\n\n\nMar 9, 2025\n\n\n童雨轩\n\n\n\n\n\n\nNo matching items"
  }
]