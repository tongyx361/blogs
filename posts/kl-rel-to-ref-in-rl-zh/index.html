<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="童雨轩">

<title>重新思考 RL 中的 KL 梯度优化 – Shawn/Yuxuan Tong 童雨轩</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-e85eba1097e763ab92a8026b31d9210d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-d4dbfae94d399dbf1014af73f007268d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-e85eba1097e763ab92a8026b31d9210d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shawn/Yuxuan Tong 童雨轩</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://tongyx361.github.io"> <i class="bi bi-person" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/tongyx361"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tongyx361"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">重新思考 RL 中的 KL 梯度优化</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">修正 GRPO 公式与流行 LLM RL 框架</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Chinese 中文</div>
                <div class="quarto-category">Technical 技术</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>童雨轩 <a href="mailto:tongyuxuan361@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">2025/03/09</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Takeways</div>
      <p>对于 LLM RL 中相对于参考策略的 KL 优化，GRPO 公式</p>
      <ol type="1">
      <li>没有处理 KL 项的 off-policy 问题，这可以通过在多轮更新时重新计算 KL 项并添加重要性采样系数解决</li>
      <li>先将 KL 估计样本量应用于动作对数条件似然再求和，而非先求和得到概率再应用估计样本量，与 John Schulman “Approximating KL Divergence”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> 分析不符（对应导出的梯度也可能因此而错误）</li>
      </ol>
      <p>目前流行的 LLM RL 框架（TRL，OpenRLHF，verl）也没有避免上述问题，且存在其他问题：</p>
      <ol start="3" type="1">
      <li>在计算 KL loss 项时默认不去除任何梯度，实际得到的梯度通常不是在优化 KL 散度</li>
      <li>KL loss 项的平均操作存在错误。</li>
      </ol>
      <p>本文基于序列决策过程（而非 bandit）建模分析了上述问题，并提供了正确的 KL loss / reward 项实现的数学推导与上述问题的修正思路。</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#sec-grpo-kl-misunderstanding" id="toc-sec-grpo-kl-misunderstanding" class="nav-link active" data-scroll-target="#sec-grpo-kl-misunderstanding"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</a></li>
  <li><a href="#sec-popular-llm-rl-kl-optim" id="toc-sec-popular-llm-rl-kl-optim" class="nav-link" data-scroll-target="#sec-popular-llm-rl-kl-optim"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</a>
  <ul class="collapse">
  <li><a href="#trlkl-reward-项" id="toc-trlkl-reward-项" class="nav-link" data-scroll-target="#trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</a></li>
  <li><a href="#openrlhf" id="toc-openrlhf" class="nav-link" data-scroll-target="#openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</a>
  <ul class="collapse">
  <li><a href="#sec-openrlhf-kl-reward" id="toc-sec-openrlhf-kl-reward" class="nav-link" data-scroll-target="#sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项" id="toc-kl-loss-项" class="nav-link" data-scroll-target="#kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#verl" id="toc-verl" class="nav-link" data-scroll-target="#verl"><span class="header-section-number">2.3</span> verl</a>
  <ul class="collapse">
  <li><a href="#kl-reward-项" id="toc-kl-reward-项" class="nav-link" data-scroll-target="#kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项-1" id="toc-kl-loss-项-1" class="nav-link" data-scroll-target="#kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#sec-why-kl-reward" id="toc-sec-why-kl-reward" class="nav-link" data-scroll-target="#sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</a>
  <ul class="collapse">
  <li><a href="#kl-reward-的流行应当源自-rlhf-与-instructgpt" id="toc-kl-reward-的流行应当源自-rlhf-与-instructgpt" class="nav-link" data-scroll-target="#kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</a></li>
  <li><a href="#sec-oai-kl-reward-src" id="toc-sec-oai-kl-reward-src" class="nav-link" data-scroll-target="#sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</a></li>
  <li><a href="#kl-reward-最早的出处" id="toc-kl-reward-最早的出处" class="nav-link" data-scroll-target="#kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-rl-kl-optim-formulation" id="toc-sec-rl-kl-optim-formulation" class="nav-link" data-scroll-target="#sec-rl-kl-optim-formulation"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</a>
  <ul class="collapse">
  <li><a href="#rl-中的-kl-散度通常定义在轨迹分布上" id="toc-rl-中的-kl-散度通常定义在轨迹分布上" class="nav-link" data-scroll-target="#rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</a></li>
  <li><a href="#将轨迹展开为状态-动作序列" id="toc-将轨迹展开为状态-动作序列" class="nav-link" data-scroll-target="#将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</a></li>
  <li><a href="#markov-决策过程中的-kl-散度" id="toc-markov-决策过程中的-kl-散度" class="nav-link" data-scroll-target="#markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</a></li>
  <li><a href="#sec-lm-as-dp" id="toc-sec-lm-as-dp" class="nav-link" data-scroll-target="#sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</a></li>
  <li><a href="#估计-kl-散度" id="toc-估计-kl-散度" class="nav-link" data-scroll-target="#估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</a>
  <ul class="collapse">
  <li><a href="#几乎不可能直接计算-kl-散度的真实值" id="toc-几乎不可能直接计算-kl-散度的真实值" class="nav-link" data-scroll-target="#几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</a></li>
  <li><a href="#通常使用-monte-carlo-方法估计-kl-散度" id="toc-通常使用-monte-carlo-方法估计-kl-散度" class="nav-link" data-scroll-target="#通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</a></li>
  <li><a href="#不同的-kl-估计量" id="toc-不同的-kl-估计量" class="nav-link" data-scroll-target="#不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#流行-on-policy-kl-优化实现的数学形式化" id="toc-流行-on-policy-kl-优化实现的数学形式化" class="nav-link" data-scroll-target="#流行-on-policy-kl-优化实现的数学形式化"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</a>
  <ul class="collapse">
  <li><a href="#sec-kl-loss-impl" id="toc-sec-kl-loss-impl" class="nav-link" data-scroll-target="#sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</a>
  <ul class="collapse">
  <li><a href="#不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" id="toc-不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" class="nav-link" data-scroll-target="#不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</a></li>
  <li><a href="#k_1-导出的梯度期望为-0" id="toc-k_1-导出的梯度期望为-0" class="nav-link" data-scroll-target="#k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <span class="math inline">\(k_1\)</span> 导出的梯度：期望为 0</a></li>
  <li><a href="#k_2-导出的梯度" id="toc-k_2-导出的梯度" class="nav-link" data-scroll-target="#k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <span class="math inline">\(k_2\)</span> 导出的梯度</a></li>
  <li><a href="#k_3-导出的梯度" id="toc-k_3-导出的梯度" class="nav-link" data-scroll-target="#k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <span class="math inline">\(k_3\)</span> 导出的梯度</a></li>
  <li><a href="#小结流行的-kl-loss-项-实现并不合理" id="toc-小结流行的-kl-loss-项-实现并不合理" class="nav-link" data-scroll-target="#小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</a></li>
  </ul></li>
  <li><a href="#分析流行的-kl-reward-项-实现" id="toc-分析流行的-kl-reward-项-实现" class="nav-link" data-scroll-target="#分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</a>
  <ul class="collapse">
  <li><a href="#sec-analogy-pg-kl" id="toc-sec-analogy-pg-kl" class="nav-link" data-scroll-target="#sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</a></li>
  <li><a href="#不同-kl-估计量导出的-reward-项的作用" id="toc-不同-kl-估计量导出的-reward-项的作用" class="nav-link" data-scroll-target="#不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</a></li>
  <li><a href="#小结在-on-policy-设置下修正-grpo-目标的-kl-项" id="toc-小结在-on-policy-设置下修正-grpo-目标的-kl-项" class="nav-link" data-scroll-target="#小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-derive-kld-grad" id="toc-sec-derive-kld-grad" class="nav-link" data-scroll-target="#sec-derive-kld-grad"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</a>
  <ul class="collapse">
  <li><a href="#在已知环境中简化-kl-梯度估计" id="toc-在已知环境中简化-kl-梯度估计" class="nav-link" data-scroll-target="#在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</a></li>
  <li><a href="#简写为-contextual-bandit" id="toc-简写为-contextual-bandit" class="nav-link" data-scroll-target="#简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</a></li>
  <li><a href="#还原为已知环境决策过程" id="toc-还原为已知环境决策过程" class="nav-link" data-scroll-target="#还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</a></li>
  <li><a href="#利用因果性技巧化简-kl-梯度估计" id="toc-利用因果性技巧化简-kl-梯度估计" class="nav-link" data-scroll-target="#利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计</a></li>
  <li><a href="#sec-kl-grad-as-kl-reward" id="toc-sec-kl-grad-as-kl-reward" class="nav-link" data-scroll-target="#sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</a></li>
  </ul></li>
  <li><a href="#off-policy-设置下如何估计-kl-散度的梯度" id="toc-off-policy-设置下如何估计-kl-散度的梯度" class="nav-link" data-scroll-target="#off-policy-设置下如何估计-kl-散度的梯度"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</a>
  <ul class="collapse">
  <li><a href="#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" id="toc-流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" class="nav-link" data-scroll-target="#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</a>
  <ul class="collapse">
  <li><a href="#trl" id="toc-trl" class="nav-link" data-scroll-target="#trl"><span class="header-section-number">6.1.1</span> TRL</a></li>
  <li><a href="#openrlhf-1" id="toc-openrlhf-1" class="nav-link" data-scroll-target="#openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</a></li>
  <li><a href="#verl-1" id="toc-verl-1" class="nav-link" data-scroll-target="#verl-1"><span class="header-section-number">6.1.3</span> verl</a></li>
  </ul></li>
  <li><a href="#利用重要性采样处理-off-policy-设置" id="toc-利用重要性采样处理-off-policy-设置" class="nav-link" data-scroll-target="#利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</a></li>
  </ul></li>
  <li><a href="#结论如何正确地在-rl-中优化-kl-散度" id="toc-结论如何正确地在-rl-中优化-kl-散度" class="nav-link" data-scroll-target="#结论如何正确地在-rl-中优化-kl-散度"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</a>
  <ul class="collapse">
  <li><a href="#修正-grpo-公式中的-kl-项" id="toc-修正-grpo-公式中的-kl-项" class="nav-link" data-scroll-target="#修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</a></li>
  <li><a href="#修正流行-llm-rl-框架中的-kl-优化实现" id="toc-修正流行-llm-rl-框架中的-kl-优化实现" class="nav-link" data-scroll-target="#修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</a></li>
  </ul></li>
  <li><a href="#讨论" id="toc-讨论" class="nav-link" data-scroll-target="#讨论"><span class="header-section-number">8</span> 讨论</a>
  <ul class="collapse">
  <li><a href="#对于-kl-梯度更好的估计样本量" id="toc-对于-kl-梯度更好的估计样本量" class="nav-link" data-scroll-target="#对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</a></li>
  <li><a href="#kl-regularized-rl-的理论优势" id="toc-kl-regularized-rl-的理论优势" class="nav-link" data-scroll-target="#kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#sec-grpo-kl-misunderstanding" id="toc-sec-grpo-kl-misunderstanding"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</a></li>
  <li><a href="#sec-popular-llm-rl-kl-optim" id="toc-sec-popular-llm-rl-kl-optim"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</a>
  <ul>
  <li><a href="#trlkl-reward-项" id="toc-trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</a></li>
  <li><a href="#openrlhf" id="toc-openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</a>
  <ul>
  <li><a href="#sec-openrlhf-kl-reward" id="toc-sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项" id="toc-kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#verl" id="toc-verl"><span class="header-section-number">2.3</span> verl</a>
  <ul>
  <li><a href="#kl-reward-项" id="toc-kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</a></li>
  <li><a href="#kl-loss-项-1" id="toc-kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</a></li>
  </ul></li>
  <li><a href="#sec-why-kl-reward" id="toc-sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</a>
  <ul>
  <li><a href="#kl-reward-的流行应当源自-rlhf-与-instructgpt" id="toc-kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</a></li>
  <li><a href="#sec-oai-kl-reward-src" id="toc-sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</a></li>
  <li><a href="#kl-reward-最早的出处" id="toc-kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-rl-kl-optim-formulation" id="toc-sec-rl-kl-optim-formulation"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</a>
  <ul>
  <li><a href="#rl-中的-kl-散度通常定义在轨迹分布上" id="toc-rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</a></li>
  <li><a href="#将轨迹展开为状态-动作序列" id="toc-将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</a></li>
  <li><a href="#markov-决策过程中的-kl-散度" id="toc-markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</a></li>
  <li><a href="#sec-lm-as-dp" id="toc-sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</a></li>
  <li><a href="#估计-kl-散度" id="toc-估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</a>
  <ul>
  <li><a href="#几乎不可能直接计算-kl-散度的真实值" id="toc-几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</a></li>
  <li><a href="#通常使用-monte-carlo-方法估计-kl-散度" id="toc-通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</a></li>
  <li><a href="#不同的-kl-估计量" id="toc-不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#流行-on-policy-kl-优化实现的数学形式化" id="toc-流行-on-policy-kl-优化实现的数学形式化"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</a>
  <ul>
  <li><a href="#sec-kl-loss-impl" id="toc-sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</a>
  <ul>
  <li><a href="#不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" id="toc-不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</a></li>
  <li><a href="#k_1-导出的梯度期望为-0" id="toc-k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <span class="math inline">\(k_1\)</span> 导出的梯度：期望为 0</a></li>
  <li><a href="#k_2-导出的梯度" id="toc-k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <span class="math inline">\(k_2\)</span> 导出的梯度</a></li>
  <li><a href="#k_3-导出的梯度" id="toc-k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <span class="math inline">\(k_3\)</span> 导出的梯度</a></li>
  <li><a href="#小结流行的-kl-loss-项-实现并不合理" id="toc-小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</a></li>
  </ul></li>
  <li><a href="#分析流行的-kl-reward-项-实现" id="toc-分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</a>
  <ul>
  <li><a href="#sec-analogy-pg-kl" id="toc-sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</a></li>
  <li><a href="#不同-kl-估计量导出的-reward-项的作用" id="toc-不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</a></li>
  <li><a href="#小结在-on-policy-设置下修正-grpo-目标的-kl-项" id="toc-小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-derive-kld-grad" id="toc-sec-derive-kld-grad"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</a>
  <ul>
  <li><a href="#在已知环境中简化-kl-梯度估计" id="toc-在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</a></li>
  <li><a href="#简写为-contextual-bandit" id="toc-简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</a></li>
  <li><a href="#还原为已知环境决策过程" id="toc-还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</a></li>
  <li><a href="#利用因果性技巧化简-kl-梯度估计" id="toc-利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计</a></li>
  <li><a href="#sec-kl-grad-as-kl-reward" id="toc-sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</a></li>
  </ul></li>
  <li><a href="#off-policy-设置下如何估计-kl-散度的梯度" id="toc-off-policy-设置下如何估计-kl-散度的梯度"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</a>
  <ul>
  <li><a href="#流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" id="toc-流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</a>
  <ul>
  <li><a href="#trl" id="toc-trl"><span class="header-section-number">6.1.1</span> TRL</a></li>
  <li><a href="#openrlhf-1" id="toc-openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</a></li>
  <li><a href="#verl-1" id="toc-verl-1"><span class="header-section-number">6.1.3</span> verl</a></li>
  </ul></li>
  <li><a href="#利用重要性采样处理-off-policy-设置" id="toc-利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</a></li>
  </ul></li>
  <li><a href="#结论如何正确地在-rl-中优化-kl-散度" id="toc-结论如何正确地在-rl-中优化-kl-散度"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</a>
  <ul>
  <li><a href="#修正-grpo-公式中的-kl-项" id="toc-修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</a></li>
  <li><a href="#修正流行-llm-rl-框架中的-kl-优化实现" id="toc-修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</a></li>
  </ul></li>
  <li><a href="#讨论" id="toc-讨论"><span class="header-section-number">8</span> 讨论</a>
  <ul>
  <li><a href="#对于-kl-梯度更好的估计样本量" id="toc-对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</a></li>
  <li><a href="#kl-regularized-rl-的理论优势" id="toc-kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</a></li>
  </ul></li>
  <li><a href="#附录" id="toc-附录"><span class="header-section-number">9</span> 附录</a>
  <ul>
  <li><a href="#相关工作" id="toc-相关工作"><span class="header-section-number">9.1</span> 相关工作</a></li>
  <li><a href="#写作契机trpoppo-与-grpo-中的-kl-为什么不一样" id="toc-写作契机trpoppo-与-grpo-中的-kl-为什么不一样"><span class="header-section-number">9.2</span> 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”</a></li>
  <li><a href="#致谢" id="toc-致谢"><span class="header-section-number">9.3</span> 致谢</a></li>
  <li><a href="#引用" id="toc-引用"><span class="header-section-number">9.4</span> 引用</a></li>
  </ul></li>
  </ul>
</nav>
<section id="sec-grpo-kl-misunderstanding" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</h1>
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(<a href="#ref-shao2024deepseekmath" role="doc-biblioref">Shao et al. 2024</a>)</span> 的优化目标公式为：</p>
<p><span id="eq-grpo-obj"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left\{\min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]-\beta \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\right\}
\end{aligned}
\tag{1}\]</span></span></p>
<p>其中</p>
<p><span id="eq-grpo-obj-kl-term"><span class="math display">\[
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]=\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1
\tag{2}\]</span></span></p>
<p>首先，<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a> 中出现了 <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>，这意味着其考虑了 off-policy 设置，但 <a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a> 中却没有相应的处理，只适用于 <span class="math inline">\(o_i \sim \pi_{\theta}\)</span>，无法正确处理 <span class="math inline">\(o_i \sim \pi_{\theta_\text{old}}\)</span>。</p>
<p>其次，<a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a> 将估计样本量 <span class="math inline">\(\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1\)</span> 写成 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]\)</span> 也并不十分恰当，因为 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]\)</span> 通常表示 KL 散度的真实值。</p>
<p>而目前流行的 LLM RL 框架，在实现 KL 优化时，通常也忽略了 off-policy 问题，同时还存在其他一系列问题：</p>
<ol type="1">
<li>误认为前向传播估计出 KL 散度，再反向传播就能得到其梯度（但实际上通常并非如此）；</li>
<li>忽略了先对动作动作对数条件似然应用 KL 估计样本量再求和并非良好定义的行为，导致梯度错误；</li>
<li>忽略了同一轨迹上 KL 对数概率必须求和以获得轨迹联合概率，而不能求平均；</li>
<li>错误地计算了平均操作。</li>
</ol>
<p>由于 on-policy 设置更加简单，但也已经暴露了上述大部分问题，我们可以先从 on-policy 设置开始讨论，后续再考虑 off-policy 设置。</p>
<aside id="footnotes" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol>
<li id="fn1"><p>http://joschu.net/blog/kl-approx.html<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="sec-popular-llm-rl-kl-optim" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</h1>
<p>我们可以先回顾目前流行的 LLM RL 框架中对于 KL 优化的实现。以下我们以</p>
<ol type="1">
<li>TRL<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>，</li>
<li>OpenRLHF<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="citation" data-cites="hu2024openrlhf">(<a href="#ref-hu2024openrlhf" role="doc-biblioref">Hu et al. 2024</a>)</span></li>
<li>verl<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="citation" data-cites="sheng2024hybridflow">(<a href="#ref-sheng2024hybridflow" role="doc-biblioref">Sheng et al. 2024</a>)</span></li>
</ol>
<p>为例。</p>
<p>熟悉这些框架的读者可以跳过本节，直接从 <a href="#sec-rl-kl-optim-formulation" class="quarto-xref">Section&nbsp;3</a> 开始阅读。</p>
<section id="trlkl-reward-项" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</h2>
<p>TRL 计算 KL 定义中的样本值 <span class="math inline">\(\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}\)</span>，并将其从 reward 中减去。对应代码可见 <a href="#lst-trl-kl-reward" class="quarto-xref">Listing&nbsp;1</a>。</p>
<div id="lst-trl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: TRL 计算 KL 样本值 <span class="math inline">\(\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}\)</span> 并从 reward 中减去<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>
</figcaption>
<div aria-describedby="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># 4. compute rewards</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>kl <span class="op">=</span> logprobs <span class="op">-</span> ref_logprobs</span>
<span id="cb1-3"><a href="#cb1-3"></a>non_score_reward <span class="op">=</span> <span class="op">-</span>args.kl_coef <span class="op">*</span> kl</span>
<span id="cb1-4"><a href="#cb1-4"></a>rewards <span class="op">=</span> non_score_reward.clone()</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># ...</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>rewards[[actual_start, actual_end]] <span class="op">+=</span> scores</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<p>这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 <a href="#sec-why-kl-reward" class="quarto-xref">Section&nbsp;2.4</a>。</p>
<aside id="footnotes-2" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="2">
<li id="fn2"><p>https://github.com/huggingface/trl<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://github.com/OpenRLHF/OpenRLHF<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://github.com/volcengine/verl<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="openrlhf" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</h2>
<section id="sec-openrlhf-kl-reward" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</h3>
<p>与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 <a href="#lst-openrlhf-calc-kl-estimator" class="quarto-xref">Listing&nbsp;2</a>。</p>
<div id="lst-openrlhf-calc-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> compute_approx_kl(</span>
<span id="cb2-2"><a href="#cb2-2"></a>    log_probs: torch.Tensor,</span>
<span id="cb2-3"><a href="#cb2-3"></a>    log_probs_base: torch.Tensor,</span>
<span id="cb2-4"><a href="#cb2-4"></a>    action_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb2-5"><a href="#cb2-5"></a>    kl_estimator: <span class="bu">str</span> <span class="op">=</span> <span class="st">"k1"</span>,</span>
<span id="cb2-6"><a href="#cb2-6"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb2-7"><a href="#cb2-7"></a>    <span class="co">"""</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">    Compute the approximate KL divergence between two distributions.</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="co">    Schulman blog: http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="co">    Args:</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co">        log_probs: Log probabilities of the new distribution.</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co">        log_probs_base: Log probabilities of the base distribution.</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="co">        action_mask: Mask for actions.</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co">    """</span></span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a>    <span class="cf">if</span> kl_estimator <span class="op">==</span> <span class="st">"k1"</span>:</span>
<span id="cb2-18"><a href="#cb2-18"></a>        log_ratio <span class="op">=</span> log_probs.<span class="bu">float</span>() <span class="op">-</span> log_probs_base.<span class="bu">float</span>()</span>
<span id="cb2-19"><a href="#cb2-19"></a>        <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-20"><a href="#cb2-20"></a>            log_ratio <span class="op">=</span> log_ratio <span class="op">*</span> action_mask</span>
<span id="cb2-21"><a href="#cb2-21"></a></span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="co"># The $k_2$ estimator is the non negative kl approximation in</span></span>
<span id="cb2-23"><a href="#cb2-23"></a>    <span class="co"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>    <span class="co"># The k2_loss is approximately equivalent to the</span></span>
<span id="cb2-25"><a href="#cb2-25"></a>    <span class="co"># one-step KL divergence penalty with the $k_1$ estimator</span></span>
<span id="cb2-26"><a href="#cb2-26"></a>    <span class="co"># used in https://arxiv.org/abs/2310.10505.</span></span>
<span id="cb2-27"><a href="#cb2-27"></a>    <span class="cf">if</span> kl_estimator <span class="op">==</span> <span class="st">"k2"</span>:</span>
<span id="cb2-28"><a href="#cb2-28"></a>        log_ratio <span class="op">=</span> log_probs.<span class="bu">float</span>() <span class="op">-</span> log_probs_base.<span class="bu">float</span>()</span>
<span id="cb2-29"><a href="#cb2-29"></a>        <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-30"><a href="#cb2-30"></a>            log_ratio <span class="op">=</span> log_ratio <span class="op">*</span> action_mask</span>
<span id="cb2-31"><a href="#cb2-31"></a>        log_ratio <span class="op">=</span> log_ratio<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="cb2-32"><a href="#cb2-32"></a></span>
<span id="cb2-33"><a href="#cb2-33"></a>    <span class="co"># The $k_3$ estimator is the non negative kl approximation in</span></span>
<span id="cb2-34"><a href="#cb2-34"></a>    <span class="co"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-35"><a href="#cb2-35"></a>    <span class="cf">if</span> kl_estimator <span class="op">==</span> <span class="st">"k3"</span>:</span>
<span id="cb2-36"><a href="#cb2-36"></a>        log_ratio <span class="op">=</span> log_probs.<span class="bu">float</span>() <span class="op">-</span> log_probs_base.<span class="bu">float</span>()</span>
<span id="cb2-37"><a href="#cb2-37"></a>        <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-38"><a href="#cb2-38"></a>            log_ratio <span class="op">=</span> log_ratio <span class="op">*</span> action_mask</span>
<span id="cb2-39"><a href="#cb2-39"></a>        log_ratio <span class="op">=</span> <span class="op">-</span>log_ratio</span>
<span id="cb2-40"><a href="#cb2-40"></a>        log_ratio <span class="op">=</span> log_ratio.exp() <span class="op">-</span> <span class="dv">1</span> <span class="op">-</span> log_ratio</span>
<span id="cb2-41"><a href="#cb2-41"></a></span>
<span id="cb2-42"><a href="#cb2-42"></a>    <span class="cf">return</span> log_ratio</span>
<span id="cb2-43"><a href="#cb2-43"></a></span>
<span id="cb2-44"><a href="#cb2-44"></a></span>
<span id="cb2-45"><a href="#cb2-45"></a><span class="kw">def</span> compute_reward(</span>
<span id="cb2-46"><a href="#cb2-46"></a>    <span class="co"># ...</span></span>
<span id="cb2-47"><a href="#cb2-47"></a>    kl_coef: <span class="bu">float</span>,</span>
<span id="cb2-48"><a href="#cb2-48"></a>    kl: Union[torch.Tensor, <span class="bu">list</span>[torch.Tensor]],</span>
<span id="cb2-49"><a href="#cb2-49"></a>    <span class="co"># ...</span></span>
<span id="cb2-50"><a href="#cb2-50"></a>    num_actions: Optional[Union[<span class="bu">int</span>, <span class="bu">list</span>[<span class="bu">int</span>]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb2-51"><a href="#cb2-51"></a>    <span class="co"># ...</span></span>
<span id="cb2-52"><a href="#cb2-52"></a>) <span class="op">-&gt;</span> Union[torch.Tensor, <span class="bu">list</span>[torch.Tensor]]:</span>
<span id="cb2-53"><a href="#cb2-53"></a>    <span class="co"># ...</span></span>
<span id="cb2-54"><a href="#cb2-54"></a>    <span class="cf">if</span> action_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-55"><a href="#cb2-55"></a>        <span class="co"># ...</span></span>
<span id="cb2-56"><a href="#cb2-56"></a>    <span class="cf">else</span>:</span>
<span id="cb2-57"><a href="#cb2-57"></a>        <span class="co"># ...</span></span>
<span id="cb2-58"><a href="#cb2-58"></a>        reward <span class="op">=</span> []</span>
<span id="cb2-59"><a href="#cb2-59"></a>        <span class="cf">for</span> i, (kl_seg, action_len) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(kl, num_actions)):</span>
<span id="cb2-60"><a href="#cb2-60"></a>            kl_reward <span class="op">=</span> <span class="op">-</span>kl_coef <span class="op">*</span> kl_seg</span>
<span id="cb2-61"><a href="#cb2-61"></a>            kl_reward[action_len <span class="op">-</span> <span class="dv">1</span>] <span class="op">+=</span> r[i]</span>
<span id="cb2-62"><a href="#cb2-62"></a>            reward.append(kl_reward)</span>
<span id="cb2-63"><a href="#cb2-63"></a></span>
<span id="cb2-64"><a href="#cb2-64"></a>    <span class="cf">return</span> reward</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<aside id="footnotes-3" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="6">
<li id="fn6"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</h3>
<p>此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 <a href="#lst-openrlhf-calc-kl-loss" class="quarto-xref">Listing&nbsp;3</a>。</p>
<div id="lst-openrlhf-calc-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> training_step_actor(<span class="va">self</span>, experience: Experience) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, <span class="bu">float</span>]:</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="va">self</span>.actor.train()</span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="co"># ...</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(experience.sequences, <span class="bu">list</span>):</span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="co"># ...</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="cf">else</span>:</span>
<span id="cb3-7"><a href="#cb3-7"></a>        sequences <span class="op">=</span> experience.sequences</span>
<span id="cb3-8"><a href="#cb3-8"></a>        old_action_log_probs <span class="op">=</span> experience.action_log_probs</span>
<span id="cb3-9"><a href="#cb3-9"></a>        advantages <span class="op">=</span> experience.advantages</span>
<span id="cb3-10"><a href="#cb3-10"></a>        num_actions <span class="op">=</span> experience.action_mask.size(<span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11"></a>        packed_seq_lens <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-12"><a href="#cb3-12"></a>        attention_mask <span class="op">=</span> experience.attention_mask</span>
<span id="cb3-13"><a href="#cb3-13"></a>        <span class="cf">if</span> <span class="va">self</span>.args.use_kl_loss <span class="kw">and</span> experience.base_action_log_probs <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-14"><a href="#cb3-14"></a>            base_action_log_probs <span class="op">=</span> experience.base_action_log_probs</span>
<span id="cb3-15"><a href="#cb3-15"></a></span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="co"># actor loss</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    action_log_probs, output <span class="op">=</span> <span class="va">self</span>.actor(</span>
<span id="cb3-18"><a href="#cb3-18"></a>        sequences,</span>
<span id="cb3-19"><a href="#cb3-19"></a>        num_actions,</span>
<span id="cb3-20"><a href="#cb3-20"></a>        <span class="co"># ...</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>    )</span>
<span id="cb3-22"><a href="#cb3-22"></a>    <span class="co"># ...</span></span>
<span id="cb3-23"><a href="#cb3-23"></a>    <span class="co"># loss function</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>    actor_loss <span class="op">=</span> <span class="va">self</span>.actor_loss_fn(</span>
<span id="cb3-25"><a href="#cb3-25"></a>        action_log_probs,</span>
<span id="cb3-26"><a href="#cb3-26"></a>        old_action_log_probs,</span>
<span id="cb3-27"><a href="#cb3-27"></a>        advantages,</span>
<span id="cb3-28"><a href="#cb3-28"></a>        <span class="co"># ...</span></span>
<span id="cb3-29"><a href="#cb3-29"></a>    )</span>
<span id="cb3-30"><a href="#cb3-30"></a></span>
<span id="cb3-31"><a href="#cb3-31"></a>    <span class="cf">if</span> <span class="va">self</span>.args.use_kl_loss:</span>
<span id="cb3-32"><a href="#cb3-32"></a>        <span class="cf">if</span> <span class="va">self</span>.initial_model <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-33"><a href="#cb3-33"></a>            kl <span class="op">=</span> compute_approx_kl(</span>
<span id="cb3-34"><a href="#cb3-34"></a>                action_log_probs,</span>
<span id="cb3-35"><a href="#cb3-35"></a>                base_action_log_probs,</span>
<span id="cb3-36"><a href="#cb3-36"></a>                <span class="co"># ...</span></span>
<span id="cb3-37"><a href="#cb3-37"></a>                kl_estimator<span class="op">=</span><span class="va">self</span>.args.kl_estimator,</span>
<span id="cb3-38"><a href="#cb3-38"></a>            )</span>
<span id="cb3-39"><a href="#cb3-39"></a>        <span class="cf">else</span>:</span>
<span id="cb3-40"><a href="#cb3-40"></a>            kl <span class="op">=</span> torch.zeros_like(action_log_probs, dtype<span class="op">=</span>action_log_probs.dtype, device<span class="op">=</span>action_log_probs.device)</span>
<span id="cb3-41"><a href="#cb3-41"></a></span>
<span id="cb3-42"><a href="#cb3-42"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.args.packing_samples:</span>
<span id="cb3-43"><a href="#cb3-43"></a>            kl_mean <span class="op">=</span> masked_mean(kl, experience.action_mask, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-44"><a href="#cb3-44"></a>        <span class="cf">else</span>:</span>
<span id="cb3-45"><a href="#cb3-45"></a>            <span class="co"># ...</span></span>
<span id="cb3-46"><a href="#cb3-46"></a></span>
<span id="cb3-47"><a href="#cb3-47"></a>        kl_loss <span class="op">=</span> kl_mean.mean()</span>
<span id="cb3-48"><a href="#cb3-48"></a>        experience.info[<span class="st">"kl"</span>] <span class="op">=</span> kl_loss.item()</span>
<span id="cb3-49"><a href="#cb3-49"></a>    <span class="cf">else</span>:</span>
<span id="cb3-50"><a href="#cb3-50"></a>        kl_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-51"><a href="#cb3-51"></a>    <span class="co"># ...</span></span>
<span id="cb3-52"><a href="#cb3-52"></a>    <span class="va">self</span>.strategy.optimizer_step(<span class="va">self</span>.actor_optim, <span class="va">self</span>.actor, <span class="va">self</span>.actor_scheduler, name<span class="op">=</span><span class="st">"actor"</span>)</span>
<span id="cb3-53"><a href="#cb3-53"></a>    <span class="co"># ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<aside id="footnotes-4" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="7">
<li id="fn7"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="verl" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="verl"><span class="header-section-number">2.3</span> verl</h2>
<section id="kl-reward-项" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</h3>
<p>verl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 <a href="#lst-verl-kl-reward" class="quarto-xref">Listing&nbsp;4</a>。</p>
<div id="lst-verl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: verl 将 KL 估计样本值从 reward 中减去 <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>
</figcaption>
<div aria-describedby="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty<span class="op">=</span><span class="st">'kl'</span>):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="co"># ...</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="co"># compute kl between ref_policy and current policy</span></span>
<span id="cb4-4"><a href="#cb4-4"></a>    <span class="cf">if</span> <span class="st">'ref_log_prob'</span> <span class="kw">in</span> data.batch.keys():</span>
<span id="cb4-5"><a href="#cb4-5"></a>        kld <span class="op">=</span> core_algos.kl_penalty(data.batch[<span class="st">'old_log_probs'</span>], data.batch[<span class="st">'ref_log_prob'</span>],</span>
<span id="cb4-6"><a href="#cb4-6"></a>                                    kl_penalty<span class="op">=</span>kl_penalty)  <span class="co"># (batch_size, response_length)</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>        kld <span class="op">=</span> kld <span class="op">*</span> response_mask</span>
<span id="cb4-8"><a href="#cb4-8"></a>        beta <span class="op">=</span> kl_ctrl.value</span>
<span id="cb4-9"><a href="#cb4-9"></a>    <span class="cf">else</span>:</span>
<span id="cb4-10"><a href="#cb4-10"></a>        beta <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-11"><a href="#cb4-11"></a>        kld <span class="op">=</span> torch.zeros_like(response_mask, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>    token_level_rewards <span class="op">=</span> token_level_scores <span class="op">-</span> beta <span class="op">*</span> kld</span>
<span id="cb4-14"><a href="#cb4-14"></a>    <span class="co"># ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<aside id="footnotes-5" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="8">
<li id="fn8"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项-1" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</h3>
<p>verl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 <a href="#lst-verl-kl-loss" class="quarto-xref">Listing&nbsp;5</a>。</p>
<div id="lst-verl-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>
</figcaption>
<div aria-describedby="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> update_policy(<span class="va">self</span>, data: DataProto):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="co"># make sure we are in training mode</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="va">self</span>.actor_module.train()</span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="co"># ...</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.config.ppo_epochs):</span>
<span id="cb5-6"><a href="#cb5-6"></a>        <span class="cf">for</span> batch_idx, data <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb5-7"><a href="#cb5-7"></a>            <span class="co"># ...</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>            <span class="va">self</span>.actor_optimizer.zero_grad()</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a>            <span class="cf">for</span> data <span class="kw">in</span> micro_batches:</span>
<span id="cb5-11"><a href="#cb5-11"></a>                <span class="co"># ...</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>                responses <span class="op">=</span> data[<span class="st">'responses'</span>]</span>
<span id="cb5-13"><a href="#cb5-13"></a>                <span class="co"># ...</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>                old_log_prob <span class="op">=</span> data[<span class="st">'old_log_probs'</span>]</span>
<span id="cb5-15"><a href="#cb5-15"></a>                <span class="co"># ...</span></span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a>                <span class="co"># all return: (bsz, response_length)</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>                entropy, log_prob <span class="op">=</span> <span class="va">self</span>._forward_micro_batch(micro_batch<span class="op">=</span>data, temperature<span class="op">=</span>temperature)</span>
<span id="cb5-19"><a href="#cb5-19"></a></span>
<span id="cb5-20"><a href="#cb5-20"></a>                pg_loss, pg_clipfrac, ppo_kl <span class="op">=</span> core_algos.compute_policy_loss(old_log_prob<span class="op">=</span>old_log_prob,</span>
<span id="cb5-21"><a href="#cb5-21"></a>                                                                                log_prob<span class="op">=</span>log_prob,</span>
<span id="cb5-22"><a href="#cb5-22"></a>                                                                                <span class="co"># ...</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>                                                                                )</span>
<span id="cb5-24"><a href="#cb5-24"></a>                <span class="co"># ...</span></span>
<span id="cb5-25"><a href="#cb5-25"></a></span>
<span id="cb5-26"><a href="#cb5-26"></a>                <span class="co"># compute policy loss</span></span>
<span id="cb5-27"><a href="#cb5-27"></a>                policy_loss <span class="op">=</span> pg_loss <span class="op">-</span> entropy_loss <span class="op">*</span> entropy_coeff</span>
<span id="cb5-28"><a href="#cb5-28"></a></span>
<span id="cb5-29"><a href="#cb5-29"></a>                <span class="cf">if</span> <span class="va">self</span>.config.use_kl_loss:</span>
<span id="cb5-30"><a href="#cb5-30"></a>                    ref_log_prob <span class="op">=</span> data[<span class="st">'ref_log_prob'</span>]</span>
<span id="cb5-31"><a href="#cb5-31"></a>                    <span class="co"># compute kl loss</span></span>
<span id="cb5-32"><a href="#cb5-32"></a>                    kld <span class="op">=</span> core_algos.kl_penalty(logprob<span class="op">=</span>log_prob,</span>
<span id="cb5-33"><a href="#cb5-33"></a>                                                ref_logprob<span class="op">=</span>ref_log_prob,</span>
<span id="cb5-34"><a href="#cb5-34"></a>                                                kl_penalty<span class="op">=</span><span class="va">self</span>.config.kl_loss_type)</span>
<span id="cb5-35"><a href="#cb5-35"></a>                    kl_loss <span class="op">=</span> masked_mean(kld, response_mask)</span>
<span id="cb5-36"><a href="#cb5-36"></a></span>
<span id="cb5-37"><a href="#cb5-37"></a>                    policy_loss <span class="op">=</span> policy_loss <span class="op">+</span> kl_loss <span class="op">*</span> <span class="va">self</span>.config.kl_loss_coef</span>
<span id="cb5-38"><a href="#cb5-38"></a>                <span class="co"># ...</span></span>
<span id="cb5-39"><a href="#cb5-39"></a>                loss.backward()</span>
<span id="cb5-40"><a href="#cb5-40"></a>            <span class="co"># ...</span></span>
<span id="cb5-41"><a href="#cb5-41"></a>            grad_norm <span class="op">=</span> <span class="va">self</span>._optimizer_step()</span>
<span id="cb5-42"><a href="#cb5-42"></a>    <span class="co"># ...</span></span>
<span id="cb5-43"><a href="#cb5-43"></a>    <span class="va">self</span>.actor_optimizer.zero_grad()</span>
<span id="cb5-44"><a href="#cb5-44"></a>    <span class="co"># ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<aside id="footnotes-6" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="9">
<li id="fn9"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="sec-why-kl-reward" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</h2>
<p>将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT <span class="citation" data-cites="ouyang2022instructgpt">(<a href="#ref-ouyang2022instructgpt" role="doc-biblioref">Ouyang et al. 2022</a>)</span>。</p>
<section id="kl-reward-的流行应当源自-rlhf-与-instructgpt" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</h3>
<p>InstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。</p>
<blockquote class="blockquote">
<p>… In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models “PPO.”</p>
<p>…</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
\text { objective }(\phi)= &amp; E_{(x, y) \sim D_\pi^{\mathrm{RL}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
&amp; \gamma E_{x \sim D_{\text {remin }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
\]</span></p>
<blockquote class="blockquote">
<p>where <span class="math inline">\(\pi_\phi^{\mathrm{RL}}\)</span>is the learned RL policy,<span class="math inline">\(\pi^{\mathrm{SFT}}\)</span> is the supervised trained model, and<span class="math inline">\(D_{\text {pretrain }}\)</span>is the pretraining distribution. The KL reward coefficient, <span class="math inline">\(\beta\)</span>, and the pretraining loss coefficient, <span class="math inline">\(\gamma\)</span>, control the strength of the KL penalty and pretraining gradients respectively. For “PPO” models, <span class="math inline">\(\gamma\)</span> is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</p>
</blockquote>
</section>
<section id="sec-oai-kl-reward-src" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</h3>
<p>然而，在OpenAI 早期的一篇论文 “Learning to summarize from human feedback” <span class="citation" data-cites="stiennon2020summarize">(<a href="#ref-stiennon2020summarize" role="doc-biblioref">Stiennon et al. 2020</a>)</span> 中，他们就已经采用了 KL reward，并提及了出处：</p>
<blockquote class="blockquote">
<p>… <strong>Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy <span class="math inline">\(\pi_\phi^{\mathrm{RL}}\)</span> with parameters <span class="math inline">\(\phi\)</span> and this original supervised model <span class="math inline">\(\pi^{\mathrm{SFT}}\)</span>, as previously done in [25].</strong> The full reward <span class="math inline">\(R\)</span> can be written as:</p>
</blockquote>
<p><span class="math display">\[
R(x, y)=r_\theta(x, y)-\beta \log \left[\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right]
\]</span></p>
<blockquote class="blockquote">
<p>This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn’t learn to produce outputs that are too different from those that the reward model has seen during training.</p>
</blockquote>
</section>
<section id="kl-reward-最早的出处" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</h3>
<p><a href="#sec-oai-kl-reward-src" class="quarto-xref">Section&nbsp;2.4.2</a> 中 OpenAI 引用的 KL reward 出处 [25] 是 “Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog” <span class="citation" data-cites="jaques2019wayoffpolicy">(<a href="#ref-jaques2019wayoffpolicy" role="doc-biblioref">Jaques et al. 2019</a>)</span>。</p>
<p>实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：</p>
<blockquote class="blockquote">
<p>Rather than simply sample from the prior, we would like the <span class="math inline">\(Q\)</span>-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior <span class="math inline">\(p(y \mid x)\)</span>, and the <span class="math inline">\(Q\)</span>-network policy <span class="math inline">\(\pi_\theta\)</span>, while still maximizing reward. Given a trajectory of actions, <span class="math inline">\(\tau=\left\{a_1, a_2, \ldots a_{t-1}\right\}\)</span>, let <span class="math inline">\(q(\tau)=\prod_{t=1}^T \pi_\theta\left(a_t, s_t\right)\)</span>be the policy of our<span class="math inline">\(Q\)</span>-learning algorithm at the trajectory level. Similarly, let <span class="math inline">\(p(\tau)=\prod_{t=1}^T p\left(a_t \mid s_t\right)\)</span>be the prior distribution over the trajectory, and<span class="math inline">\(r(\tau)\)</span> be the rewards. We seek to maximize the following KL-regularized objective:</p>
</blockquote>
<p><span class="math display">\[
L(q)=\mathbb{E}_{q(\tau)}[r(\tau)] / c-D_{\text{KL}}[q(\tau) \mid p(\tau)]
\]</span></p>
<blockquote class="blockquote">
<p>Since <span class="math inline">\(D_{\text{KL}}[q \mid p]=\sum_x q(x)(\log q(x)-\log p(x))\)</span>, we can see that this is equivalent to maximizing the following expected value function of the policy <span class="math inline">\(\pi_\theta\)</span> at the action level:</p>
</blockquote>
<p><span class="math display">\[
Q^\pi\left(s_t, a_t\right)=\mathbb{E}_\pi\left[\sum^T r\left(s_{t^{\prime}}, a_{t^{\prime}}\right) / c+\log p\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)-\log \pi\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)\right]
\]</span></p>
<blockquote class="blockquote">

</blockquote>
</section>
</section>
</section>
<section id="sec-rl-kl-optim-formulation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</h1>
<p>为了进一步分析这些 LLM RL 框架中的实现是否正确，我们需要先形式化 LLM RL 中 KL 散度的优化。</p>
<section id="rl-中的-kl-散度通常定义在轨迹分布上" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</h2>
<p>GRPO 公式 (<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a>) 中的 KL 项可以定义为：</p>
<p><span id="eq-def-kl-theta-ref"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right]
\end{aligned}
\tag{3}\]</span></span></p>
<p>其中 <span class="math inline">\(\mathbf{\tau}\)</span> 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 <span class="math inline">\(p_{\theta}\)</span> 与参考策略整体分布 <span class="math inline">\(p_{\text{ref}}\)</span> 的 KL 散度。</p>
</section>
<section id="将轨迹展开为状态-动作序列" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</h2>
<p>RL 文献中通常会将轨迹 <span class="math inline">\(\mathbf{\tau}\)</span> 展开为状态-动作序列 <span class="math inline">\(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\)</span>：<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p><span id="eq-def-kl-theta-ref-state-action-ag"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|},\right) \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|},, \mathbf{a}_{|\mathbf{\tau}|},\right)}{p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\log \frac{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] \\
\end{aligned}
\tag{4}\]</span></span></p>
<p>其中 <span class="math inline">\(|\mathbf{\tau}|\)</span> 为轨迹动作数的随机变量。</p>
<p>此处利用了联合概率的展开，以 <span class="math inline">\(p_{\theta}\)</span> 为例：</p>
<p><span id="eq-dp-expansion"><span class="math display">\[
p_{\theta}(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)
\tag{5}\]</span></span></p>
<p>注意区分整体概率分布 <span class="math inline">\(p_{\theta}\)</span>、策略（条件）概率分布 <span class="math inline">\(\pi_{\theta}\)</span> 与状态转移概率分布 <span class="math inline">\(p\)</span>。</p>
<aside id="footnotes-7" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="10">
<li id="fn10"><p>这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，<span class="math inline">\(\mathbf{q}\)</span> 对应于 <span class="math inline">\(\mathbf{s}_1\)</span>，而 <span class="math inline">\({\mathbf{o}}\)</span> 对应于 <span class="math inline">\(\mathbf{\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T}\)</span>。<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="markov-决策过程中的-kl-散度" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</h2>
<p>实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>。</p>
<p>Markov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 <span class="math inline">\(n\)</span> 个历史状态和动作，而非全部的历史信息，对应的过程称为 <span class="math inline">\(n\)</span> 阶 Markov 过程。以 <span class="math inline">\(n=1\)</span> 为例：</p>
<p><span id="eq-def-markov-prop"><span class="math display">\[
\begin{aligned}
\pi(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) &amp; = \pi(\mathbf{a}_t \mid \mathbf{s}_t) \\
p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) &amp; = p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) \\
\end{aligned}
\tag{6}\]</span></span></p>
<p>则 <a href="#eq-dp-expansion" class="quarto-xref">Equation&nbsp;5</a> 中的联合概率可以进一步简化为：</p>
<p><span id="eq-dp-expansion-markov-1"><span class="math display">\[
p(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(s_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t)
\tag{7}\]</span></span></p>
<p>如果考虑一阶 Markov 过程，则 <a href="#eq-def-kl-theta-ref-state-action-ag" class="quarto-xref">Equation&nbsp;4</a> 中的 KL 可以进一步简化为：</p>
<p><span id="eq-def-kl-theta-ref-state-action-markov-1"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] = &amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] \\
&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}\right] \\
\end{aligned}
\tag{8}\]</span></span></p>
<aside id="footnotes-8" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="11">
<li id="fn11"><p>https://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="sec-lm-as-dp" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</h2>
<p>目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。</p>
<p>尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 <span class="math inline">\(s_1\)</span> 表示 prompt 中的所有 token，对于 <span class="math inline">\(t &gt;1\)</span>，如果令 <span class="math inline">\(s_t\)</span> 表示第 <span class="math inline">\(t\)</span> 个动作 token 前的所有 token，则自回归模型满足 Markov 性质，否则不一定。</p>
<p>接下来，我们先令 <span class="math inline">\(s_t\)</span> 表示前 <span class="math inline">\(t\)</span> 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。</p>
</section>
<section id="估计-kl-散度" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</h2>
<section id="几乎不可能直接计算-kl-散度的真实值" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</h3>
<p>实际实现中，我们几乎不可能直接计算出 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\)</span>，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 <span class="math inline">\(\left|\mathcal{T}\right|\)</span> 与轨迹最大长度 <span class="math inline">\(T = \max_{\mathbf{\tau} \in \mathcal{T}} |\mathbf{\tau}|\)</span> 成指数关系： <span id="eq-def-rl-kl-avg-over-traj"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] \\
&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) \\
\end{aligned}
\tag{9}\]</span></span></p>
</section>
<section id="通常使用-monte-carlo-方法估计-kl-散度" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</h3>
<p>所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>来估计 RL 中的 KL 散度，例如：</p>
<p><span id="eq-def-rl-kl-mc-k1"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) \\
&amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{|\mathbf{\tau_{i }}|} \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)
\end{aligned}
\tag{10}\]</span></span></p>
<p>其中，<span class="math inline">\(\mathbf{\tau_{i}} = \left(\mathbf{s}_{i,1}, \mathbf{a}_{i,1}, \cdots, \mathbf{s}_{i,|\mathbf{\tau_{i}}|}, \mathbf{a}_{i,|\mathbf{\tau_{i}}|}\right) \sim p_{\theta}\)</span>，<span class="math inline">\(N\)</span> 为估计使用的轨迹样本数量。</p>
<aside id="footnotes-9" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="12">
<li id="fn12"><p>https://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="不同的-kl-估计量" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</h3>
<p>实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。</p>
<p>设 KL 估计量为 <span class="math inline">\(k\)</span>，则对应的 KL 估计值为</p>
<p><span id="eq-def-rl-kl-mc-general"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} k(\tau_i)
\end{aligned}
\tag{11}\]</span></span></p>
<p>例如 <a href="#sec-openrlhf-kl-reward" class="quarto-xref">Section&nbsp;2.2.1</a> 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 <code>k1</code>, <code>k2</code>, <code>k3</code>，这应该是主要参考了 John Schulman 的博客 “Approximating KL Divergence”。</p>
<p>verl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>，但目前还没有实现。对应代码可见 <a href="#lst-verl-kl-estimator" class="quarto-xref">Listing&nbsp;6</a>。</p>
<div id="lst-verl-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: verl 的 KL 散度 Monte Carlo 估计样本值<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>
</figcaption>
<div aria-describedby="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) <span class="op">-&gt;</span> torch.FloatTensor:</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="co"># ...</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"kl"</span>:</span>
<span id="cb6-4"><a href="#cb6-4"></a>        <span class="cf">return</span> logprob <span class="op">-</span> ref_logprob</span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"abs"</span>:</span>
<span id="cb6-7"><a href="#cb6-7"></a>        <span class="cf">return</span> (logprob <span class="op">-</span> ref_logprob).<span class="bu">abs</span>()</span>
<span id="cb6-8"><a href="#cb6-8"></a></span>
<span id="cb6-9"><a href="#cb6-9"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"mse"</span>:</span>
<span id="cb6-10"><a href="#cb6-10"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (logprob <span class="op">-</span> ref_logprob).square()</span>
<span id="cb6-11"><a href="#cb6-11"></a></span>
<span id="cb6-12"><a href="#cb6-12"></a>    <span class="co"># J. Schulman. Approximating kl divergence, 2020.</span></span>
<span id="cb6-13"><a href="#cb6-13"></a>    <span class="co"># # URL http://joschu.net/blog/kl-approx.html.</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">'low_var_kl'</span>:</span>
<span id="cb6-15"><a href="#cb6-15"></a>        kl <span class="op">=</span> ref_logprob <span class="op">-</span> logprob</span>
<span id="cb6-16"><a href="#cb6-16"></a>        ratio <span class="op">=</span> torch.exp(kl)</span>
<span id="cb6-17"><a href="#cb6-17"></a>        kld <span class="op">=</span> (ratio <span class="op">-</span> kl <span class="op">-</span> <span class="dv">1</span>).contiguous()</span>
<span id="cb6-18"><a href="#cb6-18"></a>        <span class="cf">return</span> torch.clamp(kld, <span class="bu">min</span><span class="op">=-</span><span class="dv">10</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a>    <span class="cf">if</span> kl_penalty <span class="op">==</span> <span class="st">"full"</span>:</span>
<span id="cb6-21"><a href="#cb6-21"></a>        <span class="co"># so, here logprob and ref_logprob should contain the logits for every token in vocabulary</span></span>
<span id="cb6-22"><a href="#cb6-22"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb6-23"><a href="#cb6-23"></a></span>
<span id="cb6-24"><a href="#cb6-24"></a>    <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<p>由于 <span class="math inline">\(k_1\)</span>、<span class="math inline">\(k_2\)</span>、<span class="math inline">\(k_3\)</span> 三种估计量最为流行，我们将以这三种估计量为例展开分析。</p>
<p>考虑 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} k_j(\tau_i)\)</span>，其中 <span class="math inline">\(\tau_i \sim p_{\theta}\)</span>，令 <span class="math inline">\(r = \frac{\pi_{\text{ref}}(\tau_i)}{\pi_{\theta}(\tau_i)}\)</span>，注意，此处 <span class="math inline">\(r\)</span> 并非 KL 定义中的样本量，而是其倒数，则：</p>
<p><span id="eq-def-kl-estimators"><span class="math display">\[
\begin{aligned}
k_{1} &amp; = - \log r \\
k_{2} &amp; = \frac{1}{2} (\log r)^2 \\
k_{3} &amp; = (r - 1) - \log r
\end{aligned}
\tag{12}\]</span></span></p>
<aside id="footnotes-10" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="13">
<li id="fn13"><p>这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
</section>
<section id="流行-on-policy-kl-优化实现的数学形式化" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</h1>
<p>神经网络模型普遍使用梯度法优化，因此，我们主要关注这些 KL 优化实现导出的梯度。</p>
<p>而由于 reward 项优化的实现涉及到基线（Baseline）、折扣（Discounting）、GAE <span class="citation" data-cites="schulman2018gae">(<a href="#ref-schulman2018gae" role="doc-biblioref">Schulman et al. 2018</a>)</span> 等内容，较为复杂，我们可以先分析 KL loss 项实现。</p>
<section id="sec-kl-loss-impl" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</h2>
<p>上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。</p>
<p>然而，如 <a href="#sec-grpo-kl-misunderstanding" class="quarto-xref">Section&nbsp;1</a> 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。</p>
<section id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</h3>
<p>观察 <a href="#lst-openrlhf-calc-kl-loss" class="quarto-xref">Listing&nbsp;3</a> 计算 “KL loss” 项的部分。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># ...</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>kl <span class="op">=</span> compute_approx_kl(</span>
<span id="cb7-3"><a href="#cb7-3"></a>    action_log_probs,</span>
<span id="cb7-4"><a href="#cb7-4"></a>    base_action_log_probs,</span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="co"># ...</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    kl_estimator<span class="op">=</span><span class="va">self</span>.args.kl_estimator,</span>
<span id="cb7-7"><a href="#cb7-7"></a>)</span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="co"># ...</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>kl_mean <span class="op">=</span> masked_mean(kl, experience.action_mask, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co"># ...</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>kl_loss <span class="op">=</span> kl_mean.mean()</span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="co"># ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>这些代码：</p>
<ol type="1">
<li>计算了 <code>kl</code>，对应对每个动作 token <span class="math inline">\(a_{i,t}\)</span> 计算 “KL 估计量” <span class="math inline">\(k\)</span>。</li>
<li>计算了 <code>kl_mean</code>，对应对每个轨迹 <span class="math inline">\(\tau_i\)</span> 计算均值 <span class="math inline">\(\frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k\)</span>。</li>
<li>计算了 <code>kl_loss</code>，对应对所有轨迹样本计算均值 <span class="math inline">\(\frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k\)</span>。</li>
</ol>
<p>由于其没有去除任何梯度，因此其导出的梯度估计值为</p>
<p><span id="eq-def-kl-loss-grad-estim-openrlhf"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \left( \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \frac{1}{|\tau_i|} k \right) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k
\end{aligned}
\tag{13}\]</span></span></p>
<p><a href="#lst-verl-kl-loss" class="quarto-xref">Listing&nbsp;5</a> 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：</p>
<p><span id="eq-def-kl-loss-grad-estim-verl"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \left( \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} k \right) = \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} \nabla_{\theta} k
\end{aligned}
\tag{14}\]</span></span></p>
<p>我们将平均操作一般化为权重 <span class="math inline">\(w_{\mathbf{\tau}}\)</span> 与 <span class="math inline">\(w_{t}\)</span>，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：</p>
<p><span id="eq-def-kl-loss-grad-estim-general"><span class="math display">\[
\begin{aligned}
\sum_{i=1}^{N} w_{\mathbf{\tau}_i} \sum_{t=1}^{|\tau_i|} w_{t} \nabla_{\theta} k \\
\end{aligned}
\tag{15}\]</span></span></p>
<p>则</p>
<ul>
<li>OpenRLHF 对应 <span class="math inline">\(w_{\mathbf{\tau}} = \frac{1}{N}, w_{t} = \frac{1}{|\tau|}\)</span>；</li>
<li>verl 对应 <span class="math inline">\(w_{\mathbf{\tau}} = \frac{1}{\sum_{i=1}^{N} |\tau_i|}, w_{t} = 1\)</span>。</li>
</ul>
<p>此处，我们先以 OpenRLHF 的梯度估计 (<a href="#eq-def-kl-loss-grad-estim-openrlhf" class="quarto-xref">Equation&nbsp;13</a>) 为例，分析不同 KL 估计量导出的梯度估计，其满足：</p>
<p><span id="eq-def-kl-loss-grad-expect-openrlhf"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau}_i \sim p_{\theta}} \left[ \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k \right] = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} k \right]
\tag{16}\]</span></span></p>
<p>我们会在 <a href="#sec-derive-kld-grad" class="quarto-xref">Section&nbsp;5</a> 中推导正确的 KL 梯度估计。</p>
</section>
<section id="k_1-导出的梯度期望为-0" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <span class="math inline">\(k_1\)</span> 导出的梯度：期望为 0</h3>
<p>向 <a href="#eq-def-kl-loss-grad-expect-openrlhf" class="quarto-xref">Equation&nbsp;16</a> 代入 <span class="math inline">\(k = k_1 = - \log r = \log \frac{1}{r} = \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\)</span>，导出的梯度估计为</p>
<p><span id="eq-kl-loss-grad-sample-k1"><span class="math display">\[
\begin{aligned}
&amp; \frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k \\
=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \\
=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}\log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \left( \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) + \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) + \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \right) \right) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) \right) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_\theta(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \\
=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\tau)
\end{aligned}
\tag{17}\]</span></span></p>
<p>则其导出的梯度期望满足：</p>
<p><span id="eq-kl-loss-grad-expect-k1"><span class="math display">\[
\begin{aligned}
\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})\right]
&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} \nabla_{\theta} \log p_{\theta}(\tau) \\
&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \\
&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} \nabla_{\theta} p_{\theta}(\tau) \\
&amp; = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} \\
&amp; = \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \right]
\end{aligned}
\tag{18}\]</span></span></p>
<p>此处利用了 <span class="math inline">\(p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \frac{1}{p_{\theta}(\tau)} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \nabla_{\theta} p_{\theta}(\tau)\)</span>。</p>
<p>所以 <span class="math inline">\(k_1\)</span> loss 项优化的量是 <span class="math inline">\(\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \right]\)</span>。这意味着该优化过程会降低采样轨迹的长度。</p>
<p>特别地，当不对同一轨迹中的 “<span class="math inline">\(k_1\)</span> 估计量”求均值，而是求和时，可以直接将 <span class="math inline">\(\frac{1}{|\tau|}\)</span> 这一项替换为 <span class="math inline">\(1\)</span>，得到 <span id="eq-kl-loss-grad-expect-k1-no-intra-traj-mean"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) = \sum_{\tau \in \mathcal{T}} \nabla_{\theta} p_{\theta} = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta} = \nabla_{\theta} 1 = 0
\tag{19}\]</span></span><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<p>这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。</p>
<p>无论哪种情况，<span class="math inline">\(k_1\)</span> 导出的优化量都非常奇怪，不太可能出于实现者的本意。</p>
<p>同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。接下来，我们将忽略这一操作，即将 <span class="math inline">\(\frac{1}{|\tau|}\)</span> 一项替换为 <span class="math inline">\(1\)</span>。</p>
<aside id="footnotes-11" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="15">
<li id="fn15"><p>此处对数似然的梯度的期望值为 0，是一个著名的性质，会在接下来频繁用到。<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="k_2-导出的梯度" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <span class="math inline">\(k_2\)</span> 导出的梯度</h3>
<p>向 <a href="#eq-def-kl-loss-grad-expect-openrlhf" class="quarto-xref">Equation&nbsp;16</a> 代入 <span class="math inline">\(k = k_2 = \frac{1}{2} (\log r)^2 = \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right)^2\)</span>，导出的单条轨迹 <span class="math inline">\(\mathbf{\tau} \sim p_{\theta}\)</span> 的梯度为 <span id="eq-kl-loss-grad-sample-k2"><span class="math display">\[
\begin{aligned}
&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k\\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}  \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)^2 \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \\
\end{aligned}
\tag{20}\]</span></span></p>
<p>显然，</p>
<p><span id="eq-kl-loss-grad-sample-k2-wrong"><span class="math display">\[
\begin{aligned}
&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \\
\neq &amp; \left( \sum_{t=1}^{|\mathbf{\tau}|}  \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \left( \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \right) \\
=&amp; \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})
\end{aligned}
\tag{21}\]</span></span></p>
<p>然而，</p>
<p><span id="eq-kl-loss-grad-expect-k2-wrong"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] \\
=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} \log p_{\theta}(\tau) \\
=&amp; \sum_{\tau \in \mathcal{T}} \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} p_{\theta}(\tau) \\
=&amp; \sum_{\tau \in \mathcal{T}} \left[ \left( \log p_{\theta}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) - \left( \log p_{\text{ref}}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) \right] \\
=&amp; \sum_{\tau \in \mathcal{T}}  \left[ \nabla_{\theta} (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) -  \nabla_{\theta} \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right] \\
=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  \left[ (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) - \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right] \\
=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  p_{\theta} \left[ \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} - 1 \right) \right] \\
=&amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} - 1 \right) \right] \\
= &amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right] \\
= &amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]
\end{aligned}
\tag{22}\]</span></span></p>
<p>此处利用了 <span class="math inline">\(\log p(x) \nabla_{\theta} p(x) = \nabla_{\theta} (\log p(x) - 1) p(x)\)</span></p>
<p>因此，最小化 <span class="math inline">\(k_2\)</span> loss 项 (<a href="#eq-kl-loss-grad-sample-k2-wrong" class="quarto-xref">Equation&nbsp;21</a>) ，并非在优化 <span class="math inline">\(\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\)</span>。</p>
</section>
<section id="k_3-导出的梯度" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <span class="math inline">\(k_3\)</span> 导出的梯度</h3>
<p>向 <a href="#eq-def-kl-loss-grad-expect-openrlhf" class="quarto-xref">Equation&nbsp;16</a> 代入 <span class="math inline">\(k = k_3 = (r - 1) - \log r = (\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1) - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\)</span>，导出的单条轨迹 <span class="math inline">\(\mathbf{\tau} \sim p_{\theta}\)</span> 的梯度为 <span id="eq-kl-loss-grad-sample-k3"><span class="math display">\[
\begin{aligned}
&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \left(\frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1 - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right) \\
=&amp; \sum_{t=1}^{|\mathbf{\tau}|} - \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} \\
=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} \\
=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) + \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \\
\end{aligned}
\tag{23}\]</span></span></p>
<p>其中，根据 <a href="#eq-kl-loss-grad-expect-k1-no-intra-traj-mean" class="quarto-xref">Equation&nbsp;19</a>，<span class="math inline">\(\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = 0\)</span>，不妨直接省略。</p>
<p>而剩余部分似乎很难通过消去 <span class="math inline">\(\pi_{\theta}(\mathbf{\tau})\)</span> 来提出 <span class="math inline">\(\nabla_{\theta}\)</span> 并准确分析。但显然也并非在优化 KL 散度。</p>
</section>
<section id="小结流行的-kl-loss-项-实现并不合理" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</h3>
<p>综上所述，对于 OpenRLHF 实现的 “KL loss 项”，</p>
<ol type="1">
<li>对同一轨迹内的 “KL 估计量” 求均值这一操作很可能是错误的，正确操作应当为求和，对应于根据对数条件概率求对数联合概率。</li>
<li><span class="math inline">\(k_1\)</span> 导出的梯度
<ol type="1">
<li>若对同一轨迹内的 “KL 估计量” 求均值，则会导致输出长度减小，</li>
<li>而如果修正为求和，则其期望为 0，在平均意义上不改分布。</li>
</ol></li>
<li><span class="math inline">\(k_2\)</span>，<span class="math inline">\(k_3\)</span> 导出的梯度则十分复杂，难以分析，但都并非在优化 KL 散度，这可能是因为其错误地将 KL 估计样本量应用于动作对数条件似然并求和。回顾 KL 估计量公式 (<a href="#eq-def-kl-estimators" class="quarto-xref">Equation&nbsp;12</a>) ，应当注意到这些估计量是直接作用于似然 <span class="math inline">\(p_{\theta}(\mathbf{\tau})\)</span>，而没有保证作用于概率后求积/对数和仍然有意义。</li>
</ol>
</section>
</section>
<section id="分析流行的-kl-reward-项-实现" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</h2>
<section id="sec-analogy-pg-kl" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</h3>
<p>由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是： <span id="eq-pg-est-adv"><span class="math display">\[
\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[r(\mathbf{\tau})\right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \hat{A}_t \right]
\tag{24}\]</span></span></p>
<p>其中 <span class="math inline">\(\hat{A}_t\)</span> 为优势（Advantage）的估计量。</p>
<p>为了方便观察 KL reward 项发挥的作用，我们将 <span class="math inline">\(r_{\mathbf{\tau}}\)</span> 展开，并不妨考虑一个更简单的估计，例如：</p>
<p><span id="eq-pg-est-ret"><span class="math display">\[
\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{|\mathbf{\tau}|} r(\mathbf{s}_t, \mathbf{a}_t) \right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \sum_{t'=1}^{|\tau|} r(s_{t'}, a_{t'}) \right]
\tag{25}\]</span></span></p>
<p>简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 “Policy Gradient” 一讲<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>。</p>
<p>类比 <span class="math inline">\(r_{t'}\)</span> 导出的梯度期望，将负的 KL 样本量 <span class="math inline">\(- \log \frac{\pi_\theta\left(a_t \mid s_t \right)}{\pi_{\text{ref}}\left(a_t \mid s_t \right)}\)</span> 加入 reward <span class="math inline">\(r_{t'}\)</span> 代入其中，导出的梯度期望为：</p>
<p><span id="eq-kl-grad-est-markov-1"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|}  \left( \nabla_\theta \log \pi_\theta\left(a_t \mid s_t \right) \right) \sum_{t'=1}^{|\tau|} - \log \frac{\pi_\theta\left(a_{t'} \mid s_{t'} \right)}{\pi_{\text{ref}}\left(a_{t'} \mid s_{t'} \right)} \right] = \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}\right]
\tag{26}\]</span></span></p>
<p>注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (<a href="#eq-def-markov-prop" class="quarto-xref">Equation&nbsp;6</a>)。</p>
<p>实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：</p>
<p><span id="eq-kl-grad-est-dp"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta}- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)} \right] \\
\to&amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] \\
= &amp; \nabla_{\theta} -  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{\prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{ \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] \\
= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) }{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) } \right] \\
= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p_\theta\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)}{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)} \right] \\
= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta} \left[ \log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)} \right] \\
= &amp; \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \\
\end{aligned}
\tag{27}\]</span></span></p>
<p>可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。</p>
<aside id="footnotes-12" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="16">
<li id="fn16"><p>https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="不同-kl-估计量导出的-reward-项的作用" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</h3>
<p>不难注意到，<a href="#sec-analogy-pg-kl" class="quarto-xref">Section&nbsp;4.2.1</a> 中的 KL 样本量对应于 <span class="math inline">\(k_1\)</span> 估计量。</p>
<p>一个自然的问题是，如果对动作条件似然使用 <span class="math inline">\(k_2\)</span> 或 <span class="math inline">\(k_3\)</span> 等其他估计量，会得到什么结果？</p>
<p><span class="math inline">\(k_2\)</span> 或 <span class="math inline">\(k_3\)</span> 等其他估计量导致的一个问题，求和时通常无法得到联合概率。具体来说，其他估计量分别在优化</p>
<ul>
<li><span class="math inline">\(k_2\)</span>: <span class="math inline">\(- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \frac{1}{2} \left( \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right)^{2} \right]\)</span></li>
<li><span class="math inline">\(k_3\)</span>: <span class="math inline">\(- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} (\frac{\pi_{\text{ref}} \left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} - 1 - \log \frac{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}) \right]\)</span></li>
</ul>
<p>显然，这里的求和无法得到联合概率，也就无法实现类似 <a href="#eq-kl-grad-est-dp" class="quarto-xref">Equation&nbsp;27</a> 中的效果了。</p>
</section>
<section id="小结在-on-policy-设置下修正-grpo-目标的-kl-项" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</h3>
<p>若对动作对数条件似然计算 KL 估计样本量，则由于涉及到求和，<span class="math inline">\(k_1\)</span> 之外的估计量通常没有良好定义。</p>
<p>但是若放弃对动作条件似然计算 KL 估计样本量，而是对求和之后的对数（条件）似然进行计算，则只需满足</p>
<p><span id="eq-kl-reward-grad-expect-general"><span class="math display">\[
\nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right) \right]
\approx \nabla_{\theta} - \frac{1}{N} k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right)
\approx \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]
\tag{28}\]</span></span></p>
<p>暂时不考虑 off-policy 问题，根据 <a href="#eq-kl-reward-grad-expect-general" class="quarto-xref">Equation&nbsp;28</a>, GRPO 公式 (<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a>, <a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a>) 应当修正 KL 项如下：</p>
<p><span id="eq-grpo-obj-kl-fixed"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \left\{ \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|} \min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]  \right\}  -\beta k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)
\end{aligned}
\tag{29}\]</span></span></p>
</section>
</section>
</section>
<section id="sec-derive-kld-grad" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</h1>
<p>前文中，我们分析了流行的 LLM RL 框架中对 KL 散度优化的实现，并得出了结论。另一种思路是直接推导出 KL 散度的梯度估计表达式，并据此实现代码。</p>
<p>由于我们使用的是梯度法，为了优化 KL 散度，我们需要准确估计的是 KL 散度的梯度而非其本身。类似地，在 PG 中，我们需要最大化 <span class="math inline">\(\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]\)</span>，估计的是其梯度 <span class="math inline">\(\nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]=\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau}) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})]\)</span>而不是<span class="math inline">\(r(\mathbf{\tau})\)</span> 本身。</p>
<p>同时，如 <a href="#sec-kl-loss-impl" class="quarto-xref">Section&nbsp;4.1</a> 所述，先前向传播估计 KL 散度，再直接反向传播，通常是无法直接得到 KL 散度的梯度的。所以，我们需要直接估计 KL 散度的梯度。</p>
<p>首先，展开 KL 散度的表达式：</p>
<p><span id="eq-kl-expansion-sum"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] \\
&amp; \propto \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right)
\end{aligned}
\tag{30}\]</span></span></p>
<p>再计算其梯度：</p>
<p><span id="eq-kl-grad-expansion"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \propto \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\prod_{t=1}^{|\tau|-1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right)  \\
&amp; \cdot \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) \\
&amp; = \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau| - 1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right) \\
&amp; \cdot \nabla_{\theta} \left(\left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) \right)
\end{aligned}
\tag{31}\]</span></span></p>
<p><a href="#eq-kl-grad-expansion" class="quarto-xref">Equation&nbsp;31</a> 中的梯度相当复杂，难以直接计算。接下来，我们将引入一系列合理的假设来简化它。</p>
<section id="在已知环境中简化-kl-梯度估计" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</h2>
<p>实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。</p>
<p>当状态转移概率分布已知时，<span class="math inline">\(\forall t, p_{\theta}(a_1, \cdots, s_t, a_t \mid s_1)\)</span> 都是可以计算的，则 KL 散度可以直接写成：</p>
<p><span id="eq-kl-grad-expansion-known-transition"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\mathbf{\tau} \in \mathcal{T}} p(\mathbf{s}_1) p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1) \log \frac{p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}{p_{\text{ref}}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}  \\
\end{aligned}
\tag{32}\]</span></span></p>
</section>
<section id="简写为-contextual-bandit" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</h2>
<p>为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 <span class="math inline">\(\mathbf{s}_1 = \mathbf{x} \in \mathcal{P}, (\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T) = \mathbf{y} \in \mathcal{R}\)</span>，其中 <span class="math inline">\(\mathcal{P}, \mathcal{R}\)</span> 分别表示 prompt / response 空间，则 KL 散度变为：</p>
<p><span id="eq-def-kl-cb"><span class="math display">\[
\begin{aligned}
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}}\left[\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right] \\
&amp; = \sum_{(x, y) \in \mathcal{T}} p_{\theta}(x, y) \left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \\
&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)
\end{aligned}
\tag{33}\]</span></span></p>
<p>其梯度变为：</p>
<p><span id="eq-def-kl-grad-cb"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \nabla_{\theta} \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \\
&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right)
\end{aligned}
\tag{34}\]</span></span></p>
<p>其中梯度项可以进一步展开为：</p>
<p><span id="eq-def-kl-grad-cb-grad-term"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right) \\
=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \nabla_{\theta} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \\
=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \frac{1}{\pi_\theta(y \mid x)} \nabla_{\theta} \pi_{\theta}(y \mid x) \\
=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \nabla_{\theta} \pi_{\theta}(y \mid x) \\
=&amp; \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x)
\end{aligned}
\tag{35}\]</span></span></p>
<p>代入回 KL 梯度表达式：</p>
<p><span id="eq-def-kl-grad-cb-expect"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \\
=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x) \\
=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \\
=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x) \\
=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] \\
=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] + \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] \\
=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right]
\end{aligned}
\tag{36}\]</span></span></p>
<p>这里为了重新获得期望形式，引入了 <span class="math inline">\(1 = \pi_{\theta}(y \mid x) / \pi_{\theta}(y \mid x)\)</span>，并利用了 <span class="math inline">\(\nabla_{\theta} \log \pi_{\theta}(y \mid x) = \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)}\)</span> 和 <span class="math inline">\(\mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] = 0\)</span>。</p>
<p>进行 Monte Carlo 估计：</p>
<p><span id="eq-def-kl-grad-cb-mc"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\log \frac{\pi_{\theta}(y_i \mid x_i)}{\pi_{\text{ref}}(y_i \mid x_i)}\right) \nabla_{\theta} \log \pi_{\theta}(y_i \mid x_i)
\end{aligned}
\tag{37}\]</span></span></p>
<p>其中 <span class="math inline">\((\mathbf{x}_i, \mathbf{y}_i) \sim p_{\theta}\)</span>。</p>
</section>
<section id="还原为已知环境决策过程" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</h2>
<p>将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：</p>
<p><span id="eq-def-kl-grad-kt-mc"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\\
=&amp; \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{y} \mid \mathbf{x})\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T}) \sim p_{\theta}} \left[\left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)\right)\right]
\end{aligned}
\tag{38}\]</span></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss"><span class="math display">\[
\begin{aligned}
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T}\log \frac{\pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}{\pi_{\text{ref}}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})\right)
\end{aligned}
\tag{39}\]</span></span></p>
</section>
<section id="利用因果性技巧化简-kl-梯度估计" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></h2>
<p>因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。</p>
<p>对于任何 <span class="math inline">\(0 \leq t \leq |\tau|\)</span>，我们有 <span id="eq-score-expect-zero"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) }\left[\nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \\
=&amp; \sum_{a_t \in \mathcal{A}} \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \nabla_\theta \log \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \\
=&amp; \sum_{a_j \in \mathcal{A}} \pi_\theta(a_j \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_j) \cdot 0 \\
=&amp; 0
\end{aligned}
\tag{40}\]</span></span></p>
<p>更进一步，如果 <span class="math inline">\(\mathbf{\Psi}_{t'}\)</span> 是一个与 <span class="math inline">\(\mathbf{a}_t, \mathbf{s}_{t+1}, \mathbf{a}_{t+1}, \ldots\)</span> 独立的随机变量，那么 <span id="eq-score-indep-mul-expect-zero"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{(\mathbf{a}_t, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]
\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \mathbb{E}_{
    (\mathbf{s}_{t+1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t})} \left[\mathbf{\Psi}_{t'} \right] \right]
\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]
\right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[
            \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right]
        \right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \right] \\
=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbf{\Psi}_{t'} \cdot 0 \right] \\
=&amp; 0
\end{aligned}
\tag{41}\]</span></span></p>
<p>其中，为了利用 <a href="#eq-score-expect-zero" class="quarto-xref">Equation&nbsp;40</a> 的结论，我们利用了全期望定律，即</p>
<p><span id="eq-law-of-total-expectation"><span class="math display">\[
\mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p} \left[\mathbf{x}\right] = \mathbb{E}_{\mathbf{y} \sim p} \left[\mathbb{E}_{\mathbf{x} \sim p(\cdot \mid \mathbf{y})} [\mathbf{x}] \right]
\tag{42}\]</span></span></p>
<p>来引入我们想要的期望。</p>
<p><span id="eq-score-indep-mul-expect-zero"><span class="math display">\[
\begin{aligned}
&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_i \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \\
=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \\
=&amp; \sum_{\tau \in \mathcal{T}} p_\theta(s_1, a_1, \cdots, s_t) \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) p_\theta(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \\
=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t)  \sum_{(a_{t}, s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})} \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \Psi_{t'} \nabla_\theta p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right)  \\
=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t) \sum_{a_t \in \mathcal{A}}  \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \sum_{(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})}  p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} \\
\end{aligned}
\tag{43}\]</span></span></p>
<p>考虑 Monte Carlo 估计式 <a href="#eq-def-kl-grad-kt-mc-loss" class="quarto-xref">Equation&nbsp;39</a> 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss-estimator-one-grad"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[
\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})
\right]
\tag{44}\]</span></span></p>
<p>由于序列决策过程满足因果性，即 <span class="math inline">\(\forall t' &lt; t\)</span>，<span class="math inline">\(\mathbf{s}_{t'}, \mathbf{a}_{t'}\)</span> 独立于 <span class="math inline">\(\mathbf{s}_{t}, \mathbf{a}_{t}\)</span>，则可令 <span class="math inline">\(\mathbf{\Psi}_{t'} = \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}\)</span>，其独立于 <span class="math inline">\(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}, \ldots\)</span>，利用 <a href="#eq-score-indep-mul-expect-zero" class="quarto-xref">Equation&nbsp;43</a> 的性质，则有 <span id="eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero"><span class="math display">\[
\forall t' &lt; t, \mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[
\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})
\right] = 0
\tag{45}\]</span></span></p>
<p>将 <a href="#eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero" class="quarto-xref">Equation&nbsp;45</a> 代入 KL 梯度表达式 (<a href="#eq-def-kl-grad-kt-mc" class="quarto-xref">Equation&nbsp;38</a>) ，即可简化得到：</p>
<p><span id="eq-def-kl-grad-kt-reduce"><span class="math display">\[
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{t}) \right]
\tag{46}\]</span></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc"><span class="math display">\[
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \left(\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})
\tag{47}\]</span></span></p>
<p>同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc-loss"><span class="math display">\[
\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \text{nograd}\left (\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})
\tag{48}\]</span></span></p>
<p>这里也可以看到，KL loss 项正确的实现要求：</p>
<ol type="1">
<li>在序列内 token 间，对对数条件似然先求和，得到 KL 样本值，</li>
<li>再在序列间求均值。</li>
</ol>
<p>因此 OpenRLHF (<a href="#eq-def-kl-loss-grad-estim-openrlhf" class="quarto-xref">Equation&nbsp;13</a>) 与 verl (<a href="#eq-def-kl-loss-grad-estim-verl" class="quarto-xref">Equation&nbsp;14</a>) 的权重都是错误的。</p>
<aside id="footnotes-13" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="17">
<li id="fn17"><p>https://www.wikiwand.com/en/articles/Policy_gradient_method<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="sec-kl-grad-as-kl-reward" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</h2>
<p>在 <a href="#eq-def-kl-grad-kt-reduce" class="quarto-xref">Equation&nbsp;46</a> 中，令 <span class="math inline">\(k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) = \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}\)</span>，则有： <span id="eq-def-kl-grad-kt-reduce-k"><span class="math display">\[
\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t-1}, \mathbf{s}_{t}) \right]
\tag{49}\]</span></span></p>
<p>不难注意到 <a href="#eq-def-kl-grad-kt-reduce-k" class="quarto-xref">Equation&nbsp;49</a> 中 <span class="math inline">\(k\)</span> 与 <a href="#eq-pg-est-ret" class="quarto-xref">Equation&nbsp;25</a> 中 reward <span class="math inline">\(r\)</span> 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。</p>
<p>类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS285<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> 等材料。</p>
<aside id="footnotes-14" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="18">
<li id="fn18"><p>https://rail.eecs.berkeley.edu/deeprlcourse/<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="off-policy-设置下如何估计-kl-散度的梯度" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</h1>
<p>上面的推导中，我们假设了 RL 是 on-policy 设置，即采样策略即为最新策略 <span class="math inline">\(\pi_\theta\)</span>。</p>
<p>在这一节，我们进一步考虑 off-policy 设置，即一次采样获得样本会用于多次更新，除了第一次更新，采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 与最新策略 <span class="math inline">\(\pi_\theta\)</span> 都会不同。off-policy 设置给 KL 散度优化带来的问题在于，我们需要优化最新策略 <span class="math inline">\(\pi_\theta\)</span> 的 KL 散度，但却没有来自 <span class="math inline">\(p_{\theta}\)</span> 的样本，这意味着我们无法直接使用梯度估计式 <a href="#eq-def-kl-grad-kt-reduce-mc" class="quarto-xref">Equation&nbsp;47</a>。</p>
<section id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</h2>
<p>遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 <span class="math inline">\(\pi_\theta \neq \pi_{\theta_{\text{old}}}\)</span>，尽管没有来自最新策略 <span class="math inline">\(p_{\theta}\)</span> 的样本，却仍然在使用基于 on-policy 设置的优化方式。</p>
<section id="trl" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="trl"><span class="header-section-number">6.1.1</span> TRL</h3>
<p>TRL 在 <a href="#lst-trl-kl-reward" class="quarto-xref">Listing&nbsp;1</a> 中计算 KL 样本值使用的 <code>logprobs</code> 及其对应的轨迹样本均来自采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span>。对应代码可见 <a href="#lst-trl-sample-and-calc-old-logprob" class="quarto-xref">Listing&nbsp;7</a>。</p>
<div id="lst-trl-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: TRL 使用采样样本并使用 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 计算对数似然<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>
</figcaption>
<div aria-describedby="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>queries <span class="op">=</span> data[<span class="st">"input_ids"</span>].to(device)</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># ...</span></span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="cf">with</span> unwrap_model_for_generation(</span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="va">self</span>.model, <span class="co">#...</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>) <span class="im">as</span> unwrapped_model:</span>
<span id="cb8-7"><a href="#cb8-7"></a>    query_responses, logitss <span class="op">=</span> batch_generation(</span>
<span id="cb8-8"><a href="#cb8-8"></a>        unwrapped_model.policy,</span>
<span id="cb8-9"><a href="#cb8-9"></a>        queries,</span>
<span id="cb8-10"><a href="#cb8-10"></a>        <span class="co"># ...</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>    )</span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, queries.shape[<span class="dv">0</span>], args.local_rollout_forward_batch_size):</span>
<span id="cb8-15"><a href="#cb8-15"></a>    <span class="co"># ...</span></span>
<span id="cb8-16"><a href="#cb8-16"></a>    logits <span class="op">=</span> logitss[i : i <span class="op">+</span> args.local_rollout_forward_batch_size]</span>
<span id="cb8-17"><a href="#cb8-17"></a>    logprob <span class="op">=</span> selective_log_softmax(logits, response)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<p>注意，基于 <span class="math inline">\(\mathbf{\tau} \sim \pi_{\theta_{\text{old}}}\)</span> 计算的 KL 样本值可以用于估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_{\theta_{\text{old}}} \mid \pi_{\text{ref}}\right]\)</span>，在第一次更新时，由于 <span class="math inline">\(\pi_\theta = \pi_{\theta_{\text{old}}}\)</span>，所以也可以用于估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。但问题在于，从第二次更新开始，<span class="math inline">\(\pi_\theta \neq \pi_{\theta_{\text{old}}}\)</span>，而我们仍然希望估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。</p>
<p>随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 <span class="math inline">\(\pi_{\theta}\)</span> 重新估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。对应代码可见 <a href="#lst-trl-ppo-update" class="quarto-xref">Listing&nbsp;8</a>。</p>
<div id="lst-trl-ppo-update" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: TRL PPO 多轮更新
</figcaption>
<div aria-describedby="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="cf">for</span> ppo_epoch_idx <span class="kw">in</span> <span class="bu">range</span>(args.num_ppo_epochs):</span>
<span id="cb9-3"><a href="#cb9-3"></a>    b_inds <span class="op">=</span> np.random.permutation(args.local_batch_size)</span>
<span id="cb9-4"><a href="#cb9-4"></a>    minibatch_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>    <span class="cf">for</span> mini_batch_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, args.local_batch_size, args.local_mini_batch_size):</span>
<span id="cb9-6"><a href="#cb9-6"></a>        mini_batch_end <span class="op">=</span> mini_batch_start <span class="op">+</span> args.local_mini_batch_size</span>
<span id="cb9-7"><a href="#cb9-7"></a>        mini_batch_inds <span class="op">=</span> b_inds[mini_batch_start:mini_batch_end]</span>
<span id="cb9-8"><a href="#cb9-8"></a>        gradient_accumulation_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>        <span class="cf">for</span> micro_batch_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, args.local_mini_batch_size, args.per_device_train_batch_size):</span>
<span id="cb9-10"><a href="#cb9-10"></a>            <span class="cf">with</span> accelerator.accumulate(model):</span>
<span id="cb9-11"><a href="#cb9-11"></a>                micro_batch_end <span class="op">=</span> micro_batch_start <span class="op">+</span> args.per_device_train_batch_size</span>
<span id="cb9-12"><a href="#cb9-12"></a>                micro_batch_inds <span class="op">=</span> mini_batch_inds[micro_batch_start:micro_batch_end]</span>
<span id="cb9-13"><a href="#cb9-13"></a>                mb_advantage <span class="op">=</span> advantages[micro_batch_inds]</span>
<span id="cb9-14"><a href="#cb9-14"></a>                mb_responses <span class="op">=</span> responses[micro_batch_inds]</span>
<span id="cb9-15"><a href="#cb9-15"></a>                mb_query_responses <span class="op">=</span> query_responses[micro_batch_inds]</span>
<span id="cb9-16"><a href="#cb9-16"></a>                mb_logprobs <span class="op">=</span> logprobs[micro_batch_inds]</span>
<span id="cb9-17"><a href="#cb9-17"></a>                mb_return <span class="op">=</span> returns[micro_batch_inds]</span>
<span id="cb9-18"><a href="#cb9-18"></a>                mb_values <span class="op">=</span> values[micro_batch_inds]</span>
<span id="cb9-19"><a href="#cb9-19"></a></span>
<span id="cb9-20"><a href="#cb9-20"></a></span>
<span id="cb9-21"><a href="#cb9-21"></a>                output, vpred_temp <span class="op">=</span> forward(model, mb_query_responses, processing_class.pad_token_id)</span>
<span id="cb9-22"><a href="#cb9-22"></a>                logits <span class="op">=</span> output.logits[:, context_length <span class="op">-</span> <span class="dv">1</span> : <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-23"><a href="#cb9-23"></a>                logits <span class="op">/=</span> args.temperature <span class="op">+</span> <span class="fl">1e-7</span></span>
<span id="cb9-24"><a href="#cb9-24"></a>                new_logprobs <span class="op">=</span> selective_log_softmax(logits, mb_responses)</span>
<span id="cb9-25"><a href="#cb9-25"></a>                new_logprobs <span class="op">=</span> torch.masked_fill(</span>
<span id="cb9-26"><a href="#cb9-26"></a>                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB</span>
<span id="cb9-27"><a href="#cb9-27"></a>                )</span>
<span id="cb9-28"><a href="#cb9-28"></a>                vpred <span class="op">=</span> vpred_temp[:, context_length <span class="op">-</span> <span class="dv">1</span> : <span class="op">-</span><span class="dv">1</span>].squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-29"><a href="#cb9-29"></a>                vpred <span class="op">=</span> torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], <span class="dv">0</span>)</span>
<span id="cb9-30"><a href="#cb9-30"></a>                vpredclipped <span class="op">=</span> torch.clamp(</span>
<span id="cb9-31"><a href="#cb9-31"></a>                    vpred,</span>
<span id="cb9-32"><a href="#cb9-32"></a>                    mb_values <span class="op">-</span> args.cliprange_value,</span>
<span id="cb9-33"><a href="#cb9-33"></a>                    mb_values <span class="op">+</span> args.cliprange_value,</span>
<span id="cb9-34"><a href="#cb9-34"></a>                )</span>
<span id="cb9-35"><a href="#cb9-35"></a>                vf_losses1 <span class="op">=</span> torch.square(vpred <span class="op">-</span> mb_return)</span>
<span id="cb9-36"><a href="#cb9-36"></a>                vf_losses2 <span class="op">=</span> torch.square(vpredclipped <span class="op">-</span> mb_return)</span>
<span id="cb9-37"><a href="#cb9-37"></a>                vf_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(vf_losses1, vf_losses2)</span>
<span id="cb9-38"><a href="#cb9-38"></a>                vf_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> masked_mean(vf_loss_max, <span class="op">~</span>padding_mask_p1[micro_batch_inds])</span>
<span id="cb9-39"><a href="#cb9-39"></a>                vf_clipfrac <span class="op">=</span> masked_mean(</span>
<span id="cb9-40"><a href="#cb9-40"></a>                    (vf_losses2 <span class="op">&gt;</span> vf_losses1).<span class="bu">float</span>(), <span class="op">~</span>padding_mask_p1[micro_batch_inds]</span>
<span id="cb9-41"><a href="#cb9-41"></a>                )</span>
<span id="cb9-42"><a href="#cb9-42"></a>                logprobs_diff <span class="op">=</span> new_logprobs <span class="op">-</span> mb_logprobs</span>
<span id="cb9-43"><a href="#cb9-43"></a>                ratio <span class="op">=</span> torch.exp(logprobs_diff)</span>
<span id="cb9-44"><a href="#cb9-44"></a>                pg_losses <span class="op">=</span> <span class="op">-</span>mb_advantage <span class="op">*</span> ratio</span>
<span id="cb9-45"><a href="#cb9-45"></a>                pg_losses2 <span class="op">=</span> <span class="op">-</span>mb_advantage <span class="op">*</span> torch.clamp(ratio, <span class="fl">1.0</span> <span class="op">-</span> args.cliprange, <span class="fl">1.0</span> <span class="op">+</span> args.cliprange)</span>
<span id="cb9-46"><a href="#cb9-46"></a>                pg_loss_max <span class="op">=</span> torch.<span class="bu">max</span>(pg_losses, pg_losses2)</span>
<span id="cb9-47"><a href="#cb9-47"></a>                pg_loss <span class="op">=</span> masked_mean(pg_loss_max, <span class="op">~</span>padding_mask[micro_batch_inds])</span>
<span id="cb9-48"><a href="#cb9-48"></a>                loss <span class="op">=</span> pg_loss <span class="op">+</span> args.vf_coef <span class="op">*</span> vf_loss</span>
<span id="cb9-49"><a href="#cb9-49"></a>                accelerator.backward(loss)</span>
<span id="cb9-50"><a href="#cb9-50"></a>                optimizer.step()</span>
<span id="cb9-51"><a href="#cb9-51"></a>                optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<aside id="footnotes-15" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="19">
<li id="fn19"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="openrlhf-1" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</h3>
<p>类似地，OpenRLHF 在 <a href="#lst-openrlhf-calc-kl-estimator" class="quarto-xref">Listing&nbsp;2</a> 中计算 KL 样本值使用的 <code>log_probs</code> 在 <code>make_experience</code> 时被计算，和对应的样本 <code>sequences</code> 都来自采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span>，而非当前策略 <span class="math inline">\(\pi_{\theta}\)</span>。对应代码可见 <a href="#lst-openrlhf-sample-and-calc-old-logprob" class="quarto-xref">Listing&nbsp;9</a>。</p>
<div id="lst-openrlhf-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;9: OpenRLHF 采样样本并使用 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 计算对数似然
</figcaption>
<div aria-describedby="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="kw">def</span> make_experience(<span class="va">self</span>, samples: Samples) <span class="op">-&gt;</span> Experience:</span>
<span id="cb10-3"><a href="#cb10-3"></a>    <span class="co">"""</span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co">    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">    """</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>    <span class="co"># ...</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>    <span class="co"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>    action_log_probs <span class="op">=</span> <span class="va">self</span>.actor(</span>
<span id="cb10-9"><a href="#cb10-9"></a>        sequences,</span>
<span id="cb10-10"><a href="#cb10-10"></a>        num_actions,</span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="co"># ...</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>    )</span>
<span id="cb10-13"><a href="#cb10-13"></a>    <span class="co"># ...</span></span>
<span id="cb10-14"><a href="#cb10-14"></a>    <span class="co"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709</span></span>
<span id="cb10-15"><a href="#cb10-15"></a>    kl <span class="op">=</span> compute_approx_kl(</span>
<span id="cb10-16"><a href="#cb10-16"></a>        action_log_probs,</span>
<span id="cb10-17"><a href="#cb10-17"></a>        base_action_log_probs,</span>
<span id="cb10-18"><a href="#cb10-18"></a>        <span class="co"># ...</span></span>
<span id="cb10-19"><a href="#cb10-19"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<p>从 <a href="#lst-openrlhf-calc-kl-loss" class="quarto-xref">Listing&nbsp;3</a> 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 <span class="math inline">\(\pi_{\theta}\)</span> 计算的对数似然，但如 <a href="#sec-kl-loss-impl" class="quarto-xref">Section&nbsp;4.1</a> 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。</p>
</section>
<section id="verl-1" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="verl-1"><span class="header-section-number">6.1.3</span> verl</h3>
<p>从 <a href="#lst-verl-kl-reward" class="quarto-xref">Listing&nbsp;4</a> 可见，verl 同样使用 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 计算 KL 样本值。</p>
<p>从 <a href="#lst-verl-kl-loss" class="quarto-xref">Listing&nbsp;5</a> 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 的 KL 样本值。</p>
</section>
</section>
<section id="利用重要性采样处理-off-policy-设置" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</h2>
<p>off-policy 设置下，我们没有来自最新策略 <span class="math inline">\(\pi_{\theta}\)</span> 的样本，而只能使用来自采样策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 的样本，但我们仍然希望估计 <span class="math inline">\(\nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right]\)</span>。</p>
<p>熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即</p>
<p><span id="eq-is-off-policy-kl"><span class="math display">\[
\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[f(\mathbf{\tau})\right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) f(\tau)  = \sum_{\tau \in \mathcal{T}} p_{\theta_{\text{old}}}(\tau) \frac{p_{\theta}(\tau)}{p_{\theta_{\text{old}}}(\tau)} f(\tau) = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}} \left[\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} f(\mathbf{\tau})\right]
\tag{50}\]</span></span></p>
<p>此处，重要性采样系数 <span class="math inline">\(\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})}\)</span> 可以仿照 <a href="#eq-dp-expansion" class="quarto-xref">Equation&nbsp;5</a> 展开为：</p>
<p><span id="eq-is-coef-expansion"><span class="math display">\[
\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} = \prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{\pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}
\tag{51}\]</span></span> <a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p>利用重要性采样 (<a href="#eq-is-off-policy-kl" class="quarto-xref">Equation&nbsp;50</a>, <a href="#eq-is-coef-expansion" class="quarto-xref">Equation&nbsp;51</a>) ，KL 梯度表达式 <a href="#eq-def-kl-grad-kt-reduce" class="quarto-xref">Equation&nbsp;46</a> 可以转化为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right] \\
=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right] \\
=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \frac{p_{\theta}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}{p_{\theta_{\text{old}}}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}  \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})  \right] \\
=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \left(\prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}\right) \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right]
\end{aligned}
\tag{52}\]</span></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc"><span class="math display">\[
\begin{aligned}
&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \\
\approx&amp; \frac{1}{N} \sum_{i=1}^{N} \left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) \\
=&amp; \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})
\end{aligned}
\tag{53}\]</span></span></p>
<p>对应的 loss 函数为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc-loss"><span class="math display">\[
\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_{i}|} \text{nograd}\left(\left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right)\sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})
\tag{54}\]</span></span></p>
<p>类似 <a href="#eq-def-kl-grad-kt-reduce-k" class="quarto-xref">Equation&nbsp;49</a>，我们可以令</p>
<p><span id="eq-def-kl-reward-is"><span class="math display">\[
k(\mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t}) = \left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}
\tag{55}\]</span></span></p>
<p>注意，<a href="#eq-def-kl-reward-is" class="quarto-xref">Equation&nbsp;55</a> 中的 <span class="math inline">\(k\)</span> 需要对于每个新的 <span class="math inline">\(\pi_{\theta}\)</span> 重新计算。</p>
<aside id="footnotes-16" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="20">
<li id="fn20"><p>实际计算中，<a href="#eq-is-coef-expansion" class="quarto-xref">Equation&nbsp;51</a> 由于涉及到 <span class="math inline">\(|\mathbf{\tau}|\)</span> 次连乘，方差大且数值稳定性差，需要利用因果性、近似等技术来化简。本文目前省略该部分，后续将会更新相关内容。<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
</section>
<section id="结论如何正确地在-rl-中优化-kl-散度" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</h1>
<section id="修正-grpo-公式中的-kl-项" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</h2>
<p>GRPO 公式 (<a href="#eq-grpo-obj" class="quarto-xref">Equation&nbsp;1</a>, <a href="#eq-grpo-obj-kl-term" class="quarto-xref">Equation&nbsp;2</a>) 对于 KL 优化主要存在两个错误：</p>
<ol type="1">
<li>忽略了 KL 优化的 off-policy 问题</li>
<li>先将 <span class="math inline">\(k_{3}\)</span> 估计样本量应用于动作条件似然再求和，导致得到异常的梯度</li>
</ol>
<p>对于这两个问题，在 <a href="#eq-grpo-obj-kl-fixed" class="quarto-xref">Equation&nbsp;29</a> 的基础上，仿照 <a href="#eq-def-kl-reward-is" class="quarto-xref">Equation&nbsp;55</a>，我们可以按如下方式修正：</p>
<p><span id="eq-grpo-obj-kl-fixed-is"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \left\{ \sum_{t=1}^{\left|o_i\right|} \min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old}}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right] \right\} -\beta \left(\prod_{t=1}^{|o_{i}|}\frac{\pi_{\theta}(o_{i, t} | q, o_{i,\lt t})}{ \pi_{\theta_{\text{old}}}(o_{i, t} | q, o_{i,\lt t})}\right) k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)
\end{aligned}
\tag{56}\]</span></span></p>
</section>
<section id="修正流行-llm-rl-框架中的-kl-优化实现" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</h2>
<p>目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：</p>
<ol type="1">
<li>实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）</li>
<li>错误地实现了平均操作</li>
</ol>
<p>对于这些问题，可以按照如下思路修正：</p>
<ol type="1">
<li>为 KL 项添加重要性采样，这需要从第二轮更新开始，每次基于新的 <span class="math inline">\(\pi_\theta\)</span> 重新计算 KL loss / reward 项，包括重要性采样系数</li>
<li>应用 KL 估计样本量时，先对于序列内 token 间的对数条件似然求和，得到轨迹联合概率，再代入公式</li>
<li>如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 <a href="#eq-def-kl-reward-is" class="quarto-xref">Equation&nbsp;55</a> 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）</li>
<li>如果不希望应用 reward 优化的其他技术，可以按 <a href="#eq-def-kl-grad-kt-reduce-is-mc-loss" class="quarto-xref">Equation&nbsp;54</a> 实现为 KL loss 项</li>
</ol>
</section>
</section>
<section id="讨论" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> 讨论</h1>
<section id="对于-kl-梯度更好的估计样本量" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</h2>
<p>如 <a href="#sec-kl-grad-as-kl-reward" class="quarto-xref">Section&nbsp;5.5</a> 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？</p>
<p>此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？</p>
</section>
<section id="kl-regularized-rl-的理论优势" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</h2>
<p>最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。</p>
<p>然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 <span class="citation" data-cites="zhao2025logregretkl">Zhao et al. (<a href="#ref-zhao2025logregretkl" role="doc-biblioref">2025</a>)</span> 证明了 KL-regularized RL 的 regret 只有 <span class="math inline">\(\mathcal{O}(\log T)\)</span>，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 <span class="math inline">\(\mathcal{O}(\sqrt{T})\)</span>。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="附录" class="level1 appendix" data-number="9"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9</span> 附录</h2><div class="quarto-appendix-contents">

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>本文的作者（童雨轩）仍在寻求北美的 Ph.D.&nbsp;或 RA 机会。如果你觉得本文对你有帮助，欢迎浏览其主页<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>来获取进一步了解。</p>
</div>
</div>




</div></section><section id="相关工作" class="level2 appendix" data-number="9.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.1</span> 相关工作</h2><div class="quarto-appendix-contents">

<p>与本文同期也有许多精彩的讨论，由于笔者还没能通读全文，此处仅提供链接，不作概括，欢迎感兴趣的读者自行阅读：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/28440962040">GRPO 中的 KL Loss 实现细节问题 - Hongyu Zang @ 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28735759256">k2 loss就是比k3 loss好！以及GRPO_off-policy - Yiming Liu @ 知乎</a></li>
</ul>
<aside id="footnotes-17" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="21">
<li id="fn21"><p>https://tongyx361.github.io<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</div></section><section id="写作契机trpoppo-与-grpo-中的-kl-为什么不一样" class="level2 appendix" data-number="9.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.2</span> 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”</h2><div class="quarto-appendix-contents">

<p>笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>：</p>
<blockquote class="blockquote">
<p>A small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?</p>
<p>TRPO <span class="citation" data-cites="schulman2015trpo">(<a href="#ref-schulman2015trpo" role="doc-biblioref">Schulman et al. 2015</a>)</span></p>
</blockquote>
<p><span id="eq-trpo"><span class="math display">\[
\begin{aligned}
&amp; \underset{\theta}{\text{maximize}}~L_{\theta_{\text {old }}}(\theta) \\
&amp; \text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta
\end{aligned}
\tag{57}\]</span></span></p>
<blockquote class="blockquote">
<p>PPO <span class="citation" data-cites="schulman2017ppo">(<a href="#ref-schulman2017ppo" role="doc-biblioref">Schulman et al. 2017</a>)</span></p>
</blockquote>
<p><span id="eq-ppo-klpen"><span class="math display">\[
L^{K L P E N}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(\mathbf{y}_t \mid \mathbf{x}_t\right)}{\pi_{\theta_{\text {old }}}\left(\mathbf{y}_t \mid \mathbf{x}_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid \mathbf{x}_t\right), \pi_\theta\left(\cdot \mid \mathbf{x}_t\right)\right]\right]
\tag{58}\]</span></span></p>
<blockquote class="blockquote">
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(<a href="#ref-shao2024deepseekmath" role="doc-biblioref">Shao et al. 2024</a>)</span></p>
</blockquote>
<p><span id="eq-grpo"><span class="math display">\[
\begin{aligned}
&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left\{\min \left[\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]-\beta \mathbb{D}_{K L}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\right\}
\end{aligned}
\tag{59}\]</span></span></p>
<p>这个问题本身的答案是非常简单的。</p>
<p>首先，这个问题混淆了两种不同的 KL 惩罚项：</p>
<ol type="1">
<li><span class="math inline">\(\text{KL}[\pi_{\theta_{\text{old}}},\pi_{\theta}]\)</span>，其作用是约束最新策略 <span class="math inline">\(\pi_{\theta}\)</span>不要离采样策略<span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。</li>
<li><span class="math inline">\(\text{KL}[\pi_{\theta},\pi_{\theta_{\text{ref}}}]\)</span>，其作用是约束最新策略 <span class="math inline">\(\pi_{\theta}\)</span>不要离参考策略<span class="math inline">\(\pi_{\theta_{\text{ref}}}\)</span> 太远，从而更充分地利用参考策略中的先验。</li>
</ol>
<p>另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 <span class="math inline">\(\text{KL}[\pi_{\theta_{\text{old}}},\pi_{\theta}]\)</span>。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：</p>
<blockquote class="blockquote">
<p>Let <span class="math inline">\(r_t(\theta)\)</span> denote the probability ratio <span class="math inline">\(r_{t}(\theta)=\frac{\pi_{\theta}\left(a_t \mid s_t\right)}{\left(\pi_{\theta_{\text {old }}}\left|a_t\right| s_t\right)}\)</span>, so <span class="math inline">\(r\left(\theta_{\text{old}}\right)=1\)</span>. TRPO maximizes a “surrogate” objective</p>
</blockquote>
<p><span class="math display">\[
L^{\text{CPI}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t\right]=\hat{\mathbb{E}}_t\left[r_t(\theta) \hat{A}_t\right] .
\]</span></p>
<blockquote class="blockquote">
<p>…</p>
<p>The main objective we propose is the following:</p>
</blockquote>
<p><span class="math display">\[
L^{\text{CLIP}}(\theta)=\hat{\mathbb{E}}_t\left[\min \left(r_t(\theta) \hat{A}_t, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right]
\]</span></p>
<blockquote class="blockquote">
<p>where epsilon is a hyperparameter, say, <span class="math inline">\(\epsilon=0.2\)</span>. The motivation for this objective is as follows. The first term inside the <span class="math inline">\(\min\)</span> is <span class="math inline">\(L^{\text{CPI}}\)</span>. The second term, <span class="math inline">\(\text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\)</span>, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving <span class="math inline">\(r_t\)</span> outside of the interval <span class="math inline">\([1-\epsilon, 1+\epsilon]\)</span>.</p>
<p>…</p>
<p><strong>Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence</strong>, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence <span class="math inline">\(d_{\text{targ}}\)</span> each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve included it here because it’s an important baseline.</p>
<p>In the simplest instantiation of this algorithm, we perform the following steps in each policy update:</p>
<ul>
<li>Using several epochs of minibatch SGD, optimize the KL-penalized objective</li>
</ul>
</blockquote>
<p><span class="math display">\[
L^{\text{KLPEN}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid s_t\right)\right]\right]
\]</span></p>
<blockquote class="blockquote">

</blockquote>
<p>顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 <span class="math inline">\(r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}\)</span>就是<span class="math inline">\(K L\left[\pi_{\theta_{d d}}, \pi_\theta\right]=\mathbb{E}_{a_t \sim \pi_{\theta_{d t}}\left(\cdot \mid s_t\right)}\left[\log \frac{\pi_{\theta_{d t}}\left(a_t \mid s_t\right)}{\pi_\theta\left(a_t \mid s_t\right)}\right]\)</span> 中对单个样本 <span class="math inline">\((s_t, a_t)\)</span> 的值中 <span class="math inline">\(\log\)</span> 的真数。</p>
<aside id="footnotes-18" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="22">
<li id="fn22"><p>https://x.com/pufanyi/status/1888845956684370202<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</div></section><section id="致谢" class="level2 appendix" data-number="9.3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.3</span> 致谢</h2><div class="quarto-appendix-contents">

<p>感谢王浩然、YuMS 对本文提供的重要反馈。</p>
<p>感谢生广明、Wei Xiong、刘仁彪、刘威、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论以及对于本文的有益反馈。</p>
<p>感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。</p>
</div></section><section id="引用" class="level2 appendix" data-number="9.4"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.4</span> 引用</h2><div class="quarto-appendix-contents">

<p>BibTeX:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode numberSource bibtex number-lines code-with-copy"><code class="sourceCode bibtex"><span id="cb11-1"><a href="#cb11-1"></a><span class="va">@online</span>{<span class="ot">tong2025kl</span>,</span>
<span id="cb11-2"><a href="#cb11-2"></a>  <span class="dt">author</span> = {童雨轩},</span>
<span id="cb11-3"><a href="#cb11-3"></a>  <span class="dt">title</span> = {重新思考 {RL} 中的 {KL} 梯度优化},</span>
<span id="cb11-4"><a href="#cb11-4"></a>  <span class="dt">year</span> = {2025},</span>
<span id="cb11-5"><a href="#cb11-5"></a>  <span class="dt">url</span> = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},</span>
<span id="cb11-6"><a href="#cb11-6"></a>  <span class="dt">urldate</span> = {2025-03-09},</span>
<span id="cb11-7"><a href="#cb11-7"></a>  <span class="dt">language</span> = {Chinese},</span>
<span id="cb11-8"><a href="#cb11-8"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>文本：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb12-1"><a href="#cb12-1"></a>童雨轩. 2025. “重新思考 RL 中的 KL 梯度优化.” https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh.</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>


<!-- -->


</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-hu2024openrlhf" class="csl-entry" role="listitem">
Hu, Jian, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. <span>“OpenRLHF: An Easy-to-Use, Scalable and High-Performance RLHF Framework.”</span> <em>arXiv Preprint arXiv:2405.11143</em>.
</div>
<div id="ref-jaques2019wayoffpolicy" class="csl-entry" role="listitem">
Jaques, Natasha, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. <span>“Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.”</span> <a href="https://arxiv.org/abs/1907.00456">https://arxiv.org/abs/1907.00456</a>.
</div>
<div id="ref-ouyang2022instructgpt" class="csl-entry" role="listitem">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. <a href="https://openreview.net/forum?id=TG8KACxEON">https://openreview.net/forum?id=TG8KACxEON</a>.
</div>
<div id="ref-schulman2015trpo" class="csl-entry" role="listitem">
Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 1889–97. PMLR.
</div>
<div id="ref-schulman2018gae" class="csl-entry" role="listitem">
Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2018. <span>“High-Dimensional Continuous Control Using Generalized Advantage Estimation.”</span> <a href="https://arxiv.org/abs/1506.02438">https://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-schulman2017ppo" class="csl-entry" role="listitem">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv:1707.06347</em>.
</div>
<div id="ref-shao2024deepseekmath" class="csl-entry" role="listitem">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, et al. 2024. <span>“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.”</span> <em>arXiv Preprint arXiv:2402.03300</em>.
</div>
<div id="ref-sheng2024hybridflow" class="csl-entry" role="listitem">
Sheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. <span>“HybridFlow: A Flexible and Efficient RLHF Framework.”</span> <em>arXiv Preprint arXiv: 2409.19256</em>.
</div>
<div id="ref-stiennon2020summarize" class="csl-entry" role="listitem">
Stiennon, Nisan, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020. <span>“Learning to Summarize with Human Feedback.”</span> In <em>NeurIPS</em>. <a href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html</a>.
</div>
<div id="ref-zhao2025logregretkl" class="csl-entry" role="listitem">
Zhao, Heyang, Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. 2025. <span>“Logarithmic Regret for Online KL-Regularized Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2502.07460">https://arxiv.org/abs/2502.07460</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tongyx361\.github\.io\/blogs");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "tongyx361/blogs";
    script.dataset.repoId = "R_kgDOOFf8xw";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOOFf8x84CoCU7";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1"></a><span class="co">---</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="co"># title</span></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="an">title:</span><span class="co"> "重新思考 RL 中的 KL 梯度优化"</span></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="an">subtitle:</span><span class="co"> "修正 GRPO 公式与流行 LLM RL 框架"</span></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="an">date:</span><span class="co"> "2025-03-09"</span></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co">  name: "童雨轩"</span></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="co">  email: "tongyuxuan361@gmail.com"</span></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="an">copyright:</span></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co">  holder: "童雨轩"</span></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="co">  year: 2025</span></span>
<span id="cb13-12"><a href="#cb13-12"></a><span class="co"># abstract</span></span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="an">abstract-title:</span><span class="co"> Takeways</span></span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb13-15"><a href="#cb13-15"></a><span class="co">    对于 LLM RL 中相对于参考策略的 KL 优化，GRPO 公式</span></span>
<span id="cb13-16"><a href="#cb13-16"></a></span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="co">    1) 没有处理 KL 项的 off-policy 问题，这可以通过在多轮更新时重新计算 KL 项并添加重要性采样系数解决</span></span>
<span id="cb13-18"><a href="#cb13-18"></a><span class="co">    2) 先将 KL 估计样本量应用于动作对数条件似然再求和，而非先求和得到概率再应用估计样本量，与 John Schulman "Approximating KL Divergence"^[http://joschu.net/blog/kl-approx.html] 分析不符（对应导出的梯度也可能因此而错误）</span></span>
<span id="cb13-19"><a href="#cb13-19"></a><span class="co">    </span></span>
<span id="cb13-20"><a href="#cb13-20"></a><span class="co">    目前流行的 LLM RL 框架（TRL，OpenRLHF，verl）也没有避免上述问题，且存在其他问题：</span></span>
<span id="cb13-21"><a href="#cb13-21"></a></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="co">    3) 在计算 KL loss 项时默认不去除任何梯度，实际得到的梯度通常不是在优化 KL 散度</span></span>
<span id="cb13-23"><a href="#cb13-23"></a><span class="co">    4) KL loss 项的平均操作存在错误。</span></span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="co">  </span></span>
<span id="cb13-25"><a href="#cb13-25"></a><span class="co">    本文基于序列决策过程（而非 bandit）建模分析了上述问题，并提供了正确的 KL loss / reward 项实现的数学推导与上述问题的修正思路。</span></span>
<span id="cb13-26"><a href="#cb13-26"></a><span class="an">toc-expand:</span><span class="co"> false</span></span>
<span id="cb13-27"><a href="#cb13-27"></a><span class="co"># citation</span></span>
<span id="cb13-28"><a href="#cb13-28"></a><span class="an">citation:</span><span class="co"> false # Manual citation</span></span>
<span id="cb13-29"><a href="#cb13-29"></a><span class="co"># comments</span></span>
<span id="cb13-30"><a href="#cb13-30"></a><span class="an">comments:</span><span class="co"> # c.f. https://thedatasavvycorner.com/blogs/17-quarto-comments</span></span>
<span id="cb13-31"><a href="#cb13-31"></a><span class="co">  hypothesis: true</span></span>
<span id="cb13-32"><a href="#cb13-32"></a><span class="co">  giscus: </span></span>
<span id="cb13-33"><a href="#cb13-33"></a><span class="co">    repo: tongyx361/blogs</span></span>
<span id="cb13-34"><a href="#cb13-34"></a><span class="co"># language</span></span>
<span id="cb13-35"><a href="#cb13-35"></a><span class="an">toc-title:</span><span class="co"> 目录</span></span>
<span id="cb13-36"><a href="#cb13-36"></a><span class="an">categories:</span></span>
<span id="cb13-37"><a href="#cb13-37"></a><span class="co">    - Chinese 中文</span></span>
<span id="cb13-38"><a href="#cb13-38"></a><span class="co">    - Technical 技术</span></span>
<span id="cb13-39"><a href="#cb13-39"></a><span class="co">---</span></span>
<span id="cb13-40"><a href="#cb13-40"></a></span>
<span id="cb13-41"><a href="#cb13-41"></a><span class="fu"># 引言：GRPO 公式的“错误”{#sec-grpo-kl-misunderstanding}</span></span>
<span id="cb13-42"><a href="#cb13-42"></a></span>
<span id="cb13-43"><a href="#cb13-43"></a>GRPO <span class="co">[</span><span class="ot">@shao2024deepseekmath</span><span class="co">]</span> 的优化目标公式为：</span>
<span id="cb13-44"><a href="#cb13-44"></a></span>
<span id="cb13-45"><a href="#cb13-45"></a>$$</span>
<span id="cb13-46"><a href="#cb13-46"></a>\begin{aligned}</span>
<span id="cb13-47"><a href="#cb13-47"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb13-48"><a href="#cb13-48"></a>&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left<span class="sc">\{</span>\min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span>-\beta \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]\right<span class="sc">\}</span></span>
<span id="cb13-49"><a href="#cb13-49"></a>\end{aligned}</span>
<span id="cb13-50"><a href="#cb13-50"></a>$$ {#eq-grpo-obj}</span>
<span id="cb13-51"><a href="#cb13-51"></a></span>
<span id="cb13-52"><a href="#cb13-52"></a>其中</span>
<span id="cb13-53"><a href="#cb13-53"></a></span>
<span id="cb13-54"><a href="#cb13-54"></a>$$</span>
<span id="cb13-55"><a href="#cb13-55"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]=\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1</span>
<span id="cb13-56"><a href="#cb13-56"></a>$$ {#eq-grpo-obj-kl-term}</span>
<span id="cb13-57"><a href="#cb13-57"></a></span>
<span id="cb13-58"><a href="#cb13-58"></a></span>
<span id="cb13-59"><a href="#cb13-59"></a>首先，<span class="co">[</span><span class="ot">@eq-grpo-obj</span><span class="co">]</span> 中出现了 $\pi_{\theta_\text{old}}$，这意味着其考虑了 off-policy 设置，但 <span class="co">[</span><span class="ot">@eq-grpo-obj-kl-term</span><span class="co">]</span> 中却没有相应的处理，只适用于 $o_i \sim \pi_{\theta}$，无法正确处理 $o_i \sim \pi_{\theta_\text{old}}$。</span>
<span id="cb13-60"><a href="#cb13-60"></a></span>
<span id="cb13-61"><a href="#cb13-61"></a>其次，<span class="co">[</span><span class="ot">@eq-grpo-obj-kl-term</span><span class="co">]</span> 将估计样本量 $\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, i} \mid q, o_{i, \alpha}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i, e t}\right)}-1$ 写成 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]$ 也并不十分恰当，因为 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{r e f}\right]$ 通常表示 KL 散度的真实值。</span>
<span id="cb13-62"><a href="#cb13-62"></a></span>
<span id="cb13-63"><a href="#cb13-63"></a>而目前流行的 LLM RL 框架，在实现 KL 优化时，通常也忽略了 off-policy 问题，同时还存在其他一系列问题：</span>
<span id="cb13-64"><a href="#cb13-64"></a></span>
<span id="cb13-65"><a href="#cb13-65"></a><span class="ss">1. </span>误认为前向传播估计出 KL 散度，再反向传播就能得到其梯度（但实际上通常并非如此）；</span>
<span id="cb13-66"><a href="#cb13-66"></a><span class="ss">2. </span>忽略了先对动作动作对数条件似然应用 KL 估计样本量再求和并非良好定义的行为，导致梯度错误；</span>
<span id="cb13-67"><a href="#cb13-67"></a><span class="ss">3. </span>忽略了同一轨迹上 KL 对数概率必须求和以获得轨迹联合概率，而不能求平均；</span>
<span id="cb13-68"><a href="#cb13-68"></a><span class="ss">4. </span>错误地计算了平均操作。</span>
<span id="cb13-69"><a href="#cb13-69"></a></span>
<span id="cb13-70"><a href="#cb13-70"></a>由于 on-policy 设置更加简单，但也已经暴露了上述大部分问题，我们可以先从 on-policy 设置开始讨论，后续再考虑 off-policy 设置。</span>
<span id="cb13-71"><a href="#cb13-71"></a></span>
<span id="cb13-72"><a href="#cb13-72"></a><span class="fu"># 流行 LLM RL 框架中 on-policy KL 优化的实现 {#sec-popular-llm-rl-kl-optim}</span></span>
<span id="cb13-73"><a href="#cb13-73"></a></span>
<span id="cb13-74"><a href="#cb13-74"></a>我们可以先回顾目前流行的 LLM RL 框架中对于 KL 优化的实现。以下我们以</span>
<span id="cb13-75"><a href="#cb13-75"></a></span>
<span id="cb13-76"><a href="#cb13-76"></a><span class="ss">1. </span>TRL^<span class="co">[</span><span class="ot">https://github.com/huggingface/trl</span><span class="co">]</span>，</span>
<span id="cb13-77"><a href="#cb13-77"></a><span class="ss">2. </span>OpenRLHF^<span class="co">[</span><span class="ot">https://github.com/OpenRLHF/OpenRLHF</span><span class="co">] [@hu2024openrlhf]</span></span>
<span id="cb13-78"><a href="#cb13-78"></a><span class="ss">3. </span>verl^<span class="co">[</span><span class="ot">https://github.com/volcengine/verl</span><span class="co">] [@sheng2024hybridflow]</span></span>
<span id="cb13-79"><a href="#cb13-79"></a></span>
<span id="cb13-80"><a href="#cb13-80"></a>为例。</span>
<span id="cb13-81"><a href="#cb13-81"></a></span>
<span id="cb13-82"><a href="#cb13-82"></a>熟悉这些框架的读者可以跳过本节，直接从 <span class="co">[</span><span class="ot">@sec-rl-kl-optim-formulation</span><span class="co">]</span> 开始阅读。</span>
<span id="cb13-83"><a href="#cb13-83"></a></span>
<span id="cb13-84"><a href="#cb13-84"></a><span class="fu">## TRL：KL reward 项</span></span>
<span id="cb13-85"><a href="#cb13-85"></a></span>
<span id="cb13-86"><a href="#cb13-86"></a>TRL 计算 KL 定义中的样本值 $\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}$，并将其从 reward 中减去。对应代码可见 <span class="co">[</span><span class="ot">@lst-trl-kl-reward</span><span class="co">]</span>。</span>
<span id="cb13-87"><a href="#cb13-87"></a></span>
<span id="cb13-88"><a href="#cb13-88"></a><span class="in">```{#lst-trl-kl-reward .python lst-cap="TRL 计算 KL 样本值 $\log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,t})}{\pi_{\theta_{\text{ref}}}(a_{i,t} \mid s_{i,t})}$ 并从 reward 中减去^[https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506]"}</span></span>
<span id="cb13-89"><a href="#cb13-89"></a><span class="in"># 4. compute rewards</span></span>
<span id="cb13-90"><a href="#cb13-90"></a><span class="in">kl = logprobs - ref_logprobs</span></span>
<span id="cb13-91"><a href="#cb13-91"></a><span class="in">non_score_reward = -args.kl_coef * kl</span></span>
<span id="cb13-92"><a href="#cb13-92"></a><span class="in">rewards = non_score_reward.clone()</span></span>
<span id="cb13-93"><a href="#cb13-93"></a><span class="in"># ...</span></span>
<span id="cb13-94"><a href="#cb13-94"></a><span class="in">rewards[[actual_start, actual_end]] += scores</span></span>
<span id="cb13-95"><a href="#cb13-95"></a><span class="in">```</span></span>
<span id="cb13-96"><a href="#cb13-96"></a></span>
<span id="cb13-97"><a href="#cb13-97"></a>这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 @sec-why-kl-reward。</span>
<span id="cb13-98"><a href="#cb13-98"></a></span>
<span id="cb13-99"><a href="#cb13-99"></a><span class="fu">## OpenRLHF</span></span>
<span id="cb13-100"><a href="#cb13-100"></a></span>
<span id="cb13-101"><a href="#cb13-101"></a><span class="fu">### KL reward 项 {#sec-openrlhf-kl-reward}</span></span>
<span id="cb13-102"><a href="#cb13-102"></a></span>
<span id="cb13-103"><a href="#cb13-103"></a>与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-estimator</span><span class="co">]</span>。</span>
<span id="cb13-104"><a href="#cb13-104"></a></span>
<span id="cb13-105"><a href="#cb13-105"></a><span class="in">```{#lst-openrlhf-calc-kl-estimator .python lst-cap="OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 ^[https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88]"}</span></span>
<span id="cb13-106"><a href="#cb13-106"></a><span class="in">def compute_approx_kl(</span></span>
<span id="cb13-107"><a href="#cb13-107"></a><span class="in">    log_probs: torch.Tensor,</span></span>
<span id="cb13-108"><a href="#cb13-108"></a><span class="in">    log_probs_base: torch.Tensor,</span></span>
<span id="cb13-109"><a href="#cb13-109"></a><span class="in">    action_mask: Optional[torch.Tensor] = None,</span></span>
<span id="cb13-110"><a href="#cb13-110"></a><span class="in">    kl_estimator: str = "k1",</span></span>
<span id="cb13-111"><a href="#cb13-111"></a><span class="in">) -&gt; torch.Tensor:</span></span>
<span id="cb13-112"><a href="#cb13-112"></a><span class="in">    """</span></span>
<span id="cb13-113"><a href="#cb13-113"></a><span class="in">    Compute the approximate KL divergence between two distributions.</span></span>
<span id="cb13-114"><a href="#cb13-114"></a><span class="in">    Schulman blog: http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb13-115"><a href="#cb13-115"></a></span>
<span id="cb13-116"><a href="#cb13-116"></a><span class="in">    Args:</span></span>
<span id="cb13-117"><a href="#cb13-117"></a><span class="in">        log_probs: Log probabilities of the new distribution.</span></span>
<span id="cb13-118"><a href="#cb13-118"></a><span class="in">        log_probs_base: Log probabilities of the base distribution.</span></span>
<span id="cb13-119"><a href="#cb13-119"></a><span class="in">        action_mask: Mask for actions.</span></span>
<span id="cb13-120"><a href="#cb13-120"></a><span class="in">    """</span></span>
<span id="cb13-121"><a href="#cb13-121"></a></span>
<span id="cb13-122"><a href="#cb13-122"></a><span class="in">    if kl_estimator == "k1":</span></span>
<span id="cb13-123"><a href="#cb13-123"></a><span class="in">        log_ratio = log_probs.float() - log_probs_base.float()</span></span>
<span id="cb13-124"><a href="#cb13-124"></a><span class="in">        if action_mask is not None:</span></span>
<span id="cb13-125"><a href="#cb13-125"></a><span class="in">            log_ratio = log_ratio * action_mask</span></span>
<span id="cb13-126"><a href="#cb13-126"></a></span>
<span id="cb13-127"><a href="#cb13-127"></a><span class="in">    # The $k_2$ estimator is the non negative kl approximation in</span></span>
<span id="cb13-128"><a href="#cb13-128"></a><span class="in">    # http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb13-129"><a href="#cb13-129"></a><span class="in">    # The k2_loss is approximately equivalent to the</span></span>
<span id="cb13-130"><a href="#cb13-130"></a><span class="in">    # one-step KL divergence penalty with the $k_1$ estimator</span></span>
<span id="cb13-131"><a href="#cb13-131"></a><span class="in">    # used in https://arxiv.org/abs/2310.10505.</span></span>
<span id="cb13-132"><a href="#cb13-132"></a><span class="in">    if kl_estimator == "k2":</span></span>
<span id="cb13-133"><a href="#cb13-133"></a><span class="in">        log_ratio = log_probs.float() - log_probs_base.float()</span></span>
<span id="cb13-134"><a href="#cb13-134"></a><span class="in">        if action_mask is not None:</span></span>
<span id="cb13-135"><a href="#cb13-135"></a><span class="in">            log_ratio = log_ratio * action_mask</span></span>
<span id="cb13-136"><a href="#cb13-136"></a><span class="in">        log_ratio = log_ratio**2 / 2.0</span></span>
<span id="cb13-137"><a href="#cb13-137"></a></span>
<span id="cb13-138"><a href="#cb13-138"></a><span class="in">    # The $k_3$ estimator is the non negative kl approximation in</span></span>
<span id="cb13-139"><a href="#cb13-139"></a><span class="in">    # http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb13-140"><a href="#cb13-140"></a><span class="in">    if kl_estimator == "k3":</span></span>
<span id="cb13-141"><a href="#cb13-141"></a><span class="in">        log_ratio = log_probs.float() - log_probs_base.float()</span></span>
<span id="cb13-142"><a href="#cb13-142"></a><span class="in">        if action_mask is not None:</span></span>
<span id="cb13-143"><a href="#cb13-143"></a><span class="in">            log_ratio = log_ratio * action_mask</span></span>
<span id="cb13-144"><a href="#cb13-144"></a><span class="in">        log_ratio = -log_ratio</span></span>
<span id="cb13-145"><a href="#cb13-145"></a><span class="in">        log_ratio = log_ratio.exp() - 1 - log_ratio</span></span>
<span id="cb13-146"><a href="#cb13-146"></a></span>
<span id="cb13-147"><a href="#cb13-147"></a><span class="in">    return log_ratio</span></span>
<span id="cb13-148"><a href="#cb13-148"></a></span>
<span id="cb13-149"><a href="#cb13-149"></a></span>
<span id="cb13-150"><a href="#cb13-150"></a><span class="in">def compute_reward(</span></span>
<span id="cb13-151"><a href="#cb13-151"></a><span class="in">    # ...</span></span>
<span id="cb13-152"><a href="#cb13-152"></a><span class="in">    kl_coef: float,</span></span>
<span id="cb13-153"><a href="#cb13-153"></a><span class="in">    kl: Union[torch.Tensor, list[torch.Tensor]],</span></span>
<span id="cb13-154"><a href="#cb13-154"></a><span class="in">    # ...</span></span>
<span id="cb13-155"><a href="#cb13-155"></a><span class="in">    num_actions: Optional[Union[int, list[int]]] = None,</span></span>
<span id="cb13-156"><a href="#cb13-156"></a><span class="in">    # ...</span></span>
<span id="cb13-157"><a href="#cb13-157"></a><span class="in">) -&gt; Union[torch.Tensor, list[torch.Tensor]]:</span></span>
<span id="cb13-158"><a href="#cb13-158"></a><span class="in">    # ...</span></span>
<span id="cb13-159"><a href="#cb13-159"></a><span class="in">    if action_mask is not None:</span></span>
<span id="cb13-160"><a href="#cb13-160"></a><span class="in">        # ...</span></span>
<span id="cb13-161"><a href="#cb13-161"></a><span class="in">    else:</span></span>
<span id="cb13-162"><a href="#cb13-162"></a><span class="in">        # ...</span></span>
<span id="cb13-163"><a href="#cb13-163"></a><span class="in">        reward = []</span></span>
<span id="cb13-164"><a href="#cb13-164"></a><span class="in">        for i, (kl_seg, action_len) in enumerate(zip(kl, num_actions)):</span></span>
<span id="cb13-165"><a href="#cb13-165"></a><span class="in">            kl_reward = -kl_coef * kl_seg</span></span>
<span id="cb13-166"><a href="#cb13-166"></a><span class="in">            kl_reward[action_len - 1] += r[i]</span></span>
<span id="cb13-167"><a href="#cb13-167"></a><span class="in">            reward.append(kl_reward)</span></span>
<span id="cb13-168"><a href="#cb13-168"></a></span>
<span id="cb13-169"><a href="#cb13-169"></a><span class="in">    return reward</span></span>
<span id="cb13-170"><a href="#cb13-170"></a><span class="in">```</span></span>
<span id="cb13-171"><a href="#cb13-171"></a></span>
<span id="cb13-172"><a href="#cb13-172"></a><span class="fu">### KL loss 项</span></span>
<span id="cb13-173"><a href="#cb13-173"></a></span>
<span id="cb13-174"><a href="#cb13-174"></a>此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-loss</span><span class="co">]</span>。</span>
<span id="cb13-175"><a href="#cb13-175"></a></span>
<span id="cb13-176"><a href="#cb13-176"></a><span class="in">```{#lst-openrlhf-calc-kl-loss .python lst-cap="OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 ^[https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470]"}</span></span>
<span id="cb13-177"><a href="#cb13-177"></a><span class="in">def training_step_actor(self, experience: Experience) -&gt; Dict[str, float]:</span></span>
<span id="cb13-178"><a href="#cb13-178"></a><span class="in">    self.actor.train()</span></span>
<span id="cb13-179"><a href="#cb13-179"></a><span class="in">    # ...</span></span>
<span id="cb13-180"><a href="#cb13-180"></a><span class="in">    if isinstance(experience.sequences, list):</span></span>
<span id="cb13-181"><a href="#cb13-181"></a><span class="in">        # ...</span></span>
<span id="cb13-182"><a href="#cb13-182"></a><span class="in">    else:</span></span>
<span id="cb13-183"><a href="#cb13-183"></a><span class="in">        sequences = experience.sequences</span></span>
<span id="cb13-184"><a href="#cb13-184"></a><span class="in">        old_action_log_probs = experience.action_log_probs</span></span>
<span id="cb13-185"><a href="#cb13-185"></a><span class="in">        advantages = experience.advantages</span></span>
<span id="cb13-186"><a href="#cb13-186"></a><span class="in">        num_actions = experience.action_mask.size(1)</span></span>
<span id="cb13-187"><a href="#cb13-187"></a><span class="in">        packed_seq_lens = None</span></span>
<span id="cb13-188"><a href="#cb13-188"></a><span class="in">        attention_mask = experience.attention_mask</span></span>
<span id="cb13-189"><a href="#cb13-189"></a><span class="in">        if self.args.use_kl_loss and experience.base_action_log_probs is not None:</span></span>
<span id="cb13-190"><a href="#cb13-190"></a><span class="in">            base_action_log_probs = experience.base_action_log_probs</span></span>
<span id="cb13-191"><a href="#cb13-191"></a></span>
<span id="cb13-192"><a href="#cb13-192"></a><span class="in">    # actor loss</span></span>
<span id="cb13-193"><a href="#cb13-193"></a><span class="in">    action_log_probs, output = self.actor(</span></span>
<span id="cb13-194"><a href="#cb13-194"></a><span class="in">        sequences,</span></span>
<span id="cb13-195"><a href="#cb13-195"></a><span class="in">        num_actions,</span></span>
<span id="cb13-196"><a href="#cb13-196"></a><span class="in">        # ...</span></span>
<span id="cb13-197"><a href="#cb13-197"></a><span class="in">    )</span></span>
<span id="cb13-198"><a href="#cb13-198"></a><span class="in">    # ...</span></span>
<span id="cb13-199"><a href="#cb13-199"></a><span class="in">    # loss function</span></span>
<span id="cb13-200"><a href="#cb13-200"></a><span class="in">    actor_loss = self.actor_loss_fn(</span></span>
<span id="cb13-201"><a href="#cb13-201"></a><span class="in">        action_log_probs,</span></span>
<span id="cb13-202"><a href="#cb13-202"></a><span class="in">        old_action_log_probs,</span></span>
<span id="cb13-203"><a href="#cb13-203"></a><span class="in">        advantages,</span></span>
<span id="cb13-204"><a href="#cb13-204"></a><span class="in">        # ...</span></span>
<span id="cb13-205"><a href="#cb13-205"></a><span class="in">    )</span></span>
<span id="cb13-206"><a href="#cb13-206"></a></span>
<span id="cb13-207"><a href="#cb13-207"></a><span class="in">    if self.args.use_kl_loss:</span></span>
<span id="cb13-208"><a href="#cb13-208"></a><span class="in">        if self.initial_model is not None:</span></span>
<span id="cb13-209"><a href="#cb13-209"></a><span class="in">            kl = compute_approx_kl(</span></span>
<span id="cb13-210"><a href="#cb13-210"></a><span class="in">                action_log_probs,</span></span>
<span id="cb13-211"><a href="#cb13-211"></a><span class="in">                base_action_log_probs,</span></span>
<span id="cb13-212"><a href="#cb13-212"></a><span class="in">                # ...</span></span>
<span id="cb13-213"><a href="#cb13-213"></a><span class="in">                kl_estimator=self.args.kl_estimator,</span></span>
<span id="cb13-214"><a href="#cb13-214"></a><span class="in">            )</span></span>
<span id="cb13-215"><a href="#cb13-215"></a><span class="in">        else:</span></span>
<span id="cb13-216"><a href="#cb13-216"></a><span class="in">            kl = torch.zeros_like(action_log_probs, dtype=action_log_probs.dtype, device=action_log_probs.device)</span></span>
<span id="cb13-217"><a href="#cb13-217"></a></span>
<span id="cb13-218"><a href="#cb13-218"></a><span class="in">        if not self.args.packing_samples:</span></span>
<span id="cb13-219"><a href="#cb13-219"></a><span class="in">            kl_mean = masked_mean(kl, experience.action_mask, dim=-1)</span></span>
<span id="cb13-220"><a href="#cb13-220"></a><span class="in">        else:</span></span>
<span id="cb13-221"><a href="#cb13-221"></a><span class="in">            # ...</span></span>
<span id="cb13-222"><a href="#cb13-222"></a></span>
<span id="cb13-223"><a href="#cb13-223"></a><span class="in">        kl_loss = kl_mean.mean()</span></span>
<span id="cb13-224"><a href="#cb13-224"></a><span class="in">        experience.info["kl"] = kl_loss.item()</span></span>
<span id="cb13-225"><a href="#cb13-225"></a><span class="in">    else:</span></span>
<span id="cb13-226"><a href="#cb13-226"></a><span class="in">        kl_loss = 0</span></span>
<span id="cb13-227"><a href="#cb13-227"></a><span class="in">    # ...</span></span>
<span id="cb13-228"><a href="#cb13-228"></a><span class="in">    self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name="actor")</span></span>
<span id="cb13-229"><a href="#cb13-229"></a><span class="in">    # ...</span></span>
<span id="cb13-230"><a href="#cb13-230"></a><span class="in">```</span></span>
<span id="cb13-231"><a href="#cb13-231"></a></span>
<span id="cb13-232"><a href="#cb13-232"></a><span class="fu">## verl</span></span>
<span id="cb13-233"><a href="#cb13-233"></a></span>
<span id="cb13-234"><a href="#cb13-234"></a><span class="fu">### KL reward 项</span></span>
<span id="cb13-235"><a href="#cb13-235"></a></span>
<span id="cb13-236"><a href="#cb13-236"></a>verl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 <span class="co">[</span><span class="ot">@lst-verl-kl-reward</span><span class="co">]</span>。</span>
<span id="cb13-237"><a href="#cb13-237"></a></span>
<span id="cb13-238"><a href="#cb13-238"></a><span class="in">```{#lst-verl-kl-reward .python lst-cap="verl 将 KL 估计样本值从 reward 中减去 ^[https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160]"}</span></span>
<span id="cb13-239"><a href="#cb13-239"></a><span class="in">def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):</span></span>
<span id="cb13-240"><a href="#cb13-240"></a><span class="in">    # ...</span></span>
<span id="cb13-241"><a href="#cb13-241"></a><span class="in">    # compute kl between ref_policy and current policy</span></span>
<span id="cb13-242"><a href="#cb13-242"></a><span class="in">    if 'ref_log_prob' in data.batch.keys():</span></span>
<span id="cb13-243"><a href="#cb13-243"></a><span class="in">        kld = core_algos.kl_penalty(data.batch['old_log_probs'], data.batch['ref_log_prob'],</span></span>
<span id="cb13-244"><a href="#cb13-244"></a><span class="in">                                    kl_penalty=kl_penalty)  # (batch_size, response_length)</span></span>
<span id="cb13-245"><a href="#cb13-245"></a><span class="in">        kld = kld * response_mask</span></span>
<span id="cb13-246"><a href="#cb13-246"></a><span class="in">        beta = kl_ctrl.value</span></span>
<span id="cb13-247"><a href="#cb13-247"></a><span class="in">    else:</span></span>
<span id="cb13-248"><a href="#cb13-248"></a><span class="in">        beta = 0</span></span>
<span id="cb13-249"><a href="#cb13-249"></a><span class="in">        kld = torch.zeros_like(response_mask, dtype=torch.float32)</span></span>
<span id="cb13-250"><a href="#cb13-250"></a></span>
<span id="cb13-251"><a href="#cb13-251"></a><span class="in">    token_level_rewards = token_level_scores - beta * kld</span></span>
<span id="cb13-252"><a href="#cb13-252"></a><span class="in">    # ...</span></span>
<span id="cb13-253"><a href="#cb13-253"></a><span class="in">```</span></span>
<span id="cb13-254"><a href="#cb13-254"></a></span>
<span id="cb13-255"><a href="#cb13-255"></a><span class="fu">### KL loss 项</span></span>
<span id="cb13-256"><a href="#cb13-256"></a></span>
<span id="cb13-257"><a href="#cb13-257"></a>verl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 <span class="co">[</span><span class="ot">@lst-verl-kl-loss</span><span class="co">]</span>。</span>
<span id="cb13-258"><a href="#cb13-258"></a></span>
<span id="cb13-259"><a href="#cb13-259"></a><span class="in">```{#lst-verl-kl-loss .python lst-cap="verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 ^[https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327]"}</span></span>
<span id="cb13-260"><a href="#cb13-260"></a><span class="in">def update_policy(self, data: DataProto):</span></span>
<span id="cb13-261"><a href="#cb13-261"></a><span class="in">    # make sure we are in training mode</span></span>
<span id="cb13-262"><a href="#cb13-262"></a><span class="in">    self.actor_module.train()</span></span>
<span id="cb13-263"><a href="#cb13-263"></a><span class="in">    # ...</span></span>
<span id="cb13-264"><a href="#cb13-264"></a><span class="in">    for epoch in range(self.config.ppo_epochs):</span></span>
<span id="cb13-265"><a href="#cb13-265"></a><span class="in">        for batch_idx, data in enumerate(dataloader):</span></span>
<span id="cb13-266"><a href="#cb13-266"></a><span class="in">            # ...</span></span>
<span id="cb13-267"><a href="#cb13-267"></a><span class="in">            self.actor_optimizer.zero_grad()</span></span>
<span id="cb13-268"><a href="#cb13-268"></a></span>
<span id="cb13-269"><a href="#cb13-269"></a><span class="in">            for data in micro_batches:</span></span>
<span id="cb13-270"><a href="#cb13-270"></a><span class="in">                # ...</span></span>
<span id="cb13-271"><a href="#cb13-271"></a><span class="in">                responses = data['responses']</span></span>
<span id="cb13-272"><a href="#cb13-272"></a><span class="in">                # ...</span></span>
<span id="cb13-273"><a href="#cb13-273"></a><span class="in">                old_log_prob = data['old_log_probs']</span></span>
<span id="cb13-274"><a href="#cb13-274"></a><span class="in">                # ...</span></span>
<span id="cb13-275"><a href="#cb13-275"></a></span>
<span id="cb13-276"><a href="#cb13-276"></a><span class="in">                # all return: (bsz, response_length)</span></span>
<span id="cb13-277"><a href="#cb13-277"></a><span class="in">                entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature)</span></span>
<span id="cb13-278"><a href="#cb13-278"></a></span>
<span id="cb13-279"><a href="#cb13-279"></a><span class="in">                pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=old_log_prob,</span></span>
<span id="cb13-280"><a href="#cb13-280"></a><span class="in">                                                                                log_prob=log_prob,</span></span>
<span id="cb13-281"><a href="#cb13-281"></a><span class="in">                                                                                # ...</span></span>
<span id="cb13-282"><a href="#cb13-282"></a><span class="in">                                                                                )</span></span>
<span id="cb13-283"><a href="#cb13-283"></a><span class="in">                # ...</span></span>
<span id="cb13-284"><a href="#cb13-284"></a></span>
<span id="cb13-285"><a href="#cb13-285"></a><span class="in">                # compute policy loss</span></span>
<span id="cb13-286"><a href="#cb13-286"></a><span class="in">                policy_loss = pg_loss - entropy_loss * entropy_coeff</span></span>
<span id="cb13-287"><a href="#cb13-287"></a></span>
<span id="cb13-288"><a href="#cb13-288"></a><span class="in">                if self.config.use_kl_loss:</span></span>
<span id="cb13-289"><a href="#cb13-289"></a><span class="in">                    ref_log_prob = data['ref_log_prob']</span></span>
<span id="cb13-290"><a href="#cb13-290"></a><span class="in">                    # compute kl loss</span></span>
<span id="cb13-291"><a href="#cb13-291"></a><span class="in">                    kld = core_algos.kl_penalty(logprob=log_prob,</span></span>
<span id="cb13-292"><a href="#cb13-292"></a><span class="in">                                                ref_logprob=ref_log_prob,</span></span>
<span id="cb13-293"><a href="#cb13-293"></a><span class="in">                                                kl_penalty=self.config.kl_loss_type)</span></span>
<span id="cb13-294"><a href="#cb13-294"></a><span class="in">                    kl_loss = masked_mean(kld, response_mask)</span></span>
<span id="cb13-295"><a href="#cb13-295"></a></span>
<span id="cb13-296"><a href="#cb13-296"></a><span class="in">                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef</span></span>
<span id="cb13-297"><a href="#cb13-297"></a><span class="in">                # ...</span></span>
<span id="cb13-298"><a href="#cb13-298"></a><span class="in">                loss.backward()</span></span>
<span id="cb13-299"><a href="#cb13-299"></a><span class="in">            # ...</span></span>
<span id="cb13-300"><a href="#cb13-300"></a><span class="in">            grad_norm = self._optimizer_step()</span></span>
<span id="cb13-301"><a href="#cb13-301"></a><span class="in">    # ...</span></span>
<span id="cb13-302"><a href="#cb13-302"></a><span class="in">    self.actor_optimizer.zero_grad()</span></span>
<span id="cb13-303"><a href="#cb13-303"></a><span class="in">    # ...</span></span>
<span id="cb13-304"><a href="#cb13-304"></a><span class="in">```</span></span>
<span id="cb13-305"><a href="#cb13-305"></a></span>
<span id="cb13-306"><a href="#cb13-306"></a></span>
<span id="cb13-307"><a href="#cb13-307"></a><span class="fu">## 为什么要将 KL 从 reward 中减去 {#sec-why-kl-reward}</span></span>
<span id="cb13-308"><a href="#cb13-308"></a></span>
<span id="cb13-309"><a href="#cb13-309"></a>将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT <span class="co">[</span><span class="ot">@ouyang2022instructgpt</span><span class="co">]</span>。</span>
<span id="cb13-310"><a href="#cb13-310"></a></span>
<span id="cb13-311"><a href="#cb13-311"></a><span class="fu">### KL reward 的流行应当源自 RLHF 与 InstructGPT</span></span>
<span id="cb13-312"><a href="#cb13-312"></a></span>
<span id="cb13-313"><a href="#cb13-313"></a>InstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。</span>
<span id="cb13-314"><a href="#cb13-314"></a></span>
<span id="cb13-315"><a href="#cb13-315"></a><span class="at">&gt; ... In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models "PPO."</span></span>
<span id="cb13-316"><a href="#cb13-316"></a><span class="at">&gt;</span></span>
<span id="cb13-317"><a href="#cb13-317"></a><span class="at">&gt; ...</span></span>
<span id="cb13-318"><a href="#cb13-318"></a></span>
<span id="cb13-319"><a href="#cb13-319"></a>$$</span>
<span id="cb13-320"><a href="#cb13-320"></a>\begin{aligned}</span>
<span id="cb13-321"><a href="#cb13-321"></a>\text { objective }(\phi)= &amp; E_{(x, y) \sim D_\pi^{\mathrm{RL}}}\left<span class="co">[</span><span class="ot">r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right</span><span class="co">]</span>+ <span class="sc">\\</span></span>
<span id="cb13-322"><a href="#cb13-322"></a>&amp; \gamma E_{x \sim D_{\text {remin }}}\left<span class="co">[</span><span class="ot">\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right</span><span class="co">]</span></span>
<span id="cb13-323"><a href="#cb13-323"></a>\end{aligned}</span>
<span id="cb13-324"><a href="#cb13-324"></a>$$</span>
<span id="cb13-325"><a href="#cb13-325"></a></span>
<span id="cb13-326"><a href="#cb13-326"></a><span class="at">&gt; where $\pi_\phi^{\mathrm{RL}}$is the learned RL policy,$\pi^{\mathrm{SFT}}$ is the supervised trained model, and$D_{\text {pretrain }}$is the pretraining distribution. The KL reward coefficient, $\beta$, and the pretraining loss coefficient, $\gamma$, control the strength of the KL penalty and pretraining gradients respectively. For "PPO" models, $\gamma$ is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</span></span>
<span id="cb13-327"><a href="#cb13-327"></a></span>
<span id="cb13-328"><a href="#cb13-328"></a><span class="fu">### OpenAI 论文中 KL reward 的出处 {#sec-oai-kl-reward-src}</span></span>
<span id="cb13-329"><a href="#cb13-329"></a></span>
<span id="cb13-330"><a href="#cb13-330"></a>然而，在OpenAI 早期的一篇论文 "Learning to summarize from human feedback" <span class="co">[</span><span class="ot">@stiennon2020summarize</span><span class="co">]</span> 中，他们就已经采用了 KL reward，并提及了出处：</span>
<span id="cb13-331"><a href="#cb13-331"></a></span>
<span id="cb13-332"><a href="#cb13-332"></a><span class="at">&gt; ... **Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy $\pi_\phi^{\mathrm{RL}}$ with parameters $\phi$ and this original supervised model $\pi^{\mathrm{SFT}}$, as previously done in [25].** The full reward $R$ can be written as:</span></span>
<span id="cb13-333"><a href="#cb13-333"></a></span>
<span id="cb13-334"><a href="#cb13-334"></a>$$</span>
<span id="cb13-335"><a href="#cb13-335"></a>R(x, y)=r_\theta(x, y)-\beta \log \left<span class="co">[</span><span class="ot">\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right</span><span class="co">]</span></span>
<span id="cb13-336"><a href="#cb13-336"></a>$$</span>
<span id="cb13-337"><a href="#cb13-337"></a></span>
<span id="cb13-338"><a href="#cb13-338"></a><span class="at">&gt; This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn't learn to produce outputs that are too different from those that the reward model has seen during training.</span></span>
<span id="cb13-339"><a href="#cb13-339"></a></span>
<span id="cb13-340"><a href="#cb13-340"></a><span class="fu">### KL reward 最早的出处</span></span>
<span id="cb13-341"><a href="#cb13-341"></a></span>
<span id="cb13-342"><a href="#cb13-342"></a><span class="co">[</span><span class="ot">@sec-oai-kl-reward-src</span><span class="co">]</span> 中 OpenAI 引用的 KL reward 出处 <span class="co">[</span><span class="ot">25</span><span class="co">]</span> 是 "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog" <span class="co">[</span><span class="ot">@jaques2019wayoffpolicy</span><span class="co">]</span>。</span>
<span id="cb13-343"><a href="#cb13-343"></a></span>
<span id="cb13-344"><a href="#cb13-344"></a>实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：</span>
<span id="cb13-345"><a href="#cb13-345"></a></span>
<span id="cb13-346"><a href="#cb13-346"></a><span class="at">&gt; Rather than simply sample from the prior, we would like the $Q$-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior $p(y \mid x)$, and the $Q$-network policy $\pi_\theta$, while still maximizing reward. Given a trajectory of actions, $\tau=\left\{a_1, a_2, \ldots a_{t-1}\right\}$, let $q(\tau)=\prod_{t=1}^T \pi_\theta\left(a_t, s_t\right)$be the policy of our$Q$-learning algorithm at the trajectory level. Similarly, let $p(\tau)=\prod_{t=1}^T p\left(a_t \mid s_t\right)$be the prior distribution over the trajectory, and$r(\tau)$ be the rewards. We seek to maximize the following KL-regularized objective:</span></span>
<span id="cb13-347"><a href="#cb13-347"></a></span>
<span id="cb13-348"><a href="#cb13-348"></a>$$</span>
<span id="cb13-349"><a href="#cb13-349"></a>L(q)=\mathbb{E}_{q(\tau)}[r(\tau)] / c-D_{\text{KL}}<span class="co">[</span><span class="ot">q(\tau) \mid p(\tau)</span><span class="co">]</span></span>
<span id="cb13-350"><a href="#cb13-350"></a>$$</span>
<span id="cb13-351"><a href="#cb13-351"></a></span>
<span id="cb13-352"><a href="#cb13-352"></a><span class="at">&gt; Since $D_{\text{KL}}[q \mid p]=\sum_x q(x)(\log q(x)-\log p(x))$, we can see that this is equivalent to maximizing the following expected value function of the policy $\pi_\theta$ at the action level:</span></span>
<span id="cb13-353"><a href="#cb13-353"></a></span>
<span id="cb13-354"><a href="#cb13-354"></a>$$</span>
<span id="cb13-355"><a href="#cb13-355"></a>Q^\pi\left(s_t, a_t\right)=\mathbb{E}_\pi\left[\sum^T r\left(s_{t^{\prime}}, a_{t^{\prime}}\right) / c+\log p\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)-\log \pi\left(a_{t^{\prime}} \mid s_{t^{\prime}}\right)\right]</span>
<span id="cb13-356"><a href="#cb13-356"></a>$$</span>
<span id="cb13-357"><a href="#cb13-357"></a></span>
<span id="cb13-358"><a href="#cb13-358"></a><span class="at">&gt; </span></span>
<span id="cb13-359"><a href="#cb13-359"></a></span>
<span id="cb13-360"><a href="#cb13-360"></a><span class="fu"># LLM RL 中 KL 优化的数学形式化 {#sec-rl-kl-optim-formulation}</span></span>
<span id="cb13-361"><a href="#cb13-361"></a></span>
<span id="cb13-362"><a href="#cb13-362"></a>为了进一步分析这些 LLM RL 框架中的实现是否正确，我们需要先形式化 LLM RL 中 KL 散度的优化。</span>
<span id="cb13-363"><a href="#cb13-363"></a></span>
<span id="cb13-364"><a href="#cb13-364"></a><span class="fu">## RL 中的 KL 散度通常定义在轨迹分布上</span></span>
<span id="cb13-365"><a href="#cb13-365"></a></span>
<span id="cb13-366"><a href="#cb13-366"></a>GRPO 公式 (@eq-grpo-obj) 中的 KL 项可以定义为：</span>
<span id="cb13-367"><a href="#cb13-367"></a></span>
<span id="cb13-368"><a href="#cb13-368"></a>$$</span>
<span id="cb13-369"><a href="#cb13-369"></a>\begin{aligned}</span>
<span id="cb13-370"><a href="#cb13-370"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right]</span>
<span id="cb13-371"><a href="#cb13-371"></a>\end{aligned}</span>
<span id="cb13-372"><a href="#cb13-372"></a>$$ {#eq-def-kl-theta-ref}</span>
<span id="cb13-373"><a href="#cb13-373"></a></span>
<span id="cb13-374"><a href="#cb13-374"></a>其中 $\mathbf{\tau}$ 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 $p_{\theta}$ 与参考策略整体分布 $p_{\text{ref}}$ 的 KL 散度。</span>
<span id="cb13-375"><a href="#cb13-375"></a></span>
<span id="cb13-376"><a href="#cb13-376"></a><span class="fu">## 将轨迹展开为状态-动作序列</span></span>
<span id="cb13-377"><a href="#cb13-377"></a></span>
<span id="cb13-378"><a href="#cb13-378"></a>RL 文献中通常会将轨迹 $\mathbf{\tau}$ 展开为状态-动作序列 $\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}$：^<span class="co">[</span><span class="ot">这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，$\mathbf{q}$ 对应于 $\mathbf{s}_1$，而 ${\mathbf{o}}$ 对应于 $\mathbf{\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T}$。</span><span class="co">]</span></span>
<span id="cb13-379"><a href="#cb13-379"></a></span>
<span id="cb13-380"><a href="#cb13-380"></a>$$</span>
<span id="cb13-381"><a href="#cb13-381"></a>\begin{aligned}</span>
<span id="cb13-382"><a href="#cb13-382"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; =\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)}\right] <span class="sc">\\</span></span>
<span id="cb13-383"><a href="#cb13-383"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|},\right) \sim p_{\theta}}\left[\log \frac{p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|},, \mathbf{a}_{|\mathbf{\tau}|},\right)}{p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right)}\right] <span class="sc">\\</span></span>
<span id="cb13-384"><a href="#cb13-384"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\log \frac{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}{p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)}\right] <span class="sc">\\</span></span>
<span id="cb13-385"><a href="#cb13-385"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb13-386"><a href="#cb13-386"></a>\end{aligned}</span>
<span id="cb13-387"><a href="#cb13-387"></a>$$ {#eq-def-kl-theta-ref-state-action-ag}</span>
<span id="cb13-388"><a href="#cb13-388"></a></span>
<span id="cb13-389"><a href="#cb13-389"></a>其中 $|\mathbf{\tau}|$ 为轨迹动作数的随机变量。</span>
<span id="cb13-390"><a href="#cb13-390"></a></span>
<span id="cb13-391"><a href="#cb13-391"></a>此处利用了联合概率的展开，以 $p_{\theta}$ 为例：</span>
<span id="cb13-392"><a href="#cb13-392"></a></span>
<span id="cb13-393"><a href="#cb13-393"></a>$$</span>
<span id="cb13-394"><a href="#cb13-394"></a>p_{\theta}(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t)</span>
<span id="cb13-395"><a href="#cb13-395"></a>$$ {#eq-dp-expansion}</span>
<span id="cb13-396"><a href="#cb13-396"></a></span>
<span id="cb13-397"><a href="#cb13-397"></a>注意区分整体概率分布 $p_{\theta}$、策略（条件）概率分布 $\pi_{\theta}$ 与状态转移概率分布 $p$。</span>
<span id="cb13-398"><a href="#cb13-398"></a></span>
<span id="cb13-399"><a href="#cb13-399"></a><span class="fu">## Markov 决策过程中的 KL 散度</span></span>
<span id="cb13-400"><a href="#cb13-400"></a></span>
<span id="cb13-401"><a href="#cb13-401"></a>实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP^<span class="co">[</span><span class="ot">https://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B</span><span class="co">]</span>。</span>
<span id="cb13-402"><a href="#cb13-402"></a></span>
<span id="cb13-403"><a href="#cb13-403"></a>Markov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 $n$ 个历史状态和动作，而非全部的历史信息，对应的过程称为 $n$ 阶 Markov 过程。以 $n=1$ 为例：</span>
<span id="cb13-404"><a href="#cb13-404"></a></span>
<span id="cb13-405"><a href="#cb13-405"></a>$$</span>
<span id="cb13-406"><a href="#cb13-406"></a>\begin{aligned}</span>
<span id="cb13-407"><a href="#cb13-407"></a>\pi(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) &amp; = \pi(\mathbf{a}_t \mid \mathbf{s}_t) <span class="sc">\\</span></span>
<span id="cb13-408"><a href="#cb13-408"></a>p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) &amp; = p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) <span class="sc">\\</span></span>
<span id="cb13-409"><a href="#cb13-409"></a>\end{aligned}</span>
<span id="cb13-410"><a href="#cb13-410"></a>$$ {#eq-def-markov-prop}</span>
<span id="cb13-411"><a href="#cb13-411"></a></span>
<span id="cb13-412"><a href="#cb13-412"></a>则 <span class="co">[</span><span class="ot">@eq-dp-expansion</span><span class="co">]</span> 中的联合概率可以进一步简化为：</span>
<span id="cb13-413"><a href="#cb13-413"></a></span>
<span id="cb13-414"><a href="#cb13-414"></a>$$</span>
<span id="cb13-415"><a href="#cb13-415"></a>p(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) = p(s_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t)</span>
<span id="cb13-416"><a href="#cb13-416"></a>$$ {#eq-dp-expansion-markov-1}</span>
<span id="cb13-417"><a href="#cb13-417"></a></span>
<span id="cb13-418"><a href="#cb13-418"></a>如果考虑一阶 Markov 过程，则 <span class="co">[</span><span class="ot">@eq-def-kl-theta-ref-state-action-ag</span><span class="co">]</span> 中的 KL 可以进一步简化为：</span>
<span id="cb13-419"><a href="#cb13-419"></a></span>
<span id="cb13-420"><a href="#cb13-420"></a>$$</span>
<span id="cb13-421"><a href="#cb13-421"></a>\begin{aligned}</span>
<span id="cb13-422"><a href="#cb13-422"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] = &amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb13-423"><a href="#cb13-423"></a>&amp; = \mathbb{E}_{\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}\right) \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb13-424"><a href="#cb13-424"></a>\end{aligned}</span>
<span id="cb13-425"><a href="#cb13-425"></a>$$ {#eq-def-kl-theta-ref-state-action-markov-1}</span>
<span id="cb13-426"><a href="#cb13-426"></a></span>
<span id="cb13-427"><a href="#cb13-427"></a><span class="fu">## 语言模型作为序列决策过程 {#sec-lm-as-dp}</span></span>
<span id="cb13-428"><a href="#cb13-428"></a></span>
<span id="cb13-429"><a href="#cb13-429"></a>目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。</span>
<span id="cb13-430"><a href="#cb13-430"></a></span>
<span id="cb13-431"><a href="#cb13-431"></a>尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 $s_1$ 表示 prompt 中的所有 token，对于 $t &gt;1$，如果令 $s_t$ 表示第 $t$ 个动作 token 前的所有 token，则自回归模型满足 Markov 性质，否则不一定。</span>
<span id="cb13-432"><a href="#cb13-432"></a></span>
<span id="cb13-433"><a href="#cb13-433"></a>接下来，我们先令 $s_t$ 表示前 $t$ 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。</span>
<span id="cb13-434"><a href="#cb13-434"></a></span>
<span id="cb13-435"><a href="#cb13-435"></a><span class="fu">## 估计 KL 散度</span></span>
<span id="cb13-436"><a href="#cb13-436"></a></span>
<span id="cb13-437"><a href="#cb13-437"></a><span class="fu">### 几乎不可能直接计算 KL 散度的真实值</span></span>
<span id="cb13-438"><a href="#cb13-438"></a></span>
<span id="cb13-439"><a href="#cb13-439"></a>实际实现中，我们几乎不可能直接计算出 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]$，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 $\left|\mathcal{T}\right|$ 与轨迹最大长度 $T = \max_{\mathbf{\tau} \in \mathcal{T}} |\mathbf{\tau}|$ 成指数关系：</span>
<span id="cb13-440"><a href="#cb13-440"></a>$$</span>
<span id="cb13-441"><a href="#cb13-441"></a>\begin{aligned}</span>
<span id="cb13-442"><a href="#cb13-442"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb13-443"><a href="#cb13-443"></a>&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) <span class="sc">\\</span></span>
<span id="cb13-444"><a href="#cb13-444"></a>\end{aligned}</span>
<span id="cb13-445"><a href="#cb13-445"></a>$$ {#eq-def-rl-kl-avg-over-traj}</span>
<span id="cb13-446"><a href="#cb13-446"></a></span>
<span id="cb13-447"><a href="#cb13-447"></a><span class="fu">### 通常使用 Monte Carlo 方法估计 KL 散度</span></span>
<span id="cb13-448"><a href="#cb13-448"></a></span>
<span id="cb13-449"><a href="#cb13-449"></a>所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法^<span class="co">[</span><span class="ot">https://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95</span><span class="co">]</span>来估计 RL 中的 KL 散度，例如：</span>
<span id="cb13-450"><a href="#cb13-450"></a></span>
<span id="cb13-451"><a href="#cb13-451"></a>$$</span>
<span id="cb13-452"><a href="#cb13-452"></a>\begin{aligned}</span>
<span id="cb13-453"><a href="#cb13-453"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\tau \in \mathcal{T}} p_{\theta} (\mathbf{\tau}) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right) <span class="sc">\\</span></span>
<span id="cb13-454"><a href="#cb13-454"></a>&amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{|\mathbf{\tau_{i }}|} \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)</span>
<span id="cb13-455"><a href="#cb13-455"></a>\end{aligned}</span>
<span id="cb13-456"><a href="#cb13-456"></a>$$ {#eq-def-rl-kl-mc-k1}</span>
<span id="cb13-457"><a href="#cb13-457"></a></span>
<span id="cb13-458"><a href="#cb13-458"></a>其中，$\mathbf{\tau_{i}} = \left(\mathbf{s}_{i,1}, \mathbf{a}_{i,1}, \cdots, \mathbf{s}_{i,|\mathbf{\tau_{i}}|}, \mathbf{a}_{i,|\mathbf{\tau_{i}}|}\right) \sim p_{\theta}$，$N$ 为估计使用的轨迹样本数量。</span>
<span id="cb13-459"><a href="#cb13-459"></a></span>
<span id="cb13-460"><a href="#cb13-460"></a><span class="fu">### 不同的 KL 估计量</span></span>
<span id="cb13-461"><a href="#cb13-461"></a></span>
<span id="cb13-462"><a href="#cb13-462"></a>实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。</span>
<span id="cb13-463"><a href="#cb13-463"></a></span>
<span id="cb13-464"><a href="#cb13-464"></a>设 KL 估计量为 $k$，则对应的 KL 估计值为</span>
<span id="cb13-465"><a href="#cb13-465"></a></span>
<span id="cb13-466"><a href="#cb13-466"></a>$$</span>
<span id="cb13-467"><a href="#cb13-467"></a>\begin{aligned}</span>
<span id="cb13-468"><a href="#cb13-468"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} k(\tau_i)</span>
<span id="cb13-469"><a href="#cb13-469"></a>\end{aligned}</span>
<span id="cb13-470"><a href="#cb13-470"></a>$$ {#eq-def-rl-kl-mc-general}</span>
<span id="cb13-471"><a href="#cb13-471"></a></span>
<span id="cb13-472"><a href="#cb13-472"></a>例如 <span class="co">[</span><span class="ot">@sec-openrlhf-kl-reward</span><span class="co">]</span> 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 <span class="in">`k1`</span>, <span class="in">`k2`</span>, <span class="in">`k3`</span>，这应该是主要参考了 John Schulman 的博客 "Approximating KL Divergence"。</span>
<span id="cb13-473"><a href="#cb13-473"></a></span>
<span id="cb13-474"><a href="#cb13-474"></a>verl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度^<span class="co">[</span><span class="ot">这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。</span><span class="co">]</span>，但目前还没有实现。对应代码可见 <span class="co">[</span><span class="ot">@lst-verl-kl-estimator</span><span class="co">]</span>。</span>
<span id="cb13-475"><a href="#cb13-475"></a></span>
<span id="cb13-476"><a href="#cb13-476"></a><span class="in">```{#lst-verl-kl-estimator .python lst-cap="verl 的 KL 散度 Monte Carlo 估计样本值^[https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383]"}</span></span>
<span id="cb13-477"><a href="#cb13-477"></a><span class="in">def kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) -&gt; torch.FloatTensor:</span></span>
<span id="cb13-478"><a href="#cb13-478"></a><span class="in">    # ...</span></span>
<span id="cb13-479"><a href="#cb13-479"></a><span class="in">    if kl_penalty == "kl":</span></span>
<span id="cb13-480"><a href="#cb13-480"></a><span class="in">        return logprob - ref_logprob</span></span>
<span id="cb13-481"><a href="#cb13-481"></a></span>
<span id="cb13-482"><a href="#cb13-482"></a><span class="in">    if kl_penalty == "abs":</span></span>
<span id="cb13-483"><a href="#cb13-483"></a><span class="in">        return (logprob - ref_logprob).abs()</span></span>
<span id="cb13-484"><a href="#cb13-484"></a></span>
<span id="cb13-485"><a href="#cb13-485"></a><span class="in">    if kl_penalty == "mse":</span></span>
<span id="cb13-486"><a href="#cb13-486"></a><span class="in">        return 0.5 * (logprob - ref_logprob).square()</span></span>
<span id="cb13-487"><a href="#cb13-487"></a></span>
<span id="cb13-488"><a href="#cb13-488"></a><span class="in">    # J. Schulman. Approximating kl divergence, 2020.</span></span>
<span id="cb13-489"><a href="#cb13-489"></a><span class="in">    # # URL http://joschu.net/blog/kl-approx.html.</span></span>
<span id="cb13-490"><a href="#cb13-490"></a><span class="in">    if kl_penalty == 'low_var_kl':</span></span>
<span id="cb13-491"><a href="#cb13-491"></a><span class="in">        kl = ref_logprob - logprob</span></span>
<span id="cb13-492"><a href="#cb13-492"></a><span class="in">        ratio = torch.exp(kl)</span></span>
<span id="cb13-493"><a href="#cb13-493"></a><span class="in">        kld = (ratio - kl - 1).contiguous()</span></span>
<span id="cb13-494"><a href="#cb13-494"></a><span class="in">        return torch.clamp(kld, min=-10, max=10)</span></span>
<span id="cb13-495"><a href="#cb13-495"></a></span>
<span id="cb13-496"><a href="#cb13-496"></a><span class="in">    if kl_penalty == "full":</span></span>
<span id="cb13-497"><a href="#cb13-497"></a><span class="in">        # so, here logprob and ref_logprob should contain the logits for every token in vocabulary</span></span>
<span id="cb13-498"><a href="#cb13-498"></a><span class="in">        raise NotImplementedError</span></span>
<span id="cb13-499"><a href="#cb13-499"></a></span>
<span id="cb13-500"><a href="#cb13-500"></a><span class="in">    raise NotImplementedError</span></span>
<span id="cb13-501"><a href="#cb13-501"></a><span class="in">```</span></span>
<span id="cb13-502"><a href="#cb13-502"></a></span>
<span id="cb13-503"><a href="#cb13-503"></a>由于 $k_1$、$k_2$、$k_3$ 三种估计量最为流行，我们将以这三种估计量为例展开分析。</span>
<span id="cb13-504"><a href="#cb13-504"></a></span>
<span id="cb13-505"><a href="#cb13-505"></a>考虑 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} k_j(\tau_i)$，其中 $\tau_i \sim p_{\theta}$，令 $r = \frac{\pi_{\text{ref}}(\tau_i)}{\pi_{\theta}(\tau_i)}$，注意，此处 $r$ 并非 KL 定义中的样本量，而是其倒数，则：</span>
<span id="cb13-506"><a href="#cb13-506"></a></span>
<span id="cb13-507"><a href="#cb13-507"></a>$$</span>
<span id="cb13-508"><a href="#cb13-508"></a>\begin{aligned}</span>
<span id="cb13-509"><a href="#cb13-509"></a>k_{1} &amp; = - \log r <span class="sc">\\</span></span>
<span id="cb13-510"><a href="#cb13-510"></a>k_{2} &amp; = \frac{1}{2} (\log r)^2 <span class="sc">\\</span></span>
<span id="cb13-511"><a href="#cb13-511"></a>k_{3} &amp; = (r - 1) - \log r</span>
<span id="cb13-512"><a href="#cb13-512"></a>\end{aligned}</span>
<span id="cb13-513"><a href="#cb13-513"></a>$$ {#eq-def-kl-estimators}</span>
<span id="cb13-514"><a href="#cb13-514"></a></span>
<span id="cb13-515"><a href="#cb13-515"></a><span class="fu"># 流行 on-policy KL 优化实现的数学形式化</span></span>
<span id="cb13-516"><a href="#cb13-516"></a></span>
<span id="cb13-517"><a href="#cb13-517"></a>神经网络模型普遍使用梯度法优化，因此，我们主要关注这些 KL 优化实现导出的梯度。</span>
<span id="cb13-518"><a href="#cb13-518"></a></span>
<span id="cb13-519"><a href="#cb13-519"></a>而由于 reward 项优化的实现涉及到基线（Baseline）、折扣（Discounting）、GAE <span class="co">[</span><span class="ot">@schulman2018gae</span><span class="co">]</span> 等内容，较为复杂，我们可以先分析 KL loss 项实现。</span>
<span id="cb13-520"><a href="#cb13-520"></a></span>
<span id="cb13-521"><a href="#cb13-521"></a><span class="fu">## 分析流行的 “KL loss 项” 实现 {#sec-kl-loss-impl}</span></span>
<span id="cb13-522"><a href="#cb13-522"></a></span>
<span id="cb13-523"><a href="#cb13-523"></a>上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。</span>
<span id="cb13-524"><a href="#cb13-524"></a></span>
<span id="cb13-525"><a href="#cb13-525"></a>然而，如 <span class="co">[</span><span class="ot">@sec-grpo-kl-misunderstanding</span><span class="co">]</span> 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。</span>
<span id="cb13-526"><a href="#cb13-526"></a></span>
<span id="cb13-527"><a href="#cb13-527"></a><span class="fu">### 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</span></span>
<span id="cb13-528"><a href="#cb13-528"></a></span>
<span id="cb13-529"><a href="#cb13-529"></a>观察 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-loss</span><span class="co">]</span> 计算 “KL loss” 项的部分。</span>
<span id="cb13-530"><a href="#cb13-530"></a></span>
<span id="cb13-531"><a href="#cb13-531"></a><span class="in">```python</span></span>
<span id="cb13-532"><a href="#cb13-532"></a><span class="co"># ...</span></span>
<span id="cb13-533"><a href="#cb13-533"></a>kl <span class="op">=</span> compute_approx_kl(</span>
<span id="cb13-534"><a href="#cb13-534"></a>    action_log_probs,</span>
<span id="cb13-535"><a href="#cb13-535"></a>    base_action_log_probs,</span>
<span id="cb13-536"><a href="#cb13-536"></a>    <span class="co"># ...</span></span>
<span id="cb13-537"><a href="#cb13-537"></a>    kl_estimator<span class="op">=</span><span class="va">self</span>.args.kl_estimator,</span>
<span id="cb13-538"><a href="#cb13-538"></a>)</span>
<span id="cb13-539"><a href="#cb13-539"></a><span class="co"># ...</span></span>
<span id="cb13-540"><a href="#cb13-540"></a>kl_mean <span class="op">=</span> masked_mean(kl, experience.action_mask, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-541"><a href="#cb13-541"></a><span class="co"># ...</span></span>
<span id="cb13-542"><a href="#cb13-542"></a>kl_loss <span class="op">=</span> kl_mean.mean()</span>
<span id="cb13-543"><a href="#cb13-543"></a><span class="co"># ...</span></span>
<span id="cb13-544"><a href="#cb13-544"></a><span class="in">```</span></span>
<span id="cb13-545"><a href="#cb13-545"></a></span>
<span id="cb13-546"><a href="#cb13-546"></a>这些代码：</span>
<span id="cb13-547"><a href="#cb13-547"></a></span>
<span id="cb13-548"><a href="#cb13-548"></a><span class="ss">1. </span>计算了 <span class="in">`kl`</span>，对应对每个动作 token $a_{i,t}$ 计算 “KL 估计量” $k$。</span>
<span id="cb13-549"><a href="#cb13-549"></a><span class="ss">2. </span>计算了 <span class="in">`kl_mean`</span>，对应对每个轨迹 $\tau_i$ 计算均值 $\frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k$。</span>
<span id="cb13-550"><a href="#cb13-550"></a><span class="ss">3. </span>计算了 <span class="in">`kl_loss`</span>，对应对所有轨迹样本计算均值 $\frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|} k$。</span>
<span id="cb13-551"><a href="#cb13-551"></a></span>
<span id="cb13-552"><a href="#cb13-552"></a>由于其没有去除任何梯度，因此其导出的梯度估计值为</span>
<span id="cb13-553"><a href="#cb13-553"></a></span>
<span id="cb13-554"><a href="#cb13-554"></a>$$</span>
<span id="cb13-555"><a href="#cb13-555"></a>\begin{aligned}</span>
<span id="cb13-556"><a href="#cb13-556"></a>\nabla_{\theta} \left( \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \frac{1}{|\tau_i|} k \right) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k</span>
<span id="cb13-557"><a href="#cb13-557"></a>\end{aligned}</span>
<span id="cb13-558"><a href="#cb13-558"></a>$$ {#eq-def-kl-loss-grad-estim-openrlhf}</span>
<span id="cb13-559"><a href="#cb13-559"></a></span>
<span id="cb13-560"><a href="#cb13-560"></a><span class="co">[</span><span class="ot">@lst-verl-kl-loss</span><span class="co">]</span> 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：</span>
<span id="cb13-561"><a href="#cb13-561"></a></span>
<span id="cb13-562"><a href="#cb13-562"></a>$$</span>
<span id="cb13-563"><a href="#cb13-563"></a>\begin{aligned}</span>
<span id="cb13-564"><a href="#cb13-564"></a>\nabla_{\theta} \left( \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} k \right) = \frac{1}{\sum_{i=1}^{N} |\tau_i|} \sum_{i=1}^{N} \nabla_{\theta} k</span>
<span id="cb13-565"><a href="#cb13-565"></a>\end{aligned}</span>
<span id="cb13-566"><a href="#cb13-566"></a>$$ {#eq-def-kl-loss-grad-estim-verl}</span>
<span id="cb13-567"><a href="#cb13-567"></a></span>
<span id="cb13-568"><a href="#cb13-568"></a>我们将平均操作一般化为权重 $w_{\mathbf{\tau}}$ 与 $w_{t}$，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：</span>
<span id="cb13-569"><a href="#cb13-569"></a></span>
<span id="cb13-570"><a href="#cb13-570"></a>$$</span>
<span id="cb13-571"><a href="#cb13-571"></a>\begin{aligned}</span>
<span id="cb13-572"><a href="#cb13-572"></a>\sum_{i=1}^{N} w_{\mathbf{\tau}_i} \sum_{t=1}^{|\tau_i|} w_{t} \nabla_{\theta} k <span class="sc">\\</span></span>
<span id="cb13-573"><a href="#cb13-573"></a>\end{aligned}</span>
<span id="cb13-574"><a href="#cb13-574"></a>$$ {#eq-def-kl-loss-grad-estim-general}</span>
<span id="cb13-575"><a href="#cb13-575"></a></span>
<span id="cb13-576"><a href="#cb13-576"></a>则</span>
<span id="cb13-577"><a href="#cb13-577"></a></span>
<span id="cb13-578"><a href="#cb13-578"></a><span class="ss">- </span>OpenRLHF 对应 $w_{\mathbf{\tau}} = \frac{1}{N}, w_{t} = \frac{1}{|\tau|}$；</span>
<span id="cb13-579"><a href="#cb13-579"></a><span class="ss">- </span>verl 对应 $w_{\mathbf{\tau}} = \frac{1}{\sum_{i=1}^{N} |\tau_i|}, w_{t} = 1$。</span>
<span id="cb13-580"><a href="#cb13-580"></a></span>
<span id="cb13-581"><a href="#cb13-581"></a>此处，我们先以 OpenRLHF 的梯度估计 (@eq-def-kl-loss-grad-estim-openrlhf) 为例，分析不同 KL 估计量导出的梯度估计，其满足：</span>
<span id="cb13-582"><a href="#cb13-582"></a></span>
<span id="cb13-583"><a href="#cb13-583"></a>$$</span>
<span id="cb13-584"><a href="#cb13-584"></a>\mathbb{E}_{\mathbf{\tau}_i \sim p_{\theta}} \left[ \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\tau_i|} \sum_{t=1}^{|\tau_i|}  \nabla_{\theta} k \right] = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} k \right]</span>
<span id="cb13-585"><a href="#cb13-585"></a>$$ {#eq-def-kl-loss-grad-expect-openrlhf}</span>
<span id="cb13-586"><a href="#cb13-586"></a></span>
<span id="cb13-587"><a href="#cb13-587"></a>我们会在 <span class="co">[</span><span class="ot">@sec-derive-kld-grad</span><span class="co">]</span> 中推导正确的 KL 梯度估计。</span>
<span id="cb13-588"><a href="#cb13-588"></a></span>
<span id="cb13-589"><a href="#cb13-589"></a><span class="fu">### $k_1$ 导出的梯度：期望为 0</span></span>
<span id="cb13-590"><a href="#cb13-590"></a></span>
<span id="cb13-591"><a href="#cb13-591"></a>向 <span class="co">[</span><span class="ot">@eq-def-kl-loss-grad-expect-openrlhf</span><span class="co">]</span> 代入 $k = k_1 = - \log r = \log \frac{1}{r} = \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}$，导出的梯度估计为</span>
<span id="cb13-592"><a href="#cb13-592"></a></span>
<span id="cb13-593"><a href="#cb13-593"></a>$$</span>
<span id="cb13-594"><a href="#cb13-594"></a>\begin{aligned}</span>
<span id="cb13-595"><a href="#cb13-595"></a>&amp; \frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k <span class="sc">\\</span></span>
<span id="cb13-596"><a href="#cb13-596"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} <span class="sc">\\</span></span>
<span id="cb13-597"><a href="#cb13-597"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}\log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) <span class="sc">\\</span></span>
<span id="cb13-598"><a href="#cb13-598"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) <span class="sc">\\</span></span>
<span id="cb13-599"><a href="#cb13-599"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \left( \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) + \nabla_{\theta} \log \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) + \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \right) \right) <span class="sc">\\</span></span>
<span id="cb13-600"><a href="#cb13-600"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log \left( p(\mathbf{s}_{1}) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t}) \right) <span class="sc">\\</span></span>
<span id="cb13-601"><a href="#cb13-601"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_\theta(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) <span class="sc">\\</span></span>
<span id="cb13-602"><a href="#cb13-602"></a>=&amp;\frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\tau)</span>
<span id="cb13-603"><a href="#cb13-603"></a>\end{aligned}</span>
<span id="cb13-604"><a href="#cb13-604"></a>$$ {#eq-kl-loss-grad-sample-k1}</span>
<span id="cb13-605"><a href="#cb13-605"></a></span>
<span id="cb13-606"><a href="#cb13-606"></a>则其导出的梯度期望满足：</span>
<span id="cb13-607"><a href="#cb13-607"></a></span>
<span id="cb13-608"><a href="#cb13-608"></a>$$</span>
<span id="cb13-609"><a href="#cb13-609"></a>\begin{aligned}</span>
<span id="cb13-610"><a href="#cb13-610"></a>\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \frac{1}{|\mathbf{\tau}|} \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})\right] </span>
<span id="cb13-611"><a href="#cb13-611"></a>&amp; = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} \nabla_{\theta} \log p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb13-612"><a href="#cb13-612"></a>&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb13-613"><a href="#cb13-613"></a>&amp; = \sum_{\tau \in \mathcal{T}} \frac{1}{|\tau|} \nabla_{\theta} p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb13-614"><a href="#cb13-614"></a>&amp; = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \frac{1}{|\tau|} <span class="sc">\\</span></span>
<span id="cb13-615"><a href="#cb13-615"></a>&amp; = \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left<span class="co">[</span><span class="ot"> \frac{1}{|\mathbf{\tau}|} \right</span><span class="co">]</span></span>
<span id="cb13-616"><a href="#cb13-616"></a>\end{aligned}</span>
<span id="cb13-617"><a href="#cb13-617"></a>$$ {#eq-kl-loss-grad-expect-k1}</span>
<span id="cb13-618"><a href="#cb13-618"></a></span>
<span id="cb13-619"><a href="#cb13-619"></a>此处利用了 $p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \frac{1}{p_{\theta}(\tau)} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) = \nabla_{\theta} p_{\theta}(\tau)$。</span>
<span id="cb13-620"><a href="#cb13-620"></a></span>
<span id="cb13-621"><a href="#cb13-621"></a>所以 $k_1$ loss 项优化的量是 $\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left<span class="co">[</span><span class="ot"> \frac{1}{|\mathbf{\tau}|} \right</span><span class="co">]</span>$。这意味着该优化过程会降低采样轨迹的长度。</span>
<span id="cb13-622"><a href="#cb13-622"></a></span>
<span id="cb13-623"><a href="#cb13-623"></a>特别地，当不对同一轨迹中的 “$k_1$ 估计量”求均值，而是求和时，可以直接将 $\frac{1}{|\tau|}$ 这一项替换为 $1$，得到</span>
<span id="cb13-624"><a href="#cb13-624"></a>$$</span>
<span id="cb13-625"><a href="#cb13-625"></a>\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) = \sum_{\tau \in \mathcal{T}} \nabla_{\theta} p_{\theta} = \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p_{\theta} = \nabla_{\theta} 1 = 0</span>
<span id="cb13-626"><a href="#cb13-626"></a>$$ {#eq-kl-loss-grad-expect-k1-no-intra-traj-mean}^<span class="co">[</span><span class="ot">此处对数似然的梯度的期望值为 0，是一个著名的性质，会在接下来频繁用到。</span><span class="co">]</span></span>
<span id="cb13-627"><a href="#cb13-627"></a></span>
<span id="cb13-628"><a href="#cb13-628"></a>这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。</span>
<span id="cb13-629"><a href="#cb13-629"></a></span>
<span id="cb13-630"><a href="#cb13-630"></a>无论哪种情况，$k_1$ 导出的优化量都非常奇怪，不太可能出于实现者的本意。</span>
<span id="cb13-631"><a href="#cb13-631"></a></span>
<span id="cb13-632"><a href="#cb13-632"></a>同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。接下来，我们将忽略这一操作，即将 $\frac{1}{|\tau|}$ 一项替换为 $1$。</span>
<span id="cb13-633"><a href="#cb13-633"></a></span>
<span id="cb13-634"><a href="#cb13-634"></a><span class="fu">### $k_2$ 导出的梯度</span></span>
<span id="cb13-635"><a href="#cb13-635"></a></span>
<span id="cb13-636"><a href="#cb13-636"></a>向 <span class="co">[</span><span class="ot">@eq-def-kl-loss-grad-expect-openrlhf</span><span class="co">]</span> 代入 $k = k_2 = \frac{1}{2} (\log r)^2 = \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right)^2$，导出的单条轨迹 $\mathbf{\tau} \sim p_{\theta}$ 的梯度为</span>
<span id="cb13-637"><a href="#cb13-637"></a>$$</span>
<span id="cb13-638"><a href="#cb13-638"></a>\begin{aligned}</span>
<span id="cb13-639"><a href="#cb13-639"></a>&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k<span class="sc">\\</span></span>
<span id="cb13-640"><a href="#cb13-640"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta}  \frac{1}{2} \left(\log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}\right)^2 <span class="sc">\\</span></span>
<span id="cb13-641"><a href="#cb13-641"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \frac{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} <span class="sc">\\</span></span>
<span id="cb13-642"><a href="#cb13-642"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) <span class="sc">\\</span></span>
<span id="cb13-643"><a href="#cb13-643"></a>\end{aligned}</span>
<span id="cb13-644"><a href="#cb13-644"></a>$$ {#eq-kl-loss-grad-sample-k2}</span>
<span id="cb13-645"><a href="#cb13-645"></a></span>
<span id="cb13-646"><a href="#cb13-646"></a>显然，</span>
<span id="cb13-647"><a href="#cb13-647"></a></span>
<span id="cb13-648"><a href="#cb13-648"></a>$$</span>
<span id="cb13-649"><a href="#cb13-649"></a>\begin{aligned}</span>
<span id="cb13-650"><a href="#cb13-650"></a>&amp; \sum_{t=1}^{|\mathbf{\tau}|} \left( \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) <span class="sc">\\</span></span>
<span id="cb13-651"><a href="#cb13-651"></a>\neq &amp; \left( \sum_{t=1}^{|\mathbf{\tau}|}  \log \frac{\pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})}{\pi_{\text{ref}}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t})} \right) \left( \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \log \pi_{\theta}(a_{i,t} \mid s_{i,1}, a_{i,1}, \cdots, s_{i,t}) \right) <span class="sc">\\</span></span>
<span id="cb13-652"><a href="#cb13-652"></a>=&amp; \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})</span>
<span id="cb13-653"><a href="#cb13-653"></a>\end{aligned}</span>
<span id="cb13-654"><a href="#cb13-654"></a>$$ {#eq-kl-loss-grad-sample-k2-wrong}</span>
<span id="cb13-655"><a href="#cb13-655"></a></span>
<span id="cb13-656"><a href="#cb13-656"></a>然而，</span>
<span id="cb13-657"><a href="#cb13-657"></a></span>
<span id="cb13-658"><a href="#cb13-658"></a>$$</span>
<span id="cb13-659"><a href="#cb13-659"></a>\begin{aligned}</span>
<span id="cb13-660"><a href="#cb13-660"></a>&amp; \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] <span class="sc">\\</span></span>
<span id="cb13-661"><a href="#cb13-661"></a>=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} \log p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb13-662"><a href="#cb13-662"></a>=&amp; \sum_{\tau \in \mathcal{T}} \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} \right) \nabla_{\theta} p_{\theta}(\tau) <span class="sc">\\</span></span>
<span id="cb13-663"><a href="#cb13-663"></a>=&amp; \sum_{\tau \in \mathcal{T}} \left<span class="co">[</span><span class="ot"> \left( \log p_{\theta}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) - \left( \log p_{\text{ref}}(\tau) \right) \nabla_{\theta} p_{\theta}(\tau) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb13-664"><a href="#cb13-664"></a>=&amp; \sum_{\tau \in \mathcal{T}}  \left<span class="co">[</span><span class="ot"> \nabla_{\theta} (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) -  \nabla_{\theta} \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb13-665"><a href="#cb13-665"></a>=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  \left<span class="co">[</span><span class="ot"> (\log p_{\theta}(\tau) - 1) p_{\theta}(\tau) - \log p_{\text{ref}}(\tau) p_{\theta}(\tau) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb13-666"><a href="#cb13-666"></a>=&amp; \nabla_{\theta} \sum_{\tau \in \mathcal{T}}  p_{\theta} \left<span class="co">[</span><span class="ot"> \left( \log \frac{p_{\theta}(\tau)}{p_{\text{ref}}(\tau)} - 1 \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb13-667"><a href="#cb13-667"></a>=&amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \left( \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} - 1 \right) \right] <span class="sc">\\</span></span>
<span id="cb13-668"><a href="#cb13-668"></a>= &amp; \nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[  \log \frac{p_{\theta}(\mathbf{\tau})}{p_{\text{ref}}(\mathbf{\tau})} \right] <span class="sc">\\</span></span>
<span id="cb13-669"><a href="#cb13-669"></a>= &amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]</span>
<span id="cb13-670"><a href="#cb13-670"></a>\end{aligned}</span>
<span id="cb13-671"><a href="#cb13-671"></a>$$ {#eq-kl-loss-grad-expect-k2-wrong}</span>
<span id="cb13-672"><a href="#cb13-672"></a></span>
<span id="cb13-673"><a href="#cb13-673"></a>此处利用了 $\log p(x) \nabla_{\theta} p(x) = \nabla_{\theta} (\log p(x) - 1) p(x)$</span>
<span id="cb13-674"><a href="#cb13-674"></a></span>
<span id="cb13-675"><a href="#cb13-675"></a>因此，最小化 $k_2$ loss 项 (@eq-kl-loss-grad-sample-k2-wrong) ，并非在优化 $\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]$。</span>
<span id="cb13-676"><a href="#cb13-676"></a></span>
<span id="cb13-677"><a href="#cb13-677"></a><span class="fu">### $k_3$ 导出的梯度</span></span>
<span id="cb13-678"><a href="#cb13-678"></a></span>
<span id="cb13-679"><a href="#cb13-679"></a>向 <span class="co">[</span><span class="ot">@eq-def-kl-loss-grad-expect-openrlhf</span><span class="co">]</span> 代入 $k = k_3 = (r - 1) - \log r = (\log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1) - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}$，导出的单条轨迹 $\mathbf{\tau} \sim p_{\theta}$ 的梯度为</span>
<span id="cb13-680"><a href="#cb13-680"></a>$$</span>
<span id="cb13-681"><a href="#cb13-681"></a>\begin{aligned}</span>
<span id="cb13-682"><a href="#cb13-682"></a>&amp; \sum_{t=1}^{|\mathbf{\tau}|}  \nabla_{\theta} k <span class="sc">\\</span></span>
<span id="cb13-683"><a href="#cb13-683"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} \nabla_{\theta} \left(\frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} - 1 - \log \frac{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}\right) <span class="sc">\\</span></span>
<span id="cb13-684"><a href="#cb13-684"></a>=&amp; \sum_{t=1}^{|\mathbf{\tau}|} - \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} <span class="sc">\\</span></span>
<span id="cb13-685"><a href="#cb13-685"></a>=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) - \nabla_{\theta} \log \frac{p_{\text{ref}}(\mathbf{\tau})}{p_{\theta}(\mathbf{\tau})} <span class="sc">\\</span></span>
<span id="cb13-686"><a href="#cb13-686"></a>=&amp; - \left( \sum_{t=1}^{|\mathbf{\tau}|} \frac{ \pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})}{\pi_{\theta}^{2}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t})} \nabla_{\theta}  \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \right) + \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) <span class="sc">\\</span></span>
<span id="cb13-687"><a href="#cb13-687"></a>\end{aligned}</span>
<span id="cb13-688"><a href="#cb13-688"></a>$$ {#eq-kl-loss-grad-sample-k3}</span>
<span id="cb13-689"><a href="#cb13-689"></a></span>
<span id="cb13-690"><a href="#cb13-690"></a>其中，根据 <span class="co">[</span><span class="ot">@eq-kl-loss-grad-expect-k1-no-intra-traj-mean</span><span class="co">]</span>，$\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[ \nabla_{\theta} \log p_{\theta}(\mathbf{\tau}) \right] = 0$，不妨直接省略。</span>
<span id="cb13-691"><a href="#cb13-691"></a></span>
<span id="cb13-692"><a href="#cb13-692"></a>而剩余部分似乎很难通过消去 $\pi_{\theta}(\mathbf{\tau})$ 来提出 $\nabla_{\theta}$ 并准确分析。但显然也并非在优化 KL 散度。</span>
<span id="cb13-693"><a href="#cb13-693"></a></span>
<span id="cb13-694"><a href="#cb13-694"></a><span class="fu">### 小结：流行的 ”KL loss 项“ 实现并不合理</span></span>
<span id="cb13-695"><a href="#cb13-695"></a></span>
<span id="cb13-696"><a href="#cb13-696"></a>综上所述，对于 OpenRLHF 实现的 “KL loss 项”，</span>
<span id="cb13-697"><a href="#cb13-697"></a></span>
<span id="cb13-698"><a href="#cb13-698"></a></span>
<span id="cb13-699"><a href="#cb13-699"></a><span class="ss">1. </span>对同一轨迹内的 “KL 估计量” 求均值这一操作很可能是错误的，正确操作应当为求和，对应于根据对数条件概率求对数联合概率。</span>
<span id="cb13-700"><a href="#cb13-700"></a><span class="ss">2. </span>$k_1$ 导出的梯度</span>
<span id="cb13-701"><a href="#cb13-701"></a><span class="ss">   1. </span>若对同一轨迹内的 “KL 估计量” 求均值，则会导致输出长度减小，</span>
<span id="cb13-702"><a href="#cb13-702"></a><span class="ss">   2. </span>而如果修正为求和，则其期望为 0，在平均意义上不改分布。</span>
<span id="cb13-703"><a href="#cb13-703"></a><span class="ss">3. </span>$k_2$，$k_3$ 导出的梯度则十分复杂，难以分析，但都并非在优化 KL 散度，这可能是因为其错误地将 KL 估计样本量应用于动作对数条件似然并求和。回顾 KL 估计量公式 (@eq-def-kl-estimators) ，应当注意到这些估计量是直接作用于似然 $p_{\theta}(\mathbf{\tau})$，而没有保证作用于概率后求积/对数和仍然有意义。</span>
<span id="cb13-704"><a href="#cb13-704"></a></span>
<span id="cb13-705"><a href="#cb13-705"></a><span class="fu">## 分析流行的 “KL reward 项“ 实现</span></span>
<span id="cb13-706"><a href="#cb13-706"></a></span>
<span id="cb13-707"><a href="#cb13-707"></a><span class="fu">### 类比 PG 优化 reward 来分析 KL reward 的作用 {#sec-analogy-pg-kl}</span></span>
<span id="cb13-708"><a href="#cb13-708"></a></span>
<span id="cb13-709"><a href="#cb13-709"></a>由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是：</span>
<span id="cb13-710"><a href="#cb13-710"></a>$$</span>
<span id="cb13-711"><a href="#cb13-711"></a>\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[r(\mathbf{\tau})\right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \hat{A}_t \right]</span>
<span id="cb13-712"><a href="#cb13-712"></a>$$ {#eq-pg-est-adv}</span>
<span id="cb13-713"><a href="#cb13-713"></a></span>
<span id="cb13-714"><a href="#cb13-714"></a>其中 $\hat{A}_t$ 为优势（Advantage）的估计量。</span>
<span id="cb13-715"><a href="#cb13-715"></a></span>
<span id="cb13-716"><a href="#cb13-716"></a>为了方便观察 KL reward 项发挥的作用，我们将 $r_{\mathbf{\tau}}$ 展开，并不妨考虑一个更简单的估计，例如：</span>
<span id="cb13-717"><a href="#cb13-717"></a></span>
<span id="cb13-718"><a href="#cb13-718"></a>$$</span>
<span id="cb13-719"><a href="#cb13-719"></a>\nabla_\theta \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{|\mathbf{\tau}|} r(\mathbf{s}_t, \mathbf{a}_t) \right] = \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right) \sum_{t'=1}^{|\tau|} r(s_{t'}, a_{t'}) \right]</span>
<span id="cb13-720"><a href="#cb13-720"></a>$$ {#eq-pg-est-ret}</span>
<span id="cb13-721"><a href="#cb13-721"></a></span>
<span id="cb13-722"><a href="#cb13-722"></a>简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 "Policy Gradient" 一讲^<span class="co">[</span><span class="ot">https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf</span><span class="co">]</span>。</span>
<span id="cb13-723"><a href="#cb13-723"></a></span>
<span id="cb13-724"><a href="#cb13-724"></a>类比 $r_{t'}$ 导出的梯度期望，将负的 KL 样本量 $- \log \frac{\pi_\theta\left(a_t \mid s_t \right)}{\pi_{\text{ref}}\left(a_t \mid s_t \right)}$ 加入 reward $r_{t'}$ 代入其中，导出的梯度期望为：</span>
<span id="cb13-725"><a href="#cb13-725"></a></span>
<span id="cb13-726"><a href="#cb13-726"></a>$$</span>
<span id="cb13-727"><a href="#cb13-727"></a>\mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\tau|}  \left( \nabla_\theta \log \pi_\theta\left(a_t \mid s_t \right) \right) \sum_{t'=1}^{|\tau|} - \log \frac{\pi_\theta\left(a_{t'} \mid s_{t'} \right)}{\pi_{\text{ref}}\left(a_{t'} \mid s_{t'} \right)} \right] = \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}\right]</span>
<span id="cb13-728"><a href="#cb13-728"></a>$$ {#eq-kl-grad-est-markov-1}</span>
<span id="cb13-729"><a href="#cb13-729"></a></span>
<span id="cb13-730"><a href="#cb13-730"></a>注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (@eq-def-markov-prop)。</span>
<span id="cb13-731"><a href="#cb13-731"></a></span>
<span id="cb13-732"><a href="#cb13-732"></a>实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：</span>
<span id="cb13-733"><a href="#cb13-733"></a></span>
<span id="cb13-734"><a href="#cb13-734"></a>$$</span>
<span id="cb13-735"><a href="#cb13-735"></a>\begin{aligned}</span>
<span id="cb13-736"><a href="#cb13-736"></a>&amp; \nabla_{\theta}- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t \right)} \right] <span class="sc">\\</span></span>
<span id="cb13-737"><a href="#cb13-737"></a>\to&amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] <span class="sc">\\</span></span>
<span id="cb13-738"><a href="#cb13-738"></a>= &amp; \nabla_{\theta} -  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{\prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{ \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right] <span class="sc">\\</span></span>
<span id="cb13-739"><a href="#cb13-739"></a>= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) }{ p(\mathbf{s}_1) \prod_{t=1}^{|\mathbf{\tau}|} \pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right) \prod_{t=1}^{|\mathbf{\tau}|-1} p(\mathbf{s}_{t+1} \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t) } \right] <span class="sc">\\</span></span>
<span id="cb13-740"><a href="#cb13-740"></a>= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  \log \frac{ p_\theta\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)}{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}  \right)} \right] <span class="sc">\\</span></span>
<span id="cb13-741"><a href="#cb13-741"></a>= &amp; \nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta} \left[ \log \frac{p_{\theta}\left(\mathbf{\tau}\right)}{p_{\text{ref}}\left(\mathbf{\tau}\right)} \right] <span class="sc">\\</span></span>
<span id="cb13-742"><a href="#cb13-742"></a>= &amp; \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb13-743"><a href="#cb13-743"></a>\end{aligned}</span>
<span id="cb13-744"><a href="#cb13-744"></a>$$ {#eq-kl-grad-est-dp}</span>
<span id="cb13-745"><a href="#cb13-745"></a></span>
<span id="cb13-746"><a href="#cb13-746"></a>可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。</span>
<span id="cb13-747"><a href="#cb13-747"></a></span>
<span id="cb13-748"><a href="#cb13-748"></a><span class="fu">### 不同 KL 估计量导出的 reward 项的作用</span></span>
<span id="cb13-749"><a href="#cb13-749"></a></span>
<span id="cb13-750"><a href="#cb13-750"></a>不难注意到，<span class="co">[</span><span class="ot">@sec-analogy-pg-kl</span><span class="co">]</span> 中的 KL 样本量对应于 $k_1$ 估计量。</span>
<span id="cb13-751"><a href="#cb13-751"></a></span>
<span id="cb13-752"><a href="#cb13-752"></a>一个自然的问题是，如果对动作条件似然使用 $k_2$ 或 $k_3$ 等其他估计量，会得到什么结果？</span>
<span id="cb13-753"><a href="#cb13-753"></a></span>
<span id="cb13-754"><a href="#cb13-754"></a>$k_2$ 或 $k_3$ 等其他估计量导致的一个问题，求和时通常无法得到联合概率。具体来说，其他估计量分别在优化</span>
<span id="cb13-755"><a href="#cb13-755"></a></span>
<span id="cb13-756"><a href="#cb13-756"></a><span class="ss">- </span>$k_2$: $- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} \frac{1}{2} \left( \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} \right)^{2} \right]$</span>
<span id="cb13-757"><a href="#cb13-757"></a><span class="ss">- </span>$k_3$: $- \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[ \sum_{t=1}^{|\mathbf{\tau}|} (\frac{\pi_{\text{ref}} \left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)} - 1 - \log \frac{\pi_{\text{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}{\pi_{\theta}\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t \right)}) \right]$</span>
<span id="cb13-758"><a href="#cb13-758"></a></span>
<span id="cb13-759"><a href="#cb13-759"></a>显然，这里的求和无法得到联合概率，也就无法实现类似 <span class="co">[</span><span class="ot">@eq-kl-grad-est-dp</span><span class="co">]</span> 中的效果了。</span>
<span id="cb13-760"><a href="#cb13-760"></a></span>
<span id="cb13-761"><a href="#cb13-761"></a><span class="fu">### 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</span></span>
<span id="cb13-762"><a href="#cb13-762"></a></span>
<span id="cb13-763"><a href="#cb13-763"></a>若对动作对数条件似然计算 KL 估计样本量，则由于涉及到求和，$k_1$ 之外的估计量通常没有良好定义。</span>
<span id="cb13-764"><a href="#cb13-764"></a></span>
<span id="cb13-765"><a href="#cb13-765"></a>但是若放弃对动作条件似然计算 KL 估计样本量，而是对求和之后的对数（条件）似然进行计算，则只需满足</span>
<span id="cb13-766"><a href="#cb13-766"></a></span>
<span id="cb13-767"><a href="#cb13-767"></a>$$</span>
<span id="cb13-768"><a href="#cb13-768"></a>\nabla_{\theta} - \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[  k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right) \right] </span>
<span id="cb13-769"><a href="#cb13-769"></a>\approx \nabla_{\theta} - \frac{1}{N} k\left(\frac{ p_{\text{ref}}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}{ p_{\theta}\left(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t, \mathbf{a}_t  \right)}\right)</span>
<span id="cb13-770"><a href="#cb13-770"></a>\approx \nabla_{\theta} - \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]</span>
<span id="cb13-771"><a href="#cb13-771"></a>$$ {#eq-kl-reward-grad-expect-general}</span>
<span id="cb13-772"><a href="#cb13-772"></a></span>
<span id="cb13-773"><a href="#cb13-773"></a>暂时不考虑 off-policy 问题，根据 <span class="co">[</span><span class="ot">@eq-kl-reward-grad-expect-general</span><span class="co">]</span>, GRPO 公式 (@eq-grpo-obj, @eq-grpo-obj-kl-term) 应当修正 KL 项如下：</span>
<span id="cb13-774"><a href="#cb13-774"></a></span>
<span id="cb13-775"><a href="#cb13-775"></a>$$</span>
<span id="cb13-776"><a href="#cb13-776"></a>\begin{aligned}</span>
<span id="cb13-777"><a href="#cb13-777"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb13-778"><a href="#cb13-778"></a>&amp; \frac{1}{G} \sum_{i=1}^G \left<span class="sc">\{</span> \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|} \min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span>  \right<span class="sc">\}</span>  -\beta k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)</span>
<span id="cb13-779"><a href="#cb13-779"></a>\end{aligned}</span>
<span id="cb13-780"><a href="#cb13-780"></a>$$ {#eq-grpo-obj-kl-fixed}</span>
<span id="cb13-781"><a href="#cb13-781"></a></span>
<span id="cb13-782"><a href="#cb13-782"></a><span class="fu"># 推导 on-policy 设置下 KL 散度的梯度估计 {#sec-derive-kld-grad}</span></span>
<span id="cb13-783"><a href="#cb13-783"></a></span>
<span id="cb13-784"><a href="#cb13-784"></a>前文中，我们分析了流行的 LLM RL 框架中对 KL 散度优化的实现，并得出了结论。另一种思路是直接推导出 KL 散度的梯度估计表达式，并据此实现代码。</span>
<span id="cb13-785"><a href="#cb13-785"></a></span>
<span id="cb13-786"><a href="#cb13-786"></a>由于我们使用的是梯度法，为了优化 KL 散度，我们需要准确估计的是 KL 散度的梯度而非其本身。类似地，在 PG 中，我们需要最大化 $\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]$，估计的是其梯度 $\nabla_{\theta} \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau})]=\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}[r(\mathbf{\tau}) \nabla_{\theta} \log p_{\theta}(\mathbf{\tau})]$而不是$r(\mathbf{\tau})$ 本身。</span>
<span id="cb13-787"><a href="#cb13-787"></a></span>
<span id="cb13-788"><a href="#cb13-788"></a>同时，如 <span class="co">[</span><span class="ot">@sec-kl-loss-impl</span><span class="co">]</span> 所述，先前向传播估计 KL 散度，再直接反向传播，通常是无法直接得到 KL 散度的梯度的。所以，我们需要直接估计 KL 散度的梯度。</span>
<span id="cb13-789"><a href="#cb13-789"></a></span>
<span id="cb13-790"><a href="#cb13-790"></a>首先，展开 KL 散度的表达式：</span>
<span id="cb13-791"><a href="#cb13-791"></a></span>
<span id="cb13-792"><a href="#cb13-792"></a>$$</span>
<span id="cb13-793"><a href="#cb13-793"></a>\begin{aligned}</span>
<span id="cb13-794"><a href="#cb13-794"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_t \mid  \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots,\mathbf{s}_t)}\right] <span class="sc">\\</span></span>
<span id="cb13-795"><a href="#cb13-795"></a>&amp; \propto \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid s_1, a_1, \cdots, s_t)}\right)</span>
<span id="cb13-796"><a href="#cb13-796"></a>\end{aligned}</span>
<span id="cb13-797"><a href="#cb13-797"></a>$$ {#eq-kl-expansion-sum}</span>
<span id="cb13-798"><a href="#cb13-798"></a></span>
<span id="cb13-799"><a href="#cb13-799"></a>再计算其梯度：</span>
<span id="cb13-800"><a href="#cb13-800"></a></span>
<span id="cb13-801"><a href="#cb13-801"></a>$$</span>
<span id="cb13-802"><a href="#cb13-802"></a>\begin{aligned}</span>
<span id="cb13-803"><a href="#cb13-803"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \propto \nabla_{\theta} \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\prod_{t=1}^{|\tau|-1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right)  <span class="sc">\\</span></span>
<span id="cb13-804"><a href="#cb13-804"></a>&amp; \cdot \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) <span class="sc">\\</span></span>
<span id="cb13-805"><a href="#cb13-805"></a>&amp; = \sum_{\tau \in \mathcal{T}} p(s_1) \left(\prod_{t=1}^{|\tau| - 1} p(s_{t+1} \mid  s_1, a_1, \cdots, s_t, a_t)\right) <span class="sc">\\</span></span>
<span id="cb13-806"><a href="#cb13-806"></a>&amp; \cdot \nabla_{\theta} \left(\left(\prod_{t=1}^{|\tau|} \pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t) \right) \left(\sum_{t=1}^{|\tau|} \log \frac{\pi_{\theta}(a_t \mid  s_1, a_1, \cdots, s_t)}{\pi_{\text{ref}}(a_t \mid  s_1, a_1, \cdots, s_t)}\right) \right)</span>
<span id="cb13-807"><a href="#cb13-807"></a>\end{aligned}</span>
<span id="cb13-808"><a href="#cb13-808"></a>$$ {#eq-kl-grad-expansion}</span>
<span id="cb13-809"><a href="#cb13-809"></a></span>
<span id="cb13-810"><a href="#cb13-810"></a><span class="co">[</span><span class="ot">@eq-kl-grad-expansion</span><span class="co">]</span> 中的梯度相当复杂，难以直接计算。接下来，我们将引入一系列合理的假设来简化它。</span>
<span id="cb13-811"><a href="#cb13-811"></a></span>
<span id="cb13-812"><a href="#cb13-812"></a><span class="fu">## 在已知环境中简化 KL 梯度估计</span></span>
<span id="cb13-813"><a href="#cb13-813"></a></span>
<span id="cb13-814"><a href="#cb13-814"></a>实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。</span>
<span id="cb13-815"><a href="#cb13-815"></a></span>
<span id="cb13-816"><a href="#cb13-816"></a>当状态转移概率分布已知时，$\forall t, p_{\theta}(a_1, \cdots, s_t, a_t \mid s_1)$ 都是可以计算的，则 KL 散度可以直接写成：</span>
<span id="cb13-817"><a href="#cb13-817"></a></span>
<span id="cb13-818"><a href="#cb13-818"></a>$$</span>
<span id="cb13-819"><a href="#cb13-819"></a>\begin{aligned}</span>
<span id="cb13-820"><a href="#cb13-820"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \sum_{\mathbf{\tau} \in \mathcal{T}} p(\mathbf{s}_1) p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1) \log \frac{p_{\theta}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}{p_{\text{ref}}(\mathbf{a}_1, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|} \mid \mathbf{s}_1)}  <span class="sc">\\</span></span>
<span id="cb13-821"><a href="#cb13-821"></a>\end{aligned}</span>
<span id="cb13-822"><a href="#cb13-822"></a>$$ {#eq-kl-grad-expansion-known-transition}</span>
<span id="cb13-823"><a href="#cb13-823"></a></span>
<span id="cb13-824"><a href="#cb13-824"></a><span class="fu">## 简写为 Contextual Bandit</span></span>
<span id="cb13-825"><a href="#cb13-825"></a></span>
<span id="cb13-826"><a href="#cb13-826"></a>为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 $\mathbf{s}_1 = \mathbf{x} \in \mathcal{P}, (\mathbf{a}_1, \cdots, \mathbf{s}_T, \mathbf{a}_T) = \mathbf{y} \in \mathcal{R}$，其中 $\mathcal{P}, \mathcal{R}$ 分别表示 prompt / response 空间，则 KL 散度变为：</span>
<span id="cb13-827"><a href="#cb13-827"></a></span>
<span id="cb13-828"><a href="#cb13-828"></a>$$</span>
<span id="cb13-829"><a href="#cb13-829"></a>\begin{aligned}</span>
<span id="cb13-830"><a href="#cb13-830"></a>\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}}\left[\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right] <span class="sc">\\</span></span>
<span id="cb13-831"><a href="#cb13-831"></a>&amp; = \sum_{(x, y) \in \mathcal{T}} p_{\theta}(x, y) \left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) <span class="sc">\\</span></span>
<span id="cb13-832"><a href="#cb13-832"></a>&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)</span>
<span id="cb13-833"><a href="#cb13-833"></a>\end{aligned}</span>
<span id="cb13-834"><a href="#cb13-834"></a>$$ {#eq-def-kl-cb}</span>
<span id="cb13-835"><a href="#cb13-835"></a></span>
<span id="cb13-836"><a href="#cb13-836"></a>其梯度变为：</span>
<span id="cb13-837"><a href="#cb13-837"></a></span>
<span id="cb13-838"><a href="#cb13-838"></a>$$</span>
<span id="cb13-839"><a href="#cb13-839"></a>\begin{aligned}</span>
<span id="cb13-840"><a href="#cb13-840"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; = \nabla_{\theta} \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) <span class="sc">\\</span></span>
<span id="cb13-841"><a href="#cb13-841"></a>&amp; = \sum_{(x, y) \in \mathcal{T}} p(s) \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right)</span>
<span id="cb13-842"><a href="#cb13-842"></a>\end{aligned}</span>
<span id="cb13-843"><a href="#cb13-843"></a>$$ {#eq-def-kl-grad-cb}</span>
<span id="cb13-844"><a href="#cb13-844"></a></span>
<span id="cb13-845"><a href="#cb13-845"></a>其中梯度项可以进一步展开为：</span>
<span id="cb13-846"><a href="#cb13-846"></a></span>
<span id="cb13-847"><a href="#cb13-847"></a>$$</span>
<span id="cb13-848"><a href="#cb13-848"></a>\begin{aligned}</span>
<span id="cb13-849"><a href="#cb13-849"></a>&amp; \nabla_{\theta} \left(\pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right)\right) <span class="sc">\\</span></span>
<span id="cb13-850"><a href="#cb13-850"></a>=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \nabla_{\theta} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) <span class="sc">\\</span></span>
<span id="cb13-851"><a href="#cb13-851"></a>=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \pi_{\theta}(y \mid x) \frac{1}{\pi_\theta(y \mid x)} \nabla_{\theta} \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb13-852"><a href="#cb13-852"></a>=&amp; \left(\nabla_{\theta} \pi_{\theta}(y \mid x)\right) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) + \nabla_{\theta} \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb13-853"><a href="#cb13-853"></a>=&amp; \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x)</span>
<span id="cb13-854"><a href="#cb13-854"></a>\end{aligned}</span>
<span id="cb13-855"><a href="#cb13-855"></a>$$ {#eq-def-kl-grad-cb-grad-term}</span>
<span id="cb13-856"><a href="#cb13-856"></a></span>
<span id="cb13-857"><a href="#cb13-857"></a>代入回 KL 梯度表达式：</span>
<span id="cb13-858"><a href="#cb13-858"></a></span>
<span id="cb13-859"><a href="#cb13-859"></a>$$</span>
<span id="cb13-860"><a href="#cb13-860"></a>\begin{aligned}</span>
<span id="cb13-861"><a href="#cb13-861"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb13-862"><a href="#cb13-862"></a>=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb13-863"><a href="#cb13-863"></a>=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)} \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) <span class="sc">\\</span></span>
<span id="cb13-864"><a href="#cb13-864"></a>=&amp; \sum_{(x, y) \in \mathcal{T}} p(s) \pi_{\theta}(y \mid x) \left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x) <span class="sc">\\</span></span>
<span id="cb13-865"><a href="#cb13-865"></a>=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + 1\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] <span class="sc">\\</span></span>
<span id="cb13-866"><a href="#cb13-866"></a>=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] + \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] <span class="sc">\\</span></span>
<span id="cb13-867"><a href="#cb13-867"></a>=&amp; \mathbb{E}_{(x, y) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right) \nabla_{\theta} \log \pi_{\theta}(y \mid x)\right]</span>
<span id="cb13-868"><a href="#cb13-868"></a>\end{aligned}</span>
<span id="cb13-869"><a href="#cb13-869"></a>$$ {#eq-def-kl-grad-cb-expect}</span>
<span id="cb13-870"><a href="#cb13-870"></a></span>
<span id="cb13-871"><a href="#cb13-871"></a>这里为了重新获得期望形式，引入了 $1 = \pi_{\theta}(y \mid x) / \pi_{\theta}(y \mid x)$，并利用了 $\nabla_{\theta} \log \pi_{\theta}(y \mid x) = \frac{\nabla_{\theta} \pi_{\theta}(y \mid x)}{\pi_{\theta}(y \mid x)}$ 和 $\mathbb{E}_{(x, y) \sim p_{\theta}} \left[\nabla_{\theta} \log \pi_{\theta}(y \mid x)\right] = 0$。</span>
<span id="cb13-872"><a href="#cb13-872"></a></span>
<span id="cb13-873"><a href="#cb13-873"></a>进行 Monte Carlo 估计：</span>
<span id="cb13-874"><a href="#cb13-874"></a></span>
<span id="cb13-875"><a href="#cb13-875"></a>$$</span>
<span id="cb13-876"><a href="#cb13-876"></a>\begin{aligned}</span>
<span id="cb13-877"><a href="#cb13-877"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N} \left(\log \frac{\pi_{\theta}(y_i \mid x_i)}{\pi_{\text{ref}}(y_i \mid x_i)}\right) \nabla_{\theta} \log \pi_{\theta}(y_i \mid x_i)</span>
<span id="cb13-878"><a href="#cb13-878"></a>\end{aligned}</span>
<span id="cb13-879"><a href="#cb13-879"></a>$$ {#eq-def-kl-grad-cb-mc}</span>
<span id="cb13-880"><a href="#cb13-880"></a></span>
<span id="cb13-881"><a href="#cb13-881"></a>其中 $(\mathbf{x}_i, \mathbf{y}_i) \sim p_{\theta}$。</span>
<span id="cb13-882"><a href="#cb13-882"></a></span>
<span id="cb13-883"><a href="#cb13-883"></a><span class="fu">## 还原为已知环境决策过程</span></span>
<span id="cb13-884"><a href="#cb13-884"></a></span>
<span id="cb13-885"><a href="#cb13-885"></a>将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：</span>
<span id="cb13-886"><a href="#cb13-886"></a></span>
<span id="cb13-887"><a href="#cb13-887"></a>$$</span>
<span id="cb13-888"><a href="#cb13-888"></a>\begin{aligned}</span>
<span id="cb13-889"><a href="#cb13-889"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]<span class="sc">\\</span></span>
<span id="cb13-890"><a href="#cb13-890"></a>=&amp; \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p_{\theta}} \left[\left(\log \frac{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})}\right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{y} \mid \mathbf{x})\right] <span class="sc">\\</span></span>
<span id="cb13-891"><a href="#cb13-891"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T}) \sim p_{\theta}} \left[\left(\sum_{t=1}^{T} \log \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_t)\right)\right]</span>
<span id="cb13-892"><a href="#cb13-892"></a>\end{aligned}</span>
<span id="cb13-893"><a href="#cb13-893"></a>$$ {#eq-def-kl-grad-kt-mc}</span>
<span id="cb13-894"><a href="#cb13-894"></a></span>
<span id="cb13-895"><a href="#cb13-895"></a>对应的 Monte Carlo 估计式为：</span>
<span id="cb13-896"><a href="#cb13-896"></a></span>
<span id="cb13-897"><a href="#cb13-897"></a>$$</span>
<span id="cb13-898"><a href="#cb13-898"></a>\begin{aligned}</span>
<span id="cb13-899"><a href="#cb13-899"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] &amp; \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T}\log \frac{\pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}{\pi_{\text{ref}}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})}\right) \left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{1, t}, \cdots, a_{i, t-1}, s_{i, t})\right)</span>
<span id="cb13-900"><a href="#cb13-900"></a>\end{aligned}</span>
<span id="cb13-901"><a href="#cb13-901"></a>$$ {#eq-def-kl-grad-kt-mc-loss}</span>
<span id="cb13-902"><a href="#cb13-902"></a></span>
<span id="cb13-903"><a href="#cb13-903"></a><span class="fu">## 利用因果性技巧化简 KL 梯度估计^[https://www.wikiwand.com/en/articles/Policy_gradient_method]</span></span>
<span id="cb13-904"><a href="#cb13-904"></a></span>
<span id="cb13-905"><a href="#cb13-905"></a>因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。</span>
<span id="cb13-906"><a href="#cb13-906"></a></span>
<span id="cb13-907"><a href="#cb13-907"></a>对于任何 $0 \leq t \leq |\tau|$，我们有</span>
<span id="cb13-908"><a href="#cb13-908"></a>$$</span>
<span id="cb13-909"><a href="#cb13-909"></a>\begin{aligned}</span>
<span id="cb13-910"><a href="#cb13-910"></a>&amp; \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) }\left[\nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] <span class="sc">\\</span></span>
<span id="cb13-911"><a href="#cb13-911"></a>=&amp; \sum_{a_t \in \mathcal{A}} \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \nabla_\theta \log \pi_\theta(a_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) <span class="sc">\\</span></span>
<span id="cb13-912"><a href="#cb13-912"></a>=&amp; \sum_{a_j \in \mathcal{A}} \pi_\theta(a_j \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_j) \cdot 0 <span class="sc">\\</span></span>
<span id="cb13-913"><a href="#cb13-913"></a>=&amp; 0</span>
<span id="cb13-914"><a href="#cb13-914"></a>\end{aligned}</span>
<span id="cb13-915"><a href="#cb13-915"></a>$$ {#eq-score-expect-zero}</span>
<span id="cb13-916"><a href="#cb13-916"></a></span>
<span id="cb13-917"><a href="#cb13-917"></a>更进一步，如果 $\mathbf{\Psi}_{t'}$ 是一个与 $\mathbf{a}_t, \mathbf{s}_{t+1}, \mathbf{a}_{t+1}, \ldots$ 独立的随机变量，那么</span>
<span id="cb13-918"><a href="#cb13-918"></a>$$</span>
<span id="cb13-919"><a href="#cb13-919"></a>\begin{aligned}</span>
<span id="cb13-920"><a href="#cb13-920"></a>&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] <span class="sc">\\</span></span>
<span id="cb13-921"><a href="#cb13-921"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{(\mathbf{a}_t, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]</span>
<span id="cb13-922"><a href="#cb13-922"></a> \right] <span class="sc">\\</span></span>
<span id="cb13-923"><a href="#cb13-923"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \mathbb{E}_{</span>
<span id="cb13-924"><a href="#cb13-924"></a>    (\mathbf{s}_{t+1}, \cdots, \mathbf{s}_{|\mathbf{\tau}|}, \mathbf{a}_{|\mathbf{\tau}|}) \sim p_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}, \mathbf{a}_{t})} \left[\mathbf{\Psi}_{t'} \right] \right]</span>
<span id="cb13-925"><a href="#cb13-925"></a> \right] <span class="sc">\\</span></span>
<span id="cb13-926"><a href="#cb13-926"></a>=&amp; \mathbb{E}_{(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t}) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t} )} \left[ \mathbf{\Psi}_{t'} \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \right]</span>
<span id="cb13-927"><a href="#cb13-927"></a> \right] <span class="sc">\\</span></span>
<span id="cb13-928"><a href="#cb13-928"></a>=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[</span>
<span id="cb13-929"><a href="#cb13-929"></a>            \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] </span>
<span id="cb13-930"><a href="#cb13-930"></a>        \right] <span class="sc">\\</span></span>
<span id="cb13-931"><a href="#cb13-931"></a>=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbb{E}_{\mathbf{a}_t \sim \pi_\theta(\cdot \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t)}\left[\mathbf{\Psi}_{t'} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] \right] <span class="sc">\\</span></span>
<span id="cb13-932"><a href="#cb13-932"></a>=&amp; \mathbb{E}_{(\mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t) \sim p_\theta} \left[ \mathbf{\Psi}_{t'} \cdot 0 \right] <span class="sc">\\</span></span>
<span id="cb13-933"><a href="#cb13-933"></a>=&amp; 0</span>
<span id="cb13-934"><a href="#cb13-934"></a>\end{aligned}</span>
<span id="cb13-935"><a href="#cb13-935"></a>$$ {#eq-score-indep-mul-expect-zero}</span>
<span id="cb13-936"><a href="#cb13-936"></a></span>
<span id="cb13-937"><a href="#cb13-937"></a>其中，为了利用 <span class="co">[</span><span class="ot">@eq-score-expect-zero</span><span class="co">]</span> 的结论，我们利用了全期望定律，即</span>
<span id="cb13-938"><a href="#cb13-938"></a></span>
<span id="cb13-939"><a href="#cb13-939"></a>$$</span>
<span id="cb13-940"><a href="#cb13-940"></a>\mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p} \left[\mathbf{x}\right] = \mathbb{E}_{\mathbf{y} \sim p} \left[\mathbb{E}_{\mathbf{x} \sim p(\cdot \mid \mathbf{y})} <span class="co">[</span><span class="ot">\mathbf{x}</span><span class="co">]</span> \right]</span>
<span id="cb13-941"><a href="#cb13-941"></a>$$ {#eq-law-of-total-expectation}</span>
<span id="cb13-942"><a href="#cb13-942"></a></span>
<span id="cb13-943"><a href="#cb13-943"></a>来引入我们想要的期望。</span>
<span id="cb13-944"><a href="#cb13-944"></a></span>
<span id="cb13-945"><a href="#cb13-945"></a>$$</span>
<span id="cb13-946"><a href="#cb13-946"></a>\begin{aligned}</span>
<span id="cb13-947"><a href="#cb13-947"></a>&amp; \mathbb{E}_{\tau \sim p_\theta}\left[\mathbf{\Psi}_i \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_1, \mathbf{a}_1, \cdots, \mathbf{s}_t\right) \right] <span class="sc">\\</span></span>
<span id="cb13-948"><a href="#cb13-948"></a>=&amp; \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) <span class="sc">\\</span></span>
<span id="cb13-949"><a href="#cb13-949"></a>=&amp; \sum_{\tau \in \mathcal{T}} p_\theta(s_1, a_1, \cdots, s_t) \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) p_\theta(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) <span class="sc">\\</span></span>
<span id="cb13-950"><a href="#cb13-950"></a>=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t)  \sum_{(a_{t}, s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})} \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \Psi_{t'} \nabla_\theta p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right)  <span class="sc">\\</span></span>
<span id="cb13-951"><a href="#cb13-951"></a>=&amp; \sum_{(s_{1}, a_{1}, \cdots, s_{t})} p_\theta(s_1, a_1, \cdots, s_t) \sum_{a_t \in \mathcal{A}}  \pi_\theta(a_t \mid s_1, a_1, \cdots, s_t) \nabla_\theta \log \pi_\theta\left(a_t \mid s_1, a_1, \cdots, s_t\right) \sum_{(s_{t+1}, \cdots, s_{|\tau|}, a_{|\tau|})}  p_\theta(s_{t+1}, \cdots, a_{|\tau|} \mid s_1, a_1, \cdots, s_t, a_t) \Psi_{t'} <span class="sc">\\</span></span>
<span id="cb13-952"><a href="#cb13-952"></a>\end{aligned}</span>
<span id="cb13-953"><a href="#cb13-953"></a>$$ {#eq-score-indep-mul-expect-zero}</span>
<span id="cb13-954"><a href="#cb13-954"></a></span>
<span id="cb13-955"><a href="#cb13-955"></a></span>
<span id="cb13-956"><a href="#cb13-956"></a>考虑 Monte Carlo 估计式 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-mc-loss</span><span class="co">]</span> 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：</span>
<span id="cb13-957"><a href="#cb13-957"></a></span>
<span id="cb13-958"><a href="#cb13-958"></a>$$</span>
<span id="cb13-959"><a href="#cb13-959"></a>\mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[</span>
<span id="cb13-960"><a href="#cb13-960"></a>\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})</span>
<span id="cb13-961"><a href="#cb13-961"></a>\right]</span>
<span id="cb13-962"><a href="#cb13-962"></a>$$ {#eq-def-kl-grad-kt-mc-loss-estimator-one-grad}</span>
<span id="cb13-963"><a href="#cb13-963"></a></span>
<span id="cb13-964"><a href="#cb13-964"></a></span>
<span id="cb13-965"><a href="#cb13-965"></a>由于序列决策过程满足因果性，即 $\forall t' &lt; t$，$\mathbf{s}_{t'}, \mathbf{a}_{t'}$ 独立于 $\mathbf{s}_{t}, \mathbf{a}_{t}$，则可令 $\mathbf{\Psi}_{t'} = \nabla_{\theta} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t'})}$，其独立于 $\mathbf{s}_{i, t}, \mathbf{a}_{i, t}, \ldots$，利用 <span class="co">[</span><span class="ot">@eq-score-indep-mul-expect-zero</span><span class="co">]</span> 的性质，则有</span>
<span id="cb13-966"><a href="#cb13-966"></a>$$</span>
<span id="cb13-967"><a href="#cb13-967"></a>\forall t' &lt; t, \mathbb{E}_{\mathbf{\tau_{i}} \sim p_{\theta}} \left[</span>
<span id="cb13-968"><a href="#cb13-968"></a>\log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'})} \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})</span>
<span id="cb13-969"><a href="#cb13-969"></a>\right] = 0</span>
<span id="cb13-970"><a href="#cb13-970"></a>$$ {#eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero}</span>
<span id="cb13-971"><a href="#cb13-971"></a></span>
<span id="cb13-972"><a href="#cb13-972"></a></span>
<span id="cb13-973"><a href="#cb13-973"></a>将 <span class="co">[</span><span class="ot">@eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero</span><span class="co">]</span> 代入 KL 梯度表达式 (@eq-def-kl-grad-kt-mc) ，即可简化得到：</span>
<span id="cb13-974"><a href="#cb13-974"></a></span>
<span id="cb13-975"><a href="#cb13-975"></a>$$</span>
<span id="cb13-976"><a href="#cb13-976"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{t}) \right]</span>
<span id="cb13-977"><a href="#cb13-977"></a>$$ {#eq-def-kl-grad-kt-reduce}</span>
<span id="cb13-978"><a href="#cb13-978"></a></span>
<span id="cb13-979"><a href="#cb13-979"></a>对应的 Monte Carlo 估计式为：</span>
<span id="cb13-980"><a href="#cb13-980"></a></span>
<span id="cb13-981"><a href="#cb13-981"></a>$$</span>
<span id="cb13-982"><a href="#cb13-982"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \left(\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \nabla_{\theta} \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})</span>
<span id="cb13-983"><a href="#cb13-983"></a>$$ {#eq-def-kl-grad-kt-reduce-mc}</span>
<span id="cb13-984"><a href="#cb13-984"></a></span>
<span id="cb13-985"><a href="#cb13-985"></a>同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：</span>
<span id="cb13-986"><a href="#cb13-986"></a></span>
<span id="cb13-987"><a href="#cb13-987"></a>$$</span>
<span id="cb13-988"><a href="#cb13-988"></a>\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_i|} \text{nograd}\left (\sum_{t'=t}^{|\tau_i|} \log \frac{\pi_{\theta}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})}{\pi_{\text{ref}}(a_{i, t'} \mid s_{i, 1}, \cdots, a_{i, t'-1}, s_{i, t'})} \right) \log \pi_{\theta}(a_{i, t} \mid s_{i, 1}, \cdots, a_{i, t-1}, s_{i, t})</span>
<span id="cb13-989"><a href="#cb13-989"></a>$$ {#eq-def-kl-grad-kt-reduce-mc-loss}</span>
<span id="cb13-990"><a href="#cb13-990"></a></span>
<span id="cb13-991"><a href="#cb13-991"></a>这里也可以看到，KL loss 项正确的实现要求：</span>
<span id="cb13-992"><a href="#cb13-992"></a></span>
<span id="cb13-993"><a href="#cb13-993"></a><span class="ss">1. </span>在序列内 token 间，对对数条件似然先求和，得到 KL 样本值，</span>
<span id="cb13-994"><a href="#cb13-994"></a><span class="ss">2. </span>再在序列间求均值。</span>
<span id="cb13-995"><a href="#cb13-995"></a></span>
<span id="cb13-996"><a href="#cb13-996"></a>因此 OpenRLHF (@eq-def-kl-loss-grad-estim-openrlhf) 与 verl (@eq-def-kl-loss-grad-estim-verl) 的权重都是错误的。</span>
<span id="cb13-997"><a href="#cb13-997"></a></span>
<span id="cb13-998"><a href="#cb13-998"></a><span class="fu">## KL 梯度优化可以实现为 KL 样本值 reward {#sec-kl-grad-as-kl-reward}</span></span>
<span id="cb13-999"><a href="#cb13-999"></a></span>
<span id="cb13-1000"><a href="#cb13-1000"></a>在 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce</span><span class="co">]</span> 中，令 $k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) = \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'-1}, \mathbf{s}_{t'})}$，则有：</span>
<span id="cb13-1001"><a href="#cb13-1001"></a>$$</span>
<span id="cb13-1002"><a href="#cb13-1002"></a>\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] =  \mathbb{E}_{\mathbf{\tau} \sim p_\theta}\left[\sum_{t=1}^{T} \left(\sum_{t'=t}^{T} k\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t'}, \mathbf{a}_{t'}\right) \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{t-1}, \mathbf{s}_{t}) \right]</span>
<span id="cb13-1003"><a href="#cb13-1003"></a>$$ {#eq-def-kl-grad-kt-reduce-k}</span>
<span id="cb13-1004"><a href="#cb13-1004"></a></span>
<span id="cb13-1005"><a href="#cb13-1005"></a>不难注意到 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-k</span><span class="co">]</span> 中 $k$ 与 <span class="co">[</span><span class="ot">@eq-pg-est-ret</span><span class="co">]</span> 中 reward $r$ 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。</span>
<span id="cb13-1006"><a href="#cb13-1006"></a></span>
<span id="cb13-1007"><a href="#cb13-1007"></a>类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS285^<span class="co">[</span><span class="ot">https://rail.eecs.berkeley.edu/deeprlcourse/</span><span class="co">]</span> 等材料。</span>
<span id="cb13-1008"><a href="#cb13-1008"></a></span>
<span id="cb13-1009"><a href="#cb13-1009"></a><span class="fu"># off-policy 设置下如何估计 KL 散度的梯度</span></span>
<span id="cb13-1010"><a href="#cb13-1010"></a></span>
<span id="cb13-1011"><a href="#cb13-1011"></a>上面的推导中，我们假设了 RL 是 on-policy 设置，即采样策略即为最新策略 $\pi_\theta$。</span>
<span id="cb13-1012"><a href="#cb13-1012"></a></span>
<span id="cb13-1013"><a href="#cb13-1013"></a>在这一节，我们进一步考虑 off-policy 设置，即一次采样获得样本会用于多次更新，除了第一次更新，采样策略 $\pi_{\theta_{\text{old}}}$ 与最新策略 $\pi_\theta$ 都会不同。off-policy 设置给 KL 散度优化带来的问题在于，我们需要优化最新策略 $\pi_\theta$ 的 KL 散度，但却没有来自 $p_{\theta}$ 的样本，这意味着我们无法直接使用梯度估计式 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-mc</span><span class="co">]</span>。</span>
<span id="cb13-1014"><a href="#cb13-1014"></a></span>
<span id="cb13-1015"><a href="#cb13-1015"></a><span class="fu">## 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</span></span>
<span id="cb13-1016"><a href="#cb13-1016"></a></span>
<span id="cb13-1017"><a href="#cb13-1017"></a>遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 $\pi_\theta \neq \pi_{\theta_{\text{old}}}$，尽管没有来自最新策略 $p_{\theta}$ 的样本，却仍然在使用基于 on-policy 设置的优化方式。</span>
<span id="cb13-1018"><a href="#cb13-1018"></a></span>
<span id="cb13-1019"><a href="#cb13-1019"></a><span class="fu">### TRL</span></span>
<span id="cb13-1020"><a href="#cb13-1020"></a></span>
<span id="cb13-1021"><a href="#cb13-1021"></a>TRL 在 <span class="co">[</span><span class="ot">@lst-trl-kl-reward</span><span class="co">]</span> 中计算 KL 样本值使用的 <span class="in">`logprobs`</span> 及其对应的轨迹样本均来自采样策略 $\pi_{\theta_{\text{old}}}$。对应代码可见 <span class="co">[</span><span class="ot">@lst-trl-sample-and-calc-old-logprob</span><span class="co">]</span>。</span>
<span id="cb13-1022"><a href="#cb13-1022"></a></span>
<span id="cb13-1023"><a href="#cb13-1023"></a><span class="in">```{#lst-trl-sample-and-calc-old-logprob .python lst-cap="TRL 使用采样样本并使用 $\pi_{\theta_{\text{old}}}$ 计算对数似然^[https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432]"}</span></span>
<span id="cb13-1024"><a href="#cb13-1024"></a><span class="in">queries = data["input_ids"].to(device)</span></span>
<span id="cb13-1025"><a href="#cb13-1025"></a><span class="in"># ...</span></span>
<span id="cb13-1026"><a href="#cb13-1026"></a></span>
<span id="cb13-1027"><a href="#cb13-1027"></a><span class="in">with unwrap_model_for_generation(</span></span>
<span id="cb13-1028"><a href="#cb13-1028"></a><span class="in">    self.model, #...</span></span>
<span id="cb13-1029"><a href="#cb13-1029"></a><span class="in">) as unwrapped_model:</span></span>
<span id="cb13-1030"><a href="#cb13-1030"></a><span class="in">    query_responses, logitss = batch_generation(</span></span>
<span id="cb13-1031"><a href="#cb13-1031"></a><span class="in">        unwrapped_model.policy,</span></span>
<span id="cb13-1032"><a href="#cb13-1032"></a><span class="in">        queries,</span></span>
<span id="cb13-1033"><a href="#cb13-1033"></a><span class="in">        # ...</span></span>
<span id="cb13-1034"><a href="#cb13-1034"></a><span class="in">    )</span></span>
<span id="cb13-1035"><a href="#cb13-1035"></a></span>
<span id="cb13-1036"><a href="#cb13-1036"></a></span>
<span id="cb13-1037"><a href="#cb13-1037"></a><span class="in">for i in range(0, queries.shape[0], args.local_rollout_forward_batch_size):</span></span>
<span id="cb13-1038"><a href="#cb13-1038"></a><span class="in">    # ...</span></span>
<span id="cb13-1039"><a href="#cb13-1039"></a><span class="in">    logits = logitss[i : i + args.local_rollout_forward_batch_size]</span></span>
<span id="cb13-1040"><a href="#cb13-1040"></a><span class="in">    logprob = selective_log_softmax(logits, response)</span></span>
<span id="cb13-1041"><a href="#cb13-1041"></a><span class="in">```</span></span>
<span id="cb13-1042"><a href="#cb13-1042"></a></span>
<span id="cb13-1043"><a href="#cb13-1043"></a>注意，基于 $\mathbf{\tau} \sim \pi_{\theta_{\text{old}}}$ 计算的 KL 样本值可以用于估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_{\theta_{\text{old}}} \mid \pi_{\text{ref}}\right]$，在第一次更新时，由于 $\pi_\theta = \pi_{\theta_{\text{old}}}$，所以也可以用于估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]$。但问题在于，从第二次更新开始，$\pi_\theta \neq \pi_{\theta_{\text{old}}}$，而我们仍然希望估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]$。</span>
<span id="cb13-1044"><a href="#cb13-1044"></a></span>
<span id="cb13-1045"><a href="#cb13-1045"></a>随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 $\pi_{\theta}$ 重新估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \mid \pi_{\text{ref}}\right]$。对应代码可见 <span class="co">[</span><span class="ot">@lst-trl-ppo-update</span><span class="co">]</span>。</span>
<span id="cb13-1046"><a href="#cb13-1046"></a></span>
<span id="cb13-1047"><a href="#cb13-1047"></a><span class="in">```{#lst-trl-ppo-update .python lst-cap="TRL PPO 多轮更新"}</span></span>
<span id="cb13-1048"><a href="#cb13-1048"></a><span class="in"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span>
<span id="cb13-1049"><a href="#cb13-1049"></a><span class="in">for ppo_epoch_idx in range(args.num_ppo_epochs):</span></span>
<span id="cb13-1050"><a href="#cb13-1050"></a><span class="in">    b_inds = np.random.permutation(args.local_batch_size)</span></span>
<span id="cb13-1051"><a href="#cb13-1051"></a><span class="in">    minibatch_idx = 0</span></span>
<span id="cb13-1052"><a href="#cb13-1052"></a><span class="in">    for mini_batch_start in range(0, args.local_batch_size, args.local_mini_batch_size):</span></span>
<span id="cb13-1053"><a href="#cb13-1053"></a><span class="in">        mini_batch_end = mini_batch_start + args.local_mini_batch_size</span></span>
<span id="cb13-1054"><a href="#cb13-1054"></a><span class="in">        mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]</span></span>
<span id="cb13-1055"><a href="#cb13-1055"></a><span class="in">        gradient_accumulation_idx = 0</span></span>
<span id="cb13-1056"><a href="#cb13-1056"></a><span class="in">        for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):</span></span>
<span id="cb13-1057"><a href="#cb13-1057"></a><span class="in">            with accelerator.accumulate(model):</span></span>
<span id="cb13-1058"><a href="#cb13-1058"></a><span class="in">                micro_batch_end = micro_batch_start + args.per_device_train_batch_size</span></span>
<span id="cb13-1059"><a href="#cb13-1059"></a><span class="in">                micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]</span></span>
<span id="cb13-1060"><a href="#cb13-1060"></a><span class="in">                mb_advantage = advantages[micro_batch_inds]</span></span>
<span id="cb13-1061"><a href="#cb13-1061"></a><span class="in">                mb_responses = responses[micro_batch_inds]</span></span>
<span id="cb13-1062"><a href="#cb13-1062"></a><span class="in">                mb_query_responses = query_responses[micro_batch_inds]</span></span>
<span id="cb13-1063"><a href="#cb13-1063"></a><span class="in">                mb_logprobs = logprobs[micro_batch_inds]</span></span>
<span id="cb13-1064"><a href="#cb13-1064"></a><span class="in">                mb_return = returns[micro_batch_inds]</span></span>
<span id="cb13-1065"><a href="#cb13-1065"></a><span class="in">                mb_values = values[micro_batch_inds]</span></span>
<span id="cb13-1066"><a href="#cb13-1066"></a></span>
<span id="cb13-1067"><a href="#cb13-1067"></a></span>
<span id="cb13-1068"><a href="#cb13-1068"></a><span class="in">                output, vpred_temp = forward(model, mb_query_responses, processing_class.pad_token_id)</span></span>
<span id="cb13-1069"><a href="#cb13-1069"></a><span class="in">                logits = output.logits[:, context_length - 1 : -1]</span></span>
<span id="cb13-1070"><a href="#cb13-1070"></a><span class="in">                logits /= args.temperature + 1e-7</span></span>
<span id="cb13-1071"><a href="#cb13-1071"></a><span class="in">                new_logprobs = selective_log_softmax(logits, mb_responses)</span></span>
<span id="cb13-1072"><a href="#cb13-1072"></a><span class="in">                new_logprobs = torch.masked_fill(</span></span>
<span id="cb13-1073"><a href="#cb13-1073"></a><span class="in">                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB</span></span>
<span id="cb13-1074"><a href="#cb13-1074"></a><span class="in">                )</span></span>
<span id="cb13-1075"><a href="#cb13-1075"></a><span class="in">                vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)</span></span>
<span id="cb13-1076"><a href="#cb13-1076"></a><span class="in">                vpred = torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], 0)</span></span>
<span id="cb13-1077"><a href="#cb13-1077"></a><span class="in">                vpredclipped = torch.clamp(</span></span>
<span id="cb13-1078"><a href="#cb13-1078"></a><span class="in">                    vpred,</span></span>
<span id="cb13-1079"><a href="#cb13-1079"></a><span class="in">                    mb_values - args.cliprange_value,</span></span>
<span id="cb13-1080"><a href="#cb13-1080"></a><span class="in">                    mb_values + args.cliprange_value,</span></span>
<span id="cb13-1081"><a href="#cb13-1081"></a><span class="in">                )</span></span>
<span id="cb13-1082"><a href="#cb13-1082"></a><span class="in">                vf_losses1 = torch.square(vpred - mb_return)</span></span>
<span id="cb13-1083"><a href="#cb13-1083"></a><span class="in">                vf_losses2 = torch.square(vpredclipped - mb_return)</span></span>
<span id="cb13-1084"><a href="#cb13-1084"></a><span class="in">                vf_loss_max = torch.max(vf_losses1, vf_losses2)</span></span>
<span id="cb13-1085"><a href="#cb13-1085"></a><span class="in">                vf_loss = 0.5 * masked_mean(vf_loss_max, ~padding_mask_p1[micro_batch_inds])</span></span>
<span id="cb13-1086"><a href="#cb13-1086"></a><span class="in">                vf_clipfrac = masked_mean(</span></span>
<span id="cb13-1087"><a href="#cb13-1087"></a><span class="in">                    (vf_losses2 &gt; vf_losses1).float(), ~padding_mask_p1[micro_batch_inds]</span></span>
<span id="cb13-1088"><a href="#cb13-1088"></a><span class="in">                )</span></span>
<span id="cb13-1089"><a href="#cb13-1089"></a><span class="in">                logprobs_diff = new_logprobs - mb_logprobs</span></span>
<span id="cb13-1090"><a href="#cb13-1090"></a><span class="in">                ratio = torch.exp(logprobs_diff)</span></span>
<span id="cb13-1091"><a href="#cb13-1091"></a><span class="in">                pg_losses = -mb_advantage * ratio</span></span>
<span id="cb13-1092"><a href="#cb13-1092"></a><span class="in">                pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)</span></span>
<span id="cb13-1093"><a href="#cb13-1093"></a><span class="in">                pg_loss_max = torch.max(pg_losses, pg_losses2)</span></span>
<span id="cb13-1094"><a href="#cb13-1094"></a><span class="in">                pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])</span></span>
<span id="cb13-1095"><a href="#cb13-1095"></a><span class="in">                loss = pg_loss + args.vf_coef * vf_loss</span></span>
<span id="cb13-1096"><a href="#cb13-1096"></a><span class="in">                accelerator.backward(loss)</span></span>
<span id="cb13-1097"><a href="#cb13-1097"></a><span class="in">                optimizer.step()</span></span>
<span id="cb13-1098"><a href="#cb13-1098"></a><span class="in">                optimizer.zero_grad()</span></span>
<span id="cb13-1099"><a href="#cb13-1099"></a><span class="in">```</span></span>
<span id="cb13-1100"><a href="#cb13-1100"></a></span>
<span id="cb13-1101"><a href="#cb13-1101"></a><span class="fu">### OpenRLHF</span></span>
<span id="cb13-1102"><a href="#cb13-1102"></a></span>
<span id="cb13-1103"><a href="#cb13-1103"></a>类似地，OpenRLHF 在 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-estimator</span><span class="co">]</span> 中计算 KL 样本值使用的 <span class="in">`log_probs`</span> 在 <span class="in">`make_experience`</span> 时被计算，和对应的样本 <span class="in">`sequences`</span> 都来自采样策略 $\pi_{\theta_{\text{old}}}$，而非当前策略 $\pi_{\theta}$。对应代码可见 <span class="co">[</span><span class="ot">@lst-openrlhf-sample-and-calc-old-logprob</span><span class="co">]</span>。</span>
<span id="cb13-1104"><a href="#cb13-1104"></a></span>
<span id="cb13-1105"><a href="#cb13-1105"></a><span class="in">```{#lst-openrlhf-sample-and-calc-old-logprob .python lst-cap="OpenRLHF 采样样本并使用 $\pi_{\theta_{\text{old}}}$ 计算对数似然"}</span></span>
<span id="cb13-1106"><a href="#cb13-1106"></a><span class="in"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595</span></span>
<span id="cb13-1107"><a href="#cb13-1107"></a><span class="in">def make_experience(self, samples: Samples) -&gt; Experience:</span></span>
<span id="cb13-1108"><a href="#cb13-1108"></a><span class="in">    """</span></span>
<span id="cb13-1109"><a href="#cb13-1109"></a><span class="in">    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.</span></span>
<span id="cb13-1110"><a href="#cb13-1110"></a><span class="in">    """</span></span>
<span id="cb13-1111"><a href="#cb13-1111"></a><span class="in">    # ...</span></span>
<span id="cb13-1112"><a href="#cb13-1112"></a><span class="in">    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680</span></span>
<span id="cb13-1113"><a href="#cb13-1113"></a><span class="in">    action_log_probs = self.actor(</span></span>
<span id="cb13-1114"><a href="#cb13-1114"></a><span class="in">        sequences,</span></span>
<span id="cb13-1115"><a href="#cb13-1115"></a><span class="in">        num_actions,</span></span>
<span id="cb13-1116"><a href="#cb13-1116"></a><span class="in">        # ...</span></span>
<span id="cb13-1117"><a href="#cb13-1117"></a><span class="in">    )</span></span>
<span id="cb13-1118"><a href="#cb13-1118"></a><span class="in">    # ...</span></span>
<span id="cb13-1119"><a href="#cb13-1119"></a><span class="in">    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709</span></span>
<span id="cb13-1120"><a href="#cb13-1120"></a><span class="in">    kl = compute_approx_kl(</span></span>
<span id="cb13-1121"><a href="#cb13-1121"></a><span class="in">        action_log_probs,</span></span>
<span id="cb13-1122"><a href="#cb13-1122"></a><span class="in">        base_action_log_probs,</span></span>
<span id="cb13-1123"><a href="#cb13-1123"></a><span class="in">        # ...</span></span>
<span id="cb13-1124"><a href="#cb13-1124"></a><span class="in">    )</span></span>
<span id="cb13-1125"><a href="#cb13-1125"></a><span class="in">```</span></span>
<span id="cb13-1126"><a href="#cb13-1126"></a></span>
<span id="cb13-1127"><a href="#cb13-1127"></a>从 <span class="co">[</span><span class="ot">@lst-openrlhf-calc-kl-loss</span><span class="co">]</span> 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 $\pi_{\theta_{\text{old}}}$ 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 $\pi_{\theta}$ 计算的对数似然，但如 <span class="co">[</span><span class="ot">@sec-kl-loss-impl</span><span class="co">]</span> 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。</span>
<span id="cb13-1128"><a href="#cb13-1128"></a></span>
<span id="cb13-1129"><a href="#cb13-1129"></a></span>
<span id="cb13-1130"><a href="#cb13-1130"></a><span class="fu">### verl</span></span>
<span id="cb13-1131"><a href="#cb13-1131"></a></span>
<span id="cb13-1132"><a href="#cb13-1132"></a>从 <span class="co">[</span><span class="ot">@lst-verl-kl-reward</span><span class="co">]</span> 可见，verl 同样使用 $\pi_{\theta_{\text{old}}}$ 计算 KL 样本值。</span>
<span id="cb13-1133"><a href="#cb13-1133"></a></span>
<span id="cb13-1134"><a href="#cb13-1134"></a>从 <span class="co">[</span><span class="ot">@lst-verl-kl-loss</span><span class="co">]</span> 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 $\pi_{\theta_{\text{old}}}$ 的 KL 样本值。</span>
<span id="cb13-1135"><a href="#cb13-1135"></a></span>
<span id="cb13-1136"><a href="#cb13-1136"></a><span class="fu">## 利用重要性采样处理 off-policy 设置</span></span>
<span id="cb13-1137"><a href="#cb13-1137"></a></span>
<span id="cb13-1138"><a href="#cb13-1138"></a>off-policy 设置下，我们没有来自最新策略 $\pi_{\theta}$ 的样本，而只能使用来自采样策略 $\pi_{\theta_{\text{old}}}$ 的样本，但我们仍然希望估计 $\nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right]$。</span>
<span id="cb13-1139"><a href="#cb13-1139"></a></span>
<span id="cb13-1140"><a href="#cb13-1140"></a>熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即</span>
<span id="cb13-1141"><a href="#cb13-1141"></a></span>
<span id="cb13-1142"><a href="#cb13-1142"></a>$$</span>
<span id="cb13-1143"><a href="#cb13-1143"></a>\mathbb{E}_{\mathbf{\tau} \sim p_{\theta}} \left[f(\mathbf{\tau})\right] = \sum_{\tau \in \mathcal{T}} p_{\theta}(\tau) f(\tau)  = \sum_{\tau \in \mathcal{T}} p_{\theta_{\text{old}}}(\tau) \frac{p_{\theta}(\tau)}{p_{\theta_{\text{old}}}(\tau)} f(\tau) = \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}} \left[\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} f(\mathbf{\tau})\right]</span>
<span id="cb13-1144"><a href="#cb13-1144"></a>$$ {#eq-is-off-policy-kl}</span>
<span id="cb13-1145"><a href="#cb13-1145"></a></span>
<span id="cb13-1146"><a href="#cb13-1146"></a>此处，重要性采样系数 $\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})}$ 可以仿照 <span class="co">[</span><span class="ot">@eq-dp-expansion</span><span class="co">]</span> 展开为：</span>
<span id="cb13-1147"><a href="#cb13-1147"></a></span>
<span id="cb13-1148"><a href="#cb13-1148"></a>$$</span>
<span id="cb13-1149"><a href="#cb13-1149"></a>\frac{p_{\theta}(\mathbf{\tau})}{p_{\theta_{\text{old}}}(\mathbf{\tau})} = \prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{\pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}</span>
<span id="cb13-1150"><a href="#cb13-1150"></a>$$ {#eq-is-coef-expansion}</span>
<span id="cb13-1151"><a href="#cb13-1151"></a>^<span class="co">[</span><span class="ot">实际计算中，[@eq-is-coef-expansion] 由于涉及到 $|\mathbf{\tau}|$ 次连乘，方差大且数值稳定性差，需要利用因果性、近似等技术来化简。本文目前省略该部分，后续将会更新相关内容。</span><span class="co">]</span></span>
<span id="cb13-1152"><a href="#cb13-1152"></a></span>
<span id="cb13-1153"><a href="#cb13-1153"></a>利用重要性采样 (@eq-is-off-policy-kl, @eq-is-coef-expansion) ，KL 梯度表达式 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce</span><span class="co">]</span> 可以转化为：</span>
<span id="cb13-1154"><a href="#cb13-1154"></a></span>
<span id="cb13-1155"><a href="#cb13-1155"></a>$$</span>
<span id="cb13-1156"><a href="#cb13-1156"></a>\begin{aligned}</span>
<span id="cb13-1157"><a href="#cb13-1157"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}} \left[\pi_\theta \mid \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb13-1158"><a href="#cb13-1158"></a>=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta}}\left[\sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right] <span class="sc">\\</span></span>
<span id="cb13-1159"><a href="#cb13-1159"></a>=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \frac{p_{\theta}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}{p_{\theta_{\text{old}}}(\mathbf{s}_{1}, \mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T})}  \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})  \right] <span class="sc">\\</span></span>
<span id="cb13-1160"><a href="#cb13-1160"></a>=&amp;  \mathbb{E}_{\mathbf{\tau} \sim p_{\theta_{\text{old}}}}\left[ \left(\prod_{t=1}^{|\mathbf{\tau}|} \frac{\pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t})}\right) \sum_{t=1}^{|\mathbf{\tau}|} \left(\sum_{t'=t}^{|\mathbf{\tau}|} \log \frac{\pi_{\theta}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})}{\pi_{\text{ref}}(\mathbf{a}_{t'} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t'-1}, \mathbf{s}_{t'})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{t} \mid \mathbf{s}_{1}, \cdots, \mathbf{a}_{t-1}, \mathbf{s}_{t}) \right]</span>
<span id="cb13-1161"><a href="#cb13-1161"></a>\end{aligned}</span>
<span id="cb13-1162"><a href="#cb13-1162"></a>$$ {#eq-def-kl-grad-kt-reduce-is}</span>
<span id="cb13-1163"><a href="#cb13-1163"></a></span>
<span id="cb13-1164"><a href="#cb13-1164"></a></span>
<span id="cb13-1165"><a href="#cb13-1165"></a>对应的 Monte Carlo 估计式为：</span>
<span id="cb13-1166"><a href="#cb13-1166"></a></span>
<span id="cb13-1167"><a href="#cb13-1167"></a>$$</span>
<span id="cb13-1168"><a href="#cb13-1168"></a>\begin{aligned}</span>
<span id="cb13-1169"><a href="#cb13-1169"></a>&amp; \nabla_{\theta} \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] <span class="sc">\\</span></span>
<span id="cb13-1170"><a href="#cb13-1170"></a>\approx&amp; \frac{1}{N} \sum_{i=1}^{N} \left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t'-1}, \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) <span class="sc">\\</span></span>
<span id="cb13-1171"><a href="#cb13-1171"></a>=&amp; \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\mathbf{\tau}_{i}|} \left(\left(\prod_{t=1}^{|\mathbf{\tau}_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\mathbf{\tau}_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1}) }{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})</span>
<span id="cb13-1172"><a href="#cb13-1172"></a>\end{aligned}</span>
<span id="cb13-1173"><a href="#cb13-1173"></a>$$ {#eq-def-kl-grad-kt-reduce-is-mc}</span>
<span id="cb13-1174"><a href="#cb13-1174"></a></span>
<span id="cb13-1175"><a href="#cb13-1175"></a>对应的 loss 函数为：</span>
<span id="cb13-1176"><a href="#cb13-1176"></a></span>
<span id="cb13-1177"><a href="#cb13-1177"></a>$$</span>
<span id="cb13-1178"><a href="#cb13-1178"></a>\mathcal{L}^{KL}_{\theta} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{|\tau_{i}|} \text{nograd}\left(\left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right)\sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})} \right) \log \pi_{\theta}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})</span>
<span id="cb13-1179"><a href="#cb13-1179"></a>$$ {#eq-def-kl-grad-kt-reduce-is-mc-loss}</span>
<span id="cb13-1180"><a href="#cb13-1180"></a></span>
<span id="cb13-1181"><a href="#cb13-1181"></a></span>
<span id="cb13-1182"><a href="#cb13-1182"></a>类似 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-k</span><span class="co">]</span>，我们可以令 </span>
<span id="cb13-1183"><a href="#cb13-1183"></a></span>
<span id="cb13-1184"><a href="#cb13-1184"></a>$$</span>
<span id="cb13-1185"><a href="#cb13-1185"></a>k(\mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t}) = \left(\prod_{t=1}^{|\tau_{i}|}\frac{\pi_{\theta}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}{ \pi_{\theta_{\text{old}}}(\mathbf{a}_{i, t} | \mathbf{s}_{i, 1}, \cdots, \mathbf{a}_{i, t-1}, \mathbf{s}_{i, t})}\right) \sum_{t'=t}^{|\tau_{i}|} \log \frac{\pi_{\theta}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}{\pi_{\text{ref}}(\mathbf{a}_{i, t'} | \mathbf{s}_{i, t'}, \cdots, \mathbf{a}_{i, t-1})}</span>
<span id="cb13-1186"><a href="#cb13-1186"></a>$$ {#eq-def-kl-reward-is}</span>
<span id="cb13-1187"><a href="#cb13-1187"></a></span>
<span id="cb13-1188"><a href="#cb13-1188"></a>注意，<span class="co">[</span><span class="ot">@eq-def-kl-reward-is</span><span class="co">]</span> 中的 $k$ 需要对于每个新的 $\pi_{\theta}$ 重新计算。</span>
<span id="cb13-1189"><a href="#cb13-1189"></a></span>
<span id="cb13-1190"><a href="#cb13-1190"></a><span class="fu"># 结论：如何正确地在 RL 中优化 KL 散度</span></span>
<span id="cb13-1191"><a href="#cb13-1191"></a></span>
<span id="cb13-1192"><a href="#cb13-1192"></a><span class="fu">## 修正 GRPO 公式中的 KL 项</span></span>
<span id="cb13-1193"><a href="#cb13-1193"></a></span>
<span id="cb13-1194"><a href="#cb13-1194"></a>GRPO 公式 (@eq-grpo-obj, @eq-grpo-obj-kl-term) 对于 KL 优化主要存在两个错误：</span>
<span id="cb13-1195"><a href="#cb13-1195"></a></span>
<span id="cb13-1196"><a href="#cb13-1196"></a><span class="ss">1. </span>忽略了 KL 优化的 off-policy 问题</span>
<span id="cb13-1197"><a href="#cb13-1197"></a><span class="ss">2. </span>先将 $k_{3}$ 估计样本量应用于动作条件似然再求和，导致得到异常的梯度</span>
<span id="cb13-1198"><a href="#cb13-1198"></a></span>
<span id="cb13-1199"><a href="#cb13-1199"></a>对于这两个问题，在 <span class="co">[</span><span class="ot">@eq-grpo-obj-kl-fixed</span><span class="co">]</span> 的基础上，仿照 <span class="co">[</span><span class="ot">@eq-def-kl-reward-is</span><span class="co">]</span>，我们可以按如下方式修正：</span>
<span id="cb13-1200"><a href="#cb13-1200"></a></span>
<span id="cb13-1201"><a href="#cb13-1201"></a>$$</span>
<span id="cb13-1202"><a href="#cb13-1202"></a>\begin{aligned}</span>
<span id="cb13-1203"><a href="#cb13-1203"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb13-1204"><a href="#cb13-1204"></a>&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \left<span class="sc">\{</span> \sum_{t=1}^{\left|o_i\right|} \min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old}}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span> \right<span class="sc">\}</span> -\beta \left(\prod_{t=1}^{|o_{i}|}\frac{\pi_{\theta}(o_{i, t} | q, o_{i,\lt t})}{ \pi_{\theta_{\text{old}}}(o_{i, t} | q, o_{i,\lt t})}\right) k\left( \frac{\prod_{t=1}^{|o_i|} \pi_{\text{ref}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\prod_{t=1}^{|o_i|} \pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \right)</span>
<span id="cb13-1205"><a href="#cb13-1205"></a>\end{aligned}</span>
<span id="cb13-1206"><a href="#cb13-1206"></a>$$ {#eq-grpo-obj-kl-fixed-is}</span>
<span id="cb13-1207"><a href="#cb13-1207"></a></span>
<span id="cb13-1208"><a href="#cb13-1208"></a><span class="fu">## 修正流行 LLM RL 框架中的 KL 优化实现</span></span>
<span id="cb13-1209"><a href="#cb13-1209"></a></span>
<span id="cb13-1210"><a href="#cb13-1210"></a>目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：</span>
<span id="cb13-1211"><a href="#cb13-1211"></a></span>
<span id="cb13-1212"><a href="#cb13-1212"></a><span class="ss">1. </span>实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）</span>
<span id="cb13-1213"><a href="#cb13-1213"></a><span class="ss">2. </span>错误地实现了平均操作</span>
<span id="cb13-1214"><a href="#cb13-1214"></a></span>
<span id="cb13-1215"><a href="#cb13-1215"></a>对于这些问题，可以按照如下思路修正：</span>
<span id="cb13-1216"><a href="#cb13-1216"></a></span>
<span id="cb13-1217"><a href="#cb13-1217"></a><span class="ss">1. </span>为 KL 项添加重要性采样，这需要从第二轮更新开始，每次基于新的 $\pi_\theta$ 重新计算 KL loss / reward 项，包括重要性采样系数</span>
<span id="cb13-1218"><a href="#cb13-1218"></a><span class="ss">2. </span>应用 KL 估计样本量时，先对于序列内 token 间的对数条件似然求和，得到轨迹联合概率，再代入公式</span>
<span id="cb13-1219"><a href="#cb13-1219"></a><span class="ss">3. </span>如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 <span class="co">[</span><span class="ot">@eq-def-kl-reward-is</span><span class="co">]</span> 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）</span>
<span id="cb13-1220"><a href="#cb13-1220"></a><span class="ss">4. </span>如果不希望应用 reward 优化的其他技术，可以按 <span class="co">[</span><span class="ot">@eq-def-kl-grad-kt-reduce-is-mc-loss</span><span class="co">]</span> 实现为 KL loss 项</span>
<span id="cb13-1221"><a href="#cb13-1221"></a></span>
<span id="cb13-1222"><a href="#cb13-1222"></a><span class="fu"># 讨论</span></span>
<span id="cb13-1223"><a href="#cb13-1223"></a></span>
<span id="cb13-1224"><a href="#cb13-1224"></a><span class="fu">## 对于 KL 梯度更好的估计样本量</span></span>
<span id="cb13-1225"><a href="#cb13-1225"></a></span>
<span id="cb13-1226"><a href="#cb13-1226"></a>如 <span class="co">[</span><span class="ot">@sec-kl-grad-as-kl-reward</span><span class="co">]</span> 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？</span>
<span id="cb13-1227"><a href="#cb13-1227"></a></span>
<span id="cb13-1228"><a href="#cb13-1228"></a>此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？</span>
<span id="cb13-1229"><a href="#cb13-1229"></a></span>
<span id="cb13-1230"><a href="#cb13-1230"></a><span class="fu">## KL-Regularized RL 的理论优势</span></span>
<span id="cb13-1231"><a href="#cb13-1231"></a></span>
<span id="cb13-1232"><a href="#cb13-1232"></a>最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。</span>
<span id="cb13-1233"><a href="#cb13-1233"></a></span>
<span id="cb13-1234"><a href="#cb13-1234"></a>然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 @zhao2025logregretkl 证明了 KL-regularized RL 的 regret 只有 $\mathcal{O}(\log T)$，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 $\mathcal{O}(\sqrt{T})$。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。</span>
<span id="cb13-1235"><a href="#cb13-1235"></a></span>
<span id="cb13-1236"><a href="#cb13-1236"></a><span class="fu"># 附录 {.appendix}</span></span>
<span id="cb13-1237"><a href="#cb13-1237"></a></span>
<span id="cb13-1238"><a href="#cb13-1238"></a>::: {.callout-tip}</span>
<span id="cb13-1239"><a href="#cb13-1239"></a></span>
<span id="cb13-1240"><a href="#cb13-1240"></a>本文的作者（童雨轩）仍在寻求北美的 Ph.D. 或 RA 机会。如果你觉得本文对你有帮助，欢迎浏览其主页^<span class="co">[</span><span class="ot">https://tongyx361.github.io</span><span class="co">]</span>来获取进一步了解。</span>
<span id="cb13-1241"><a href="#cb13-1241"></a></span>
<span id="cb13-1242"><a href="#cb13-1242"></a>:::</span>
<span id="cb13-1243"><a href="#cb13-1243"></a></span>
<span id="cb13-1244"><a href="#cb13-1244"></a><span class="fu">## 相关工作 {.appendix}</span></span>
<span id="cb13-1245"><a href="#cb13-1245"></a></span>
<span id="cb13-1246"><a href="#cb13-1246"></a>与本文同期也有许多精彩的讨论，由于笔者还没能通读全文，此处仅提供链接，不作概括，欢迎感兴趣的读者自行阅读：</span>
<span id="cb13-1247"><a href="#cb13-1247"></a></span>
<span id="cb13-1248"><a href="#cb13-1248"></a><span class="ss">- </span><span class="co">[</span><span class="ot">GRPO 中的 KL Loss 实现细节问题 - Hongyu Zang @ 知乎</span><span class="co">](https://zhuanlan.zhihu.com/p/28440962040)</span></span>
<span id="cb13-1249"><a href="#cb13-1249"></a><span class="ss">- </span><span class="co">[</span><span class="ot">k2 loss就是比k3 loss好！以及GRPO_off-policy - Yiming Liu @ 知乎</span><span class="co">](https://zhuanlan.zhihu.com/p/28735759256)</span></span>
<span id="cb13-1250"><a href="#cb13-1250"></a></span>
<span id="cb13-1251"><a href="#cb13-1251"></a><span class="fu">## 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？” {.appendix}</span></span>
<span id="cb13-1252"><a href="#cb13-1252"></a></span>
<span id="cb13-1253"><a href="#cb13-1253"></a>笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题^<span class="co">[</span><span class="ot">https://x.com/pufanyi/status/1888845956684370202</span><span class="co">]</span>：</span>
<span id="cb13-1254"><a href="#cb13-1254"></a></span>
<span id="cb13-1255"><a href="#cb13-1255"></a><span class="at">&gt; A small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new </span><span class="pp">||</span><span class="at"> old), while TRPO and PPO use KL(old </span><span class="pp">||</span><span class="at"> new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?</span></span>
<span id="cb13-1256"><a href="#cb13-1256"></a><span class="at">&gt;</span></span>
<span id="cb13-1257"><a href="#cb13-1257"></a><span class="at">&gt; TRPO </span><span class="co">[</span><span class="ot">@schulman2015trpo</span><span class="co">]</span></span>
<span id="cb13-1258"><a href="#cb13-1258"></a></span>
<span id="cb13-1259"><a href="#cb13-1259"></a>$$</span>
<span id="cb13-1260"><a href="#cb13-1260"></a>\begin{aligned}</span>
<span id="cb13-1261"><a href="#cb13-1261"></a>&amp; \underset{\theta}{\text{maximize}}~L_{\theta_{\text {old }}}(\theta) <span class="sc">\\</span></span>
<span id="cb13-1262"><a href="#cb13-1262"></a>&amp; \text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta</span>
<span id="cb13-1263"><a href="#cb13-1263"></a>\end{aligned}</span>
<span id="cb13-1264"><a href="#cb13-1264"></a>$$ {#eq-trpo}</span>
<span id="cb13-1265"><a href="#cb13-1265"></a></span>
<span id="cb13-1266"><a href="#cb13-1266"></a><span class="at">&gt; PPO </span><span class="co">[</span><span class="ot">@schulman2017ppo</span><span class="co">]</span></span>
<span id="cb13-1267"><a href="#cb13-1267"></a></span>
<span id="cb13-1268"><a href="#cb13-1268"></a>$$ </span>
<span id="cb13-1269"><a href="#cb13-1269"></a>L^{K L P E N}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(\mathbf{y}_t \mid \mathbf{x}_t\right)}{\pi_{\theta_{\text {old }}}\left(\mathbf{y}_t \mid \mathbf{x}_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid \mathbf{x}_t\right), \pi_\theta\left(\cdot \mid \mathbf{x}_t\right)\right]\right]</span>
<span id="cb13-1270"><a href="#cb13-1270"></a>$$ {#eq-ppo-klpen}</span>
<span id="cb13-1271"><a href="#cb13-1271"></a></span>
<span id="cb13-1272"><a href="#cb13-1272"></a><span class="at">&gt; GRPO </span><span class="co">[</span><span class="ot">@shao2024deepseekmath</span><span class="co">]</span></span>
<span id="cb13-1273"><a href="#cb13-1273"></a></span>
<span id="cb13-1274"><a href="#cb13-1274"></a>$$</span>
<span id="cb13-1275"><a href="#cb13-1275"></a>\begin{aligned}</span>
<span id="cb13-1276"><a href="#cb13-1276"></a>&amp; \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] <span class="sc">\\</span></span>
<span id="cb13-1277"><a href="#cb13-1277"></a>&amp; \frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\left<span class="sc">\{</span>\min \left<span class="co">[</span><span class="ot">\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{o l d}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)} \hat{A}_{i, t}, \text{clip}\left(\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,\lt t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,\lt t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right</span><span class="co">]</span>-\beta \mathbb{D}_{K L}\left[\pi_\theta \mid \pi_{\text{ref}}\right]\right<span class="sc">\}</span></span>
<span id="cb13-1278"><a href="#cb13-1278"></a>\end{aligned}</span>
<span id="cb13-1279"><a href="#cb13-1279"></a>$$ {#eq-grpo}</span>
<span id="cb13-1280"><a href="#cb13-1280"></a></span>
<span id="cb13-1281"><a href="#cb13-1281"></a>这个问题本身的答案是非常简单的。</span>
<span id="cb13-1282"><a href="#cb13-1282"></a></span>
<span id="cb13-1283"><a href="#cb13-1283"></a>首先，这个问题混淆了两种不同的 KL 惩罚项：</span>
<span id="cb13-1284"><a href="#cb13-1284"></a></span>
<span id="cb13-1285"><a href="#cb13-1285"></a><span class="ss">1. </span>$\text{KL}<span class="co">[</span><span class="ot">\pi_{\theta_{\text{old}}},\pi_{\theta}</span><span class="co">]</span>$，其作用是约束最新策略 $\pi_{\theta}$不要离采样策略$\pi_{\theta_{\text{old}}}$ 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。</span>
<span id="cb13-1286"><a href="#cb13-1286"></a><span class="ss">2. </span>$\text{KL}<span class="co">[</span><span class="ot">\pi_{\theta},\pi_{\theta_{\text{ref}}}</span><span class="co">]</span>$，其作用是约束最新策略 $\pi_{\theta}$不要离参考策略$\pi_{\theta_{\text{ref}}}$ 太远，从而更充分地利用参考策略中的先验。</span>
<span id="cb13-1287"><a href="#cb13-1287"></a></span>
<span id="cb13-1288"><a href="#cb13-1288"></a>另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 $\text{KL}<span class="co">[</span><span class="ot">\pi_{\theta_{\text{old}}},\pi_{\theta}</span><span class="co">]</span>$。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：</span>
<span id="cb13-1289"><a href="#cb13-1289"></a></span>
<span id="cb13-1290"><a href="#cb13-1290"></a><span class="at">&gt; Let $r_t(\theta)$ denote the probability ratio $r_{t}(\theta)=\frac{\pi_{\theta}\left(a_t \mid s_t\right)}{\left(\pi_{\theta_{\text {old }}}\left|a_t\right| s_t\right)}$, so $r\left(\theta_{\text{old}}\right)=1$. TRPO maximizes a "surrogate" objective</span></span>
<span id="cb13-1291"><a href="#cb13-1291"></a></span>
<span id="cb13-1292"><a href="#cb13-1292"></a>$$</span>
<span id="cb13-1293"><a href="#cb13-1293"></a>L^{\text{CPI}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t\right]=\hat{\mathbb{E}}_t\left<span class="co">[</span><span class="ot">r_t(\theta) \hat{A}_t\right</span><span class="co">]</span> .</span>
<span id="cb13-1294"><a href="#cb13-1294"></a>$$</span>
<span id="cb13-1295"><a href="#cb13-1295"></a></span>
<span id="cb13-1296"><a href="#cb13-1296"></a><span class="at">&gt; ...</span></span>
<span id="cb13-1297"><a href="#cb13-1297"></a><span class="at">&gt;</span></span>
<span id="cb13-1298"><a href="#cb13-1298"></a><span class="at">&gt; The main objective we propose is the following:</span></span>
<span id="cb13-1299"><a href="#cb13-1299"></a></span>
<span id="cb13-1300"><a href="#cb13-1300"></a>$$</span>
<span id="cb13-1301"><a href="#cb13-1301"></a>L^{\text{CLIP}}(\theta)=\hat{\mathbb{E}}_t\left<span class="co">[</span><span class="ot">\min \left(r_t(\theta) \hat{A}_t, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right</span><span class="co">]</span></span>
<span id="cb13-1302"><a href="#cb13-1302"></a>$$</span>
<span id="cb13-1303"><a href="#cb13-1303"></a></span>
<span id="cb13-1304"><a href="#cb13-1304"></a><span class="at">&gt; where epsilon is a hyperparameter, say, $\epsilon=0.2$. The motivation for this objective is as follows. The first term inside the $\min$ is $L^{\text{CPI}}$. The second term, $\text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t$, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving $r_t$ outside of the interval $</span><span class="co">[</span><span class="ot">1-\epsilon, 1+\epsilon</span><span class="co">]</span><span class="at">$.</span></span>
<span id="cb13-1305"><a href="#cb13-1305"></a><span class="at">&gt;</span></span>
<span id="cb13-1306"><a href="#cb13-1306"></a><span class="at">&gt; ...</span></span>
<span id="cb13-1307"><a href="#cb13-1307"></a><span class="at">&gt;</span></span>
<span id="cb13-1308"><a href="#cb13-1308"></a><span class="at">&gt; **Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence**, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence $d_{\text{targ}}$ each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we've included it here because it's an important baseline.</span></span>
<span id="cb13-1309"><a href="#cb13-1309"></a><span class="at">&gt;</span></span>
<span id="cb13-1310"><a href="#cb13-1310"></a><span class="at">&gt; In the simplest instantiation of this algorithm, we perform the following steps in each policy update:</span></span>
<span id="cb13-1311"><a href="#cb13-1311"></a><span class="at">&gt;</span></span>
<span id="cb13-1312"><a href="#cb13-1312"></a><span class="at">&gt; - Using several epochs of minibatch SGD, optimize the KL-penalized objective</span></span>
<span id="cb13-1313"><a href="#cb13-1313"></a></span>
<span id="cb13-1314"><a href="#cb13-1314"></a>$$</span>
<span id="cb13-1315"><a href="#cb13-1315"></a>L^{\text{KLPEN}}(\theta)=\hat{\mathbb{E}}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} \hat{A}_t-\beta \mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_t\right), \pi_\theta\left(\cdot \mid s_t\right)\right]\right]</span>
<span id="cb13-1316"><a href="#cb13-1316"></a>$$</span>
<span id="cb13-1317"><a href="#cb13-1317"></a></span>
<span id="cb13-1318"><a href="#cb13-1318"></a><span class="at">&gt; </span></span>
<span id="cb13-1319"><a href="#cb13-1319"></a></span>
<span id="cb13-1320"><a href="#cb13-1320"></a>顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 $r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}$就是$K L\left<span class="co">[</span><span class="ot">\pi_{\theta_{d d}}, \pi_\theta\right</span><span class="co">]</span>=\mathbb{E}_{a_t \sim \pi_{\theta_{d t}}\left(\cdot \mid s_t\right)}\left[\log \frac{\pi_{\theta_{d t}}\left(a_t \mid s_t\right)}{\pi_\theta\left(a_t \mid s_t\right)}\right]$ 中对单个样本 $(s_t, a_t)$ 的值中 $\log$ 的真数。</span>
<span id="cb13-1321"><a href="#cb13-1321"></a></span>
<span id="cb13-1322"><a href="#cb13-1322"></a><span class="fu">## 致谢 {.appendix}</span></span>
<span id="cb13-1323"><a href="#cb13-1323"></a></span>
<span id="cb13-1324"><a href="#cb13-1324"></a>感谢王浩然、YuMS 对本文提供的重要反馈。</span>
<span id="cb13-1325"><a href="#cb13-1325"></a></span>
<span id="cb13-1326"><a href="#cb13-1326"></a>感谢生广明、Wei Xiong、刘仁彪、刘威、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论以及对于本文的有益反馈。</span>
<span id="cb13-1327"><a href="#cb13-1327"></a></span>
<span id="cb13-1328"><a href="#cb13-1328"></a>感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。</span>
<span id="cb13-1329"><a href="#cb13-1329"></a></span>
<span id="cb13-1330"><a href="#cb13-1330"></a><span class="fu">## 引用 {.appendix}</span></span>
<span id="cb13-1331"><a href="#cb13-1331"></a></span>
<span id="cb13-1332"><a href="#cb13-1332"></a>BibTeX:</span>
<span id="cb13-1333"><a href="#cb13-1333"></a></span>
<span id="cb13-1334"><a href="#cb13-1334"></a><span class="in">```BibTeX</span></span>
<span id="cb13-1335"><a href="#cb13-1335"></a><span class="in">@online{tong2025kl,</span></span>
<span id="cb13-1336"><a href="#cb13-1336"></a><span class="in">  author = {童雨轩},</span></span>
<span id="cb13-1337"><a href="#cb13-1337"></a><span class="in">  title = {重新思考 {RL} 中的 {KL} 梯度优化},</span></span>
<span id="cb13-1338"><a href="#cb13-1338"></a><span class="in">  year = {2025},</span></span>
<span id="cb13-1339"><a href="#cb13-1339"></a><span class="in">  url = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},</span></span>
<span id="cb13-1340"><a href="#cb13-1340"></a><span class="in">  urldate = {2025-03-09},</span></span>
<span id="cb13-1341"><a href="#cb13-1341"></a><span class="in">  language = {Chinese},</span></span>
<span id="cb13-1342"><a href="#cb13-1342"></a><span class="in">}</span></span>
<span id="cb13-1343"><a href="#cb13-1343"></a><span class="in">```</span></span>
<span id="cb13-1344"><a href="#cb13-1344"></a></span>
<span id="cb13-1345"><a href="#cb13-1345"></a>文本：</span>
<span id="cb13-1346"><a href="#cb13-1346"></a></span>
<span id="cb13-1347"><a href="#cb13-1347"></a><span class="in">```text</span></span>
<span id="cb13-1348"><a href="#cb13-1348"></a><span class="in">童雨轩. 2025. “重新思考 RL 中的 KL 梯度优化.” https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh.</span></span>
<span id="cb13-1349"><a href="#cb13-1349"></a><span class="in">```</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>