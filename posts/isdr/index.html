<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxuan Tong">
<meta name="author" content="Yingru Li">
<meta name="author" content="Guangming Sheng">
<meta name="keywords" content="Importance Sampling, Off-Policy Reinforcement Learning, Large Language Model">

<title>[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems – Shawn/Yuxuan Tong 童雨轩</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-e85eba1097e763ab92a8026b31d9210d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-d4dbfae94d399dbf1014af73f007268d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-e85eba1097e763ab92a8026b31d9210d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shawn/Yuxuan Tong 童雨轩</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://tongyx361.github.io"> <i class="bi bi-person" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/tongyx361"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tongyx361"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">English 英文</div>
                <div class="quarto-category">Technical 技术</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxuan Tong <a href="mailto:tongyuxuan361@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
               <p>Yingru Li </p>
               <p>Guangming Sheng </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">2026/01/12</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Abstract</div>
      <p>In the pursuit of maximizing hardware utilization for Large Language Model (LLM) Reinforcement Learning, systems introduces some new sampling strategies like Partial Rollout and Asynchornous Rollout. This introduces complex off-policy data structures, which we categorize into <strong>Sliding Latest Policy Trajectories (SLAPTs)</strong> and <strong>Multiple Consistent Stale Policy Trajectories (MCSPTs)</strong>. This post discusses the properties of applying <strong>Importance Sampling (IS)</strong> to these distinct data forms, combining theoretical notations like Multiple Importance Sampling and Rényi divergence and practical observations like the (approximate) indentical distribution structure of SLAPTs of similar lengths.</p>
    </div>
  </div>

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Importance Sampling, Off-Policy Reinforcement Learning, Large Language Model</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#off-policy-data-in-fully-utilized-llm-rl-systems" id="toc-off-policy-data-in-fully-utilized-llm-rl-systems" class="nav-link active" data-scroll-target="#off-policy-data-in-fully-utilized-llm-rl-systems"><span class="header-section-number">1</span> Off-Policy Data in Fully-Utilized LLM RL Systems</a>
  <ul class="collapse">
  <li><a href="#sliding-latest-policy-trajectories-slapts" id="toc-sliding-latest-policy-trajectories-slapts" class="nav-link" data-scroll-target="#sliding-latest-policy-trajectories-slapts"><span class="header-section-number">1.1</span> Sliding Latest Policy Trajectories (SLAPTs)</a></li>
  <li><a href="#multiple-consistent-stale-policy-trajectories-mcspts" id="toc-multiple-consistent-stale-policy-trajectories-mcspts" class="nav-link" data-scroll-target="#multiple-consistent-stale-policy-trajectories-mcspts"><span class="header-section-number">1.2</span> Multiple Consistent Stale Policy Trajectories (MCSPTs)</a></li>
  </ul></li>
  <li><a href="#sec-is" id="toc-sec-is" class="nav-link" data-scroll-target="#sec-is"><span class="header-section-number">2</span> Importance Sampling with Off-Policy Data in (Fully-Utilized) LLM RL Systems</a>
  <ul class="collapse">
  <li><a href="#sec-intractable-global-behav-policy" id="toc-sec-intractable-global-behav-policy" class="nav-link" data-scroll-target="#sec-intractable-global-behav-policy"><span class="header-section-number">2.1</span> Simple but Intractable: Global Behavior Policy <span class="math inline">\(\mu^{*}(\cdot \mid \boldsymbol{s})\)</span></a></li>
  <li><a href="#sec-is-mcspt" id="toc-sec-is-mcspt" class="nav-link" data-scroll-target="#sec-is-mcspt"><span class="header-section-number">2.2</span> IS with MCSPTs</a>
  <ul class="collapse">
  <li><a href="#mixture-importance-sampling" id="toc-mixture-importance-sampling" class="nav-link" data-scroll-target="#mixture-importance-sampling"><span class="header-section-number">2.2.1</span> Mixture Importance Sampling</a></li>
  <li><a href="#multiple-importance-sampling" id="toc-multiple-importance-sampling" class="nav-link" data-scroll-target="#multiple-importance-sampling"><span class="header-section-number">2.2.2</span> Multiple Importance Sampling</a></li>
  <li><a href="#balance-heuristic" id="toc-balance-heuristic" class="nav-link" data-scroll-target="#balance-heuristic"><span class="header-section-number">2.2.3</span> Balance Heuristic</a></li>
  </ul></li>
  <li><a href="#is-with-slapts" id="toc-is-with-slapts" class="nav-link" data-scroll-target="#is-with-slapts"><span class="header-section-number">2.3</span> IS with SLAPTs</a>
  <ul class="collapse">
  <li><a href="#trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols" id="toc-trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols" class="nav-link" data-scroll-target="#trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols"><span class="header-section-number">2.3.1</span> Trajectory-dependent Behavior Policy <span class="math inline">\(\mu_{i}(\cdot \mid \boldsymbol{s})\)</span></a></li>
  <li><a href="#variance-of-single-sample-estimate-mcspt-vs.-slapt" id="toc-variance-of-single-sample-estimate-mcspt-vs.-slapt" class="nav-link" data-scroll-target="#variance-of-single-sample-estimate-mcspt-vs.-slapt"><span class="header-section-number">2.3.2</span> Variance of Single-Sample Estimate – MCSPT vs.&nbsp;SLAPT</a></li>
  <li><a href="#minimizing-variance-by-optimizing-the-weighting" id="toc-minimizing-variance-by-optimizing-the-weighting" class="nav-link" data-scroll-target="#minimizing-variance-by-optimizing-the-weighting"><span class="header-section-number">2.3.3</span> Minimizing Variance by Optimizing the Weighting</a></li>
  </ul></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#off-policy-data-in-fully-utilized-llm-rl-systems" id="toc-off-policy-data-in-fully-utilized-llm-rl-systems"><span class="header-section-number">1</span> Off-Policy Data in Fully-Utilized LLM RL Systems</a>
  <ul>
  <li><a href="#sliding-latest-policy-trajectories-slapts" id="toc-sliding-latest-policy-trajectories-slapts"><span class="header-section-number">1.1</span> Sliding Latest Policy Trajectories (SLAPTs)</a></li>
  <li><a href="#multiple-consistent-stale-policy-trajectories-mcspts" id="toc-multiple-consistent-stale-policy-trajectories-mcspts"><span class="header-section-number">1.2</span> Multiple Consistent Stale Policy Trajectories (MCSPTs)</a></li>
  </ul></li>
  <li><a href="#sec-is" id="toc-sec-is"><span class="header-section-number">2</span> Importance Sampling with Off-Policy Data in (Fully-Utilized) LLM RL Systems</a>
  <ul>
  <li><a href="#sec-intractable-global-behav-policy" id="toc-sec-intractable-global-behav-policy"><span class="header-section-number">2.1</span> Simple but Intractable: Global Behavior Policy <span class="math inline">\(\mu^{*}(\cdot \mid \boldsymbol{s})\)</span></a></li>
  <li><a href="#sec-is-mcspt" id="toc-sec-is-mcspt"><span class="header-section-number">2.2</span> IS with MCSPTs</a>
  <ul>
  <li><a href="#mixture-importance-sampling" id="toc-mixture-importance-sampling"><span class="header-section-number">2.2.1</span> Mixture Importance Sampling</a></li>
  <li><a href="#multiple-importance-sampling" id="toc-multiple-importance-sampling"><span class="header-section-number">2.2.2</span> Multiple Importance Sampling</a></li>
  <li><a href="#balance-heuristic" id="toc-balance-heuristic"><span class="header-section-number">2.2.3</span> Balance Heuristic</a></li>
  </ul></li>
  <li><a href="#is-with-slapts" id="toc-is-with-slapts"><span class="header-section-number">2.3</span> IS with SLAPTs</a>
  <ul>
  <li><a href="#trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols" id="toc-trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols"><span class="header-section-number">2.3.1</span> Trajectory-dependent Behavior Policy <span class="math inline">\(\mu_{i}(\cdot \mid \boldsymbol{s})\)</span></a></li>
  <li><a href="#variance-of-single-sample-estimate-mcspt-vs.-slapt" id="toc-variance-of-single-sample-estimate-mcspt-vs.-slapt"><span class="header-section-number">2.3.2</span> Variance of Single-Sample Estimate – MCSPT vs.&nbsp;SLAPT</a></li>
  <li><a href="#minimizing-variance-by-optimizing-the-weighting" id="toc-minimizing-variance-by-optimizing-the-weighting"><span class="header-section-number">2.3.3</span> Minimizing Variance by Optimizing the Weighting</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#citation" id="toc-citation">Citation</a></li>
  </ul>
</nav>
<section id="off-policy-data-in-fully-utilized-llm-rl-systems" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Off-Policy Data in Fully-Utilized LLM RL Systems</h1>
<p>In Reinforcement Learning (RL) systems for Large Language Models (LLMs), especially those that pursue full utilization of the hardware resources, it is common that we have to utilize <strong>off-policy</strong> data, typically a batch of trajectory samples collected from one or multiple stale policies.</p>
<p>So far, there have been two main kinds of off-policy data emerging in such systems, which in this post we refer to as:</p>
<ol type="1">
<li>Sliding Latest Policy Trajectories (SLAPTs)</li>
<li>Multiple Consistent Stale Policy Trajectories (MCSPTs)</li>
</ol>
<section id="sliding-latest-policy-trajectories-slapts" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sliding-latest-policy-trajectories-slapts"><span class="header-section-number">1.1</span> Sliding Latest Policy Trajectories (SLAPTs)</h2>
<p>We use <em>Sliding Latest Policy Trajectories</em> to refer to such batches of trajectories that are sampled in an RL system where we always use the latest policy <span class="math inline">\(\pi_{\theta_{t}}\)</span> at each moment to sample the actions.</p>
<p>Formally, a SLAPT can be defined as a trajectory <span class="math inline">\(\tau = (s_{0}, a_{0,\pi_{\theta_{n}}}, \ldots, s_{t}, a_{t,\pi_{\theta_{n+m}}}, \ldots, s_{T}, a_{T,\pi_{\theta_{n+M}}})\)</span>, where <span class="math inline">\(s_{t}\)</span> is the state at timestep <span class="math inline">\(t\)</span> and <span class="math inline">\(a_{t,\pi_{\theta_{n+m}}}\)</span> is the action sampled from the policy <span class="math inline">\(\pi_{\theta_{n+m}}\)</span> that is the latest policy available in the system when sampling. So for <span class="math inline">\(l &gt; m\)</span>, <span class="math inline">\(\pi_{\theta_{n+l}}\)</span> is usually more on-policy than <span class="math inline">\(\pi_{\theta_{n+m}}\)</span>.</p>
<p>SLAPTs in a batch usually have different policy compositions, depending on the system dynamics.</p>
<p>SLAPTs are a natural result of a popular design choice called <em>Partial Rollout</em> in LLM RL systems nowadays, such as Kimi k1.5 <span class="citation" data-cites="kimi2025k15">(<a href="#ref-kimi2025k15" role="doc-biblioref">Kimi Team et al. 2025</a>)</span>, AReaL <span class="citation" data-cites="fu2025areal">(<a href="#ref-fu2025areal" role="doc-biblioref">Fu et al. 2025</a>)</span> and PipelineRL <span class="citation" data-cites="piché2025pipelinerl">(<a href="#ref-piché2025pipelinerl" role="doc-biblioref">Piché et al. 2025</a>)</span>, etc., which can date back to a traditional distributed RL system called SEED RL <span class="citation" data-cites="espeholt2020seedrl">(<a href="#ref-espeholt2020seedrl" role="doc-biblioref">Espeholt et al. 2020</a>)</span>:</p>
<p>The ongoing trajectory rollouts are</p>
<ol type="1">
<li>aborted when
<ul>
<li>either, in synchoronous systems like Kimi k1.5, there are enough samples collected for training, releasing the resources for the training engine to update the model weights,</li>
<li>or, in asynchronous systems like AReaL and PipelineRL, a new version of model weights is produced by the trainer;</li>
</ul></li>
<li>continued with the latest version of model weights.</li>
</ol>
<p>Partial Rollout is motivated by its efficiency and simplicity. Let me explain in detail.</p>
<p>The earliest LLM RL systems <span class="citation" data-cites="yao2023dschat">Hu et al. (<a href="#ref-hu2025openrlhf" role="doc-biblioref">2025</a>)</span> all adopt synchoronous architectures, where the trainer always waits for all the trajectories are finished before updating the weights with these data. However, as the context length of LLMs scales up, the skewness of the trajectory length distribution becomes increasing heavy. If the distribution is very skewed but all the trajectories are required to finish in the same rollout stage, there can be only a few long-tail requests remaining in the system, causing severe under-utilization (typically &lt;30% in practice).</p>
<p>Kimi k1.5 proposes to fix this issue by aborting all the ongoing rollouts once enough training samples are collected and directly update the weights with these data, instead of waiting for all the trajectories to finish, and then continue the rollouts with the new model weights.</p>
<p>In asynchornous RL systems with an experience buffer, it is troublesome to mamage multiple versions of model weights within the rollout engine.</p>
<p>AReaL proposes to always only maintain the latest model weights across all the instances. Once a new version of model weights is produced by the trainer, all the rollouts of the stale policy are aborted and then continued with the latest policy.</p>
<p>This can be simply implemented by always loading the latest weights into all the inference engine instances, avoiding the bothering to manage the requests across each instance.</p>
<p>Despite Partial Rollout’s efficiency and simplicity, there have been worries about mixing multiple policies within a single trajectory, since most previous works formulate IS with a single consistent behavior policy <span class="math inline">\(\mu(\cdot \mid \boldsymbol{s})\)</span> while such formulation is rather under-explored. We defer a more detailed discussion about this to <a href="#sec-is" class="quarto-xref">Section&nbsp;2</a>.</p>
</section>
<section id="multiple-consistent-stale-policy-trajectories-mcspts" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="multiple-consistent-stale-policy-trajectories-mcspts"><span class="header-section-number">1.2</span> Multiple Consistent Stale Policy Trajectories (MCSPTs)</h2>
<p>We use <em>Multiple Consistent Stale Policy Trajectories</em> (MCSPTs) to refer to such batches of trajectories that</p>
<ul>
<li>each of them is sampled with a consistent stale policy <span class="math inline">\(\pi_{\theta_{n}}\)</span>,</li>
<li>while there might be different <span class="math inline">\(n\)</span> for different trajectories.</li>
</ul>
<p>The consistency of policy within a trajectory makes the formulation simpler, which is widely used in traditional distributed RL systems like IMPALA <span class="citation" data-cites="espeholt2018impala">(<a href="#ref-espeholt2018impala" role="doc-biblioref">Espeholt et al. 2018</a>)</span>.</p>
<p>In contrast, MCSPT sampling is more difficult to implement efficiently against long-tail bubbles in LLM RL systems, because this requires managing multiple versions of model weights at the same time within the rollout engine and dynamically transferring the requests of various lengths.</p>
<p><span class="citation" data-cites="sheng2025laminar">Sheng, Tong, et al. (<a href="#ref-sheng2025laminar" role="doc-biblioref">2025</a>)</span> implement an LLM RL system that samples MCSPTs with full utilization of the hardware resources.</p>
</section>
</section>
<section id="sec-is" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Importance Sampling with Off-Policy Data in (Fully-Utilized) LLM RL Systems</h1>
<p><a href="https://en.wikipedia.org/wiki/Importance_sampling">Importance Sampling (IS)</a> has been widely used in LLM RL systems.</p>
<p>It is useful for estimating the expection of some random function, typically the gradient of the return respect to the policy parameters, on the target policy distribution. In practice, with a batch of trajectory samples, the IS estimate is often formulated as:</p>
<p><span id="eq-is-batch"><span class="math display">\[
\begin{aligned}
\hat{\mathbb{E}}_{\mu}(f(\boldsymbol{\tau})) =&amp; \frac{1}{N} \sum_{i=1}^{N} \frac{p_{\theta}(\boldsymbol{\tau}_i)}{q_{\mu}(\boldsymbol{\tau}_i)}f(\boldsymbol{\tau}_i) \\
=&amp; \frac{1}{N} \sum_{i=1}^{N} \frac{p(\boldsymbol{s}_{0}) \prod_{t=0}^{T-1} \pi_{\theta}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})p(\boldsymbol{s}_{t+1} \mid \boldsymbol{s}_{t}, \boldsymbol{a}_{t})}{p(\boldsymbol{s}_{0}) \prod_{t=0}^{T-1} \mu(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})p(\boldsymbol{s}_{t+1} \mid \boldsymbol{s}_{t}, \boldsymbol{a}_{t})}f(\boldsymbol{\tau}_i) \\
=&amp; \frac{1}{N} \sum_{i=1}^{N} \prod_{t=0}^{T-1} \frac{\pi_{\theta}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}{\mu(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}f(\boldsymbol{\tau}_i)
\end{aligned}
\tag{1}\]</span></span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(N\)</span> is the batch size,</li>
<li><span class="math inline">\(f(\cdot)\)</span> can be any function of the trajectory sample <span class="math inline">\(\boldsymbol{\tau}\)</span>, of which the most important one is the gradient function of the optimization objective <span class="math inline">\(\mathbb{E}_{\boldsymbol{\tau} \sim p_{\theta}}[J(\cdot)]\)</span> relative to the policy parameters <span class="math inline">\(\theta\)</span>,</li>
<li><span class="math inline">\(p_{\theta}(\cdot)\)</span> is the probability density function of the trajectory distribution induced by the target policy <span class="math inline">\(\pi_{\theta}\)</span> parameterized by <span class="math inline">\(\theta\)</span> and the enviroment state transition distribution <span class="math inline">\(p(\cdot \mid \boldsymbol{s}, \boldsymbol{a})\)</span>,</li>
<li><span class="math inline">\(q_{\mu}(\cdot)\)</span> is the probability density function of the trajectory distribution induced by the behavior policy <span class="math inline">\(\mu\)</span> and <span class="math inline">\(p(\cdot \mid \boldsymbol{s}, \boldsymbol{a})\)</span>.</li>
</ul>
<p>One important property of <a href="#eq-is-batch" class="quarto-xref">Equation&nbsp;1</a> is that it is unbiased, i.e.,</p>
<p><span id="eq-is-unbiased"><span class="math display">\[
\mathbb{E}_{\boldsymbol{\tau} \sim q_{\mu}}[\hat{\mathbb{E}}_{\mu}(f(\boldsymbol{\tau}))] = \mathbb{E}_{\boldsymbol{\tau} \sim p_{\theta}}[f(\boldsymbol{\tau})]
\tag{2}\]</span></span></p>
<p>Of the components mentioned above, the only one that is ambiguous for calculation is the behavior policy <span class="math inline">\(\mu(\cdot \mid \boldsymbol{s})\)</span>.</p>
<p>However, the correct calculation of <span class="math inline">\(\mu(\cdot \mid \boldsymbol{s})\)</span> with the off-policy data in fully-utilized LLM RL systems mentioned above is not straightforward.</p>
<section id="sec-intractable-global-behav-policy" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-intractable-global-behav-policy"><span class="header-section-number">2.1</span> Simple but Intractable: Global Behavior Policy <span class="math inline">\(\mu^{*}(\cdot \mid \boldsymbol{s})\)</span></h2>
<p>Since there is always an actual distribution we are sampling from, we can always formulate the IS estimate with a global behavior policy <span class="math inline">\(\mu^{*}(\cdot \mid \boldsymbol{s})\)</span>.</p>
<p>The most direct thought is to use <span class="math inline">\(\mu^{*}(\cdot \mid \boldsymbol{s})\)</span> for IS in practice. But this requires us to calculate the probabilities under it.</p>
<p><span class="citation" data-cites="fu2025areal">Fu et al. (<a href="#ref-fu2025areal" role="doc-biblioref">2025</a>)</span> Proposition 1 resorts to constructing a behavior policy <span class="math inline">\(\mu(\boldsymbol{a} \mid \boldsymbol{s})\)</span> that satisfies <span class="math inline">\(\mu^{*}(a_{t} \mid s_{t}) = \pi_{\theta_{n+m}}(a_{t} \mid s_{t})\)</span> for each <span class="math inline">\((s_{t}, a_{t})\)</span> pair in the trajectory samples. This sounds reasonable for LLM RL since the LLM is usually auto-regressive and thus never revisits a past state within the same trajectory.</p>
<p>However, it might be confusing when we also notice that, the same state <span class="math inline">\(s\)</span> might appear in two different trajectory samples <span class="math inline">\(\tau_{i}\)</span> and <span class="math inline">\(\tau_{j}\)</span>, especially for the initial states <span class="math inline">\(s_{0}\)</span>, i.e., the prompts, and the same action <span class="math inline">\(a\)</span> might be sampled from different policies <span class="math inline">\(\pi_{\theta_{n+m}}\)</span> and <span class="math inline">\(\pi_{\theta_{n+l}}\)</span> (<span class="math inline">\(l \neq m\)</span>) respectively. It is very likely that <span class="math inline">\(\pi_{\theta_{n+m}}(a \mid s) \neq \pi_{\theta_{n+l}}(a \mid s)\)</span>, making it infeasible to simply construct the same behavior policy for both trajectory samples, i.e., <span class="math inline">\(\mu^{*}(a \mid s)=\pi_{\theta_{n+m}}(a \mid s)\)</span> constradicts <span class="math inline">\(\mu(a \mid s)=\pi_{\theta_{n+l}}(a \mid s)\)</span>.</p>
<p>So where is the problem in the construction for <span class="math inline">\(\mu(\cdot \mid \boldsymbol{s})\)</span> mentioned above?</p>
<p>The problem hidden here is that, <span class="math inline">\(\mu(\cdot \mid \boldsymbol{s})\)</span> does not consider the probability distribution of which LLM policy <span class="math inline">\(\pi_{\theta_{n+m}}\)</span> is used. Furthermore, this distribution is actually intractable since it depends on the system dynamics.</p>
</section>
<section id="sec-is-mcspt" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-is-mcspt"><span class="header-section-number">2.2</span> IS with MCSPTs</h2>
<p>IS with MCSPTs is simpler to formulate since each trajectory is sampled with a consistent stale policy <span class="math inline">\(\pi_{\theta_{n}}\)</span>, and we only need to correctly formulate the policy used by each trajectory.</p>
<section id="mixture-importance-sampling" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="mixture-importance-sampling"><span class="header-section-number">2.2.1</span> Mixture Importance Sampling</h3>
<p>In some simple cases, the distribution of which policy is used, i.e., the hyper-policy distribution, is known, where we can formulate the IS estimate as <em>Mixture Importance Sampling</em> <span class="citation" data-cites="owen2013mcbook">(<a href="#ref-owen2013mcbook" role="doc-biblioref">Owen 2013</a>)</span>.</p>
</section>
<section id="multiple-importance-sampling" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="multiple-importance-sampling"><span class="header-section-number">2.2.2</span> Multiple Importance Sampling</h3>
<p>However, in more general cases, we can only know ad hoc that the number of trajectories sampled from each policy <span class="math inline">\(\pi_{\theta_{j}}\)</span> is <span class="math inline">\(n_{j}\)</span>, where <span class="math inline">\(N = \sum_{j} n_{j}\)</span>. For such cases, we can formulate the IS estimate as <em>Multiple Importance Sampling</em> <span class="citation" data-cites="owen2013mcbook">(<a href="#ref-owen2013mcbook" role="doc-biblioref">Owen 2013</a>)</span>:</p>
<blockquote class="blockquote">
<p>Suppose that <span class="math inline">\(\boldsymbol{X}_{i j} \sim q_j\)</span> for <span class="math inline">\(i=1, \ldots, n_j\)</span> and <span class="math inline">\(j=1, \ldots, J\)</span> and that <span class="math inline">\(\omega_j\)</span> are a partition of unity. The multiple importance sampling estimate is</p>
<p><span class="math display">\[\widetilde{\mu}_\omega=\sum_{j=1}^J \frac{1}{n_j} \sum_{i=1}^{n_j} \omega_j\left(\boldsymbol{X}_{i j}\right) \frac{f\left(\boldsymbol{X}_{i j}\right) p\left(\boldsymbol{X}_{i j}\right)}{q_j\left(\boldsymbol{X}_{i j}\right)} .\]</span></p>
<p>Now assume that <span class="math inline">\(q_j(\boldsymbol{x})&gt;0\)</span> whenever <span class="math inline">\(\omega_j(\boldsymbol{x}) p(\boldsymbol{x}) f(\boldsymbol{x}) \neq 0\)</span>. Then multiple importance sampling is unbiased, because <span class="math display">\[\mathbb{E}\left(\widetilde{\mu}_\omega\right)=\sum_{j=1}^J \mathbb{E}_{q_j}\left(\omega_j(\boldsymbol{X}) \frac{f(\boldsymbol{X}) p(\boldsymbol{X})}{q_j(\boldsymbol{X})}\right)=\sum_{j=1}^J \int \omega_j(\boldsymbol{x}) f(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=\mu .\]</span></p>
</blockquote>
</section>
<section id="balance-heuristic" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="balance-heuristic"><span class="header-section-number">2.2.3</span> Balance Heuristic</h3>
<p>The natural problem following is how to choose the partition of unity <span class="math inline">\(\omega_j\)</span>:</p>
<blockquote class="blockquote">
<p>Among the proposals for functions <span class="math inline">\(\omega_j(\boldsymbol{x})\)</span>, the most studied one is the balance heuristic with <span class="math inline">\(\omega_j(\boldsymbol{x}) \propto n_j q_j(\boldsymbol{x})\)</span>, that is</p>
<p><span class="math display">\[\omega_j(\boldsymbol{x})=\omega_j^{\mathrm{BH}}(\boldsymbol{x}) \equiv \frac{n_j q_j(\boldsymbol{x})}{\sum_{k=1}^J n_k q_k(\boldsymbol{x})} .\]</span></p>
<p>By construction <span class="math inline">\(q_j(\boldsymbol{x})&gt;0\)</span> holds whenever <span class="math inline">\(\left(\omega_j^{\mathrm{BH}} p f\right)(\boldsymbol{x}) \neq 0\)</span>. Let <span class="math inline">\(n=\sum_{j=1}^J n_j\)</span> and define <span class="math inline">\(\alpha_j=n_j / n\)</span>. Then using the balance heuristic, <span class="math inline">\(\widetilde{\mu}_{\omega^{\text {ВН }}}\)</span> simplifies to <span class="math display">\[\widetilde{\mu}_\alpha=\frac{1}{n} \sum_{j=1}^J \sum_{i=1}^{n_j} \frac{f\left(\boldsymbol{X}_{i j}\right) p\left(\boldsymbol{X}_{i j}\right)}{\sum_{j=1}^J \alpha_j q_j\left(\boldsymbol{X}_{i j}\right)} .\]</span></p>
<p>In other words, multiple importance sampling, with weights from the balance heuristic reduces to the same estimator we would use in mixture importance sampling with mixture weights <span class="math inline">\(\alpha_j=n_j / n\)</span>. Once again, the weight on a given sampled value <span class="math inline">\(\boldsymbol{X}_{i j}\)</span> does not depend on which mixture component it came from. The balance heuristic is nearly optimal in the following sense:</p>
<p>Theorem 9.8. Let <span class="math inline">\(n_j \geqslant 1\)</span> be positive integers for <span class="math inline">\(j=1, \ldots, J\)</span>. Let <span class="math inline">\(\omega_1, \ldots, \omega_J\)</span> be a partition of unity and let <span class="math inline">\(\omega^{\mathrm{BH}}\)</span> be the balance heuristic. Suppose that <span class="math inline">\(q_j(\boldsymbol{x})&gt;\)</span> 0 whenever <span class="math inline">\(\omega_j(\boldsymbol{x}) p(\boldsymbol{x}) f(\boldsymbol{x}) \neq 0\)</span>. Then</p>
<p><span class="math display">\[\operatorname{Var}\left(\widetilde{\mu}_{\omega^{\mathrm{BH}}}\right) \leqslant \operatorname{Var}\left(\widetilde{\mu}_\omega\right)+\left(\frac{1}{\min _j n_j}-\frac{1}{\sum_j n_j}\right) \mu^2 .\]</span></p>
<p><span class="citation" data-cites="owen2013mcbook">(<a href="#ref-owen2013mcbook" role="doc-biblioref">Owen 2013</a>)</span></p>
</blockquote>
<p>The heuristic behind the balance heuristic to make <span class="math inline">\(\omega_j^{\mathrm{BH}} \propto n_{j}q_j(\boldsymbol{x})\)</span> can be also understood as the more samples we have from a policy, the more information we have about it, thus the more weight it should have.</p>
</section>
</section>
<section id="is-with-slapts" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="is-with-slapts"><span class="header-section-number">2.3</span> IS with SLAPTs</h2>
<p>The IS with SLAPTs is more difficult to formulate since we need to also consider the policy composition within a trajectory. As far as we know, the related discussion is in absense in previous works including <span class="citation" data-cites="espeholt2020seedrl">Espeholt et al. (<a href="#ref-espeholt2020seedrl" role="doc-biblioref">2020</a>)</span> that first proposed the usage of SLAPTs.</p>
<p>In this section, we try to discuss the formulation and some properties of IS with SLAPTs that might be useful in practice.</p>
<section id="trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="trajectory-dependent-behavior-policy-mu_icdot-mid-boldsymbols"><span class="header-section-number">2.3.1</span> Trajectory-dependent Behavior Policy <span class="math inline">\(\mu_{i}(\cdot \mid \boldsymbol{s})\)</span></h3>
<p>A general but trivial formulation for IS estimate with a batch of trajectory samples is to make the behavior policy trajectory-dependent, i.e., use <span class="math inline">\(\mu_{i}(\boldsymbol{a} \mid \boldsymbol{s})\)</span> for each trajectory <span class="math inline">\(\tau_{i}\)</span>.</p>
<p>Now for the counter-example mentioned in <a href="#sec-intractable-global-behav-policy" class="quarto-xref">Section&nbsp;2.1</a>, <span class="math inline">\(\mu_i(a \mid s)=\pi_{\theta_{n+m}}(a \mid s)\)</span> and <span class="math inline">\(\mu_j(a \mid s)=\pi_{\theta_{n+l}}(a \mid s)\)</span> are obviously compatible.</p>
<p>With a batch of <span class="math inline">\(N\)</span> trajectory samples sampled from each own behavior policy <span class="math inline">\(\boldsymbol{\tau}_1 \sim q_{\mu_1}, \ldots, \boldsymbol{\tau}_N \sim q_{\mu_N}\)</span>, we can only use the special case of the batch estimate <a href="#eq-is-batch" class="quarto-xref">Equation&nbsp;1</a> with <span class="math inline">\(N=1\)</span>, i.e., the single-sample estimate</p>
<p><span id="eq-is-single-sample"><span class="math display">\[
\hat{\mathbb{E}}_{\mu_i}(\boldsymbol{\tau}_i) = \prod_{t=0}^{T-1} \frac{\pi_{\theta_{n+m}}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}{\mu_i(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}f(\boldsymbol{\tau}_i)
\tag{3}\]</span></span></p>
<p>Note that the single-sample estimate is also unbiased, i.e., the unbiasedness of <a href="#eq-is-batch" class="quarto-xref">Equation&nbsp;1</a> does not depend on the sample size <span class="math inline">\(N\)</span>.</p>
<p>The common practice to combine them into an estimate with the batch of samples is to average the single-sample estimates <a href="#eq-is-single-sample" class="quarto-xref">Equation&nbsp;3</a>, i.e.,</p>
<p><span id="eq-is-single-sample-avg"><span class="math display">\[
\hat{\mathbb{E}}_{\text{avg}} = \frac{1}{N} \sum_{i=1}^{N} \hat{\mathbb{E}}_{\mu_i} = \frac{1}{N} \sum_{i=1}^{N} \prod_{t=0}^{T-1} \frac{\pi_{\theta_{n+m}}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}{\mu_i(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}f(\boldsymbol{\tau}_i)
\tag{4}\]</span></span></p>
<p>With the linearity of expectation, it is obvious to see the unbiasedness of the average estimate <span class="math inline">\(\hat{\mathbb{E}}_{\text{avg}}\)</span>.</p>
<p>Now let’s take a look at the variance. Let the variance of each single-sample estimate be <span class="math inline">\(\sigma^2_{\mu_i}\)</span>, since the samples are independent, the variance of <span class="math inline">\(\hat{\mathbb{E}}_{\text{avg}}\)</span> is:</p>
<p><span id="eq-is-single-sample-avg-var"><span class="math display">\[
\sigma^2_{\text{avg}} = \frac{1}{N^2} \sum_{i=1}^{N} \sigma^2_{\mu_i}
\tag{5}\]</span></span></p>
<p>Given the batch variance is determined by composing the variances of the single-sample estimates, we can first try to analyze the variance of the single-sample estimate.</p>
</section>
<section id="variance-of-single-sample-estimate-mcspt-vs.-slapt" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="variance-of-single-sample-estimate-mcspt-vs.-slapt"><span class="header-section-number">2.3.2</span> Variance of Single-Sample Estimate – MCSPT vs.&nbsp;SLAPT</h3>
<p>MCSPT and SLAPT just form two different cases of the single-sample estimate. We might wonder that, given a trajectory <span class="math inline">\(\boldsymbol{\tau}_{i}\)</span>, which of</p>
<ol type="1">
<li>the consistent old policy <span class="math inline">\(\pi_{\theta_{n+m}}\)</span> and</li>
<li>the sliding latest policy <span class="math inline">\(\mu_{\theta_{n},M}\)</span> using <span class="math inline">\(\pi_{\theta_{n}},\ldots,\pi_{\theta_{n+M}}\)</span> successively</li>
</ol>
<p>is better for the unbiased single-sample IS estimate <a href="#eq-is-single-sample" class="quarto-xref">Equation&nbsp;3</a>, i.e., leads to a lower variance?</p>
<p><span class="citation" data-cites="metelli2020pois">Metelli et al. (<a href="#ref-metelli2020pois" role="doc-biblioref">2020</a>)</span> provided a family of bounds of the variance of IS estimate in terms of the <a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy?oldformat=true#R%C3%A9nyi_divergence">Rényi divergence</a>:</p>
<blockquote class="blockquote">
<p>Lemma 1. Let <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> be two probability measures on the measurable space <span class="math inline">\((\mathcal{X}, \mathscr{F})\)</span> such that <span class="math inline">\(P \ll Q\)</span>. Let <span class="math inline">\(\alpha \in[1,+\infty], \mathbf{x}=\left(x_1, x_2, \ldots, x_N\right)^T\)</span> be i.i.d. random variables sampled from <span class="math inline">\(Q\)</span> and <span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span> be a function with bounded <span class="math inline">\(\frac{2 \alpha}{\alpha-1}\)</span>-moment under <span class="math inline">\(Q\left(\|f\|_{Q, \frac{2 \alpha}{\alpha-1}}&lt;+\infty\right)\)</span>. Then, for any <span class="math inline">\(N&gt;0\)</span>, the variance of the IS estimator <span class="math inline">\(\widehat{\mu}_{P / Q}\)</span> can be upper bounded as:</p>
<p><span class="math display">\[\operatorname{Var}_{\mathbf{x} \sim Q}\left[\hat{\mu}_{P / Q}\right] \leqslant \frac{1}{N}\|f\|_{Q, \frac{2 \alpha}{\alpha-1}}^2 d_{2 \alpha}(P \| Q)^{2-\frac{1}{\alpha}},\]</span></p>
<p>where we used the abbreviation <span class="math inline">\(\mathbf{x} \sim Q\)</span> for denoting <span class="math inline">\(x_i \sim Q\)</span> for all <span class="math inline">\(i=1,2, \ldots, N\)</span> all independent.</p>
<p>This result generalizes Lemma 4.1 of Metelli et al.&nbsp;(2018), that can be recovered by setting <span class="math inline">\(\alpha=1\)</span> under the condition that <span class="math inline">\(\|f\|_{\infty}&lt;+\infty\)</span> :</p>
<p><span class="math display">\[\operatorname{Var}_{\mathbf{x} \sim Q}\left[\widehat{\mu}_{P / Q}\right] \leqslant \frac{1}{N}\|f\|_{\infty}^2 d_2(P \| Q) .\]</span></p>
</blockquote>
<p>When <span class="math inline">\(\alpha = 1\)</span>, the Rényi divergence is the <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> widely used in RL analysis.</p>
<blockquote class="blockquote">
<p>When <span class="math inline">\(P=Q\)</span> almost everywhere, we get <span class="math inline">\(\operatorname{Var}_{\mathbf{x} \sim Q}\left[\hat{\mu}_{Q / Q}\right] \leqslant \frac{1}{N}\|f\|_{\infty}^2\)</span>, a well-known upper bound to the variance of a Monte Carlo estimator. Recalling the definition of ESS (Equation 7) we can rewrite the previous bound as:</p>
<p><span class="math display">\[\underset{\mathbf{x} \sim Q}{\operatorname{Var}}\left[\hat{\mu}_{P / Q}\right] \leqslant \frac{\|f\|_{\infty}^2}{\operatorname{ESS}(P \| Q)} .\]</span></p>
<p>Thus, the variance scales with ESS instead of <span class="math inline">\(N\)</span>, justifying the definition of ESS.</p>
</blockquote>
<p>In our context, <span class="math inline">\(P\)</span> is the target distribution <span class="math inline">\(p_{\theta}\)</span> and <span class="math inline">\(Q\)</span> is <span class="math inline">\(\mu_{\theta_{n},M}\)</span> or <span class="math inline">\(\pi_{\theta_{n}}\)</span>.</p>
<p>It is possible that <span class="math inline">\(d_{2 \alpha}(p_{\theta} \| q_{\theta_{n},M}) &lt; d_{2 \alpha}(p_{\theta} \| p_{\theta_{n}})\)</span>, since the newer the policy is, the more similar its induced distribution is to <span class="math inline">\(p_{\theta}\)</span>. Then as long as the <span class="math inline">\(\|f\|_{q_{\theta_{n},M}, \frac{2 \alpha}{\alpha-1}}\)</span> is not much larger than <span class="math inline">\(\|f\|_{p_{\theta_{n}}, \frac{2 \alpha}{\alpha-1}}\)</span>, the estimate using <span class="math inline">\(\mu_{\theta_{n},M}\)</span> has better guarantee than the estimate using <span class="math inline">\(\pi_{\theta_{n}}\)</span>.</p>
<p>In practice, the final effectiveness of the IS estimate usually relies on empirical diagnostic metrics like <a href="https://mc-stan.org/docs/reference-manual/analysis.html#effective-sample-size.section">Effective Sample Size (ESS)</a>. For example, <span class="citation" data-cites="piché2025pipelinerl">Piché et al. (<a href="#ref-piché2025pipelinerl" role="doc-biblioref">2025</a>)</span> measured the ESS of different sampling policies they used.</p>
</section>
<section id="minimizing-variance-by-optimizing-the-weighting" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="minimizing-variance-by-optimizing-the-weighting"><span class="header-section-number">2.3.3</span> Minimizing Variance by Optimizing the Weighting</h3>
<p>Beyond the comparison on each single-sample estimate, we can also consider the optimal weighting of them. In <a href="#eq-is-single-sample-avg" class="quarto-xref">Equation&nbsp;4</a>, we use the simplest average weight <span class="math inline">\(\frac{1}{N}\)</span> for each single-sample estimate. But we can actually use any other weight functions <span class="math inline">\(\omega_i(\boldsymbol{x})\)</span> to combine them, as long as they form a <em>partition of unity</em>, i.e., a collection of <span class="math inline">\(J \geqslant 1\)</span> weight functions <span class="math inline">\(\omega_j(\boldsymbol{x}) \geqslant 0\)</span> which satisfy <span class="math inline">\(\sum_{j=1}^J \omega_j(\boldsymbol{x})=1\)</span> for all <span class="math inline">\(\boldsymbol{x}\)</span>. Different partitions of unity will lead to estimates that are all unbiased but with different variances.</p>
<p>The optimal weighting should be still an open problem that is beyond our scope. However, there is still some properties that we can exploit from the SLAPT structure. For example, in practical systems, SLAPTs of similar lengths often conform to identical distributions (approximately, but this can be exact if we limit the timesteps where a trajectory can be aborted), we can also apply the formulation of Multiple IS to SLAPT like in <a href="#sec-is-mcspt" class="quarto-xref">Section&nbsp;2.2</a>. This also supports the comparability of SLAPT and MCSPT.</p>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="citation" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Citation</h2><div class="quarto-appendix-contents">

<p>BibTeX:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode numberSource bibtex number-lines code-with-copy"><code class="sourceCode bibtex"><span id="cb1-1"><a href="#cb1-1"></a><span class="va">@article</span>{<span class="ot">tong2026isdr</span>,</span>
<span id="cb1-2"><a href="#cb1-2"></a>  <span class="dt">author</span> = {Yuxuan Tong and Yingru Li and Guangming Sheng},</span>
<span id="cb1-3"><a href="#cb1-3"></a>  <span class="dt">title</span> = {{Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems}},</span>
<span id="cb1-4"><a href="#cb1-4"></a>  <span class="dt">journal</span> = {Blog},</span>
<span id="cb1-5"><a href="#cb1-5"></a>  <span class="dt">date</span> = {2026-01-12},</span>
<span id="cb1-6"><a href="#cb1-6"></a>  <span class="dt">url</span> = {https://tongyx361.github.io/posts/isdr},</span>
<span id="cb1-7"><a href="#cb1-7"></a>  <span class="dt">language</span> = {English},</span>
<span id="cb1-8"><a href="#cb1-8"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>


<!-- -->


</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-espeholt2020seedrl" class="csl-entry" role="listitem">
Espeholt, Lasse, Raphaël Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski‎. 2020. <span>“SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference.”</span> In <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=rkgvXlrKwH">https://openreview.net/forum?id=rkgvXlrKwH</a>.
</div>
<div id="ref-espeholt2018impala" class="csl-entry" role="listitem">
Espeholt, Lasse, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, et al. 2018. <span>“<span>IMPALA</span>: Scalable Distributed Deep-<span>RL</span> with Importance Weighted Actor-Learner Architectures.”</span> In <em>Proceedings of the 35th International Conference on Machine Learning</em>, edited by Jennifer Dy and Andreas Krause, 80:1407–16. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v80/espeholt18a.html">https://proceedings.mlr.press/v80/espeholt18a.html</a>.
</div>
<div id="ref-fu2025areal" class="csl-entry" role="listitem">
Fu, Wei, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, et al. 2025. <span>“<span>AREAL</span>: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning.”</span> In <em>The Thirty-Ninth Annual Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=X9diEuva9R">https://openreview.net/forum?id=X9diEuva9R</a>.
</div>
<div id="ref-hu2025openrlhf" class="csl-entry" role="listitem">
Hu, Jian, Xibin Wu, Wei Shen, Jason Klein Liu, Weixun Wang, Songlin Jiang, Haoran Wang, et al. 2025. <span>“<span>O</span>pen<span>RLHF</span>: A Ray-Based Easy-to-Use, Scalable and High-Performance <span>RLHF</span> Framework.”</span> In <em>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, edited by Ivan Habernal, Peter Schulam, and Jörg Tiedemann, 656–66. Suzhou, China: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2025.emnlp-demos.48">https://doi.org/10.18653/v1/2025.emnlp-demos.48</a>.
</div>
<div id="ref-kimi2025k15" class="csl-entry" role="listitem">
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, et al. 2025. <span>“Kimi K1.5: Scaling Reinforcement Learning with LLMs.”</span> <a href="https://arxiv.org/abs/2501.12599">https://arxiv.org/abs/2501.12599</a>.
</div>
<div id="ref-metelli2020pois" class="csl-entry" role="listitem">
Metelli, Alberto Maria, Matteo Papini, Nico Montali, and Marcello Restelli. 2020. <span>“Importance Sampling Techniques for Policy Optimization.”</span> <em>Journal of Machine Learning Research</em> 21 (141): 1–75. <a href="http://jmlr.org/papers/v21/20-124.html">http://jmlr.org/papers/v21/20-124.html</a>.
</div>
<div id="ref-owen2013mcbook" class="csl-entry" role="listitem">
Owen, Art B. 2013. <em>Monte Carlo Theory, Methods and Examples</em>. <a href="https://artowen.su.domains/mc/" class="uri">https://artowen.su.domains/mc/</a>.
</div>
<div id="ref-piché2025pipelinerl" class="csl-entry" role="listitem">
Piché, Alexandre, Ehsan Kamalloo, Rafael Pardinas, Xiaoyin Chen, and Dzmitry Bahdanau. 2025. <span>“PipelineRL: Faster on-Policy Reinforcement Learning for Long Sequence Generation.”</span> <a href="https://arxiv.org/abs/2509.19128">https://arxiv.org/abs/2509.19128</a>.
</div>
<div id="ref-shen2024nemoaligner" class="csl-entry" role="listitem">
Shen, Gerald, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, et al. 2024. <span>“NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment.”</span> In <em>First Conference on Language Modeling</em>. <a href="https://openreview.net/forum?id=yK2eGE8QVW">https://openreview.net/forum?id=yK2eGE8QVW</a>.
</div>
<div id="ref-sheng2025laminar" class="csl-entry" role="listitem">
Sheng, Guangming, Yuxuan Tong, Borui Wan, Wang Zhang, Chaobo Jia, Xibin Wu, Yuqi Wu, et al. 2025. <span>“Laminar: A Scalable Asynchronous RL Post-Training Framework.”</span> <a href="https://arxiv.org/abs/2510.12633">https://arxiv.org/abs/2510.12633</a>.
</div>
<div id="ref-sheng2025hybridflow" class="csl-entry" role="listitem">
Sheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. <span>“HybridFlow: A Flexible and Efficient RLHF Framework.”</span> In <em>Proceedings of the Twentieth European Conference on Computer Systems</em>, 1279–97. EuroSys ’25. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3689031.3696075">https://doi.org/10.1145/3689031.3696075</a>.
</div>
<div id="ref-yao2023dschat" class="csl-entry" role="listitem">
Yao, Zhewei, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, et al. 2023. <span>“DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-Like Models at All Scales.”</span> <a href="https://arxiv.org/abs/2308.01320">https://arxiv.org/abs/2308.01320</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tongyx361\.github\.io\/blogs");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "tongyx361/blogs";
    script.dataset.repoId = "R_kgDOOFf8xw";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOOFf8x84CoCU7";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="co"># title</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="an">title:</span><span class="co"> '[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems'</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="an">date:</span><span class="co"> "2026-01-12"</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">  - name: "Yuxuan Tong"</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">    email: "tongyuxuan361@gmail.com"</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">  - name: "Yingru Li"</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="co">  - name: "Guangming Sheng"</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="an">copyright:</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="co">  holder: "Yuxuan Tong, Yingru Li, Guangming Sheng"</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co">  year: 2026</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="an">keywords:</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="co">  - Importance Sampling</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co">  - Off-Policy Reinforcement Learning</span></span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="co">  - Large Language Model</span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb2-18"><a href="#cb2-18"></a></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="co">  In the pursuit of maximizing hardware utilization for Large Language Model (LLM) Reinforcement Learning, systems introduces some new sampling strategies like Partial Rollout and Asynchornous Rollout.</span></span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="co">  This introduces complex off-policy data structures, which we categorize into **Sliding Latest Policy Trajectories (SLAPTs)** and **Multiple Consistent Stale Policy Trajectories (MCSPTs)**.</span></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="co">  This post discusses the properties of applying **Importance Sampling (IS)** to these distinct data forms, combining theoretical notations like Multiple Importance Sampling and Rényi divergence and practical observations like the (approximate) indentical distribution structure of SLAPTs of similar lengths.</span></span>
<span id="cb2-22"><a href="#cb2-22"></a></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="an">toc-expand:</span><span class="co"> false</span></span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="co"># citation</span></span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="an">citation:</span><span class="co"> false # Manual citation</span></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="co"># comments</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="an">comments:</span><span class="co"> # c.f. https://thedatasavvycorner.com/blogs/17-quarto-comments</span></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="co">  hypothesis: true</span></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="co">  giscus: </span></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="co">    repo: tongyx361/blogs</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="co"># language</span></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="an">toc-title:</span><span class="co"> Table of Contents</span></span>
<span id="cb2-33"><a href="#cb2-33"></a><span class="an">categories:</span></span>
<span id="cb2-34"><a href="#cb2-34"></a><span class="co">    - English 英文</span></span>
<span id="cb2-35"><a href="#cb2-35"></a><span class="co">    - Technical 技术</span></span>
<span id="cb2-36"><a href="#cb2-36"></a><span class="co"># redirect</span></span>
<span id="cb2-37"><a href="#cb2-37"></a><span class="an">aliases:</span></span>
<span id="cb2-38"><a href="#cb2-38"></a><span class="co">  - /posts/is-bet</span></span>
<span id="cb2-39"><a href="#cb2-39"></a><span class="co">  - /posts/is-slapt</span></span>
<span id="cb2-40"><a href="#cb2-40"></a><span class="co">---</span></span>
<span id="cb2-41"><a href="#cb2-41"></a></span>
<span id="cb2-42"><a href="#cb2-42"></a><span class="fu"># Off-Policy Data in Fully-Utilized LLM RL Systems</span></span>
<span id="cb2-43"><a href="#cb2-43"></a></span>
<span id="cb2-44"><a href="#cb2-44"></a>In Reinforcement Learning (RL) systems for Large Language Models (LLMs), especially those that pursue full utilization of the hardware resources, it is common that we have to utilize **off-policy** data, typically a batch of trajectory samples collected from one or multiple stale policies.</span>
<span id="cb2-45"><a href="#cb2-45"></a></span>
<span id="cb2-46"><a href="#cb2-46"></a>So far, there have been two main kinds of off-policy data emerging in such systems, which in this post we refer to as:</span>
<span id="cb2-47"><a href="#cb2-47"></a></span>
<span id="cb2-48"><a href="#cb2-48"></a><span class="ss">1. </span>Sliding Latest Policy Trajectories (SLAPTs)</span>
<span id="cb2-49"><a href="#cb2-49"></a><span class="ss">2. </span>Multiple Consistent Stale Policy Trajectories (MCSPTs)</span>
<span id="cb2-50"><a href="#cb2-50"></a></span>
<span id="cb2-51"><a href="#cb2-51"></a><span class="fu">## Sliding Latest Policy Trajectories (SLAPTs)</span></span>
<span id="cb2-52"><a href="#cb2-52"></a></span>
<span id="cb2-53"><a href="#cb2-53"></a>We use *Sliding Latest Policy Trajectories* to refer to such batches of trajectories that are sampled in an RL system where we always use the latest policy $\pi_{\theta_{t}}$ at each moment to sample the actions.</span>
<span id="cb2-54"><a href="#cb2-54"></a></span>
<span id="cb2-55"><a href="#cb2-55"></a>Formally, a SLAPT can be defined as a trajectory $\tau = (s_{0}, a_{0,\pi_{\theta_{n}}}, \ldots, s_{t}, a_{t,\pi_{\theta_{n+m}}}, \ldots, s_{T}, a_{T,\pi_{\theta_{n+M}}})$, where $s_{t}$ is the state at timestep $t$ and $a_{t,\pi_{\theta_{n+m}}}$ is the action sampled from the policy $\pi_{\theta_{n+m}}$ that is the latest policy available in the system when sampling. So for $l &gt; m$, $\pi_{\theta_{n+l}}$ is usually more on-policy than $\pi_{\theta_{n+m}}$.</span>
<span id="cb2-56"><a href="#cb2-56"></a></span>
<span id="cb2-57"><a href="#cb2-57"></a>SLAPTs in a batch usually have different policy compositions, depending on the system dynamics.</span>
<span id="cb2-58"><a href="#cb2-58"></a></span>
<span id="cb2-59"><a href="#cb2-59"></a>SLAPTs are a natural result of a popular design choice called *Partial Rollout* in LLM RL systems nowadays, such as Kimi k1.5 <span class="co">[</span><span class="ot">@kimi2025k15</span><span class="co">]</span>, AReaL <span class="co">[</span><span class="ot">@fu2025areal</span><span class="co">]</span> and PipelineRL <span class="co">[</span><span class="ot">@piché2025pipelinerl</span><span class="co">]</span>, etc., which can date back to a traditional distributed RL system called SEED RL <span class="co">[</span><span class="ot">@espeholt2020seedrl</span><span class="co">]</span>:</span>
<span id="cb2-60"><a href="#cb2-60"></a></span>
<span id="cb2-61"><a href="#cb2-61"></a>The ongoing trajectory rollouts are</span>
<span id="cb2-62"><a href="#cb2-62"></a></span>
<span id="cb2-63"><a href="#cb2-63"></a>1) aborted when</span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="ss">    - </span>either, in synchoronous systems like Kimi k1.5, there are enough samples collected for training, releasing the resources for the training engine to update the model weights,</span>
<span id="cb2-65"><a href="#cb2-65"></a><span class="ss">    - </span>or, in asynchronous systems like AReaL and PipelineRL, a new version of model weights is produced by the trainer;</span>
<span id="cb2-66"><a href="#cb2-66"></a>2) continued with the latest version of model weights.</span>
<span id="cb2-67"><a href="#cb2-67"></a></span>
<span id="cb2-68"><a href="#cb2-68"></a>Partial Rollout is motivated by its efficiency and simplicity. Let me explain in detail.</span>
<span id="cb2-69"><a href="#cb2-69"></a></span>
<span id="cb2-70"><a href="#cb2-70"></a>The earliest LLM RL systems <span class="co">[</span><span class="ot">@yao2023dschat, @shen2024nemoaligner, @sheng2025hybridflow, @hu2025openrlhf</span><span class="co">]</span> all adopt synchoronous architectures, where the trainer always waits for all the trajectories are finished before updating the weights with these data. However, as the context length of LLMs scales up, the skewness of the trajectory length distribution becomes increasing heavy. If the distribution is very skewed but all the trajectories are required to finish in the same rollout stage, there can be only a few long-tail requests remaining in the system, causing severe under-utilization (typically &lt;30% in practice).</span>
<span id="cb2-71"><a href="#cb2-71"></a></span>
<span id="cb2-72"><a href="#cb2-72"></a>Kimi k1.5 proposes to fix this issue by aborting all the ongoing rollouts once enough training samples are collected and directly update the weights with these data, instead of waiting for all the trajectories to finish, and then continue the rollouts with the new model weights.</span>
<span id="cb2-73"><a href="#cb2-73"></a></span>
<span id="cb2-74"><a href="#cb2-74"></a>In asynchornous RL systems with an experience buffer, it is troublesome to mamage multiple versions of model weights within the rollout engine.</span>
<span id="cb2-75"><a href="#cb2-75"></a></span>
<span id="cb2-76"><a href="#cb2-76"></a>AReaL proposes to always only maintain the latest model weights across all the instances. Once a new version of model weights is produced by the trainer, all the rollouts of the stale policy are aborted and then continued with the latest policy.</span>
<span id="cb2-77"><a href="#cb2-77"></a></span>
<span id="cb2-78"><a href="#cb2-78"></a>This can be simply implemented by always loading the latest weights into all the inference engine instances, avoiding the bothering to manage the requests across each instance.</span>
<span id="cb2-79"><a href="#cb2-79"></a></span>
<span id="cb2-80"><a href="#cb2-80"></a>Despite Partial Rollout's efficiency and simplicity, there have been worries about mixing multiple policies within a single trajectory, since most previous works formulate IS with a single consistent behavior policy $\mu(\cdot \mid \boldsymbol{s})$ while such formulation is rather under-explored. We defer a more detailed discussion about this to @sec-is.</span>
<span id="cb2-81"><a href="#cb2-81"></a></span>
<span id="cb2-82"><a href="#cb2-82"></a><span class="fu">## Multiple Consistent Stale Policy Trajectories (MCSPTs)</span></span>
<span id="cb2-83"><a href="#cb2-83"></a></span>
<span id="cb2-84"><a href="#cb2-84"></a>We use *Multiple Consistent Stale Policy Trajectories* (MCSPTs) to refer to such batches of trajectories that</span>
<span id="cb2-85"><a href="#cb2-85"></a></span>
<span id="cb2-86"><a href="#cb2-86"></a><span class="ss">- </span>each of them is sampled with a consistent stale policy $\pi_{\theta_{n}}$,</span>
<span id="cb2-87"><a href="#cb2-87"></a><span class="ss">- </span>while there might be different $n$ for different trajectories.</span>
<span id="cb2-88"><a href="#cb2-88"></a></span>
<span id="cb2-89"><a href="#cb2-89"></a>The consistency of policy within a trajectory makes the formulation simpler, which is widely used in traditional distributed RL systems like IMPALA <span class="co">[</span><span class="ot">@espeholt2018impala</span><span class="co">]</span>.</span>
<span id="cb2-90"><a href="#cb2-90"></a></span>
<span id="cb2-91"><a href="#cb2-91"></a>In contrast, MCSPT sampling is more difficult to implement efficiently against long-tail bubbles in LLM RL systems, because this requires managing multiple versions of model weights at the same time within the rollout engine and dynamically transferring the requests of various lengths.</span>
<span id="cb2-92"><a href="#cb2-92"></a></span>
<span id="cb2-93"><a href="#cb2-93"></a>@sheng2025laminar implement an LLM RL system that samples MCSPTs with full utilization of the hardware resources.</span>
<span id="cb2-94"><a href="#cb2-94"></a></span>
<span id="cb2-95"><a href="#cb2-95"></a><span class="fu"># Importance Sampling with Off-Policy Data in (Fully-Utilized) LLM RL Systems {#sec-is}</span></span>
<span id="cb2-96"><a href="#cb2-96"></a></span>
<span id="cb2-97"><a href="#cb2-97"></a><span class="co">[</span><span class="ot">Importance Sampling (IS)</span><span class="co">](https://en.wikipedia.org/wiki/Importance_sampling)</span> has been widely used in LLM RL systems.</span>
<span id="cb2-98"><a href="#cb2-98"></a></span>
<span id="cb2-99"><a href="#cb2-99"></a>It is useful for estimating the expection of some random function, typically the gradient of the return respect to the policy parameters, on the target policy distribution. In practice, with a batch of trajectory samples, the IS estimate is often formulated as:</span>
<span id="cb2-100"><a href="#cb2-100"></a></span>
<span id="cb2-101"><a href="#cb2-101"></a>$$</span>
<span id="cb2-102"><a href="#cb2-102"></a>\begin{aligned}</span>
<span id="cb2-103"><a href="#cb2-103"></a>\hat{\mathbb{E}}_{\mu}(f(\boldsymbol{\tau})) =&amp; \frac{1}{N} \sum_{i=1}^{N} \frac{p_{\theta}(\boldsymbol{\tau}_i)}{q_{\mu}(\boldsymbol{\tau}_i)}f(\boldsymbol{\tau}_i) <span class="sc">\\</span></span>
<span id="cb2-104"><a href="#cb2-104"></a>=&amp; \frac{1}{N} \sum_{i=1}^{N} \frac{p(\boldsymbol{s}_{0}) \prod_{t=0}^{T-1} \pi_{\theta}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})p(\boldsymbol{s}_{t+1} \mid \boldsymbol{s}_{t}, \boldsymbol{a}_{t})}{p(\boldsymbol{s}_{0}) \prod_{t=0}^{T-1} \mu(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})p(\boldsymbol{s}_{t+1} \mid \boldsymbol{s}_{t}, \boldsymbol{a}_{t})}f(\boldsymbol{\tau}_i) <span class="sc">\\</span></span>
<span id="cb2-105"><a href="#cb2-105"></a>=&amp; \frac{1}{N} \sum_{i=1}^{N} \prod_{t=0}^{T-1} \frac{\pi_{\theta}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}{\mu(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}f(\boldsymbol{\tau}_i)</span>
<span id="cb2-106"><a href="#cb2-106"></a>\end{aligned}</span>
<span id="cb2-107"><a href="#cb2-107"></a>$$ {#eq-is-batch}</span>
<span id="cb2-108"><a href="#cb2-108"></a></span>
<span id="cb2-109"><a href="#cb2-109"></a>where </span>
<span id="cb2-110"><a href="#cb2-110"></a></span>
<span id="cb2-111"><a href="#cb2-111"></a><span class="ss">- </span>$N$ is the batch size,</span>
<span id="cb2-112"><a href="#cb2-112"></a><span class="ss">- </span>$f(\cdot)$ can be any function of the trajectory sample $\boldsymbol{\tau}$, of which the most important one is the gradient function of the optimization objective $\mathbb{E}_{\boldsymbol{\tau} \sim p_{\theta}}<span class="co">[</span><span class="ot">J(\cdot)</span><span class="co">]</span>$ relative to the policy parameters $\theta$,</span>
<span id="cb2-113"><a href="#cb2-113"></a><span class="ss">- </span>$p_{\theta}(\cdot)$ is the probability density function of the trajectory distribution induced by the target policy $\pi_{\theta}$ parameterized by $\theta$ and the enviroment state transition distribution $p(\cdot \mid \boldsymbol{s}, \boldsymbol{a})$,</span>
<span id="cb2-114"><a href="#cb2-114"></a><span class="ss">- </span>$q_{\mu}(\cdot)$ is the probability density function of the trajectory distribution induced by the behavior policy $\mu$ and $p(\cdot \mid \boldsymbol{s}, \boldsymbol{a})$.</span>
<span id="cb2-115"><a href="#cb2-115"></a></span>
<span id="cb2-116"><a href="#cb2-116"></a>One important property of @eq-is-batch is that it is unbiased, i.e.,</span>
<span id="cb2-117"><a href="#cb2-117"></a></span>
<span id="cb2-118"><a href="#cb2-118"></a>$$</span>
<span id="cb2-119"><a href="#cb2-119"></a>\mathbb{E}_{\boldsymbol{\tau} \sim q_{\mu}}[\hat{\mathbb{E}}_{\mu}(f(\boldsymbol{\tau}))] = \mathbb{E}_{\boldsymbol{\tau} \sim p_{\theta}}<span class="co">[</span><span class="ot">f(\boldsymbol{\tau})</span><span class="co">]</span></span>
<span id="cb2-120"><a href="#cb2-120"></a>$$ {#eq-is-unbiased}</span>
<span id="cb2-121"><a href="#cb2-121"></a></span>
<span id="cb2-122"><a href="#cb2-122"></a>Of the components mentioned above, the only one that is ambiguous for calculation is the behavior policy $\mu(\cdot \mid \boldsymbol{s})$.</span>
<span id="cb2-123"><a href="#cb2-123"></a></span>
<span id="cb2-124"><a href="#cb2-124"></a>However, the correct calculation of $\mu(\cdot \mid \boldsymbol{s})$ with the off-policy data in fully-utilized LLM RL systems mentioned above is not straightforward.</span>
<span id="cb2-125"><a href="#cb2-125"></a></span>
<span id="cb2-126"><a href="#cb2-126"></a><span class="fu">## Simple but Intractable: Global Behavior Policy $\mu^{*}(\cdot \mid \boldsymbol{s})$ {#sec-intractable-global-behav-policy}</span></span>
<span id="cb2-127"><a href="#cb2-127"></a></span>
<span id="cb2-128"><a href="#cb2-128"></a>Since there is always an actual distribution we are sampling from, we can always formulate the IS estimate with a global behavior policy $\mu^{*}(\cdot \mid \boldsymbol{s})$.</span>
<span id="cb2-129"><a href="#cb2-129"></a></span>
<span id="cb2-130"><a href="#cb2-130"></a>The most direct thought is to use $\mu^{*}(\cdot \mid \boldsymbol{s})$ for IS in practice. But this requires us to calculate the  probabilities under it.</span>
<span id="cb2-131"><a href="#cb2-131"></a></span>
<span id="cb2-132"><a href="#cb2-132"></a>@fu2025areal Proposition 1 resorts to constructing a behavior policy $\mu(\boldsymbol{a} \mid \boldsymbol{s})$ that satisfies $\mu^{*}(a_{t} \mid s_{t}) = \pi_{\theta_{n+m}}(a_{t} \mid s_{t})$ for each $(s_{t}, a_{t})$ pair in the trajectory samples. This sounds reasonable for LLM RL since the LLM is usually auto-regressive and thus never revisits a past state within the same trajectory.</span>
<span id="cb2-133"><a href="#cb2-133"></a></span>
<span id="cb2-134"><a href="#cb2-134"></a>However, it might be confusing when we also notice that, the same state $s$ might appear in two different trajectory samples $\tau_{i}$ and $\tau_{j}$, especially for the initial states $s_{0}$, i.e., the prompts, and the same action $a$ might be sampled from different policies $\pi_{\theta_{n+m}}$ and $\pi_{\theta_{n+l}}$ ($l \neq m$) respectively. It is very likely that $\pi_{\theta_{n+m}}(a \mid s) \neq \pi_{\theta_{n+l}}(a \mid s)$, making it infeasible to simply construct the same behavior policy for both trajectory samples, i.e., $\mu^{*}(a \mid s)=\pi_{\theta_{n+m}}(a \mid s)$ constradicts $\mu(a \mid s)=\pi_{\theta_{n+l}}(a \mid s)$.</span>
<span id="cb2-135"><a href="#cb2-135"></a></span>
<span id="cb2-136"><a href="#cb2-136"></a>So where is the problem in the construction for $\mu(\cdot \mid \boldsymbol{s})$ mentioned above?</span>
<span id="cb2-137"><a href="#cb2-137"></a></span>
<span id="cb2-138"><a href="#cb2-138"></a>The problem hidden here is that, $\mu(\cdot \mid \boldsymbol{s})$ does not consider the probability distribution of which LLM policy $\pi_{\theta_{n+m}}$ is used. Furthermore, this distribution is actually intractable since it depends on the system dynamics.</span>
<span id="cb2-139"><a href="#cb2-139"></a></span>
<span id="cb2-140"><a href="#cb2-140"></a><span class="fu">## IS with MCSPTs {#sec-is-mcspt}</span></span>
<span id="cb2-141"><a href="#cb2-141"></a></span>
<span id="cb2-142"><a href="#cb2-142"></a>IS with MCSPTs is simpler to formulate since each trajectory is sampled with a consistent stale policy $\pi_{\theta_{n}}$, and we only need to correctly formulate the policy used by each trajectory.</span>
<span id="cb2-143"><a href="#cb2-143"></a></span>
<span id="cb2-144"><a href="#cb2-144"></a><span class="fu">### Mixture Importance Sampling</span></span>
<span id="cb2-145"><a href="#cb2-145"></a></span>
<span id="cb2-146"><a href="#cb2-146"></a>In some simple cases, the distribution of which policy is used, i.e., the hyper-policy distribution, is known, where we can formulate the IS estimate as *Mixture Importance Sampling* <span class="co">[</span><span class="ot">@owen2013mcbook</span><span class="co">]</span>.</span>
<span id="cb2-147"><a href="#cb2-147"></a></span>
<span id="cb2-148"><a href="#cb2-148"></a><span class="fu">### Multiple Importance Sampling</span></span>
<span id="cb2-149"><a href="#cb2-149"></a></span>
<span id="cb2-150"><a href="#cb2-150"></a>However, in more general cases, we can only know ad hoc that the number of trajectories sampled from each policy $\pi_{\theta_{j}}$ is $n_{j}$, where $N = \sum_{j} n_{j}$. For such cases, we can formulate the IS estimate as *Multiple Importance Sampling* <span class="co">[</span><span class="ot">@owen2013mcbook</span><span class="co">]</span>:</span>
<span id="cb2-151"><a href="#cb2-151"></a></span>
<span id="cb2-152"><a href="#cb2-152"></a><span class="at">&gt; Suppose that $\boldsymbol{X}_{i j} \sim q_j$ for $i=1, \ldots, n_j$ and $j=1, \ldots, J$ and that $\omega_j$ are a partition of unity. The multiple importance sampling estimate is</span></span>
<span id="cb2-153"><a href="#cb2-153"></a><span class="at">&gt;</span></span>
<span id="cb2-154"><a href="#cb2-154"></a><span class="at">&gt; $$\widetilde{\mu}_\omega=\sum_{j=1}^J \frac{1}{n_j} \sum_{i=1}^{n_j} \omega_j\left(\boldsymbol{X}_{i j}\right) \frac{f\left(\boldsymbol{X}_{i j}\right) p\left(\boldsymbol{X}_{i j}\right)}{q_j\left(\boldsymbol{X}_{i j}\right)} .$$</span></span>
<span id="cb2-155"><a href="#cb2-155"></a><span class="at">&gt;</span></span>
<span id="cb2-156"><a href="#cb2-156"></a><span class="at">&gt; Now assume that $q_j(\boldsymbol{x})&gt;0$ whenever $\omega_j(\boldsymbol{x}) p(\boldsymbol{x}) f(\boldsymbol{x}) \neq 0$. Then multiple importance sampling is unbiased, because</span></span>
<span id="cb2-157"><a href="#cb2-157"></a><span class="at">&gt; $$\mathbb{E}\left(\widetilde{\mu}_\omega\right)=\sum_{j=1}^J \mathbb{E}_{q_j}\left(\omega_j(\boldsymbol{X}) \frac{f(\boldsymbol{X}) p(\boldsymbol{X})}{q_j(\boldsymbol{X})}\right)=\sum_{j=1}^J \int \omega_j(\boldsymbol{x}) f(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=\mu .$$</span></span>
<span id="cb2-158"><a href="#cb2-158"></a></span>
<span id="cb2-159"><a href="#cb2-159"></a><span class="fu">### Balance Heuristic</span></span>
<span id="cb2-160"><a href="#cb2-160"></a></span>
<span id="cb2-161"><a href="#cb2-161"></a>The natural problem following is how to choose the partition of unity $\omega_j$:</span>
<span id="cb2-162"><a href="#cb2-162"></a></span>
<span id="cb2-163"><a href="#cb2-163"></a><span class="at">&gt; Among the proposals for functions $\omega_j(\boldsymbol{x})$, the most studied one is the balance heuristic with $\omega_j(\boldsymbol{x}) \propto n_j q_j(\boldsymbol{x})$, that is</span></span>
<span id="cb2-164"><a href="#cb2-164"></a><span class="at">&gt; </span></span>
<span id="cb2-165"><a href="#cb2-165"></a><span class="at">&gt;$$\omega_j(\boldsymbol{x})=\omega_j^{\mathrm{BH}}(\boldsymbol{x}) \equiv \frac{n_j q_j(\boldsymbol{x})}{\sum_{k=1}^J n_k q_k(\boldsymbol{x})} .$$</span></span>
<span id="cb2-166"><a href="#cb2-166"></a><span class="at">&gt;</span></span>
<span id="cb2-167"><a href="#cb2-167"></a><span class="at">&gt; By construction $q_j(\boldsymbol{x})&gt;0$ holds whenever $\left(\omega_j^{\mathrm{BH}} p f\right)(\boldsymbol{x}) \neq 0$. Let $n=\sum_{j=1}^J n_j$ and define $\alpha_j=n_j / n$. Then using the balance heuristic, $\widetilde{\mu}_{\omega^{\text {ВН }}}$ simplifies to</span></span>
<span id="cb2-168"><a href="#cb2-168"></a><span class="at">&gt; $$\widetilde{\mu}_\alpha=\frac{1}{n} \sum_{j=1}^J \sum_{i=1}^{n_j} \frac{f\left(\boldsymbol{X}_{i j}\right) p\left(\boldsymbol{X}_{i j}\right)}{\sum_{j=1}^J \alpha_j q_j\left(\boldsymbol{X}_{i j}\right)} .$$</span></span>
<span id="cb2-169"><a href="#cb2-169"></a><span class="at">&gt;</span></span>
<span id="cb2-170"><a href="#cb2-170"></a><span class="at">&gt; In other words, multiple importance sampling, with weights from the balance heuristic reduces to the same estimator we would use in mixture importance sampling with mixture weights $\alpha_j=n_j / n$. Once again, the weight on a given sampled value $\boldsymbol{X}_{i j}$ does not depend on which mixture component it came from. The balance heuristic is nearly optimal in the following sense:</span></span>
<span id="cb2-171"><a href="#cb2-171"></a><span class="at">&gt;</span></span>
<span id="cb2-172"><a href="#cb2-172"></a><span class="at">&gt; Theorem 9.8. Let $n_j \geqslant 1$ be positive integers for $j=1, \ldots, J$. Let $\omega_1, \ldots, \omega_J$ be a partition of unity and let $\omega^{\mathrm{BH}}$ be the balance heuristic. Suppose that $q_j(\boldsymbol{x})&gt;$ 0 whenever $\omega_j(\boldsymbol{x}) p(\boldsymbol{x}) f(\boldsymbol{x}) \neq 0$. Then</span></span>
<span id="cb2-173"><a href="#cb2-173"></a><span class="at">&gt; </span></span>
<span id="cb2-174"><a href="#cb2-174"></a><span class="at">&gt; $$\operatorname{Var}\left(\widetilde{\mu}_{\omega^{\mathrm{BH}}}\right) \leqslant \operatorname{Var}\left(\widetilde{\mu}_\omega\right)+\left(\frac{1}{\min _j n_j}-\frac{1}{\sum_j n_j}\right) \mu^2 .$$</span></span>
<span id="cb2-175"><a href="#cb2-175"></a><span class="at">&gt;</span></span>
<span id="cb2-176"><a href="#cb2-176"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@owen2013mcbook</span><span class="co">]</span></span>
<span id="cb2-177"><a href="#cb2-177"></a></span>
<span id="cb2-178"><a href="#cb2-178"></a>The heuristic behind the balance heuristic to make $\omega_j^{\mathrm{BH}} \propto n_{j}q_j(\boldsymbol{x})$ can be also understood as the more samples we have from a policy, the more information we have about it, thus the more weight it should have.</span>
<span id="cb2-179"><a href="#cb2-179"></a></span>
<span id="cb2-180"><a href="#cb2-180"></a><span class="fu">## IS with SLAPTs</span></span>
<span id="cb2-181"><a href="#cb2-181"></a></span>
<span id="cb2-182"><a href="#cb2-182"></a>The IS with SLAPTs is more difficult to formulate since we need to also consider the policy composition within a trajectory. As far as we know, the related discussion is in absense in previous works including @espeholt2020seedrl that first proposed the usage of SLAPTs.</span>
<span id="cb2-183"><a href="#cb2-183"></a></span>
<span id="cb2-184"><a href="#cb2-184"></a>In this section, we try to discuss the formulation and some properties of IS with SLAPTs that might be useful in practice.</span>
<span id="cb2-185"><a href="#cb2-185"></a></span>
<span id="cb2-186"><a href="#cb2-186"></a><span class="fu">### Trajectory-dependent Behavior Policy $\mu_{i}(\cdot \mid \boldsymbol{s})$</span></span>
<span id="cb2-187"><a href="#cb2-187"></a></span>
<span id="cb2-188"><a href="#cb2-188"></a>A general but trivial formulation for IS estimate with a batch of trajectory samples is to make the behavior policy trajectory-dependent, i.e., use $\mu_{i}(\boldsymbol{a} \mid \boldsymbol{s})$ for each trajectory $\tau_{i}$.</span>
<span id="cb2-189"><a href="#cb2-189"></a></span>
<span id="cb2-190"><a href="#cb2-190"></a>Now for the counter-example mentioned in @sec-intractable-global-behav-policy, $\mu_i(a \mid s)=\pi_{\theta_{n+m}}(a \mid s)$ and $\mu_j(a \mid s)=\pi_{\theta_{n+l}}(a \mid s)$ are obviously compatible.</span>
<span id="cb2-191"><a href="#cb2-191"></a></span>
<span id="cb2-192"><a href="#cb2-192"></a>With a batch of $N$ trajectory samples sampled from each own behavior policy $\boldsymbol{\tau}_1 \sim q_{\mu_1}, \ldots, \boldsymbol{\tau}_N \sim q_{\mu_N}$, we can only use the special case of the batch estimate @eq-is-batch with $N=1$, i.e., the single-sample estimate</span>
<span id="cb2-193"><a href="#cb2-193"></a></span>
<span id="cb2-194"><a href="#cb2-194"></a>$$</span>
<span id="cb2-195"><a href="#cb2-195"></a>\hat{\mathbb{E}}_{\mu_i}(\boldsymbol{\tau}_i) = \prod_{t=0}^{T-1} \frac{\pi_{\theta_{n+m}}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}{\mu_i(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}f(\boldsymbol{\tau}_i)</span>
<span id="cb2-196"><a href="#cb2-196"></a>$$ {#eq-is-single-sample}</span>
<span id="cb2-197"><a href="#cb2-197"></a></span>
<span id="cb2-198"><a href="#cb2-198"></a>Note that the single-sample estimate is also unbiased, i.e., the unbiasedness of @eq-is-batch does not depend on the sample size $N$.</span>
<span id="cb2-199"><a href="#cb2-199"></a></span>
<span id="cb2-200"><a href="#cb2-200"></a>The common practice to combine them into an estimate with the batch of samples is to average the single-sample estimates @eq-is-single-sample, i.e., </span>
<span id="cb2-201"><a href="#cb2-201"></a></span>
<span id="cb2-202"><a href="#cb2-202"></a>$$</span>
<span id="cb2-203"><a href="#cb2-203"></a>\hat{\mathbb{E}}_{\text{avg}} = \frac{1}{N} \sum_{i=1}^{N} \hat{\mathbb{E}}_{\mu_i} = \frac{1}{N} \sum_{i=1}^{N} \prod_{t=0}^{T-1} \frac{\pi_{\theta_{n+m}}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}{\mu_i(\boldsymbol{a}_{t} \mid \boldsymbol{s}_{t})}f(\boldsymbol{\tau}_i)</span>
<span id="cb2-204"><a href="#cb2-204"></a>$$ {#eq-is-single-sample-avg}</span>
<span id="cb2-205"><a href="#cb2-205"></a></span>
<span id="cb2-206"><a href="#cb2-206"></a>With the linearity of expectation, it is obvious to see the unbiasedness of the average estimate $\hat{\mathbb{E}}_{\text{avg}}$.</span>
<span id="cb2-207"><a href="#cb2-207"></a></span>
<span id="cb2-208"><a href="#cb2-208"></a>Now let's take a look at the variance. Let the variance of each single-sample estimate be $\sigma^2_{\mu_i}$, since the samples are independent, the variance of $\hat{\mathbb{E}}_{\text{avg}}$ is:</span>
<span id="cb2-209"><a href="#cb2-209"></a></span>
<span id="cb2-210"><a href="#cb2-210"></a>$$</span>
<span id="cb2-211"><a href="#cb2-211"></a>\sigma^2_{\text{avg}} = \frac{1}{N^2} \sum_{i=1}^{N} \sigma^2_{\mu_i}</span>
<span id="cb2-212"><a href="#cb2-212"></a>$$ {#eq-is-single-sample-avg-var}</span>
<span id="cb2-213"><a href="#cb2-213"></a></span>
<span id="cb2-214"><a href="#cb2-214"></a>Given the batch variance is determined by composing the variances of the single-sample estimates, we can first try to analyze the variance of the single-sample estimate.</span>
<span id="cb2-215"><a href="#cb2-215"></a></span>
<span id="cb2-216"><a href="#cb2-216"></a><span class="fu">### Variance of Single-Sample Estimate -- MCSPT vs. SLAPT</span></span>
<span id="cb2-217"><a href="#cb2-217"></a></span>
<span id="cb2-218"><a href="#cb2-218"></a>MCSPT and SLAPT just form two different cases of the single-sample estimate. We might wonder that, given a trajectory $\boldsymbol{\tau}_{i}$, which of</span>
<span id="cb2-219"><a href="#cb2-219"></a></span>
<span id="cb2-220"><a href="#cb2-220"></a>1) the consistent old policy $\pi_{\theta_{n+m}}$ and</span>
<span id="cb2-221"><a href="#cb2-221"></a>2) the sliding latest policy $\mu_{\theta_{n},M}$ using $\pi_{\theta_{n}},\ldots,\pi_{\theta_{n+M}}$ successively</span>
<span id="cb2-222"><a href="#cb2-222"></a></span>
<span id="cb2-223"><a href="#cb2-223"></a>is better for the unbiased single-sample IS estimate @eq-is-single-sample, i.e., leads to a lower variance?</span>
<span id="cb2-224"><a href="#cb2-224"></a></span>
<span id="cb2-225"><a href="#cb2-225"></a>@metelli2020pois provided a family of bounds of the variance of IS estimate in terms of the <span class="co">[</span><span class="ot">Rényi divergence</span><span class="co">](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy?oldformat=true#R%C3%A9nyi_divergence)</span>:</span>
<span id="cb2-226"><a href="#cb2-226"></a></span>
<span id="cb2-227"><a href="#cb2-227"></a><span class="at">&gt; Lemma 1. Let $P$ and $Q$ be two probability measures on the measurable space $(\mathcal{X}, \mathscr{F})$ such that $P \ll Q$. Let $\alpha \in</span><span class="co">[</span><span class="ot">1,+\infty</span><span class="co">]</span><span class="at">, \mathbf{x}=\left(x_1, x_2, \ldots, x_N\right)^T$ be i.i.d. random variables sampled from $Q$ and $f: \mathcal{X} \rightarrow \mathbb{R}$ be a function with bounded $\frac{2 \alpha}{\alpha-1}$-moment under $Q\left(\|f\|_{Q, \frac{2 \alpha}{\alpha-1}}&lt;+\infty\right)$. Then, for any $N&gt;0$, the variance of the IS estimator $\widehat{\mu}_{P / Q}$ can be upper bounded as:</span></span>
<span id="cb2-228"><a href="#cb2-228"></a><span class="at">&gt;</span></span>
<span id="cb2-229"><a href="#cb2-229"></a><span class="at">&gt; $$\operatorname{Var}_{\mathbf{x} \sim Q}\left[\hat{\mu}_{P / Q}\right] \leqslant \frac{1}{N}\|f\|_{Q, \frac{2 \alpha}{\alpha-1}}^2 d_{2 \alpha}(P </span><span class="sc">\|</span><span class="at"> Q)^{2-\frac{1}{\alpha}},$$</span></span>
<span id="cb2-230"><a href="#cb2-230"></a><span class="at">&gt;</span></span>
<span id="cb2-231"><a href="#cb2-231"></a><span class="at">&gt; where we used the abbreviation $\mathbf{x} \sim Q$ for denoting $x_i \sim Q$ for all $i=1,2, \ldots, N$ all independent.</span></span>
<span id="cb2-232"><a href="#cb2-232"></a><span class="at">&gt;</span></span>
<span id="cb2-233"><a href="#cb2-233"></a><span class="at">&gt; This result generalizes Lemma 4.1 of Metelli et al. (2018), that can be recovered by setting $\alpha=1$ under the condition that $</span><span class="sc">\|</span><span class="at">f</span><span class="sc">\|</span><span class="at">_{\infty}&lt;+\infty$ :</span></span>
<span id="cb2-234"><a href="#cb2-234"></a><span class="at">&gt;</span></span>
<span id="cb2-235"><a href="#cb2-235"></a><span class="at">&gt; $$\operatorname{Var}_{\mathbf{x} \sim Q}\left[\widehat{\mu}_{P / Q}\right] \leqslant \frac{1}{N}\|f\|_{\infty}^2 d_2(P </span><span class="sc">\|</span><span class="at"> Q) .$$</span></span>
<span id="cb2-236"><a href="#cb2-236"></a></span>
<span id="cb2-237"><a href="#cb2-237"></a>When $\alpha = 1$, the Rényi divergence is the <span class="co">[</span><span class="ot">Kullback-Leibler divergence</span><span class="co">](https://en.wikipedia.org/wiki/Kullback-Leibler_divergence)</span> widely used in RL analysis.</span>
<span id="cb2-238"><a href="#cb2-238"></a></span>
<span id="cb2-239"><a href="#cb2-239"></a></span>
<span id="cb2-240"><a href="#cb2-240"></a><span class="at">&gt; When $P=Q$ almost everywhere, we get $\operatorname{Var}_{\mathbf{x} \sim Q}\left[\hat{\mu}_{Q / Q}\right] \leqslant \frac{1}{N}\|f\|_{\infty}^2$, a well-known upper bound to the variance of a Monte Carlo estimator. Recalling the definition of ESS (Equation 7) we can rewrite the previous bound as:</span></span>
<span id="cb2-241"><a href="#cb2-241"></a><span class="at">&gt;</span></span>
<span id="cb2-242"><a href="#cb2-242"></a><span class="at">&gt; $$\underset{\mathbf{x} \sim Q}{\operatorname{Var}}\left</span><span class="co">[</span><span class="ot">\hat{\mu}_{P / Q}\right</span><span class="co">]</span><span class="at"> \leqslant \frac{</span><span class="sc">\|</span><span class="at">f</span><span class="sc">\|</span><span class="at">_{\infty}^2}{\operatorname{ESS}(P </span><span class="sc">\|</span><span class="at"> Q)} .$$</span></span>
<span id="cb2-243"><a href="#cb2-243"></a><span class="at">&gt;</span></span>
<span id="cb2-244"><a href="#cb2-244"></a><span class="at">&gt; Thus, the variance scales with ESS instead of $N$, justifying the definition of ESS.</span></span>
<span id="cb2-245"><a href="#cb2-245"></a></span>
<span id="cb2-246"><a href="#cb2-246"></a>In our context, $P$ is the target distribution $p_{\theta}$ and $Q$ is $\mu_{\theta_{n},M}$ or $\pi_{\theta_{n}}$.</span>
<span id="cb2-247"><a href="#cb2-247"></a></span>
<span id="cb2-248"><a href="#cb2-248"></a>It is possible that $d_{2 \alpha}(p_{\theta} <span class="sc">\|</span> q_{\theta_{n},M}) &lt; d_{2 \alpha}(p_{\theta} <span class="sc">\|</span> p_{\theta_{n}})$, since the newer the policy is, the more similar its induced distribution is to $p_{\theta}$. Then as long as the $<span class="sc">\|</span>f<span class="sc">\|</span>_{q_{\theta_{n},M}, \frac{2 \alpha}{\alpha-1}}$ is not much larger than $\|f\|_{p_{\theta_{n}}, \frac{2 \alpha}{\alpha-1}}$, the estimate using $\mu_{\theta_{n},M}$ has better guarantee than the estimate using $\pi_{\theta_{n}}$.</span>
<span id="cb2-249"><a href="#cb2-249"></a></span>
<span id="cb2-250"><a href="#cb2-250"></a>In practice, the final effectiveness of the IS estimate usually relies on empirical diagnostic metrics like <span class="co">[</span><span class="ot">Effective Sample Size (ESS)</span><span class="co">](https://mc-stan.org/docs/reference-manual/analysis.html#effective-sample-size.section)</span>. For example, @piché2025pipelinerl measured the ESS of different sampling policies they used.</span>
<span id="cb2-251"><a href="#cb2-251"></a></span>
<span id="cb2-252"><a href="#cb2-252"></a><span class="fu">### Minimizing Variance by Optimizing the Weighting</span></span>
<span id="cb2-253"><a href="#cb2-253"></a></span>
<span id="cb2-254"><a href="#cb2-254"></a>Beyond the comparison on each single-sample estimate, we can also consider the optimal weighting of them. In @eq-is-single-sample-avg, we use the simplest average weight $\frac{1}{N}$ for each single-sample estimate. But we can actually use any other weight functions $\omega_i(\boldsymbol{x})$ to combine them, as long as they form a *partition of unity*, i.e., a collection of $J \geqslant 1$ weight functions $\omega_j(\boldsymbol{x}) \geqslant 0$ which satisfy $\sum_{j=1}^J \omega_j(\boldsymbol{x})=1$ for all $\boldsymbol{x}$. Different partitions of unity will lead to estimates that are all unbiased but with different variances.</span>
<span id="cb2-255"><a href="#cb2-255"></a></span>
<span id="cb2-256"><a href="#cb2-256"></a>The optimal weighting should be still an open problem that is beyond our scope. However, there is still some properties that we can exploit from the SLAPT structure. For example, in practical systems, SLAPTs of similar lengths often conform to identical distributions (approximately, but this can be exact if we limit the timesteps where a trajectory can be aborted), we can also apply the formulation of Multiple IS to SLAPT like in @sec-is-mcspt. This also supports the comparability of SLAPT and MCSPT.</span>
<span id="cb2-257"><a href="#cb2-257"></a></span>
<span id="cb2-258"><a href="#cb2-258"></a><span class="fu"># Citation {.appendix .unnumbered}</span></span>
<span id="cb2-259"><a href="#cb2-259"></a></span>
<span id="cb2-260"><a href="#cb2-260"></a>BibTeX:</span>
<span id="cb2-261"><a href="#cb2-261"></a></span>
<span id="cb2-262"><a href="#cb2-262"></a><span class="in">```BibTeX</span></span>
<span id="cb2-263"><a href="#cb2-263"></a><span class="in">@article{tong2026isdr,</span></span>
<span id="cb2-264"><a href="#cb2-264"></a><span class="in">  author = {Yuxuan Tong and Yingru Li and Guangming Sheng},</span></span>
<span id="cb2-265"><a href="#cb2-265"></a><span class="in">  title = {{Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems}},</span></span>
<span id="cb2-266"><a href="#cb2-266"></a><span class="in">  journal = {Blog},</span></span>
<span id="cb2-267"><a href="#cb2-267"></a><span class="in">  date = {2026-01-12},</span></span>
<span id="cb2-268"><a href="#cb2-268"></a><span class="in">  url = {https://tongyx361.github.io/posts/isdr},</span></span>
<span id="cb2-269"><a href="#cb2-269"></a><span class="in">  language = {English},</span></span>
<span id="cb2-270"><a href="#cb2-270"></a><span class="in">}</span></span>
<span id="cb2-271"><a href="#cb2-271"></a><span class="in">```</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>