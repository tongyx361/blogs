<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Shawn/Yuxuan Tong 童雨轩</title>
<link>https://tongyx361.github.io/blogs/</link>
<atom:link href="https://tongyx361.github.io/blogs/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Wed, 07 Jan 2026 00:00:00 GMT</lastBuildDate>
<item>
  <title>IS with BET</title>
  <dc:creator>Yuxuan Tong</dc:creator>
  <dc:creator>Yingru Li</dc:creator>
  <dc:creator>Guangming Sheng</dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/is-bet/</link>
  <description><![CDATA[ 






<section id="is-with-bet" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> IS with BET?</h1>
<section id="importance-sampling-is" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="importance-sampling-is"><span class="header-section-number">1.1</span> Importance Sampling (IS)</h2>
<p>It is a common practice to optimize LLM policy on off-policy data with Importance Sampling (IS):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Cmu%7D%20=%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cboldsymbol%7B%5Ctau%7D)%7D%7Bq_%7B%5Cmu%7D(%5Cboldsymbol%7B%5Ctau%7D)%7Df(%5Cboldsymbol%7B%5Ctau%7D),%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f(%5Cboldsymbol%7B%5Ctau%7D)"> can be the gradient of the objective function to be optimized, <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D(%5Cboldsymbol%7B%5Ctau%7D)"> is the trajectory distribution induced by the target policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> and <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cmu%7D(%5Cboldsymbol%7B%5Ctau%7D)"> is the trajectory distribution induced by the behavior policy <img src="https://latex.codecogs.com/png.latex?%5Cmu">.</p>
</section>
<section id="best-effort-trajectory-bet" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="best-effort-trajectory-bet"><span class="header-section-number">1.2</span> Best-Effort Trajectory (BET)</h2>
<p>We use <em>Best-Effort Trajectories</em> (BETs) to refer to the trajectories sampled in a Reinforcement Learning (RL) system where we always use the latest policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bt%7D%7D"> at each moment to sample the actions.</p>
<p>Formally, a BET can be defined as a trajectory <img src="https://latex.codecogs.com/png.latex?%5Ctau%20=%20(s_%7B0%7D,%20a_%7B0,%5Cpi_%7B%5Ctheta_%7Bn%7D%7D%7D,%20%5Cldots,%20s_%7Bt%7D,%20a_%7Bt,%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D%7D,%20%5Cldots,%20s_%7BT%7D,%20a_%7BT,%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D%7D)">, where <img src="https://latex.codecogs.com/png.latex?s_%7Bt%7D"> is the state at timestep <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?a_%7Bt,%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D%7D"> is the action sampled from the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D"> that is the latest policy available in the system when sampling. So for <img src="https://latex.codecogs.com/png.latex?l%20%3E%20m">, <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta+%7Bn+l%7D%7D"> is usually more on-policy than <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D">.</p>
<p>BETs are off-policy by definition, but are as close to on-policy as possible with the best effort in the system.</p>
</section>
<section id="partial-rollout" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="partial-rollout"><span class="header-section-number">1.3</span> Partial Rollout</h2>
<p>BETs are a natural result of a popular design choice called <em>Partial Rollout</em> in LLM RL systems nowadays, such as Kimi k1.5 <span class="citation" data-cites="kimi2025k15">(Kimi Team et al. 2025)</span>, AReaL <span class="citation" data-cites="fu2025areal">(Fu et al. 2025)</span> and PipelineRL <span class="citation" data-cites="piché2025pipelinerl">(Piché et al. 2025)</span>, etc., which can date back to a traditional Distributed RL system called SEED RL <span class="citation" data-cites="espeholt2020seedrl">(Espeholt et al. 2020)</span>:</p>
<p>The ongoing trajectory rollouts are</p>
<ol type="1">
<li>aborted when
<ul>
<li>either, in synchoronous systems like Kimi k1.5, there are enough samples collected for training, releasing the resources for the training engine to update the model weights,</li>
<li>or, in asynchronous systems like AReaL and PipelineRL, a new version of model weights is produced by the trainer;</li>
</ul></li>
<li>continued with the latest version of model weights.</li>
</ol>
<p>Partial Rollout is motivated by its efficiency and simplicity. Let me explain in detail.</p>
<section id="efficiency-eliminating-long-tail-bubbles" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="efficiency-eliminating-long-tail-bubbles"><span class="header-section-number">1.3.1</span> Efficiency: Eliminating Long-Tail Bubbles</h3>
<p>The earliest LLM RL systems <span class="citation" data-cites="yao2023dschat">Hu et al. (2025)</span> all adopt synchoronous architectures, where the trainer always waits for all the trajectories are finished before updating the weights with these data.</p>
<p>However, as the context length of LLMs scales up, the skewness of the trajectory length distribution becomes increasing heavy. If the distribution is very skewed but all the trajectories are required to finish in the same rollout stage, there can be only a few long-tail requests remaining in the system, causing severe under-utilization (typically &lt;30% in practice). <span class="citation" data-cites="sheng2025laminar">(Sheng, Tong, et al. 2025)</span></p>
<p>Kimi k1.5 proposes to fix this issue by aborting all the ongoing rollouts once enough training samples are collected and directly update the weights with these data, instead of waiting for all the trajectories to finish, and then continue the rollouts with the new model weights.</p>
</section>
<section id="simplicity-only-maintaining-one-version-of-model-weights" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="simplicity-only-maintaining-one-version-of-model-weights"><span class="header-section-number">1.3.2</span> Simplicity: Only Maintaining One Version of Model Weights</h3>
<p>In asynchornous RL systems with an experience buffer, it is troublesome to mamage multiple versions of model weights within the rollout engine.</p>
<p>AReaL proposes to always only maintain the latest model weights across all the instances. Once a new version of model weights is produced by the trainer, all the rollouts of the stale policy are aborted and then continued with the latest policy.</p>
<p>This can be simply implemented by always loading the latest weights into all the inference engine instances, avoiding the bothering to manage the requests across each instance.</p>
</section>
</section>
</section>
<section id="is-with-bet-1" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> IS with BET!</h1>
<p>Despite Partial Rollout’s efficiency and simplicity, there have been worries about mixing multiple importance sampling with BET, since most previous works formulate IS with a single consistent behavior policy <img src="https://latex.codecogs.com/png.latex?%5Cmu(a%20%5Cmid%20s)">, and it feels weird to mix multiple policies <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn%7D%7D,%5Cldots,%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D,%5Cldots,%5Cpi_%7B%5Ctheta_%7Bn+M%7D%7D"> within a single trajectory.</p>
<section id="is-with-global-behavior-policy-muboldsymbola-mid-boldsymbols" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="is-with-global-behavior-policy-muboldsymbola-mid-boldsymbols"><span class="header-section-number">2.1</span> IS with Global Behavior Policy <img src="https://latex.codecogs.com/png.latex?%5Cmu(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"></h2>
<p>Since there is always an actual distribution we are sampling from, we can always formulate it as a global behavior policy <img src="https://latex.codecogs.com/png.latex?%5Cmu(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)">.</p>
<p>It is natural to try to use <img src="https://latex.codecogs.com/png.latex?%5Cmu(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> for IS. So how can we calculate it?</p>
<p>It is easy to notice that, for each trajectory of an LLM policy, we can usually construct a behavior policy <img src="https://latex.codecogs.com/png.latex?%5Cforall%20t,%20%5Cmu(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> satisfies that <img src="https://latex.codecogs.com/png.latex?%5Cmu(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)%20=%20%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)"> at every timestep <img src="https://latex.codecogs.com/png.latex?t">, since the LLM is usually auto-regressive and thus never revisits a past state within the same trajectory.</p>
<p>However, it might be confusing when we also notice that, for the same state <img src="https://latex.codecogs.com/png.latex?s"> in two different BETs <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7Bj%7D">, the same action <img src="https://latex.codecogs.com/png.latex?a"> might be sampled from different policies <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn+l%7D%7D"> (<img src="https://latex.codecogs.com/png.latex?l%20%5Cneq%20m">) respectively. It is likely that <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)%20%5Cneq%20%5Cpi_%7B%5Ctheta_%7Bn+l%7D%7D(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)">, making it impossible to simply construct the same behavior policy for both BETs, i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmu(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)=%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)"> constradicts <img src="https://latex.codecogs.com/png.latex?%5Cmu(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)=%5Cpi_%7B%5Ctheta_%7Bn+l%7D%7D(a_%7Bt%7D%20%5Cmid%20s_%7Bt%7D)">.</p>
<p>So where is the problem of the construction for <img src="https://latex.codecogs.com/png.latex?%5Cmu(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> mentioned above?</p>
<p>The problem hidden here is that, <img src="https://latex.codecogs.com/png.latex?%5Cmu(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> does not consider the probability distribution of which LLM policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D"> is used, but this distribution is intractable since it depends on the system dynamics.</p>
</section>
<section id="is-with-trajectory-wise-behavior-policy-mu_iboldsymbola-mid-boldsymbols" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="is-with-trajectory-wise-behavior-policy-mu_iboldsymbola-mid-boldsymbols"><span class="header-section-number">2.2</span> IS with Trajectory-wise Behavior Policy <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bi%7D(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"></h2>
<p>A trivial solution is to construct a trajectory-wise behavior policy <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bi%7D(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> for each trajectory <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7Bi%7D">.</p>
<p>Now for the counter-example mentioned above, <img src="https://latex.codecogs.com/png.latex?%5Cmu_i(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)=%5Cpi_%7B%5Ctheta_%7Bn+m%7D%7D(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu_j(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)=%5Cpi_%7B%5Ctheta_%7Bn+l%7D%7D(%5Cboldsymbol%7Ba%7D%20%5Cmid%20%5Cboldsymbol%7Bs%7D)"> are obviously compatible.</p>
<p>With a batch of <img src="https://latex.codecogs.com/png.latex?N"> trajectory samples sampled from each own behavior policy <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5Ctau%7D_1%20%5Csim%20q_%7B%5Cmu_1%7D,%20%5Cldots,%20%5Cboldsymbol%7B%5Ctau%7D_N%20%5Csim%20q_%7B%5Cmu_N%7D">, it is easy to formulate single-sample estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Cmu_i%7D%20=%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cboldsymbol%7B%5Ctau%7D_i)%7D%7Bq_%7B%5Cmu_i%7D(%5Cboldsymbol%7B%5Ctau%7D_i)%7Df(%5Cboldsymbol%7B%5Ctau%7D_i)">.</p>
<p>Note that the single-sample estimate is already unbiased, i.e., the unbiasedness does not depend on the sample size.</p>
<p>The common practice to combine them for an estimate is to average the single-sample estimates <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Cmu_i%7D,%20i%20=%201,%20%5Cldots,%20N">, i.e., <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Ctext%7Bavg%7D%7D%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Cmu_i%7D">.</p>
<p>Due to linearity of expectation, <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Ctext%7Bavg%7D%7D"> is still unbiased.</p>
<p>Now let’s take a look at the variance. Let the variance of each single-sample estimate be <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_%7B%5Cmu_i%7D">, since the samples are independent, the variance of the average estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbb%7BE%7D%7D_%7B%5Ctext%7Bavg%7D%7D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2_%7B%5Ctext%7Bavg%7D%7D%20=%20%5Cfrac%7B1%7D%7BN%5E2%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csigma%5E2_%7B%5Cmu_i%7D%0A"></p>
</section>
<section id="mixed-best-effort-policy-might-have-lower-variance-than-consistent-old-policy" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="mixed-best-effort-policy-might-have-lower-variance-than-consistent-old-policy"><span class="header-section-number">2.3</span> Mixed Best-Effort Policy Might Have Lower Variance than Consistent Old Policy</h2>
<p>Now we wonder, given a trajectory <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5Ctau%7D_%7Bi%7D">, which of the mixed best-effort policy <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7B%5Ctheta_%7Bn%7D,M%7D"> using <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn%7D%7D,%5Cldots,%5Cpi_%7B%5Ctheta_%7Bn+M%7D%7D"> and the consistent old policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn%7D%7D"> is better for IS estimate, i.e., leads to a lower variance?</p>
<p><span class="citation" data-cites="metelli2020pois">Metelli et al. (2020)</span> provided a family of bounds of the variance of IS estimate in terms of the <a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy?oldformat=true#R%C3%A9nyi_divergence">Rényi divergence</a>:</p>
<blockquote class="blockquote">
<p>Lemma 1. Let <img src="https://latex.codecogs.com/png.latex?P"> and <img src="https://latex.codecogs.com/png.latex?Q"> be two probability measures on the measurable space <img src="https://latex.codecogs.com/png.latex?(%5Cmathcal%7BX%7D,%20%5Cmathscr%7BF%7D)"> such that <img src="https://latex.codecogs.com/png.latex?P%20%5Cll%20Q">. Let <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cin%5B1,+%5Cinfty%5D,%20%5Cmathbf%7Bx%7D=%5Cleft(x_1,%20x_2,%20%5Cldots,%20x_N%5Cright)%5ET"> be i.i.d. random variables sampled from <img src="https://latex.codecogs.com/png.latex?Q"> and <img src="https://latex.codecogs.com/png.latex?f:%20%5Cmathcal%7BX%7D%20%5Crightarrow%20%5Cmathbb%7BR%7D"> be a function with bounded <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%20%5Calpha%7D%7B%5Calpha-1%7D">-moment under <img src="https://latex.codecogs.com/png.latex?Q%5Cleft(%5C%7Cf%5C%7C_%7BQ,%20%5Cfrac%7B2%20%5Calpha%7D%7B%5Calpha-1%7D%7D%3C+%5Cinfty%5Cright)">. Then, for any <img src="https://latex.codecogs.com/png.latex?N%3E0">, the variance of the IS estimator <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmu%7D_%7BP%20/%20Q%7D"> can be upper bounded as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BVar%7D_%7B%5Cmathbf%7Bx%7D%20%5Csim%20Q%7D%5Cleft%5B%5Chat%7B%5Cmu%7D_%7BP%20/%20Q%7D%5Cright%5D%20%5Cleqslant%20%5Cfrac%7B1%7D%7BN%7D%5C%7Cf%5C%7C_%7BQ,%20%5Cfrac%7B2%20%5Calpha%7D%7B%5Calpha-1%7D%7D%5E2%20d_%7B2%20%5Calpha%7D(P%20%5C%7C%20Q)%5E%7B2-%5Cfrac%7B1%7D%7B%5Calpha%7D%7D,"></p>
<p>where we used the abbreviation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Csim%20Q"> for denoting <img src="https://latex.codecogs.com/png.latex?x_i%20%5Csim%20Q"> for all <img src="https://latex.codecogs.com/png.latex?i=1,2,%20%5Cldots,%20N"> all independent.</p>
<p>This result generalizes Lemma 4.1 of Metelli et al.&nbsp;(2018), that can be recovered by setting <img src="https://latex.codecogs.com/png.latex?%5Calpha=1"> under the condition that <img src="https://latex.codecogs.com/png.latex?%5C%7Cf%5C%7C_%7B%5Cinfty%7D%3C+%5Cinfty"> :</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BVar%7D_%7B%5Cmathbf%7Bx%7D%20%5Csim%20Q%7D%5Cleft%5B%5Cwidehat%7B%5Cmu%7D_%7BP%20/%20Q%7D%5Cright%5D%20%5Cleqslant%20%5Cfrac%7B1%7D%7BN%7D%5C%7Cf%5C%7C_%7B%5Cinfty%7D%5E2%20d_2(P%20%5C%7C%20Q)%20."></p>
</blockquote>
<p>When <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201">, the Rényi divergence is the <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> widely used in RL analysis.</p>
<blockquote class="blockquote">
<p>When <img src="https://latex.codecogs.com/png.latex?P=Q"> almost everywhere, we get <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BVar%7D_%7B%5Cmathbf%7Bx%7D%20%5Csim%20Q%7D%5Cleft%5B%5Chat%7B%5Cmu%7D_%7BQ%20/%20Q%7D%5Cright%5D%20%5Cleqslant%20%5Cfrac%7B1%7D%7BN%7D%5C%7Cf%5C%7C_%7B%5Cinfty%7D%5E2">, a well-known upper bound to the variance of a Monte Carlo estimator. Recalling the definition of ESS (Equation 7) we can rewrite the previous bound as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cunderset%7B%5Cmathbf%7Bx%7D%20%5Csim%20Q%7D%7B%5Coperatorname%7BVar%7D%7D%5Cleft%5B%5Chat%7B%5Cmu%7D_%7BP%20/%20Q%7D%5Cright%5D%20%5Cleqslant%20%5Cfrac%7B%5C%7Cf%5C%7C_%7B%5Cinfty%7D%5E2%7D%7B%5Coperatorname%7BESS%7D(P%20%5C%7C%20Q)%7D%20."></p>
<p>Thus, the variance scales with ESS instead of <img src="https://latex.codecogs.com/png.latex?N">, justifying the definition of ESS.</p>
</blockquote>
<p>In our context, <img src="https://latex.codecogs.com/png.latex?P"> is the target distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> and <img src="https://latex.codecogs.com/png.latex?Q"> is <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7B%5Ctheta_%7Bn%7D,M%7D"> or <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn%7D%7D">.</p>
<p>Intuitively, it is likely that <img src="https://latex.codecogs.com/png.latex?d_%7B2%20%5Calpha%7D(p_%7B%5Ctheta%7D%20%5C%7C%20q_%7B%5Ctheta_%7Bn%7D,M%7D)%20%3C%20d_%7B2%20%5Calpha%7D(p_%7B%5Ctheta%7D%20%5C%7C%20p_%7B%5Ctheta_%7Bn%7D%7D)">, since the newer the policy is, the more similar its induced distribution is to <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D">.</p>
<p>As long as the <img src="https://latex.codecogs.com/png.latex?%5C%7Cf%5C%7C_%7Bq_%7B%5Ctheta_%7Bn%7D,M%7D,%20%5Cfrac%7B2%20%5Calpha%7D%7B%5Calpha-1%7D%7D"> is not much larger than <img src="https://latex.codecogs.com/png.latex?%5C%7Cf%5C%7C_%7Bp_%7B%5Ctheta_%7Bn%7D%7D,%20%5Cfrac%7B2%20%5Calpha%7D%7B%5Calpha-1%7D%7D">, the estimate using <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7B%5Ctheta_%7Bn%7D,M%7D"> has better guarantee than the estimate using <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn%7D%7D">.</p>
<p>These conjectures can be verified by empirical experiments. For example, Figure 6 and 7 by <span class="citation" data-cites="piché2025pipelinerl">Piché et al. (2025)</span> show that <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7B%5Ctheta_%7Bn%7D,M%7D"> has higher ESS and lower KL divergence relative to <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> than <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7Bn%7D%7D">, respectively.</p>
<p>We are also planning experiments to further verify these conjectures.</p>
</section>
<section id="exploiting-approximatly-identical-distribution-with-multiple-is" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="exploiting-approximatly-identical-distribution-with-multiple-is"><span class="header-section-number">2.4</span> Exploiting (Approximatly) Identical Distribution with Multiple IS</h2>
<p>Besides the likely superiority for each single-sample estimate, the combination of multiple importance sampling has other properties that can be exploited.</p>
<p>Formally, the combination can actually generalize from average weight <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7BN%7D"> to any “partion of unity”, which is a collection of <img src="https://latex.codecogs.com/png.latex?J%20%5Cgeqslant%201"> weight functions <img src="https://latex.codecogs.com/png.latex?%5Comega_j(%5Cboldsymbol%7Bx%7D)%20%5Cgeqslant%200"> which satisfy <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bj=1%7D%5EJ%20%5Comega_j(%5Cboldsymbol%7Bx%7D)=1"> for all <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7Bx%7D">. Different partitions of unity will lead to estimates that are all unbiased but with different variances.</p>
<p>Given the property that BETs of similar lengths usally conform to identical distributions (approximately, but this can be exact if we limit the timesteps where a trajectory can be aborted), we can reformulate the estimation as Multiple Importance Sampling <span class="citation" data-cites="owen2013mcbook">(Owen 2013)</span>:</p>
<blockquote class="blockquote">
<p>Suppose that <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%20%5Csim%20q_j"> for <img src="https://latex.codecogs.com/png.latex?i=1,%20%5Cldots,%20n_j"> and <img src="https://latex.codecogs.com/png.latex?j=1,%20%5Cldots,%20J"> and that <img src="https://latex.codecogs.com/png.latex?%5Comega_j"> are a partition of unity. The multiple importance sampling estimate is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7B%5Cmu%7D_%5Comega=%5Csum_%7Bj=1%7D%5EJ%20%5Cfrac%7B1%7D%7Bn_j%7D%20%5Csum_%7Bi=1%7D%5E%7Bn_j%7D%20%5Comega_j%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%20%5Cfrac%7Bf%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%20p%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%7D%7Bq_j%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%7D%20."> <span class="citation" data-cites="owen2013mcbook">(Owen 2013)</span></p>
<p>Now assume that <img src="https://latex.codecogs.com/png.latex?q_j(%5Cboldsymbol%7Bx%7D)%3E0"> whenever <img src="https://latex.codecogs.com/png.latex?%5Comega_j(%5Cboldsymbol%7Bx%7D)%20p(%5Cboldsymbol%7Bx%7D)%20f(%5Cboldsymbol%7Bx%7D)%20%5Cneq%200">. Then multiple importance sampling is unbiased, because <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Cleft(%5Cwidetilde%7B%5Cmu%7D_%5Comega%5Cright)=%5Csum_%7Bj=1%7D%5EJ%20%5Cmathbb%7BE%7D_%7Bq_j%7D%5Cleft(%5Comega_j(%5Cboldsymbol%7BX%7D)%20%5Cfrac%7Bf(%5Cboldsymbol%7BX%7D)%20p(%5Cboldsymbol%7BX%7D)%7D%7Bq_j(%5Cboldsymbol%7BX%7D)%7D%5Cright)=%5Csum_%7Bj=1%7D%5EJ%20%5Cint%20%5Comega_j(%5Cboldsymbol%7Bx%7D)%20f(%5Cboldsymbol%7Bx%7D)%20p(%5Cboldsymbol%7Bx%7D)%20%5Cmathrm%7Bd%7D%20%5Cboldsymbol%7Bx%7D=%5Cmu%20."></p>
<p>Among the proposals for functions <img src="https://latex.codecogs.com/png.latex?%5Comega_j(%5Cboldsymbol%7Bx%7D)">, the most studied one is the balance heuristic with <img src="https://latex.codecogs.com/png.latex?%5Comega_j(%5Cboldsymbol%7Bx%7D)%20%5Cpropto%20n_j%20q_j(%5Cboldsymbol%7Bx%7D)">, that is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Comega_j(%5Cboldsymbol%7Bx%7D)=%5Comega_j%5E%7B%5Cmathrm%7BBH%7D%7D(%5Cboldsymbol%7Bx%7D)%20%5Cequiv%20%5Cfrac%7Bn_j%20q_j(%5Cboldsymbol%7Bx%7D)%7D%7B%5Csum_%7Bk=1%7D%5EJ%20n_k%20q_k(%5Cboldsymbol%7Bx%7D)%7D%20."></p>
<p>By construction <img src="https://latex.codecogs.com/png.latex?q_j(%5Cboldsymbol%7Bx%7D)%3E0"> holds whenever <img src="https://latex.codecogs.com/png.latex?%5Cleft(%5Comega_j%5E%7B%5Cmathrm%7BBH%7D%7D%20p%20f%5Cright)(%5Cboldsymbol%7Bx%7D)%20%5Cneq%200">. Let <img src="https://latex.codecogs.com/png.latex?n=%5Csum_%7Bj=1%7D%5EJ%20n_j"> and define <img src="https://latex.codecogs.com/png.latex?%5Calpha_j=n_j%20/%20n">. Then using the balance heuristic, <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7B%5Cmu%7D_%7B%5Comega%5E%7B%5Ctext%20%7B%D0%92%D0%9D%20%7D%7D%7D"> simplifies to <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7B%5Cmu%7D_%5Calpha=%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bj=1%7D%5EJ%20%5Csum_%7Bi=1%7D%5E%7Bn_j%7D%20%5Cfrac%7Bf%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%20p%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%7D%7B%5Csum_%7Bj=1%7D%5EJ%20%5Calpha_j%20q_j%5Cleft(%5Cboldsymbol%7BX%7D_%7Bi%20j%7D%5Cright)%7D%20."></p>
<p>In other words, multiple importance sampling, with weights from the balance heuristic reduces to the same estimator we would use in mixture importance sampling with mixture weights <img src="https://latex.codecogs.com/png.latex?%5Calpha_j=n_j%20/%20n">. Once again, the weight on a given sampled value <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7BX%7D_%7Bi%20j%7D"> does not depend on which mixture component it came from. The balance heuristic is nearly optimal in the following sense:</p>
<p>Theorem 9.8. Let <img src="https://latex.codecogs.com/png.latex?n_j%20%5Cgeqslant%201"> be positive integers for <img src="https://latex.codecogs.com/png.latex?j=1,%20%5Cldots,%20J">. Let <img src="https://latex.codecogs.com/png.latex?%5Comega_1,%20%5Cldots,%20%5Comega_J"> be a partition of unity and let <img src="https://latex.codecogs.com/png.latex?%5Comega%5E%7B%5Cmathrm%7BBH%7D%7D"> be the balance heuristic. Suppose that <img src="https://latex.codecogs.com/png.latex?q_j(%5Cboldsymbol%7Bx%7D)%3E"> 0 whenever <img src="https://latex.codecogs.com/png.latex?%5Comega_j(%5Cboldsymbol%7Bx%7D)%20p(%5Cboldsymbol%7Bx%7D)%20f(%5Cboldsymbol%7Bx%7D)%20%5Cneq%200">. Then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BVar%7D%5Cleft(%5Cwidetilde%7B%5Cmu%7D_%7B%5Comega%5E%7B%5Cmathrm%7BBH%7D%7D%7D%5Cright)%20%5Cleqslant%20%5Coperatorname%7BVar%7D%5Cleft(%5Cwidetilde%7B%5Cmu%7D_%5Comega%5Cright)+%5Cleft(%5Cfrac%7B1%7D%7B%5Cmin%20_j%20n_j%7D-%5Cfrac%7B1%7D%7B%5Csum_j%20n_j%7D%5Cright)%20%5Cmu%5E2%20."></p>
</blockquote>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="citation" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Citation</h2><div class="quarto-appendix-contents">

<p>BibTeX:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource bibtex number-lines code-with-copy"><code class="sourceCode bibtex"><span id="cb1-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">@article</span>{<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">tong2026isbet</span>,</span>
<span id="cb1-2">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">author</span> = {Yuxuan Tong and Yingru Li and Guangming Sheng},</span>
<span id="cb1-3">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span> = {{IS with BET: Justifying Importance Sampling with Best-Effort Trajectories}},</span>
<span id="cb1-4">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">journal</span> = {Blog},</span>
<span id="cb1-5">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">year</span> = {2026},</span>
<span id="cb1-6">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">url</span> = {https://tongyx361.github.io/posts/is-bet},</span>
<span id="cb1-7">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">language</span> = {English},</span>
<span id="cb1-8">}</span></code></pre></div></div>


<!-- -->


</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-espeholt2020seedrl" class="csl-entry">
Espeholt, Lasse, Raphaël Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski‎. 2020. <span>“SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference.”</span> In <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=rkgvXlrKwH">https://openreview.net/forum?id=rkgvXlrKwH</a>.
</div>
<div id="ref-fu2025areal" class="csl-entry">
Fu, Wei, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, et al. 2025. <span>“<span>AREAL</span>: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning.”</span> In <em>The Thirty-Ninth Annual Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=X9diEuva9R">https://openreview.net/forum?id=X9diEuva9R</a>.
</div>
<div id="ref-hu2025openrlhf" class="csl-entry">
Hu, Jian, Xibin Wu, Wei Shen, Jason Klein Liu, Weixun Wang, Songlin Jiang, Haoran Wang, et al. 2025. <span>“<span>O</span>pen<span>RLHF</span>: A Ray-Based Easy-to-Use, Scalable and High-Performance <span>RLHF</span> Framework.”</span> In <em>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, edited by Ivan Habernal, Peter Schulam, and Jörg Tiedemann, 656–66. Suzhou, China: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2025.emnlp-demos.48">https://doi.org/10.18653/v1/2025.emnlp-demos.48</a>.
</div>
<div id="ref-kimi2025k15" class="csl-entry">
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, et al. 2025. <span>“Kimi K1.5: Scaling Reinforcement Learning with LLMs.”</span> <a href="https://arxiv.org/abs/2501.12599">https://arxiv.org/abs/2501.12599</a>.
</div>
<div id="ref-metelli2020pois" class="csl-entry">
Metelli, Alberto Maria, Matteo Papini, Nico Montali, and Marcello Restelli. 2020. <span>“Importance Sampling Techniques for Policy Optimization.”</span> <em>Journal of Machine Learning Research</em> 21 (141): 1–75. <a href="http://jmlr.org/papers/v21/20-124.html">http://jmlr.org/papers/v21/20-124.html</a>.
</div>
<div id="ref-owen2013mcbook" class="csl-entry">
Owen, Art B. 2013. <em>Monte Carlo Theory, Methods and Examples</em>. <a href="https://artowen.su.domains/mc/" class="uri">https://artowen.su.domains/mc/</a>.
</div>
<div id="ref-piché2025pipelinerl" class="csl-entry">
Piché, Alexandre, Ehsan Kamalloo, Rafael Pardinas, Xiaoyin Chen, and Dzmitry Bahdanau. 2025. <span>“PipelineRL: Faster on-Policy Reinforcement Learning for Long Sequence Generation.”</span> <a href="https://arxiv.org/abs/2509.19128">https://arxiv.org/abs/2509.19128</a>.
</div>
<div id="ref-shen2024nemoaligner" class="csl-entry">
Shen, Gerald, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, et al. 2024. <span>“NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment.”</span> In <em>First Conference on Language Modeling</em>. <a href="https://openreview.net/forum?id=yK2eGE8QVW">https://openreview.net/forum?id=yK2eGE8QVW</a>.
</div>
<div id="ref-sheng2025laminar" class="csl-entry">
Sheng, Guangming, Yuxuan Tong, Borui Wan, Wang Zhang, Chaobo Jia, Xibin Wu, Yuqi Wu, et al. 2025. <span>“Laminar: A Scalable Asynchronous RL Post-Training Framework.”</span> <a href="https://arxiv.org/abs/2510.12633">https://arxiv.org/abs/2510.12633</a>.
</div>
<div id="ref-sheng2025hybridflow" class="csl-entry">
Sheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. <span>“HybridFlow: A Flexible and Efficient RLHF Framework.”</span> In <em>Proceedings of the Twentieth European Conference on Computer Systems</em>, 1279–97. EuroSys ’25. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3689031.3696075">https://doi.org/10.1145/3689031.3696075</a>.
</div>
<div id="ref-yao2023dschat" class="csl-entry">
Yao, Zhewei, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, et al. 2023. <span>“DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-Like Models at All Scales.”</span> <a href="https://arxiv.org/abs/2308.01320">https://arxiv.org/abs/2308.01320</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div> ]]></description>
  <category>WIP 尚在完善</category>
  <category>English 英文</category>
  <category>Technical 技术</category>
  <guid>https://tongyx361.github.io/blogs/posts/is-bet/</guid>
  <pubDate>Wed, 07 Jan 2026 00:00:00 GMT</pubDate>
</item>
<item>
  <title></title>
  <dc:creator>Yuxuan Tong (童雨轩)</dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/verl-intro/</link>
  <description><![CDATA[ undefined ]]></description>
  <guid>https://tongyx361.github.io/blogs/posts/verl-intro/</guid>
  <pubDate>Mon, 30 Jun 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>重新思考 RL 中的 KL 梯度优化</title>
  <dc:creator>童雨轩 </dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh/</link>
  <description><![CDATA[ 






<section id="sec-grpo-kl-misunderstanding" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 引言：GRPO 公式的“错误”</h1>
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(Shao et al. 2024)</span> 的优化目标公式为：</p>
<p><span id="eq-grpo-obj"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%5Cleft%5C%7B%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D-%5Cbeta%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B1%7D"></span></p>
<p>其中</p>
<p><span id="eq-grpo-obj-kl-term"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7Br%20e%20f%7D%5Cright%5D=%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D-%5Clog%20%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20i%7D%20%5Cmid%20q,%20o_%7Bi,%20%5Calpha%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%20e%20t%7D%5Cright)%7D-1%0A%5Ctag%7B2%7D"></span></p>
<p>首先，Equation&nbsp;1 中出现了 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%5Ctext%7Bold%7D%7D">，这意味着其考虑了 off-policy 设置，但 Equation&nbsp;2 中却没有相应的处理，只适用于 <img src="https://latex.codecogs.com/png.latex?o_i%20%5Csim%20%5Cpi_%7B%5Ctheta%7D">，无法正确处理 <img src="https://latex.codecogs.com/png.latex?o_i%20%5Csim%20%5Cpi_%7B%5Ctheta_%5Ctext%7Bold%7D%7D">。</p>
<p>其次，Equation&nbsp;2 将估计样本量 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%3Ct%7D%5Cright)%7D-%5Clog%20%5Cfrac%7B%5Cpi_%7Br%20e%20f%7D%5Cleft(o_%7Bi,%20i%7D%20%5Cmid%20q,%20o_%7Bi,%20%5Calpha%7D%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%20e%20t%7D%5Cright)%7D-1"> 写成 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7Br%20e%20f%7D%5Cright%5D"> 也并不十分恰当，因为 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7Br%20e%20f%7D%5Cright%5D"> 通常表示 KL 散度的真实值。</p>
<p>而目前流行的 LLM RL 框架，在实现 KL 优化时，通常也忽略了 off-policy 问题，同时还存在其他一系列问题：</p>
<ol type="1">
<li>误认为前向传播估计出 KL 散度，再反向传播就能得到其梯度（但实际上通常并非如此）；</li>
<li>忽略了先对动作动作对数条件似然应用 KL 估计样本量再求和并非良好定义的行为，导致梯度错误；</li>
<li>忽略了同一轨迹上 KL 对数概率必须求和以获得轨迹联合概率，而不能求平均；</li>
<li>错误地计算了平均操作。</li>
</ol>
<p>由于 on-policy 设置更加简单，但也已经暴露了上述大部分问题，我们可以先从 on-policy 设置开始讨论，后续再考虑 off-policy 设置。</p>
<aside id="footnotes" class="footnotes footnotes-end-of-section">
<hr>
<ol>
<li id="fn1"><p>http://joschu.net/blog/kl-approx.html↩︎</p></li>
</ol>
</aside>
</section>
<section id="sec-popular-llm-rl-kl-optim" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 流行 LLM RL 框架中 on-policy KL 优化的实现</h1>
<p>我们可以先回顾目前流行的 LLM RL 框架中对于 KL 优化的实现。以下我们以</p>
<ol type="1">
<li>TRL<sup>2</sup>，</li>
<li>OpenRLHF<sup>3</sup> <span class="citation" data-cites="hu2024openrlhf">(Hu et al. 2024)</span></li>
<li>verl<sup>4</sup> <span class="citation" data-cites="sheng2024hybridflow">(Sheng et al. 2024)</span></li>
</ol>
<p>为例。</p>
<p>熟悉这些框架的读者可以跳过本节，直接从 Section&nbsp;3 开始阅读。</p>
<section id="trlkl-reward-项" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="trlkl-reward-项"><span class="header-section-number">2.1</span> TRL：KL reward 项</h2>
<p>TRL 计算 KL 定义中的样本值 <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D">，并将其从 reward 中减去。对应代码可见 Listing&nbsp;1。</p>
<div id="lst-trl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: TRL 计算 KL 样本值 <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,t%7D)%7D"> 并从 reward 中减去<sup>5</sup>
</figcaption>
<div aria-describedby="lst-trl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. compute rewards</span></span>
<span id="cb1-2">kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprobs</span>
<span id="cb1-3">non_score_reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>args.kl_coef <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> kl</span>
<span id="cb1-4">rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> non_score_reward.clone()</span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb1-6">rewards[[actual_start, actual_end]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> scores</span></code></pre></div></div>
</div>
</figure>
</div>
<p>这可能会引起疑惑：为什么要将 KL 样本值从 reward 中减去？我们先将对此的讨论推迟到 Section&nbsp;2.4。</p>
<aside id="footnotes-2" class="footnotes footnotes-end-of-section">
<hr>
<ol start="2">
<li id="fn2"><p>https://github.com/huggingface/trl↩︎</p></li>
<li id="fn3"><p>https://github.com/OpenRLHF/OpenRLHF↩︎</p></li>
<li id="fn4"><p>https://github.com/volcengine/verl↩︎</p></li>
<li id="fn5"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506↩︎</p></li>
</ol>
</aside>
</section>
<section id="openrlhf" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="openrlhf"><span class="header-section-number">2.2</span> OpenRLHF</h2>
<section id="sec-openrlhf-kl-reward" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-openrlhf-kl-reward"><span class="header-section-number">2.2.1</span> KL reward 项</h3>
<p>与 TRL 类似，OpenRLHF 支持计算 KL 估计样本值，并从 reward 中减去，但提供了多种计算 KL 估计样本值的方法。对应代码可见 Listing&nbsp;2。</p>
<div id="lst-openrlhf-calc-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: OpenRLHF 支持计算 KL 估计样本值并从 reward 中减去 <sup>6</sup>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_approx_kl(</span>
<span id="cb2-2">    log_probs: torch.Tensor,</span>
<span id="cb2-3">    log_probs_base: torch.Tensor,</span>
<span id="cb2-4">    action_mask: Optional[torch.Tensor] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb2-5">    kl_estimator: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k1"</span>,</span>
<span id="cb2-6">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> torch.Tensor:</span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Compute the approximate KL divergence between two distributions.</span></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Schulman blog: http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-10"></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Args:</span></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        log_probs: Log probabilities of the new distribution.</span></span>
<span id="cb2-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        log_probs_base: Log probabilities of the base distribution.</span></span>
<span id="cb2-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        action_mask: Mask for actions.</span></span>
<span id="cb2-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb2-16"></span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k1"</span>:</span>
<span id="cb2-18">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_probs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_probs_base.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb2-19">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-20">            log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> action_mask</span>
<span id="cb2-21"></span>
<span id="cb2-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The $k_2$ estimator is the non negative kl approximation in</span></span>
<span id="cb2-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The k2_loss is approximately equivalent to the</span></span>
<span id="cb2-25">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># one-step KL divergence penalty with the $k_1$ estimator</span></span>
<span id="cb2-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># used in https://arxiv.org/abs/2310.10505.</span></span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k2"</span>:</span>
<span id="cb2-28">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_probs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_probs_base.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb2-29">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-30">            log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> action_mask</span>
<span id="cb2-31">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span></span>
<span id="cb2-32"></span>
<span id="cb2-33">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The $k_3$ estimator is the non negative kl approximation in</span></span>
<span id="cb2-34">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># http://joschu.net/blog/kl-approx.html</span></span>
<span id="cb2-35">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k3"</span>:</span>
<span id="cb2-36">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_probs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_probs_base.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb2-37">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-38">            log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> action_mask</span>
<span id="cb2-39">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log_ratio</span>
<span id="cb2-40">        log_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ratio.exp() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> log_ratio</span>
<span id="cb2-41"></span>
<span id="cb2-42">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> log_ratio</span>
<span id="cb2-43"></span>
<span id="cb2-44"></span>
<span id="cb2-45"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_reward(</span>
<span id="cb2-46">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-47">    kl_coef: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-48">    kl: Union[torch.Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[torch.Tensor]],</span>
<span id="cb2-49">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-50">    num_actions: Optional[Union[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>]]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb2-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-52">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Union[torch.Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[torch.Tensor]]:</span>
<span id="cb2-53">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-54">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-55">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-56">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb2-57">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb2-58">        reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-59">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, (kl_seg, action_len) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(kl, num_actions)):</span>
<span id="cb2-60">            kl_reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>kl_coef <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> kl_seg</span>
<span id="cb2-61">            kl_reward[action_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> r[i]</span>
<span id="cb2-62">            reward.append(kl_reward)</span>
<span id="cb2-63"></span>
<span id="cb2-64">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> reward</span></code></pre></div></div>
</div>
</figure>
</div>
<aside id="footnotes-3" class="footnotes footnotes-end-of-section">
<hr>
<ol start="6">
<li id="fn6"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88↩︎</p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="kl-loss-项"><span class="header-section-number">2.2.2</span> KL loss 项</h3>
<p>此外，OpenRLHF 还支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中。对应代码可见 Listing&nbsp;3。</p>
<div id="lst-openrlhf-calc-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: OpenRLHF 支持计算 KL 估计样本值，先对序列内部的 token 计算均值，再在序列之间计算均值，并加入到 loss 中 <sup>7</sup>
</figcaption>
<div aria-describedby="lst-openrlhf-calc-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> training_step_actor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, experience: Experience) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Dict[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>]:</span>
<span id="cb3-2">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor.train()</span>
<span id="cb3-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(experience.sequences, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>):</span>
<span id="cb3-5">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-7">        sequences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.sequences</span>
<span id="cb3-8">        old_action_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.action_log_probs</span>
<span id="cb3-9">        advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.advantages</span>
<span id="cb3-10">        num_actions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.action_mask.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-11">        packed_seq_lens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-12">        attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.attention_mask</span>
<span id="cb3-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.use_kl_loss <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> experience.base_action_log_probs <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-14">            base_action_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> experience.base_action_log_probs</span>
<span id="cb3-15"></span>
<span id="cb3-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># actor loss</span></span>
<span id="cb3-17">    action_log_probs, output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor(</span>
<span id="cb3-18">        sequences,</span>
<span id="cb3-19">        num_actions,</span>
<span id="cb3-20">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-21">    )</span>
<span id="cb3-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># loss function</span></span>
<span id="cb3-24">    actor_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_loss_fn(</span>
<span id="cb3-25">        action_log_probs,</span>
<span id="cb3-26">        old_action_log_probs,</span>
<span id="cb3-27">        advantages,</span>
<span id="cb3-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-29">    )</span>
<span id="cb3-30"></span>
<span id="cb3-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.use_kl_loss:</span>
<span id="cb3-32">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_model <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-33">            kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_approx_kl(</span>
<span id="cb3-34">                action_log_probs,</span>
<span id="cb3-35">                base_action_log_probs,</span>
<span id="cb3-36">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-37">                kl_estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.kl_estimator,</span>
<span id="cb3-38">            )</span>
<span id="cb3-39">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-40">            kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros_like(action_log_probs, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>action_log_probs.dtype, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>action_log_probs.device)</span>
<span id="cb3-41"></span>
<span id="cb3-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.packing_samples:</span>
<span id="cb3-43">            kl_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(kl, experience.action_mask, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-44">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-45">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-46"></span>
<span id="cb3-47">        kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_mean.mean()</span>
<span id="cb3-48">        experience.info[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"kl"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_loss.item()</span>
<span id="cb3-49">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-50">        kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb3-52">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.strategy.optimizer_step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_optim, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_scheduler, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"actor"</span>)</span>
<span id="cb3-53">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div></div>
</div>
</figure>
</div>
<aside id="footnotes-4" class="footnotes footnotes-end-of-section">
<hr>
<ol start="7">
<li id="fn7"><p>https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="verl" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="verl"><span class="header-section-number">2.3</span> verl</h2>
<section id="kl-reward-项" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="kl-reward-项"><span class="header-section-number">2.3.1</span> KL reward 项</h3>
<p>verl 同样支持计算 KL 估计样本值并从 reward 中减去。对应代码可见 Listing&nbsp;4。</p>
<div id="lst-verl-kl-reward" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: verl 将 KL 估计样本值从 reward 中减去 <sup>8</sup>
</figcaption>
<div aria-describedby="lst-verl-kl-reward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kl'</span>):</span>
<span id="cb4-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb4-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute kl between ref_policy and current policy</span></span>
<span id="cb4-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ref_log_prob'</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> data.batch.keys():</span>
<span id="cb4-5">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> core_algos.kl_penalty(data.batch[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'old_log_probs'</span>], data.batch[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ref_log_prob'</span>],</span>
<span id="cb4-6">                                    kl_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>kl_penalty)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (batch_size, response_length)</span></span>
<span id="cb4-7">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> response_mask</span>
<span id="cb4-8">        beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_ctrl.value</span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb4-10">        beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb4-11">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros_like(response_mask, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb4-12"></span>
<span id="cb4-13">    token_level_rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> token_level_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> kld</span>
<span id="cb4-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div></div>
</div>
</figure>
</div>
<aside id="footnotes-5" class="footnotes footnotes-end-of-section">
<hr>
<ol start="8">
<li id="fn8"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160↩︎</p></li>
</ol>
</aside>
</section>
<section id="kl-loss-项-1" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kl-loss-项-1"><span class="header-section-number">2.3.2</span> KL loss 项</h3>
<p>verl 也支持计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中。对应代码可见 Listing&nbsp;5。</p>
<div id="lst-verl-kl-loss" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: verl 计算 KL 估计样本值，对所有 token 计算均值，并加入到 loss 中 <sup>9</sup>
</figcaption>
<div aria-describedby="lst-verl-kl-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_policy(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, data: DataProto):</span>
<span id="cb5-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># make sure we are in training mode</span></span>
<span id="cb5-3">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_module.train()</span>
<span id="cb5-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.ppo_epochs):</span>
<span id="cb5-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> batch_idx, data <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dataloader):</span>
<span id="cb5-7">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-8">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_optimizer.zero_grad()</span>
<span id="cb5-9"></span>
<span id="cb5-10">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> data <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> micro_batches:</span>
<span id="cb5-11">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-12">                responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'responses'</span>]</span>
<span id="cb5-13">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-14">                old_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'old_log_probs'</span>]</span>
<span id="cb5-15">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-16"></span>
<span id="cb5-17">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># all return: (bsz, response_length)</span></span>
<span id="cb5-18">                entropy, log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._forward_micro_batch(micro_batch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>temperature)</span>
<span id="cb5-19"></span>
<span id="cb5-20">                pg_loss, pg_clipfrac, ppo_kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> core_algos.compute_policy_loss(old_log_prob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>old_log_prob,</span>
<span id="cb5-21">                                                                                log_prob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>log_prob,</span>
<span id="cb5-22">                                                                                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-23">                                                                                )</span>
<span id="cb5-24">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-25"></span>
<span id="cb5-26">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute policy loss</span></span>
<span id="cb5-27">                policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> entropy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> entropy_coeff</span>
<span id="cb5-28"></span>
<span id="cb5-29">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.use_kl_loss:</span>
<span id="cb5-30">                    ref_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ref_log_prob'</span>]</span>
<span id="cb5-31">                    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute kl loss</span></span>
<span id="cb5-32">                    kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> core_algos.kl_penalty(logprob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>log_prob,</span>
<span id="cb5-33">                                                ref_logprob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ref_log_prob,</span>
<span id="cb5-34">                                                kl_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.kl_loss_type)</span>
<span id="cb5-35">                    kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(kld, response_mask)</span>
<span id="cb5-36"></span>
<span id="cb5-37">                    policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.kl_loss_coef</span>
<span id="cb5-38">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-39">                loss.backward()</span>
<span id="cb5-40">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-41">            grad_norm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._optimizer_step()</span>
<span id="cb5-42">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb5-43">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor_optimizer.zero_grad()</span>
<span id="cb5-44">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div></div>
</div>
</figure>
</div>
<aside id="footnotes-6" class="footnotes footnotes-end-of-section">
<hr>
<ol start="9">
<li id="fn9"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="sec-why-kl-reward" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-why-kl-reward"><span class="header-section-number">2.4</span> 为什么要将 KL 从 reward 中减去</h2>
<p>将 KL 从 reward 中减去的做法应当主要参考的是 OpenAI 正式提出 RLHF 的论文 InstructGPT <span class="citation" data-cites="ouyang2022instructgpt">(Ouyang et al. 2022)</span>。</p>
<section id="kl-reward-的流行应当源自-rlhf-与-instructgpt" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="kl-reward-的流行应当源自-rlhf-与-instructgpt"><span class="header-section-number">2.4.1</span> KL reward 的流行应当源自 RLHF 与 InstructGPT</h3>
<p>InstructGPT 论文中提到其向 reward 添加了相对于 SFT 模型的 KL 惩罚项，但并没有提到为什么将 KL 放在 reward 而非 loss 中。</p>
<blockquote class="blockquote">
<p>… In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models “PPO.”</p>
<p>…</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Ctext%20%7B%20objective%20%7D(%5Cphi)=%20&amp;%20E_%7B(x,%20y)%20%5Csim%20D_%5Cpi%5E%7B%5Cmathrm%7BRL%7D%7D%7D%5Cleft%5Br_%5Ctheta(x,%20y)-%5Cbeta%20%5Clog%20%5Cleft(%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D(y%20%5Cmid%20x)%20/%20%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D(y%20%5Cmid%20x)%5Cright)%5Cright%5D+%20%5C%5C%0A&amp;%20%5Cgamma%20E_%7Bx%20%5Csim%20D_%7B%5Ctext%20%7Bremin%20%7D%7D%7D%5Cleft%5B%5Clog%20%5Cleft(%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D(x)%5Cright)%5Cright%5D%0A%5Cend%7Baligned%7D%0A"></p>
<blockquote class="blockquote">
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D">is the learned RL policy,<img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D"> is the supervised trained model, and<img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%20%7Bpretrain%20%7D%7D">is the pretraining distribution. The KL reward coefficient, <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and the pretraining loss coefficient, <img src="https://latex.codecogs.com/png.latex?%5Cgamma">, control the strength of the KL penalty and pretraining gradients respectively. For “PPO” models, <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</p>
</blockquote>
</section>
<section id="sec-oai-kl-reward-src" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="sec-oai-kl-reward-src"><span class="header-section-number">2.4.2</span> OpenAI 论文中 KL reward 的出处</h3>
<p>然而，在OpenAI 早期的一篇论文 “Learning to summarize from human feedback” <span class="citation" data-cites="stiennon2020summarize">(Stiennon et al. 2020)</span> 中，他们就已经采用了 KL reward，并提及了出处：</p>
<blockquote class="blockquote">
<p>… <strong>Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D"> with parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> and this original supervised model <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D">, as previously done in [25].</strong> The full reward <img src="https://latex.codecogs.com/png.latex?R"> can be written as:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(x,%20y)=r_%5Ctheta(x,%20y)-%5Cbeta%20%5Clog%20%5Cleft%5B%5Cpi_%5Cphi%5E%7B%5Cmathrm%7BRL%7D%7D(y%20%5Cmid%20x)%20/%20%5Cpi%5E%7B%5Cmathrm%7BSFT%7D%7D(y%20%5Cmid%20x)%5Cright%5D%0A"></p>
<blockquote class="blockquote">
<p>This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn’t learn to produce outputs that are too different from those that the reward model has seen during training.</p>
</blockquote>
</section>
<section id="kl-reward-最早的出处" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="kl-reward-最早的出处"><span class="header-section-number">2.4.3</span> KL reward 最早的出处</h3>
<p>Section&nbsp;2.4.2 中 OpenAI 引用的 KL reward 出处 [25] 是 “Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog” <span class="citation" data-cites="jaques2019wayoffpolicy">(Jaques et al. 2019)</span>。</p>
<p>实际上，其中引入 KL 散度时，最初的形式是 loss 项，而非 reward 项，但其指出了两者的等价性：</p>
<blockquote class="blockquote">
<p>Rather than simply sample from the prior, we would like the <img src="https://latex.codecogs.com/png.latex?Q">-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior <img src="https://latex.codecogs.com/png.latex?p(y%20%5Cmid%20x)">, and the <img src="https://latex.codecogs.com/png.latex?Q">-network policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">, while still maximizing reward. Given a trajectory of actions, <img src="https://latex.codecogs.com/png.latex?%5Ctau=%5Cleft%5C%7Ba_1,%20a_2,%20%5Cldots%20a_%7Bt-1%7D%5Cright%5C%7D">, let <img src="https://latex.codecogs.com/png.latex?q(%5Ctau)=%5Cprod_%7Bt=1%7D%5ET%20%5Cpi_%5Ctheta%5Cleft(a_t,%20s_t%5Cright)">be the policy of our<img src="https://latex.codecogs.com/png.latex?Q">-learning algorithm at the trajectory level. Similarly, let <img src="https://latex.codecogs.com/png.latex?p(%5Ctau)=%5Cprod_%7Bt=1%7D%5ET%20p%5Cleft(a_t%20%5Cmid%20s_t%5Cright)">be the prior distribution over the trajectory, and<img src="https://latex.codecogs.com/png.latex?r(%5Ctau)"> be the rewards. We seek to maximize the following KL-regularized objective:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(q)=%5Cmathbb%7BE%7D_%7Bq(%5Ctau)%7D%5Br(%5Ctau)%5D%20/%20c-D_%7B%5Ctext%7BKL%7D%7D%5Bq(%5Ctau)%20%5Cmid%20p(%5Ctau)%5D%0A"></p>
<blockquote class="blockquote">
<p>Since <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D%5Bq%20%5Cmid%20p%5D=%5Csum_x%20q(x)(%5Clog%20q(x)-%5Clog%20p(x))">, we can see that this is equivalent to maximizing the following expected value function of the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> at the action level:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ%5E%5Cpi%5Cleft(s_t,%20a_t%5Cright)=%5Cmathbb%7BE%7D_%5Cpi%5Cleft%5B%5Csum%5ET%20r%5Cleft(s_%7Bt%5E%7B%5Cprime%7D%7D,%20a_%7Bt%5E%7B%5Cprime%7D%7D%5Cright)%20/%20c+%5Clog%20p%5Cleft(a_%7Bt%5E%7B%5Cprime%7D%7D%20%5Cmid%20s_%7Bt%5E%7B%5Cprime%7D%7D%5Cright)-%5Clog%20%5Cpi%5Cleft(a_%7Bt%5E%7B%5Cprime%7D%7D%20%5Cmid%20s_%7Bt%5E%7B%5Cprime%7D%7D%5Cright)%5Cright%5D%0A"></p>
<blockquote class="blockquote">

</blockquote>
</section>
</section>
</section>
<section id="sec-rl-kl-optim-formulation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> LLM RL 中 KL 优化的数学形式化</h1>
<p>为了进一步分析这些 LLM RL 框架中的实现是否正确，我们需要先形式化 LLM RL 中 KL 散度的优化。</p>
<section id="rl-中的-kl-散度通常定义在轨迹分布上" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="rl-中的-kl-散度通常定义在轨迹分布上"><span class="header-section-number">3.1</span> RL 中的 KL 散度通常定义在轨迹分布上</h2>
<p>GRPO 公式 (Equation&nbsp;1) 中的 KL 项可以定义为：</p>
<p><span id="eq-def-kl-theta-ref"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D"> 是表示轨迹（Trajectory）的随机变量。注意，与策略梯度（Policy Gradient，PG）优化轨迹分布上奖励的期望类似，我们同样希望在轨迹分布上优化最新策略整体分布 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 与参考策略整体分布 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bref%7D%7D"> 的 KL 散度。</p>
</section>
<section id="将轨迹展开为状态-动作序列" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="将轨迹展开为状态-动作序列"><span class="header-section-number">3.2</span> 将轨迹展开为状态-动作序列</h2>
<p>RL 文献中通常会将轨迹 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D"> 展开为状态-动作序列 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D">：<sup>10</sup></p>
<p><span id="eq-def-kl-theta-ref-state-action-ag"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7Bp(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%7D%7Bp(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B4%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7B%5Ctau%7D%7C"> 为轨迹动作数的随机变量。</p>
<p>此处利用了联合概率的展开，以 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 为例：</p>
<p><span id="eq-dp-expansion"><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Ctheta%7D(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20=%20p(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%0A%5Ctag%7B5%7D"></span></p>
<p>注意区分整体概率分布 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D">、策略（条件）概率分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 与状态转移概率分布 <img src="https://latex.codecogs.com/png.latex?p">。</p>
<aside id="footnotes-7" class="footnotes footnotes-end-of-section">
<hr>
<ol start="10">
<li id="fn10"><p>这里我们离开了 GRPO 的符号系统，换用了 RL 文献中更常见的状态-动作符号系统。实际上，<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bq%7D"> 对应于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_1">，而 <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathbf%7Bo%7D%7D"> 对应于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_T,%20%5Cmathbf%7Ba%7D_T%7D">。↩︎</p></li>
</ol>
</aside>
</section>
<section id="markov-决策过程中的-kl-散度" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="markov-决策过程中的-kl-散度"><span class="header-section-number">3.3</span> Markov 决策过程中的 KL 散度</h2>
<p>实际上，RL 文献中还经常将序列决策过程建模为一阶 Markov 决策过程（Markov Decision Process, MDP<sup>11</sup>。</p>
<p>Markov 决策过程要求序列中的条件概率满足 Markov 性质，即只依赖于最新的 <img src="https://latex.codecogs.com/png.latex?n"> 个历史状态和动作，而非全部的历史信息，对应的过程称为 <img src="https://latex.codecogs.com/png.latex?n"> 阶 Markov 过程。以 <img src="https://latex.codecogs.com/png.latex?n=1"> 为例：</p>
<p><span id="eq-def-markov-prop"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cpi(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20&amp;%20=%20%5Cpi(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%20%5C%5C%0Ap(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20&amp;%20=%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B6%7D"></span></p>
<p>则 Equation&nbsp;5 中的联合概率可以进一步简化为：</p>
<p><span id="eq-dp-expansion-markov-1"><img src="https://latex.codecogs.com/png.latex?%0Ap(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20=%20p(s_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%0A%5Ctag%7B7%7D"></span></p>
<p>如果考虑一阶 Markov 过程，则 Equation&nbsp;4 中的 KL 可以进一步简化为：</p>
<p><span id="eq-def-kl-theta-ref-state-action-markov-1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20=%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B8%7D"></span></p>
<aside id="footnotes-8" class="footnotes footnotes-end-of-section">
<hr>
<ol start="11">
<li id="fn11"><p>https://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B↩︎</p></li>
</ol>
</aside>
</section>
<section id="sec-lm-as-dp" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-lm-as-dp"><span class="header-section-number">3.4</span> 语言模型作为序列决策过程</h2>
<p>目前的语言模型（Language Model, LM）通常建模为自回归模型，即当前 token 的生成依赖于所有之前的 token。</p>
<p>尽管初看起来，自回归模型似乎无法满足 Markov 性质，但实际上我们也可以将自回归模型建模为一阶 Markov 过程。具体来说：令 <img src="https://latex.codecogs.com/png.latex?s_1"> 表示 prompt 中的所有 token，对于 <img src="https://latex.codecogs.com/png.latex?t%20%3E1">，如果令 <img src="https://latex.codecogs.com/png.latex?s_t"> 表示第 <img src="https://latex.codecogs.com/png.latex?t"> 个动作 token 前的所有 token，则自回归模型满足 Markov 性质，否则不一定。</p>
<p>接下来，我们先令 <img src="https://latex.codecogs.com/png.latex?s_t"> 表示前 <img src="https://latex.codecogs.com/png.latex?t"> 个 token 组成的序列，即不依赖于 Markov 性质继续推导，以获得尽可能通用的结论。在必要时，我们会再引入 Markov 性质。</p>
</section>
<section id="估计-kl-散度" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="估计-kl-散度"><span class="header-section-number">3.5</span> 估计 KL 散度</h2>
<section id="几乎不可能直接计算-kl-散度的真实值" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="几乎不可能直接计算-kl-散度的真实值"><span class="header-section-number">3.5.1</span> 几乎不可能直接计算 KL 散度的真实值</h3>
<p>实际实现中，我们几乎不可能直接计算出 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">，因为 RL 中的 KL 散度定义要对轨迹空间求均值，而轨迹空间的大小 <img src="https://latex.codecogs.com/png.latex?%5Cleft%7C%5Cmathcal%7BT%7D%5Cright%7C"> 与轨迹最大长度 <img src="https://latex.codecogs.com/png.latex?T%20=%20%5Cmax_%7B%5Cmathbf%7B%5Ctau%7D%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%7C%5Cmathbf%7B%5Ctau%7D%7C"> 成指数关系： <span id="eq-def-rl-kl-avg-over-traj"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D%20(%5Cmathbf%7B%5Ctau%7D)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B9%7D"></span></p>
</section>
<section id="通常使用-monte-carlo-方法估计-kl-散度" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="通常使用-monte-carlo-方法估计-kl-散度"><span class="header-section-number">3.5.2</span> 通常使用 Monte Carlo 方法估计 KL 散度</h3>
<p>所以，我们通常基于若干轨迹样本使用 Monte Carlo 方法<sup>12</sup>来估计 RL 中的 KL 散度，例如：</p>
<p><span id="eq-def-rl-kl-mc-k1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D%20(%5Cmathbf%7B%5Ctau%7D)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5C%5C%0A&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau_%7Bi%20%7D%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B10%7D"></span></p>
<p>其中，<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%20=%20%5Cleft(%5Cmathbf%7Bs%7D_%7Bi,1%7D,%20%5Cmathbf%7Ba%7D_%7Bi,1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bi,%7C%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7Bi,%7C%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%7C%7D%5Cright)%20%5Csim%20p_%7B%5Ctheta%7D">，<img src="https://latex.codecogs.com/png.latex?N"> 为估计使用的轨迹样本数量。</p>
<aside id="footnotes-9" class="footnotes footnotes-end-of-section">
<hr>
<ol start="12">
<li id="fn12"><p>https://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95↩︎</p></li>
</ol>
</aside>
</section>
<section id="不同的-kl-估计量" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="不同的-kl-估计量"><span class="header-section-number">3.5.3</span> 不同的 KL 估计量</h3>
<p>实际上，Monte Carlo 方法允许使用样本导出的不同估计量，而不必是统计量定义中的样本量。不同的估计量有不同的偏差（Bias）和方差（Variance），从而构成了估计量选择之间的权衡。</p>
<p>设 KL 估计量为 <img src="https://latex.codecogs.com/png.latex?k">，则对应的 KL 估计值为</p>
<p><span id="eq-def-rl-kl-mc-general"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20k(%5Ctau_i)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B11%7D"></span></p>
<p>例如 Section&nbsp;2.2.1 提到，OpenRLHF 引入了 3 种 KL 散度的估计方法，分别称为 <code>k1</code>, <code>k2</code>, <code>k3</code>，这应该是主要参考了 John Schulman 的博客 “Approximating KL Divergence”。</p>
<p>verl 则考虑了更多估计方法。实际上，verl 还考虑了直接计算条件 KL 散度<sup>13</sup>，但目前还没有实现。对应代码可见 Listing&nbsp;6。</p>
<div id="lst-verl-kl-estimator" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: verl 的 KL 散度 Monte Carlo 估计样本值<sup>14</sup>
</figcaption>
<div aria-describedby="lst-verl-kl-estimator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> torch.FloatTensor:</span>
<span id="cb6-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"kl"</span>:</span>
<span id="cb6-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprob</span>
<span id="cb6-5"></span>
<span id="cb6-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"abs"</span>:</span>
<span id="cb6-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprob).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>()</span>
<span id="cb6-8"></span>
<span id="cb6-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mse"</span>:</span>
<span id="cb6-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ref_logprob).square()</span>
<span id="cb6-11"></span>
<span id="cb6-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># J. Schulman. Approximating kl divergence, 2020.</span></span>
<span id="cb6-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># # URL http://joschu.net/blog/kl-approx.html.</span></span>
<span id="cb6-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'low_var_kl'</span>:</span>
<span id="cb6-15">        kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ref_logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> logprob</span>
<span id="cb6-16">        ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(kl)</span>
<span id="cb6-17">        kld <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).contiguous()</span>
<span id="cb6-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.clamp(kld, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb6-19"></span>
<span id="cb6-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kl_penalty <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"full"</span>:</span>
<span id="cb6-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># so, here logprob and ref_logprob should contain the logits for every token in vocabulary</span></span>
<span id="cb6-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">NotImplementedError</span></span>
<span id="cb6-23"></span>
<span id="cb6-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">NotImplementedError</span></span></code></pre></div></div>
</div>
</figure>
</div>
<p>由于 <img src="https://latex.codecogs.com/png.latex?k_1">、<img src="https://latex.codecogs.com/png.latex?k_2">、<img src="https://latex.codecogs.com/png.latex?k_3"> 三种估计量最为流行，我们将以这三种估计量为例展开分析。</p>
<p>考虑 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20k_j(%5Ctau_i)">，其中 <img src="https://latex.codecogs.com/png.latex?%5Ctau_i%20%5Csim%20p_%7B%5Ctheta%7D">，令 <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Ctau_i)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Ctau_i)%7D">，注意，此处 <img src="https://latex.codecogs.com/png.latex?r"> 并非 KL 定义中的样本量，而是其倒数，则：</p>
<p><span id="eq-def-kl-estimators"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ak_%7B1%7D%20&amp;%20=%20-%20%5Clog%20r%20%5C%5C%0Ak_%7B2%7D%20&amp;%20=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Clog%20r)%5E2%20%5C%5C%0Ak_%7B3%7D%20&amp;%20=%20(r%20-%201)%20-%20%5Clog%20r%0A%5Cend%7Baligned%7D%0A%5Ctag%7B12%7D"></span></p>
<aside id="footnotes-10" class="footnotes footnotes-end-of-section">
<hr>
<ol start="13">
<li id="fn13"><p>这里的条件 KL 散度只需要遍历整个词表，代价可能是可以接受的。↩︎</p></li>
<li id="fn14"><p>https://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383↩︎</p></li>
</ol>
</aside>
</section>
</section>
</section>
<section id="流行-on-policy-kl-优化实现的数学形式化" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 流行 on-policy KL 优化实现的数学形式化</h1>
<p>神经网络模型普遍使用梯度法优化，因此，我们主要关注这些 KL 优化实现导出的梯度。</p>
<p>而由于 reward 项优化的实现涉及到基线（Baseline）、折扣（Discounting）、GAE <span class="citation" data-cites="schulman2018gae">(Schulman et al. 2018)</span> 等内容，较为复杂，我们可以先分析 KL loss 项实现。</p>
<section id="sec-kl-loss-impl" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-kl-loss-impl"><span class="header-section-number">4.1</span> 分析流行的 “KL loss 项” 实现</h2>
<p>上述框架中，OpenRLHF 与 verl 都实现了 “KL loss 项”，即先直接计算出 KL 估计量并加入到 loss 中，再反向传播得到梯度，期间默认没有去除梯度。</p>
<p>然而，如 Section&nbsp;1 所述，这一做法是错误的，接下来我们将通过分析这些 “KL loss 项” 实际导出的梯度估计，说明其错误之处。</p>
<section id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="不同-kl-估计量对应的-loss-项导出的梯度估计的一般形式"><span class="header-section-number">4.1.1</span> 不同 KL 估计量对应的 loss 项导出的梯度估计的一般形式</h3>
<p>观察 Listing&nbsp;3 计算 “KL loss” 项的部分。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-2">kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_approx_kl(</span>
<span id="cb7-3">    action_log_probs,</span>
<span id="cb7-4">    base_action_log_probs,</span>
<span id="cb7-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-6">    kl_estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.args.kl_estimator,</span>
<span id="cb7-7">)</span>
<span id="cb7-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-9">kl_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(kl, experience.action_mask, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb7-11">kl_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kl_mean.mean()</span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span></code></pre></div></div>
<p>这些代码：</p>
<ol type="1">
<li>计算了 <code>kl</code>，对应对每个动作 token <img src="https://latex.codecogs.com/png.latex?a_%7Bi,t%7D"> 计算 “KL 估计量” <img src="https://latex.codecogs.com/png.latex?k">。</li>
<li>计算了 <code>kl_mean</code>，对应对每个轨迹 <img src="https://latex.codecogs.com/png.latex?%5Ctau_i"> 计算均值 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20k">。</li>
<li>计算了 <code>kl_loss</code>，对应对所有轨迹样本计算均值 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20k">。</li>
</ol>
<p>由于其没有去除任何梯度，因此其导出的梯度估计值为</p>
<p><span id="eq-def-kl-loss-grad-estim-openrlhf"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20k%20%5Cright)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%0A%5Cend%7Baligned%7D%0A%5Ctag%7B13%7D"></span></p>
<p>Listing&nbsp;5 中 verl 的实现类似，但不同的是其平均是在所有 token 之间执行的，因此对应的梯度估计值为：</p>
<p><span id="eq-def-kl-loss-grad-estim-verl"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7BN%7D%20%7C%5Ctau_i%7C%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20k%20%5Cright)%20=%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7BN%7D%20%7C%5Ctau_i%7C%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cnabla_%7B%5Ctheta%7D%20k%0A%5Cend%7Baligned%7D%0A%5Ctag%7B14%7D"></span></p>
<p>我们将平均操作一般化为权重 <img src="https://latex.codecogs.com/png.latex?w_%7B%5Cmathbf%7B%5Ctau%7D%7D"> 与 <img src="https://latex.codecogs.com/png.latex?w_%7Bt%7D">，则不同 KL 估计量对应的 loss 项导出的梯度估计值的一般形式为：</p>
<p><span id="eq-def-kl-loss-grad-estim-general"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Csum_%7Bi=1%7D%5E%7BN%7D%20w_%7B%5Cmathbf%7B%5Ctau%7D_i%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20w_%7Bt%7D%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B15%7D"></span></p>
<p>则</p>
<ul>
<li>OpenRLHF 对应 <img src="https://latex.codecogs.com/png.latex?w_%7B%5Cmathbf%7B%5Ctau%7D%7D%20=%20%5Cfrac%7B1%7D%7BN%7D,%20w_%7Bt%7D%20=%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D">；</li>
<li>verl 对应 <img src="https://latex.codecogs.com/png.latex?w_%7B%5Cmathbf%7B%5Ctau%7D%7D%20=%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7BN%7D%20%7C%5Ctau_i%7C%7D,%20w_%7Bt%7D%20=%201">。</li>
</ul>
<p>此处，我们先以 OpenRLHF 的梯度估计 (Equation&nbsp;13) 为例，分析不同 KL 估计量导出的梯度估计，其满足：</p>
<p><span id="eq-def-kl-loss-grad-expect-openrlhf"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D_i%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau_i%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5Cright%5D%0A%5Ctag%7B16%7D"></span></p>
<p>我们会在 Section&nbsp;5 中推导正确的 KL 梯度估计。</p>
</section>
<section id="k_1-导出的梯度期望为-0" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="k_1-导出的梯度期望为-0"><span class="header-section-number">4.1.2</span> <img src="https://latex.codecogs.com/png.latex?k_1"> 导出的梯度：期望为 0</h3>
<p>向 Equation&nbsp;16 代入 <img src="https://latex.codecogs.com/png.latex?k%20=%20k_1%20=%20-%20%5Clog%20r%20=%20%5Clog%20%5Cfrac%7B1%7D%7Br%7D%20=%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D">，导出的梯度估计为</p>
<p><span id="eq-kl-loss-grad-sample-k1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cleft(%20p(%5Cmathbf%7Bs%7D_%7B1%7D)%20%5Cright)%20%5Cright)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cleft(%20p(%5Cmathbf%7Bs%7D_%7B1%7D)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D)%20%5Cright)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%5Ctheta(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20%5C%5C%0A=&amp;%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B17%7D"></span></p>
<p>则其导出的梯度期望满足：</p>
<p><span id="eq-kl-loss-grad-expect-k1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A&amp;%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D%20%5C%5C%0A&amp;%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B18%7D"></span></p>
<p>此处利用了 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20=%20%5Cfrac%7B1%7D%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20=%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)">。</p>
<p>所以 <img src="https://latex.codecogs.com/png.latex?k_1"> loss 项优化的量是 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cfrac%7B1%7D%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cright%5D">。这意味着该优化过程会降低采样轨迹的长度。</p>
<p>特别地，当不对同一轨迹中的 “<img src="https://latex.codecogs.com/png.latex?k_1"> 估计量”求均值，而是求和时，可以直接将 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D"> 这一项替换为 <img src="https://latex.codecogs.com/png.latex?1">，得到 <span id="eq-kl-loss-grad-expect-k1-no-intra-traj-mean"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5Cright%5D%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D%20=%20%5Cnabla_%7B%5Ctheta%7D%201%20=%200%0A%5Ctag%7B19%7D"></span><sup>15</sup></p>
<p>这意味着使用该梯度更新参数，在平均意义上不会引起参数及其导出的分布改变。</p>
<p>无论哪种情况，<img src="https://latex.codecogs.com/png.latex?k_1"> 导出的优化量都非常奇怪，不太可能出于实现者的本意。</p>
<p>同时，对同一轨迹中的 KL 估计量求均值这一操作，也很有可能是错误的。接下来，我们将忽略这一操作，即将 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%7C%5Ctau%7C%7D"> 一项替换为 <img src="https://latex.codecogs.com/png.latex?1">。</p>
<aside id="footnotes-11" class="footnotes footnotes-end-of-section">
<hr>
<ol start="15">
<li id="fn15"><p>此处对数似然的梯度的期望值为 0，是一个著名的性质，会在接下来频繁用到。↩︎</p></li>
</ol>
</aside>
</section>
<section id="k_2-导出的梯度" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="k_2-导出的梯度"><span class="header-section-number">4.1.3</span> <img src="https://latex.codecogs.com/png.latex?k_2"> 导出的梯度</h3>
<p>向 Equation&nbsp;16 代入 <img src="https://latex.codecogs.com/png.latex?k%20=%20k_2%20=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Clog%20r)%5E2%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%5Cright)%5E2">，导出的单条轨迹 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D"> 的梯度为 <span id="eq-kl-loss-grad-sample-k2"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%5Cright)%5E2%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B20%7D"></span></p>
<p>显然，</p>
<p><span id="eq-kl-loss-grad-sample-k2-wrong"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%20%5C%5C%0A%5Cneq%20&amp;%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%7D%20%5Cright)%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,t%7D%20%5Cmid%20s_%7Bi,1%7D,%20a_%7Bi,1%7D,%20%5Ccdots,%20s_%7Bi,t%7D)%20%5Cright)%20%5C%5C%0A=&amp;%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B21%7D"></span></p>
<p>然而，</p>
<p><span id="eq-kl-loss-grad-expect-k2-wrong"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%5Cleft%5B%20%5Cleft(%20%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20-%20%5Cleft(%20%5Clog%20p_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%20%5Cleft%5B%20%5Cnabla_%7B%5Ctheta%7D%20(%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20-%201)%20p_%7B%5Ctheta%7D(%5Ctau)%20-%20%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%20%5Cleft%5B%20(%5Clog%20p_%7B%5Ctheta%7D(%5Ctau)%20-%201)%20p_%7B%5Ctheta%7D(%5Ctau)%20-%20%5Clog%20p_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20%20p_%7B%5Ctheta%7D%20%5Cleft%5B%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Ctau)%7D%20-%201%20%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%20%5Cleft(%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20-%201%20%5Cright)%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B22%7D"></span></p>
<p>此处利用了 <img src="https://latex.codecogs.com/png.latex?%5Clog%20p(x)%20%5Cnabla_%7B%5Ctheta%7D%20p(x)%20=%20%5Cnabla_%7B%5Ctheta%7D%20(%5Clog%20p(x)%20-%201)%20p(x)"></p>
<p>因此，最小化 <img src="https://latex.codecogs.com/png.latex?k_2"> loss 项 (Equation&nbsp;21) ，并非在优化 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。</p>
</section>
<section id="k_3-导出的梯度" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="k_3-导出的梯度"><span class="header-section-number">4.1.4</span> <img src="https://latex.codecogs.com/png.latex?k_3"> 导出的梯度</h3>
<p>向 Equation&nbsp;16 代入 <img src="https://latex.codecogs.com/png.latex?k%20=%20k_3%20=%20(r%20-%201)%20-%20%5Clog%20r%20=%20(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20-%201)%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D">，导出的单条轨迹 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D"> 的梯度为 <span id="eq-kl-loss-grad-sample-k3"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cnabla_%7B%5Ctheta%7D%20k%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20-%201%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20-%20%5Cfrac%7B%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D%5E%7B2%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20-%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5C%5C%0A=&amp;%20-%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D%5E%7B2%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright)%20-%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20%5C%5C%0A=&amp;%20-%20%5Cleft(%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta%7D%5E%7B2%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B23%7D"></span></p>
<p>其中，根据 Equation&nbsp;19，<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%20%5Cright%5D%20=%200">，不妨直接省略。</p>
<p>而剩余部分似乎很难通过消去 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)"> 来提出 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D"> 并准确分析。但显然也并非在优化 KL 散度。</p>
</section>
<section id="小结流行的-kl-loss-项-实现并不合理" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="小结流行的-kl-loss-项-实现并不合理"><span class="header-section-number">4.1.5</span> 小结：流行的 ”KL loss 项“ 实现并不合理</h3>
<p>综上所述，对于 OpenRLHF 实现的 “KL loss 项”，</p>
<ol type="1">
<li>对同一轨迹内的 “KL 估计量” 求均值这一操作很可能是错误的，正确操作应当为求和，对应于根据对数条件概率求对数联合概率。</li>
<li><img src="https://latex.codecogs.com/png.latex?k_1"> 导出的梯度
<ol type="1">
<li>若对同一轨迹内的 “KL 估计量” 求均值，则会导致输出长度减小，</li>
<li>而如果修正为求和，则其期望为 0，在平均意义上不改分布。</li>
</ol></li>
<li><img src="https://latex.codecogs.com/png.latex?k_2">，<img src="https://latex.codecogs.com/png.latex?k_3"> 导出的梯度则十分复杂，难以分析，但都并非在优化 KL 散度，这可能是因为其错误地将 KL 估计样本量应用于动作对数条件似然并求和。回顾 KL 估计量公式 (Equation&nbsp;12) ，应当注意到这些估计量是直接作用于似然 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)">，而没有保证作用于概率后求积/对数和仍然有意义。</li>
</ol>
</section>
</section>
<section id="分析流行的-kl-reward-项-实现" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="分析流行的-kl-reward-项-实现"><span class="header-section-number">4.2</span> 分析流行的 “KL reward 项“ 实现</h2>
<section id="sec-analogy-pg-kl" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-analogy-pg-kl"><span class="header-section-number">4.2.1</span> 类比 PG 优化 reward 来分析 KL reward 的作用</h3>
<p>由于 PG 优化的就是 reward，因此我们不妨从 PG 的估计出发。最常用的 PG 估计方式应当是： <span id="eq-pg-est-adv"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5Br(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Chat%7BA%7D_t%20%5Cright%5D%0A%5Ctag%7B24%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t"> 为优势（Advantage）的估计量。</p>
<p>为了方便观察 KL reward 项发挥的作用，我们将 <img src="https://latex.codecogs.com/png.latex?r_%7B%5Cmathbf%7B%5Ctau%7D%7D"> 展开，并不妨考虑一个更简单的估计，例如：</p>
<p><span id="eq-pg-est-ret"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20r(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Csum_%7Bt'=1%7D%5E%7B%7C%5Ctau%7C%7D%20r(s_%7Bt'%7D,%20a_%7Bt'%7D)%20%5Cright%5D%0A%5Ctag%7B25%7D"></span></p>
<p>简洁起见，这里省略了该估计方式正确性的证明，有兴趣的读者可以参考 UCB CS285 “Policy Gradient” 一讲<sup>16</sup>。</p>
<p>类比 <img src="https://latex.codecogs.com/png.latex?r_%7Bt'%7D"> 导出的梯度期望，将负的 KL 样本量 <img src="https://latex.codecogs.com/png.latex?-%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%20%5Cright)%7D"> 加入 reward <img src="https://latex.codecogs.com/png.latex?r_%7Bt'%7D"> 代入其中，导出的梯度期望为：</p>
<p><span id="eq-kl-grad-est-markov-1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%20%5Cleft(%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%20%5Cright)%20%5Cright)%20%5Csum_%7Bt'=1%7D%5E%7B%7C%5Ctau%7C%7D%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_%7Bt'%7D%20%5Cmid%20s_%7Bt'%7D%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(a_%7Bt'%7D%20%5Cmid%20s_%7Bt'%7D%20%5Cright)%7D%20%5Cright%5D%20=%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%5Cright%5D%0A%5Ctag%7B26%7D"></span></p>
<p>注意，以上推导假设 RL 优化的序列决策过程满足一阶 Markov 性质 (Equation&nbsp;6)。</p>
<p>实际上，还可以扩展到任意序列决策过程，即要求条件概率依赖于所有历史状态和动作，则对应的 KL 梯度期望为：</p>
<p><span id="eq-kl-grad-est-dp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A%5Cto&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20%5Clog%20%5Cfrac%7B%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20%5Clog%20%5Cfrac%7B%20p(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%7D%7B%20p(%5Cmathbf%7Bs%7D_1)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C-1%7D%20p(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20%5Clog%20%5Cfrac%7B%20p_%5Ctheta%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cright)%7D%7B%20p_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%20%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7B%5Ctau%7D%5Cright)%7D%20%5Cright%5D%20%5C%5C%0A=%20&amp;%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B27%7D"></span></p>
<p>可见，计算 KL 样本量并放入 reward 中，导出的梯度期望即为两个分布的 KL 散度的负梯度，则最大化 reward，就会最小化 KL 散度，是正确的做法。</p>
<aside id="footnotes-12" class="footnotes footnotes-end-of-section">
<hr>
<ol start="16">
<li id="fn16"><p>https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf↩︎</p></li>
</ol>
</aside>
</section>
<section id="不同-kl-估计量导出的-reward-项的作用" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="不同-kl-估计量导出的-reward-项的作用"><span class="header-section-number">4.2.2</span> 不同 KL 估计量导出的 reward 项的作用</h3>
<p>不难注意到，Section&nbsp;4.2.1 中的 KL 样本量对应于 <img src="https://latex.codecogs.com/png.latex?k_1"> 估计量。</p>
<p>一个自然的问题是，如果对动作条件似然使用 <img src="https://latex.codecogs.com/png.latex?k_2"> 或 <img src="https://latex.codecogs.com/png.latex?k_3"> 等其他估计量，会得到什么结果？</p>
<p><img src="https://latex.codecogs.com/png.latex?k_2"> 或 <img src="https://latex.codecogs.com/png.latex?k_3"> 等其他估计量导致的一个问题，求和时通常无法得到联合概率。具体来说，其他估计量分别在优化</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?k_2">: <img src="https://latex.codecogs.com/png.latex?-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20%5Cright)%5E%7B2%7D%20%5Cright%5D"></li>
<li><img src="https://latex.codecogs.com/png.latex?k_3">: <img src="https://latex.codecogs.com/png.latex?-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20(%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%20%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%20-%201%20-%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D%7B%5Cpi_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%20%5Cright)%7D)%20%5Cright%5D"></li>
</ul>
<p>显然，这里的求和无法得到联合概率，也就无法实现类似 Equation&nbsp;27 中的效果了。</p>
</section>
<section id="小结在-on-policy-设置下修正-grpo-目标的-kl-项" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="小结在-on-policy-设置下修正-grpo-目标的-kl-项"><span class="header-section-number">4.2.3</span> 小结：在 on-policy 设置下修正 GRPO 目标的 KL 项</h3>
<p>若对动作对数条件似然计算 KL 估计样本量，则由于涉及到求和，<img src="https://latex.codecogs.com/png.latex?k_1"> 之外的估计量通常没有良好定义。</p>
<p>但是若放弃对动作条件似然计算 KL 估计样本量，而是对求和之后的对数（条件）似然进行计算，则只需满足</p>
<p><span id="eq-kl-reward-grad-expect-general"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%20%20k%5Cleft(%5Cfrac%7B%20p_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%7B%20p_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%5Cright)%20%5Cright%5D%0A%5Capprox%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cfrac%7B1%7D%7BN%7D%20k%5Cleft(%5Cfrac%7B%20p_%7B%5Ctext%7Bref%7D%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%7B%20p_%7B%5Ctheta%7D%5Cleft(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%20%20%5Cright)%7D%5Cright)%0A%5Capprox%20%5Cnabla_%7B%5Ctheta%7D%20-%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%0A%5Ctag%7B28%7D"></span></p>
<p>暂时不考虑 off-policy 问题，根据 Equation&nbsp;28, GRPO 公式 (Equation&nbsp;1, Equation&nbsp;2) 应当修正 KL 项如下：</p>
<p><span id="eq-grpo-obj-kl-fixed"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cleft%5C%7B%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D%20%20%5Cright%5C%7D%20%20-%5Cbeta%20k%5Cleft(%20%5Cfrac%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B29%7D"></span></p>
</section>
</section>
</section>
<section id="sec-derive-kld-grad" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> 推导 on-policy 设置下 KL 散度的梯度估计</h1>
<p>前文中，我们分析了流行的 LLM RL 框架中对 KL 散度优化的实现，并得出了结论。另一种思路是直接推导出 KL 散度的梯度估计表达式，并据此实现代码。</p>
<p>由于我们使用的是梯度法，为了优化 KL 散度，我们需要准确估计的是 KL 散度的梯度而非其本身。类似地，在 PG 中，我们需要最大化 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Br(%5Cmathbf%7B%5Ctau%7D)%5D">，估计的是其梯度 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Br(%5Cmathbf%7B%5Ctau%7D)%5D=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Br(%5Cmathbf%7B%5Ctau%7D)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%5D">而不是<img src="https://latex.codecogs.com/png.latex?r(%5Cmathbf%7B%5Ctau%7D)"> 本身。</p>
<p>同时，如 Section&nbsp;4.1 所述，先前向传播估计 KL 散度，再直接反向传播，通常是无法直接得到 KL 散度的梯度的。所以，我们需要直接估计 KL 散度的梯度。</p>
<p>首先，展开 KL 散度的表达式：</p>
<p><span id="eq-kl-expansion-sum"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%5Cmathbf%7Bs%7D_t)%7D%5Cright%5D%20%5C%5C%0A&amp;%20%5Cpropto%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B30%7D"></span></p>
<p>再计算其梯度：</p>
<p><span id="eq-kl-grad-expansion"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Cpropto%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s_1)%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cright)%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C-1%7D%20p(s_%7Bt+1%7D%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%5Cright)%20%20%5C%5C%0A&amp;%20%5Ccdot%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s_1)%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%20-%201%7D%20p(s_%7Bt+1%7D%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%5Cright)%20%5C%5C%0A&amp;%20%5Ccdot%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cright)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%5Cmid%20%20s_1,%20a_1,%20%5Ccdots,%20s_t)%7D%5Cright)%20%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B31%7D"></span></p>
<p>Equation&nbsp;31 中的梯度相当复杂，难以直接计算。接下来，我们将引入一系列合理的假设来简化它。</p>
<section id="在已知环境中简化-kl-梯度估计" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="在已知环境中简化-kl-梯度估计"><span class="header-section-number">5.1</span> 在已知环境中简化 KL 梯度估计</h2>
<p>实际上，LLM 的许多任务中，环境中的状态转移概率分布均为已知的，有时还可能是确定性的（Deterministic）。</p>
<p>当状态转移概率分布已知时，<img src="https://latex.codecogs.com/png.latex?%5Cforall%20t,%20p_%7B%5Ctheta%7D(a_1,%20%5Ccdots,%20s_t,%20a_t%20%5Cmid%20s_1)"> 都是可以计算的，则 KL 散度可以直接写成：</p>
<p><span id="eq-kl-grad-expansion-known-transition"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Csum_%7B%5Cmathbf%7B%5Ctau%7D%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(%5Cmathbf%7Bs%7D_1)%20p_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1)%20%5Clog%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1)%7D%7Bp_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_1)%7D%20%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B32%7D"></span></p>
</section>
<section id="简写为-contextual-bandit" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="简写为-contextual-bandit"><span class="header-section-number">5.2</span> 简写为 Contextual Bandit</h2>
<p>为了方便书写，我们可以进一步将模型简化为 contextual bandit，即令 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_1%20=%20%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathcal%7BP%7D,%20(%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_T,%20%5Cmathbf%7Ba%7D_T)%20=%20%5Cmathbf%7By%7D%20%5Cin%20%5Cmathcal%7BR%7D">，其中 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D,%20%5Cmathcal%7BR%7D"> 分别表示 prompt / response 空间，则 KL 散度变为：</p>
<p><span id="eq-def-kl-cb"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(x,%20y)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B33%7D"></span></p>
<p>其梯度变为：</p>
<p><span id="eq-def-kl-grad-cb"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5C%5C%0A&amp;%20=%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B34%7D"></span></p>
<p>其中梯度项可以进一步展开为：</p>
<p><span id="eq-def-kl-grad-cb-grad-term"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%5Cright)%20%5C%5C%0A=&amp;%20%5Cleft(%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20+%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5C%5C%0A=&amp;%20%5Cleft(%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20+%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cfrac%7B1%7D%7B%5Cpi_%5Ctheta(y%20%5Cmid%20x)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Cleft(%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20+%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B35%7D"></span></p>
<p>代入回 KL 梯度表达式：</p>
<p><span id="eq-def-kl-grad-cb-expect"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7B(x,%20y)%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p(s)%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%20+%201%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20+%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y%20%5Cmid%20x)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B36%7D"></span></p>
<p>这里为了重新获得期望形式，引入了 <img src="https://latex.codecogs.com/png.latex?1%20=%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20/%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)">，并利用了 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%20=%20%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D%7B%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%7D"> 和 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B(x,%20y)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y%20%5Cmid%20x)%5Cright%5D%20=%200">。</p>
<p>进行 Monte Carlo 估计：</p>
<p><span id="eq-def-kl-grad-cb-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(y_i%20%5Cmid%20x_i)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(y_i%20%5Cmid%20x_i)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(y_i%20%5Cmid%20x_i)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B37%7D"></span></p>
<p>其中 <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7Bx%7D_i,%20%5Cmathbf%7By%7D_i)%20%5Csim%20p_%7B%5Ctheta%7D">。</p>
</section>
<section id="还原为已知环境决策过程" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="还原为已知环境决策过程"><span class="header-section-number">5.3</span> 还原为已知环境决策过程</h2>
<p>将上面的 KL 梯度表达式还原为已知环境决策过程建模的形式：</p>
<p><span id="eq-def-kl-grad-kt-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%7D%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7BT%7D,%20%5Cmathbf%7Ba%7D_%7BT%7D)%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_t)%7D%5Cright)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_t)%5Cright)%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B38%7D"></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20&amp;%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7B1,%20t%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7B1,%20t%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%7D%5Cright)%20%5Cleft(%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7B1,%20t%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B39%7D"></span></p>
</section>
<section id="利用因果性技巧化简-kl-梯度估计" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="利用因果性技巧化简-kl-梯度估计"><span class="header-section-number">5.4</span> 利用因果性技巧化简 KL 梯度估计<sup>17</sup></h2>
<p>因果性技巧（Causality Trick）是分析序列决策过程时一个非常有用的技巧，其充分利用了因果性与“对数（条件）似然的梯度在似然（条件）概率分布上的期望为 0” 这两个性质。</p>
<p>对于任何 <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20%7C%5Ctau%7C">，我们有 <span id="eq-score-expect-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%7D%5Cleft%5B%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7Ba_t%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5C%5C%0A=&amp;%20%5Csum_%7Ba_j%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cpi_%5Ctheta(a_j%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_j)%20%5Ccdot%200%20%5C%5C%0A=&amp;%200%0A%5Cend%7Baligned%7D%0A%5Ctag%7B40%7D"></span></p>
<p>更进一步，如果 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D"> 是一个与 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D_t,%20%5Cmathbf%7Bs%7D_%7Bt+1%7D,%20%5Cmathbf%7Ba%7D_%7Bt+1%7D,%20%5Cldots"> 独立的随机变量，那么 <span id="eq-score-indep-mul-expect-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Ba%7D_t,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20%5Csim%20p_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D%20)%7D%20%5Cleft%5B%20%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Cright%5D%0A%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D%20)%7D%20%5Cleft%5B%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Cmathbb%7BE%7D_%7B%0A%20%20%20%20(%5Cmathbf%7Bs%7D_%7Bt+1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D,%20%5Cmathbf%7Ba%7D_%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D)%20%5Csim%20p_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D)%7D%20%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cright%5D%20%5Cright%5D%0A%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%7B%5Ctheta%7D(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt%7D%20)%7D%20%5Cleft%5B%20%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Cright%5D%0A%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%0A%20%20%20%20%20%20%20%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Ba%7D_t%20%5Csim%20%5Cpi_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t)%20%5Csim%20p_%5Ctheta%7D%20%5Cleft%5B%20%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20%5Ccdot%200%20%5Cright%5D%20%5C%5C%0A=&amp;%200%0A%5Cend%7Baligned%7D%0A%5Ctag%7B41%7D"></span></p>
<p>其中，为了利用 Equation&nbsp;40 的结论，我们利用了全期望定律，即</p>
<p><span id="eq-law-of-total-expectation"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B(%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D)%20%5Csim%20p%7D%20%5Cleft%5B%5Cmathbf%7Bx%7D%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7By%7D%20%5Csim%20p%7D%20%5Cleft%5B%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D%20%5Csim%20p(%5Ccdot%20%5Cmid%20%5Cmathbf%7By%7D)%7D%20%5B%5Cmathbf%7Bx%7D%5D%20%5Cright%5D%0A%5Ctag%7B42%7D"></span></p>
<p>来引入我们想要的期望。</p>
<p><span id="eq-score-indep-mul-expect-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Cmathbf%7B%5CPsi%7D_i%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_1,%20%5Cmathbf%7Ba%7D_1,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20%5CPsi_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%5Ctheta(s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20p_%5Ctheta(s_%7Bt+1%7D,%20%5Ccdots,%20s_%7B%7C%5Ctau%7C%7D,%20a_%7B%7C%5Ctau%7C%7D%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%20%5CPsi_%7Bt'%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5C%5C%0A=&amp;%20%5Csum_%7B(s_%7B1%7D,%20a_%7B1%7D,%20%5Ccdots,%20s_%7Bt%7D)%7D%20p_%5Ctheta(s_1,%20a_1,%20%5Ccdots,%20s_t)%20%20%5Csum_%7B(a_%7Bt%7D,%20s_%7Bt+1%7D,%20%5Ccdots,%20s_%7B%7C%5Ctau%7C%7D,%20a_%7B%7C%5Ctau%7C%7D)%7D%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5CPsi_%7Bt'%7D%20%5Cnabla_%5Ctheta%20p_%5Ctheta(s_%7Bt+1%7D,%20%5Ccdots,%20a_%7B%7C%5Ctau%7C%7D%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%20%5C%5C%0A=&amp;%20%5Csum_%7B(s_%7B1%7D,%20a_%7B1%7D,%20%5Ccdots,%20s_%7Bt%7D)%7D%20p_%5Ctheta(s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Csum_%7Ba_t%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%20%5Cpi_%5Ctheta(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t)%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t%5Cright)%20%5Csum_%7B(s_%7Bt+1%7D,%20%5Ccdots,%20s_%7B%7C%5Ctau%7C%7D,%20a_%7B%7C%5Ctau%7C%7D)%7D%20%20p_%5Ctheta(s_%7Bt+1%7D,%20%5Ccdots,%20a_%7B%7C%5Ctau%7C%7D%20%5Cmid%20s_1,%20a_1,%20%5Ccdots,%20s_t,%20a_t)%20%5CPsi_%7Bt'%7D%20%5C%5C%0A%5Cend%7Baligned%7D%0A%5Ctag%7B43%7D"></span></p>
<p>考虑 Monte Carlo 估计式 Equation&nbsp;39 中的估计量，将对数条件似然梯度的求和展开，考虑其中任意一项乘积的期望：</p>
<p><span id="eq-def-kl-grad-kt-mc-loss-estimator-one-grad"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%0A%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Cright%5D%0A%5Ctag%7B44%7D"></span></p>
<p>由于序列决策过程满足因果性，即 <img src="https://latex.codecogs.com/png.latex?%5Cforall%20t'%20%3C%20t">，<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bt'%7D,%20%5Cmathbf%7Ba%7D_%7Bt'%7D"> 独立于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bt%7D,%20%5Cmathbf%7Ba%7D_%7Bt%7D">，则可令 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CPsi%7D_%7Bt'%7D%20=%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D">，其独立于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bi,%20t%7D,%20%5Cmathbf%7Ba%7D_%7Bi,%20t%7D,%20%5Cldots">，利用 Equation&nbsp;43 的性质，则有 <span id="eq-thm-kl-grad-kt-mc-loss-estimator-one-grad-previous-zero"><img src="https://latex.codecogs.com/png.latex?%0A%5Cforall%20t'%20%3C%20t,%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau_%7Bi%7D%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5B%0A%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D)%7D%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Cright%5D%20=%200%0A%5Ctag%7B45%7D"></span></p>
<p>将 Equation&nbsp;45 代入 KL 梯度表达式 (Equation&nbsp;38) ，即可简化得到：</p>
<p><span id="eq-def-kl-grad-kt-reduce"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20=%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7BT%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%0A%5Ctag%7B46%7D"></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%0A%5Ctag%7B47%7D"></span></p>
<p>同样，要使用自动微分在反向传播时计算该梯度估计式，我们需要构造对应的 loss 函数：</p>
<p><span id="eq-def-kl-grad-kt-reduce-mc-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7BKL%7D_%7B%5Ctheta%7D%20=%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Ctext%7Bnograd%7D%5Cleft%20(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_i%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_%7Bi,%20t'%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t'-1%7D,%20s_%7Bi,%20t'%7D)%7D%20%5Cright)%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(a_%7Bi,%20t%7D%20%5Cmid%20s_%7Bi,%201%7D,%20%5Ccdots,%20a_%7Bi,%20t-1%7D,%20s_%7Bi,%20t%7D)%0A%5Ctag%7B48%7D"></span></p>
<p>这里也可以看到，KL loss 项正确的实现要求：</p>
<ol type="1">
<li>在序列内 token 间，对对数条件似然先求和，得到 KL 样本值，</li>
<li>再在序列间求均值。</li>
</ol>
<p>因此 OpenRLHF (Equation&nbsp;13) 与 verl (Equation&nbsp;14) 的权重都是错误的。</p>
<aside id="footnotes-13" class="footnotes footnotes-end-of-section">
<hr>
<ol start="17">
<li id="fn17"><p>https://www.wikiwand.com/en/articles/Policy_gradient_method↩︎</p></li>
</ol>
</aside>
</section>
<section id="sec-kl-grad-as-kl-reward" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-kl-grad-as-kl-reward"><span class="header-section-number">5.5</span> KL 梯度优化可以实现为 KL 样本值 reward</h2>
<p>在 Equation&nbsp;46 中，令 <img src="https://latex.codecogs.com/png.latex?k%5Cleft(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'%7D,%20%5Cmathbf%7Ba%7D_%7Bt'%7D%5Cright)%20=%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D">，则有： <span id="eq-def-kl-grad-kt-reduce-k"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20=%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7BT%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7BT%7D%20k%5Cleft(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt'%7D,%20%5Cmathbf%7Ba%7D_%7Bt'%7D%5Cright)%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%0A%5Ctag%7B49%7D"></span></p>
<p>不难注意到 Equation&nbsp;49 中 <img src="https://latex.codecogs.com/png.latex?k"> 与 Equation&nbsp;25 中 reward <img src="https://latex.codecogs.com/png.latex?r"> 在形式上的相似性，这也解释了为什么先前的工作要将 KL 样本值放进 reward。</p>
<p>类似地，我们可以利用 PG 的其他技巧，进一步减小该估计的方差，例如减去 baseline 等。感兴趣的读者可以进一步参考 UCB CS285<sup>18</sup> 等材料。</p>
<aside id="footnotes-14" class="footnotes footnotes-end-of-section">
<hr>
<ol start="18">
<li id="fn18"><p>https://rail.eecs.berkeley.edu/deeprlcourse/↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="off-policy-设置下如何估计-kl-散度的梯度" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> off-policy 设置下如何估计 KL 散度的梯度</h1>
<p>上面的推导中，我们假设了 RL 是 on-policy 设置，即采样策略即为最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">。</p>
<p>在这一节，我们进一步考虑 off-policy 设置，即一次采样获得样本会用于多次更新，除了第一次更新，采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 与最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> 都会不同。off-policy 设置给 KL 散度优化带来的问题在于，我们需要优化最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> 的 KL 散度，但却没有来自 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 的样本，这意味着我们无法直接使用梯度估计式 Equation&nbsp;47。</p>
<section id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="流行-llm-rl-框架中的-kl-优化实现忽略了-off-policy-问题"><span class="header-section-number">6.1</span> 流行 LLM RL 框架中的 KL 优化实现忽略了 off-policy 问题</h2>
<p>遗憾的是，对于 KL 优化，GRPO 等工作，以及目前流行的 LLM RL 框架中，包括 TRL，都忽略了 off-policy 问题：对于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20%5Cneq%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，尽管没有来自最新策略 <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctheta%7D"> 的样本，却仍然在使用基于 on-policy 设置的优化方式。</p>
<section id="trl" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="trl"><span class="header-section-number">6.1.1</span> TRL</h3>
<p>TRL 在 Listing&nbsp;1 中计算 KL 样本值使用的 <code>logprobs</code> 及其对应的轨迹样本均来自采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">。对应代码可见 Listing&nbsp;7。</p>
<div id="lst-trl-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: TRL 使用采样样本并使用 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算对数似然<sup>19</sup>
</figcaption>
<div aria-describedby="lst-trl-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1">queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_ids"</span>].to(device)</span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> unwrap_model_for_generation(</span>
<span id="cb8-5">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model, <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#...</span></span>
<span id="cb8-6">) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> unwrapped_model:</span>
<span id="cb8-7">    query_responses, logitss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> batch_generation(</span>
<span id="cb8-8">        unwrapped_model.policy,</span>
<span id="cb8-9">        queries,</span>
<span id="cb8-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb8-11">    )</span>
<span id="cb8-12"></span>
<span id="cb8-13"></span>
<span id="cb8-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, queries.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], args.local_rollout_forward_batch_size):</span>
<span id="cb8-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb8-16">    logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logitss[i : i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.local_rollout_forward_batch_size]</span>
<span id="cb8-17">    logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> selective_log_softmax(logits, response)</span></code></pre></div></div>
</div>
</figure>
</div>
<p>注意，基于 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctau%7D%20%5Csim%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算的 KL 样本值可以用于估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">，在第一次更新时，由于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20=%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，所以也可以用于估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。但问题在于，从第二次更新开始，<img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20%5Cneq%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，而我们仍然希望估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。</p>
<p>随后进行多轮 PPO 更新时，TRL 并没有基于当前策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 重新估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。对应代码可见 Listing&nbsp;8。</p>
<div id="lst-trl-ppo-update" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: TRL PPO 多轮更新
</figcaption>
<div aria-describedby="lst-trl-ppo-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Do multiple epochs of PPO training, with a fresh random shuffle in each epoch</span></span>
<span id="cb9-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ppo_epoch_idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(args.num_ppo_epochs):</span>
<span id="cb9-3">    b_inds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.permutation(args.local_batch_size)</span>
<span id="cb9-4">    minibatch_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> mini_batch_start <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, args.local_batch_size, args.local_mini_batch_size):</span>
<span id="cb9-6">        mini_batch_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mini_batch_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.local_mini_batch_size</span>
<span id="cb9-7">        mini_batch_inds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> b_inds[mini_batch_start:mini_batch_end]</span>
<span id="cb9-8">        gradient_accumulation_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> micro_batch_start <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, args.local_mini_batch_size, args.per_device_train_batch_size):</span>
<span id="cb9-10">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> accelerator.accumulate(model):</span>
<span id="cb9-11">                micro_batch_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> micro_batch_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.per_device_train_batch_size</span>
<span id="cb9-12">                micro_batch_inds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mini_batch_inds[micro_batch_start:micro_batch_end]</span>
<span id="cb9-13">                mb_advantage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> advantages[micro_batch_inds]</span>
<span id="cb9-14">                mb_responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> responses[micro_batch_inds]</span>
<span id="cb9-15">                mb_query_responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> query_responses[micro_batch_inds]</span>
<span id="cb9-16">                mb_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logprobs[micro_batch_inds]</span>
<span id="cb9-17">                mb_return <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns[micro_batch_inds]</span>
<span id="cb9-18">                mb_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> values[micro_batch_inds]</span>
<span id="cb9-19"></span>
<span id="cb9-20"></span>
<span id="cb9-21">                output, vpred_temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> forward(model, mb_query_responses, processing_class.pad_token_id)</span>
<span id="cb9-22">                logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output.logits[:, context_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> : <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb9-23">                logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> args.temperature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span></span>
<span id="cb9-24">                new_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> selective_log_softmax(logits, mb_responses)</span>
<span id="cb9-25">                new_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.masked_fill(</span>
<span id="cb9-26">                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB</span>
<span id="cb9-27">                )</span>
<span id="cb9-28">                vpred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vpred_temp[:, context_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> : <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].squeeze(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-29">                vpred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb9-30">                vpredclipped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.clamp(</span>
<span id="cb9-31">                    vpred,</span>
<span id="cb9-32">                    mb_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> args.cliprange_value,</span>
<span id="cb9-33">                    mb_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.cliprange_value,</span>
<span id="cb9-34">                )</span>
<span id="cb9-35">                vf_losses1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.square(vpred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mb_return)</span>
<span id="cb9-36">                vf_losses2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.square(vpredclipped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mb_return)</span>
<span id="cb9-37">                vf_loss_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(vf_losses1, vf_losses2)</span>
<span id="cb9-38">                vf_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> masked_mean(vf_loss_max, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>padding_mask_p1[micro_batch_inds])</span>
<span id="cb9-39">                vf_clipfrac <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(</span>
<span id="cb9-40">                    (vf_losses2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> vf_losses1).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>padding_mask_p1[micro_batch_inds]</span>
<span id="cb9-41">                )</span>
<span id="cb9-42">                logprobs_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> new_logprobs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mb_logprobs</span>
<span id="cb9-43">                ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(logprobs_diff)</span>
<span id="cb9-44">                pg_losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mb_advantage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ratio</span>
<span id="cb9-45">                pg_losses2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mb_advantage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.clamp(ratio, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> args.cliprange, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.cliprange)</span>
<span id="cb9-46">                pg_loss_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(pg_losses, pg_losses2)</span>
<span id="cb9-47">                pg_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> masked_mean(pg_loss_max, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>padding_mask[micro_batch_inds])</span>
<span id="cb9-48">                loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.vf_coef <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> vf_loss</span>
<span id="cb9-49">                accelerator.backward(loss)</span>
<span id="cb9-50">                optimizer.step()</span>
<span id="cb9-51">                optimizer.zero_grad()</span></code></pre></div></div>
</div>
</figure>
</div>
<aside id="footnotes-15" class="footnotes footnotes-end-of-section">
<hr>
<ol start="19">
<li id="fn19"><p>https://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432↩︎</p></li>
</ol>
</aside>
</section>
<section id="openrlhf-1" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="openrlhf-1"><span class="header-section-number">6.1.2</span> OpenRLHF</h3>
<p>类似地，OpenRLHF 在 Listing&nbsp;2 中计算 KL 样本值使用的 <code>log_probs</code> 在 <code>make_experience</code> 时被计算，和对应的样本 <code>sequences</code> 都来自采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">，而非当前策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D">。对应代码可见 Listing&nbsp;9。</p>
<div id="lst-openrlhf-sample-and-calc-old-logprob" class="python listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;9: OpenRLHF 采样样本并使用 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算对数似然
</figcaption>
<div aria-describedby="lst-openrlhf-sample-and-calc-old-logprob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595</span></span>
<span id="cb10-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> make_experience(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, samples: Samples) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Experience:</span>
<span id="cb10-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb10-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.</span></span>
<span id="cb10-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb10-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680</span></span>
<span id="cb10-8">    action_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor(</span>
<span id="cb10-9">        sequences,</span>
<span id="cb10-10">        num_actions,</span>
<span id="cb10-11">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-12">    )</span>
<span id="cb10-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709</span></span>
<span id="cb10-15">    kl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_approx_kl(</span>
<span id="cb10-16">        action_log_probs,</span>
<span id="cb10-17">        base_action_log_probs,</span>
<span id="cb10-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ...</span></span>
<span id="cb10-19">    )</span></code></pre></div></div>
</div>
</figure>
</div>
<p>从 Listing&nbsp;3 可见，OpenRLHF 在多次更新中，对于 KL reward，并没有重新计算，还是沿用了基于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 的 KL 样本值。注意，虽然其中 KL loss 项的计算使用了基于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 计算的对数似然，但如 Section&nbsp;4.1 所述，KL loss 项的实现通常是错误的，且同样依赖于 on-policy 设置。</p>
</section>
<section id="verl-1" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="verl-1"><span class="header-section-number">6.1.3</span> verl</h3>
<p>从 Listing&nbsp;4 可见，verl 同样使用 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 计算 KL 样本值。</p>
<p>从 Listing&nbsp;5 可见，verl 在多次更新中，对于 KL reward，也会沿用基于 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 的 KL 样本值。</p>
</section>
</section>
<section id="利用重要性采样处理-off-policy-设置" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="利用重要性采样处理-off-policy-设置"><span class="header-section-number">6.2</span> 利用重要性采样处理 off-policy 设置</h2>
<p>off-policy 设置下，我们没有来自最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 的样本，而只能使用来自采样策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 的样本，但我们仍然希望估计 <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%20%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D">。</p>
<p>熟悉 off-policy PG 的读者可能已经想到了，我们可以使用重要性采样（Importance Sampling，IS）技巧来解决这一问题，即</p>
<p><span id="eq-is-off-policy-kl"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%20%5Cleft%5Bf(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta%7D(%5Ctau)%20f(%5Ctau)%20%20=%20%5Csum_%7B%5Ctau%20%5Cin%20%5Cmathcal%7BT%7D%7D%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Ctau)%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Ctau)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Ctau)%7D%20f(%5Ctau)%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%20%5Cleft%5B%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20f(%5Cmathbf%7B%5Ctau%7D)%5Cright%5D%0A%5Ctag%7B50%7D"></span></p>
<p>此处，重要性采样系数 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D"> 可以仿照 Equation&nbsp;5 展开为：</p>
<p><span id="eq-is-coef-expansion"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7B%5Ctau%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7B%5Ctau%7D)%7D%20=%20%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%0A%5Ctag%7B51%7D"></span> <sup>20</sup></p>
<p>利用重要性采样 (Equation&nbsp;50, Equation&nbsp;51) ，KL 梯度表达式 Equation&nbsp;46 可以转化为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%20%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A=&amp;%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%20%5C%5C%0A=&amp;%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7BT%7D,%20%5Cmathbf%7Ba%7D_%7BT%7D)%7D%7Bp_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Cmathbf%7Ba%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Bs%7D_%7BT%7D,%20%5Cmathbf%7Ba%7D_%7BT%7D)%7D%20%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%20%5Cright%5D%20%5C%5C%0A=&amp;%20%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Ctau%7D%20%5Csim%20p_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%7D%5Cright)%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bt'%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt'%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7B1%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bt-1%7D,%20%5Cmathbf%7Bs%7D_%7Bt%7D)%20%5Cright%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B52%7D"></span></p>
<p>对应的 Monte Carlo 估计式为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_%7B%5Ctheta%7D%20%5Cmathbb%7BD%7D_%7B%5Ctext%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%20%5C%5C%0A%5Capprox&amp;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Cleft(%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%20%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t'-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%20%5C%5C%0A=&amp;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Cleft(%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%20%5Csum_%7Bt'=t%7D%5E%7B%7C%5Cmathbf%7B%5Ctau%7D_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%20%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%20%5Cright)%20%5Cnabla_%7B%5Ctheta%7D%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B53%7D"></span></p>
<p>对应的 loss 函数为：</p>
<p><span id="eq-def-kl-grad-kt-reduce-is-mc-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7BKL%7D_%7B%5Ctheta%7D%20=%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=1%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%20%5Ctext%7Bnograd%7D%5Cleft(%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%20%5Cright)%20%5Clog%20%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%0A%5Ctag%7B54%7D"></span></p>
<p>类似 Equation&nbsp;49，我们可以令</p>
<p><span id="eq-def-kl-reward-is"><img src="https://latex.codecogs.com/png.latex?%0Ak(%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%20=%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%201%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D,%20%5Cmathbf%7Bs%7D_%7Bi,%20t%7D)%7D%5Cright)%20%5Csum_%7Bt'=t%7D%5E%7B%7C%5Ctau_%7Bi%7D%7C%7D%20%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Cmathbf%7Ba%7D_%7Bi,%20t'%7D%20%7C%20%5Cmathbf%7Bs%7D_%7Bi,%20t'%7D,%20%5Ccdots,%20%5Cmathbf%7Ba%7D_%7Bi,%20t-1%7D)%7D%0A%5Ctag%7B55%7D"></span></p>
<p>注意，Equation&nbsp;55 中的 <img src="https://latex.codecogs.com/png.latex?k"> 需要对于每个新的 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D"> 重新计算。</p>
<aside id="footnotes-16" class="footnotes footnotes-end-of-section">
<hr>
<ol start="20">
<li id="fn20"><p>实际计算中，Equation&nbsp;51 由于涉及到 <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7B%5Ctau%7D%7C"> 次连乘，方差大且数值稳定性差，需要利用因果性、近似等技术来化简。本文目前省略该部分，后续将会更新相关内容。↩︎</p></li>
</ol>
</aside>
</section>
</section>
<section id="结论如何正确地在-rl-中优化-kl-散度" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> 结论：如何正确地在 RL 中优化 KL 散度</h1>
<section id="修正-grpo-公式中的-kl-项" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="修正-grpo-公式中的-kl-项"><span class="header-section-number">7.1</span> 修正 GRPO 公式中的 KL 项</h2>
<p>GRPO 公式 (Equation&nbsp;1, Equation&nbsp;2) 对于 KL 优化主要存在两个错误：</p>
<ol type="1">
<li>忽略了 KL 优化的 off-policy 问题</li>
<li>先将 <img src="https://latex.codecogs.com/png.latex?k_%7B3%7D"> 估计样本量应用于动作条件似然再求和，导致得到异常的梯度</li>
</ol>
<p>对于这两个问题，在 Equation&nbsp;29 的基础上，仿照 Equation&nbsp;55，我们可以按如下方式修正：</p>
<p><span id="eq-grpo-obj-kl-fixed-is"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Cleft%5C%7B%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D%20%5Cright%5C%7D%20-%5Cbeta%20%5Cleft(%5Cprod_%7Bt=1%7D%5E%7B%7Co_%7Bi%7D%7C%7D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(o_%7Bi,%20t%7D%20%7C%20q,%20o_%7Bi,%5Clt%20t%7D)%7D%7B%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(o_%7Bi,%20t%7D%20%7C%20q,%20o_%7Bi,%5Clt%20t%7D)%7D%5Cright)%20k%5Cleft(%20%5Cfrac%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cprod_%7Bt=1%7D%5E%7B%7Co_i%7C%7D%20%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Cright)%0A%5Cend%7Baligned%7D%0A%5Ctag%7B56%7D"></span></p>
</section>
<section id="修正流行-llm-rl-框架中的-kl-优化实现" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="修正流行-llm-rl-框架中的-kl-优化实现"><span class="header-section-number">7.2</span> 修正流行 LLM RL 框架中的 KL 优化实现</h2>
<p>目前流行的 LLM RL 框架中的 KL 优化实现，除了 GRPO 公式中体现的两个问题之外，还存在以下问题：</p>
<ol type="1">
<li>实现单独的 KL loss 项时，默认不去除任何梯度，（这可能是误以为直接前向传播估计 KL 散度，再反向传播就能得到正确的梯度导致的）</li>
<li>错误地实现了平均操作</li>
</ol>
<p>对于这些问题，可以按照如下思路修正：</p>
<ol type="1">
<li>为 KL 项添加重要性采样，这需要从第二轮更新开始，每次基于新的 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> 重新计算 KL loss / reward 项，包括重要性采样系数</li>
<li>应用 KL 估计样本量时，先对于序列内 token 间的对数条件似然求和，得到轨迹联合概率，再代入公式</li>
<li>如果希望像对于 reward 优化一样使用基线、折扣、GAE等技术，可以按 Equation&nbsp;55 实现为 KL reward 项（尽管这些技术背后的考量并不一定适合 KL 散度，例如 reward 是允许自定义的，但 KL 散度有明确的定义）</li>
<li>如果不希望应用 reward 优化的其他技术，可以按 Equation&nbsp;54 实现为 KL loss 项</li>
</ol>
</section>
</section>
<section id="讨论" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> 讨论</h1>
<section id="对于-kl-梯度更好的估计样本量" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="对于-kl-梯度更好的估计样本量"><span class="header-section-number">8.1</span> 对于 KL 梯度更好的估计样本量</h2>
<p>如 Section&nbsp;5.5 所述，PG 使用了许多其他技术来改进其梯度估计，能否使用类似技术改进 KL 梯度估计？</p>
<p>此外，John Schulman 的博客是针对估计 KL 散度分析了不同的估计样本量。但这些分析对于估计 KL 散度的梯度是否还成立？</p>
</section>
<section id="kl-regularized-rl-的理论优势" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="kl-regularized-rl-的理论优势"><span class="header-section-number">8.2</span> KL-Regularized RL 的理论优势</h2>
<p>最近基于可验证 reward 的 RL 非常流行，其很大程度上避免了 reward hacking，直觉上，我们似乎不再需要相对于参考策略的 KL 正则化。</p>
<p>然而，也有一些工作指出，KL-Regularized RL 在理论上还有许多其他优势。例如 <span class="citation" data-cites="zhao2025logregretkl">Zhao et al. (2025)</span> 证明了 KL-regularized RL 的 regret 只有 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Clog%20T)">，而常见的基于 contextual bandit 或 MDP 建模的 RL 方法 regret 通常不低于 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Csqrt%7BT%7D)">。粗浅地说，这是因为 KL 正则化目标项的存在，使得 value 分解有了特别的性质，例如凸性更强。</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="附录" class="level1 appendix" data-number="9"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9</span> 附录</h2><div class="quarto-appendix-contents">

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>本文的作者（童雨轩）仍在寻求北美的 Ph.D.&nbsp;或 RA 机会。如果你觉得本文对你有帮助，欢迎浏览其主页<sup>21</sup>来获取进一步了解。</p>
</div>
</div>




</div></section><section id="相关工作" class="level2 appendix" data-number="9.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.1</span> 相关工作</h2><div class="quarto-appendix-contents">

<p>与本文同期也有许多精彩的讨论，由于笔者还没能通读全文，此处仅提供链接，不作概括，欢迎感兴趣的读者自行阅读：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/28440962040">GRPO 中的 KL Loss 实现细节问题 - Hongyu Zang @ 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28735759256">k2 loss就是比k3 loss好！以及GRPO_off-policy - Yiming Liu @ 知乎</a></li>
</ul>
<aside id="footnotes-17" class="footnotes footnotes-end-of-section">
<hr>
<ol start="21">
<li id="fn21"><p>https://tongyx361.github.io↩︎</p></li>
</ol>
</aside>
</div></section><section id="写作契机trpoppo-与-grpo-中的-kl-为什么不一样" class="level2 appendix" data-number="9.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.2</span> 写作契机：“TRPO/PPO 与 GRPO 中的 KL 为什么不一样？”</h2><div class="quarto-appendix-contents">

<p>笔者对 RL 中 KL 优化相关问题的思考主要开始于 X 上 Fanyi Pu 提出了这样一个问题<sup>22</sup>：</p>
<blockquote class="blockquote">
<p>A small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?</p>
<p>TRPO <span class="citation" data-cites="schulman2015trpo">(Schulman et al. 2015)</span></p>
</blockquote>
<p><span id="eq-trpo"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cunderset%7B%5Ctheta%7D%7B%5Ctext%7Bmaximize%7D%7D~L_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D(%5Ctheta)%20%5C%5C%0A&amp;%20%5Ctext%20%7B%20subject%20to%20%7D%20%5Cbar%7BD%7D_%7B%5Cmathrm%7BKL%7D%7D%5E%7B%5Crho_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%7D%5Cleft(%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D,%20%5Ctheta%5Cright)%20%5Cleq%20%5Cdelta%0A%5Cend%7Baligned%7D%0A%5Ctag%7B57%7D"></span></p>
<blockquote class="blockquote">
<p>PPO <span class="citation" data-cites="schulman2017ppo">(Schulman et al. 2017)</span></p>
</blockquote>
<p><span id="eq-ppo-klpen"><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7BK%20L%20P%20E%20N%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7By%7D_t%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(%5Cmathbf%7By%7D_t%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright)%7D%20%5Chat%7BA%7D_t-%5Cbeta%20%5Cmathrm%7BKL%7D%5Cleft%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright),%20%5Cpi_%5Ctheta%5Cleft(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bx%7D_t%5Cright)%5Cright%5D%5Cright%5D%0A%5Ctag%7B58%7D"></span></p>
<blockquote class="blockquote">
<p>GRPO <span class="citation" data-cites="shao2024deepseekmath">(Shao et al. 2024)</span></p>
</blockquote>
<p><span id="eq-grpo"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cmathcal%7BJ%7D_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)=%5Cmathbb%7BE%7D%5Cleft%5Bq%20%5Csim%20P(Q),%5Cleft%5C%7Bo_i%5Cright%5C%7D_%7Bi=1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D(O%20%5Cmid%20q)%5Cright%5D%20%5C%5C%0A&amp;%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi=1%7D%5EG%20%5Cfrac%7B1%7D%7B%5Cleft%7Co_i%5Cright%7C%7D%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%7Co_i%5Cright%7C%7D%5Cleft%5C%7B%5Cmin%20%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bo%20l%20d%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%20%5Chat%7BA%7D_%7Bi,%20t%7D,%20%5Ctext%7Bclip%7D%5Cleft(%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(o_%7Bi,%20t%7D%20%5Cmid%20q,%20o_%7Bi,%5Clt%20t%7D%5Cright)%7D,%201-%5Cvarepsilon,%201+%5Cvarepsilon%5Cright)%20%5Chat%7BA%7D_%7Bi,%20t%7D%5Cright%5D-%5Cbeta%20%5Cmathbb%7BD%7D_%7BK%20L%7D%5Cleft%5B%5Cpi_%5Ctheta%20%5Cmid%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%5Cright%5D%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B59%7D"></span></p>
<p>这个问题本身的答案是非常简单的。</p>
<p>首先，这个问题混淆了两种不同的 KL 惩罚项：</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BKL%7D%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D,%5Cpi_%7B%5Ctheta%7D%5D">，其作用是约束最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D">不要离采样策略<img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> 太远，避免过大的更新导致策略崩溃，从而构成信任域（Trust Region, TR），也就是 TRPO 中的 TR。而 PPO 作为 TRPO 的近似实现，继承了这一点。</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BKL%7D%5B%5Cpi_%7B%5Ctheta%7D,%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D%5D">，其作用是约束最新策略 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta%7D">不要离参考策略<img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bref%7D%7D%7D"> 太远，从而更充分地利用参考策略中的先验。</li>
</ol>
<p>另外，这个问题忽略了 TRPO/PPO 公式中的 KL 损失项与 GRPO 公式中的 clip 函数实际上是出于同一目的，即约束 <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BKL%7D%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D,%5Cpi_%7B%5Ctheta%7D%5D">。如 PPO 论文第 3-4 节所说，两者可以相互替代或结合使用：</p>
<blockquote class="blockquote">
<p>Let <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> denote the probability ratio <img src="https://latex.codecogs.com/png.latex?r_%7Bt%7D(%5Ctheta)=%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cleft(%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft%7Ca_t%5Cright%7C%20s_t%5Cright)%7D">, so <img src="https://latex.codecogs.com/png.latex?r%5Cleft(%5Ctheta_%7B%5Ctext%7Bold%7D%7D%5Cright)=1">. TRPO maximizes a “surrogate” objective</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%20%5Chat%7BA%7D_t%5Cright%5D=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5Br_t(%5Ctheta)%20%5Chat%7BA%7D_t%5Cright%5D%20.%0A"></p>
<blockquote class="blockquote">
<p>…</p>
<p>The main objective we propose is the following:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCLIP%7D%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cmin%20%5Cleft(r_t(%5Ctheta)%20%5Chat%7BA%7D_t,%20%5Ctext%7Bclip%7D%5Cleft(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon%5Cright)%20%5Chat%7BA%7D_t%5Cright)%5Cright%5D%0A"></p>
<blockquote class="blockquote">
<p>where epsilon is a hyperparameter, say, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon=0.2">. The motivation for this objective is as follows. The first term inside the <img src="https://latex.codecogs.com/png.latex?%5Cmin"> is <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D">. The second term, <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bclip%7D%5Cleft(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon%5Cright)%20%5Chat%7BA%7D_t">, modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving <img src="https://latex.codecogs.com/png.latex?r_t"> outside of the interval <img src="https://latex.codecogs.com/png.latex?%5B1-%5Cepsilon,%201+%5Cepsilon%5D">.</p>
<p>…</p>
<p><strong>Another approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence</strong>, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence <img src="https://latex.codecogs.com/png.latex?d_%7B%5Ctext%7Btarg%7D%7D"> each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve included it here because it’s an important baseline.</p>
<p>In the simplest instantiation of this algorithm, we perform the following steps in each policy update:</p>
<ul>
<li>Using several epochs of minibatch SGD, optimize the KL-penalized objective</li>
</ul>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BKLPEN%7D%7D(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%20%5Chat%7BA%7D_t-%5Cbeta%20%5Cmathrm%7BKL%7D%5Cleft%5B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(%5Ccdot%20%5Cmid%20s_t%5Cright),%20%5Cpi_%5Ctheta%5Cleft(%5Ccdot%20%5Cmid%20s_t%5Cright)%5Cright%5D%5Cright%5D%0A"></p>
<blockquote class="blockquote">

</blockquote>
<p>顺带，还可以从以下角度理解两者的共通之处：clip 函数约束的 <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)=%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D">就是<img src="https://latex.codecogs.com/png.latex?K%20L%5Cleft%5B%5Cpi_%7B%5Ctheta_%7Bd%20d%7D%7D,%20%5Cpi_%5Ctheta%5Cright%5D=%5Cmathbb%7BE%7D_%7Ba_t%20%5Csim%20%5Cpi_%7B%5Ctheta_%7Bd%20t%7D%7D%5Cleft(%5Ccdot%20%5Cmid%20s_t%5Cright)%7D%5Cleft%5B%5Clog%20%5Cfrac%7B%5Cpi_%7B%5Ctheta_%7Bd%20t%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%5Cright%5D"> 中对单个样本 <img src="https://latex.codecogs.com/png.latex?(s_t,%20a_t)"> 的值中 <img src="https://latex.codecogs.com/png.latex?%5Clog"> 的真数。</p>
<aside id="footnotes-18" class="footnotes footnotes-end-of-section">
<hr>
<ol start="22">
<li id="fn22"><p>https://x.com/pufanyi/status/1888845956684370202↩︎</p></li>
</ol>
</aside>
</div></section><section id="致谢" class="level2 appendix" data-number="9.3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.3</span> 致谢</h2><div class="quarto-appendix-contents">

<p>感谢王浩然、YuMS 对本文提供的重要反馈。</p>
<p>感谢生广明、Wei Xiong、刘仁彪、刘威、Weixun Wang、Yiming Liu、Haibin Lin 等关于相关问题的有益讨论以及对于本文的有益反馈。</p>
<p>感谢 Cursor 和 Mathpix 在书写 LaTeX 时提供的巨大帮助。</p>
</div></section><section id="引用" class="level2 appendix" data-number="9.4"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9.4</span> 引用</h2><div class="quarto-appendix-contents">

<p>BibTeX:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode numberSource bibtex number-lines code-with-copy"><code class="sourceCode bibtex"><span id="cb11-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">@article</span>{<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">tong2025kl</span>,</span>
<span id="cb11-2">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">author</span> = {童雨轩},</span>
<span id="cb11-3">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span> = {重新思考 {RL} 中的 {KL} 梯度优化},</span>
<span id="cb11-4">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">journal</span> = {Blog},</span>
<span id="cb11-5">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">year</span> = {2025},</span>
<span id="cb11-6">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">url</span> = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},</span>
<span id="cb11-7">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">language</span> = {Chinese},</span>
<span id="cb11-8">}</span></code></pre></div></div>
<p>文本：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb12-1">童雨轩. 2025. “重新思考 RL 中的 KL 梯度优化.” https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh.</span></code></pre></div></div>


<!-- -->


</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-hu2024openrlhf" class="csl-entry">
Hu, Jian, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. <span>“OpenRLHF: An Easy-to-Use, Scalable and High-Performance RLHF Framework.”</span> <em>arXiv Preprint arXiv:2405.11143</em>.
</div>
<div id="ref-jaques2019wayoffpolicy" class="csl-entry">
Jaques, Natasha, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. <span>“Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.”</span> <a href="https://arxiv.org/abs/1907.00456">https://arxiv.org/abs/1907.00456</a>.
</div>
<div id="ref-ouyang2022instructgpt" class="csl-entry">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. <a href="https://openreview.net/forum?id=TG8KACxEON">https://openreview.net/forum?id=TG8KACxEON</a>.
</div>
<div id="ref-schulman2015trpo" class="csl-entry">
Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 1889–97. PMLR.
</div>
<div id="ref-schulman2018gae" class="csl-entry">
Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2018. <span>“High-Dimensional Continuous Control Using Generalized Advantage Estimation.”</span> <a href="https://arxiv.org/abs/1506.02438">https://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-schulman2017ppo" class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv:1707.06347</em>.
</div>
<div id="ref-shao2024deepseekmath" class="csl-entry">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, et al. 2024. <span>“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.”</span> <em>arXiv Preprint arXiv:2402.03300</em>.
</div>
<div id="ref-sheng2024hybridflow" class="csl-entry">
Sheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. <span>“HybridFlow: A Flexible and Efficient RLHF Framework.”</span> <em>arXiv Preprint arXiv: 2409.19256</em>.
</div>
<div id="ref-stiennon2020summarize" class="csl-entry">
Stiennon, Nisan, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020. <span>“Learning to Summarize with Human Feedback.”</span> In <em>NeurIPS</em>. <a href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html</a>.
</div>
<div id="ref-zhao2025logregretkl" class="csl-entry">
Zhao, Heyang, Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. 2025. <span>“Logarithmic Regret for Online KL-Regularized Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2502.07460">https://arxiv.org/abs/2502.07460</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div> ]]></description>
  <category>Chinese 中文</category>
  <category>Technical 技术</category>
  <guid>https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh/</guid>
  <pubDate>Sun, 09 Mar 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title></title>
  <dc:creator>Yuxuan Tong</dc:creator>
  <link>https://tongyx361.github.io/blogs/posts/verl-tutorial/</link>
  <description><![CDATA[ undefined ]]></description>
  <guid>https://tongyx361.github.io/blogs/posts/verl-tutorial/</guid>
  <pubDate>Wed, 07 Jan 2026 15:10:00 GMT</pubDate>
</item>
</channel>
</rss>
