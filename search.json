[
  {
    "objectID": "posts/verl-intro/index.html#learning-to-reason-with-large-scale-rl",
    "href": "posts/verl-intro/index.html#learning-to-reason-with-large-scale-rl",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "1.1 Learning to Reason with Large-Scale RL",
    "text": "1.1 Learning to Reason with Large-Scale RL\n\n\n\nTableÂ 1: Learning to reason with large-scale RL significantly boosts the performance of LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nLarge-Scale RL?\nAIME 2024\nMATH 500\nGPQA Diamond\nCode Forces\n\n\n\n\nGPT-4o (OpenAI 2024)\nâŒ\n44.6\n60.3\n50.6\n&gt;11.0%\n\n\no1 (OpenAI 2024)\nâœ…\n74.4\n94.8\n77.3\n&gt;89.0%\n\n\nR1 (DeepSeek-AI 2025)\nâœ…\n79.8\n97.3\n71.5\n&gt;96.3%"
  },
  {
    "objectID": "posts/verl-intro/index.html#learning-as-agent-with-large-scale-rl",
    "href": "posts/verl-intro/index.html#learning-as-agent-with-large-scale-rl",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "1.2 Learning as Agent with Large-Scale RL",
    "text": "1.2 Learning as Agent with Large-Scale RL\nOpenAI (2025):\n\nDeep research independently discovers, reasons about, and consolidates insights from across the web.\nTo accomplish this, it was trained on real-world tasks requiring browser and Python tool use,\nusing the same reinforcement learning methods behind OpenAI o1, our first reasoning model.\n\nCheck OpenAI Deep Researchâ€™s demo video for more details."
  },
  {
    "objectID": "posts/verl-intro/index.html#rl-is-complex-dataflow",
    "href": "posts/verl-intro/index.html#rl-is-complex-dataflow",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.1 RL is Complex Dataflow",
    "text": "2.1 RL is Complex Dataflow\n\n\n\n\n\n\nFigureÂ 1: Modelling three example RL algorithms (Schulman et al. 2017; Dai et al. 2023; Li et al. 2024) as dataflow graphs. (Source: Sheng et al. 2025)\n\n\n\nReinforcement Learning (RL) can be modelled as complex dataflow graph (Schaarschmidt et al. 2019; Liang et al. 2021; Sheng et al. 2025), consisting of:\n\nmultiple models: actor, critic, reference, reward model, etc.\nmultiple stages: generating, preparing experiences, training\nmultiple workloads: generation, inference, training"
  },
  {
    "objectID": "posts/verl-intro/index.html#llm-workloads-are-distributed",
    "href": "posts/verl-intro/index.html#llm-workloads-are-distributed",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.2 LLM Workloads Are Distributed",
    "text": "2.2 LLM Workloads Are Distributed\n\n\n\n\n\n\nFigureÂ 2: LLM workloads are often distributed, involving many GPUs and complex parallelism strategies.\n\n\n\nLLM workloads often involves:\n\nmany GPUs\ncomplex parallelism strategies"
  },
  {
    "objectID": "posts/verl-intro/index.html#rl-with-llms-is-large-scale-distributed-dataflow",
    "href": "posts/verl-intro/index.html#rl-with-llms-is-large-scale-distributed-dataflow",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.3 RL with LLMs is Large-Scale Distributed Dataflow",
    "text": "2.3 RL with LLMs is Large-Scale Distributed Dataflow\n\n\n\n\n\n\nFigureÂ 3: In RL with LLMs, each operator in the RL dataflow is a large-scale distributed computing workload itself."
  },
  {
    "objectID": "posts/verl-intro/index.html#constraints-data-dependencies-resource-limitations",
    "href": "posts/verl-intro/index.html#constraints-data-dependencies-resource-limitations",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.4 Constraints: Data Dependencies & Resource Limitations",
    "text": "2.4 Constraints: Data Dependencies & Resource Limitations\n\n\n\n\n\n\nFigureÂ 4: Implementing RL algorithm with LLMs usually requires complex trade-offs between various constraints. (Sheng et al. 2025)"
  },
  {
    "objectID": "posts/verl-intro/index.html#flexibility-single-controller",
    "href": "posts/verl-intro/index.html#flexibility-single-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.1 Flexibility: â€œSingle-Controllerâ€",
    "text": "3.1 Flexibility: â€œSingle-Controllerâ€\n\n\n\n\n\n\n\n\nFigureÂ 5: Dataflow of PPO with KL regularization, with data shown explicitly. (Source: Sheng et al. 2025)\n\n\n\n\n\n\n\nListingÂ 1: PPO core code in a few lines in verl.\n\n\nfor prompts in dataloader:\n    # Stage 1: Generation\n    batch = actor.generate_sequences(prompts)\n    # Stage 2: Experience Preparation\n    batch = reward.compute_reward(batch)\n    batch = reference.compute_log_prob(batch)\n    batch = critic.compute_values(batch)\n    batch = compute_advantage(batch, \"gae\")\n    # Stage 3: Training\n    critic.update_critic(batch)\n    actor.update_actor(batch)\n\n\n\n\n\nProgramming interface based on the â€œsingle-controllerâ€ paradigm\nRL algorithm core logic in a few lines of code!\nDiverse RL algorithms supported: PPO,Â GRPO,Â RLOO, ReMax,Â PRIME,Â DAPO, etc."
  },
  {
    "objectID": "posts/verl-intro/index.html#efficiency-multi-controller",
    "href": "posts/verl-intro/index.html#efficiency-multi-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.2 Efficiency: â€œMulti-Controllerâ€",
    "text": "3.2 Efficiency: â€œMulti-Controllerâ€\nverl is efficient for intra-operator with the â€œmulti-controllerâ€ paradigm and features like:\n\n\nParallelism Algorithms:\n\nData Parallelism\nTensor Parallelism\nPipeline Parallelism\nContext / Sequence Parallelism\n\nEfficient Kernels:\n\nFlash Attention\nTorch Compile\nLiger Kernel\n\n\nTraining Backends:\n\nFSDP\nFSDP2\nMegatron\n\nGeneration Backends:\n\nvLLM\nSGLang\nâ€¦"
  },
  {
    "objectID": "posts/verl-intro/index.html#efficiency-hybrid-engine",
    "href": "posts/verl-intro/index.html#efficiency-hybrid-engine",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.3 Efficiency: â€œHybrid Engineâ€",
    "text": "3.3 Efficiency: â€œHybrid Engineâ€\nverl is efficient for inter-operator with the â€œhybrid engineâ€ paradigm, utilizing features like:\n\noffloading & reloading enables fully utilizing the GPU memory\nresharding enables switching for the optimal parallelism strategy\n\n\n\n\n\n\n\n\nFigureÂ 6: Example of hybrid engine switching between workloads, changing DP for TP."
  },
  {
    "objectID": "posts/verl-intro/index.html#open-source-community",
    "href": "posts/verl-intro/index.html#open-source-community",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.4 Open-Source Community",
    "text": "3.4 Open-Source Community"
  },
  {
    "objectID": "posts/verl-intro/index.html#background-single-controller-vs.-multi-controller",
    "href": "posts/verl-intro/index.html#background-single-controller-vs.-multi-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.1 Background: Single-Controller vs.Â Multi-Controller",
    "text": "4.1 Background: Single-Controller vs.Â Multi-Controller\n\n\n\n\n\n\n\n\n\n\n\n(a) Single-Controller (MPMD)\n\n\n\n\n\n\n\n\n\n\n\n(b) Multi-Controller (SPMD)\n\n\n\n\n\n\n\nFigureÂ 7: Single-Controller (Multi-Program-Multi-Data) vs.Â Multi-Controller (Single-Program-Multi-Data) (Barham et al. 2022)\n\n\n\n\nSingle-Controller (MPMD): A centralized controller manages all the workers, running different programs.\nMulti-Controller (SPMD): Each worker has its own controller, running the same program with different data."
  },
  {
    "objectID": "posts/verl-intro/index.html#trade-off-single-controller-or-multi-controller",
    "href": "posts/verl-intro/index.html#trade-off-single-controller-or-multi-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.2 Trade-off: Single-Controller or Multi-Controller?",
    "text": "4.2 Trade-off: Single-Controller or Multi-Controller?\n\n\n\nTableÂ 2: Trade-off between single-controller and multi-controller.\n\n\n\n\n\nParadigm\nPro\nCon\n\n\n\n\nSingle-Controller\nFlexible\nCommunication Overhead\n\n\nMulti-Controller\nEfficient\nComplex Programming\n\n\n\n\n\n\nğŸ¤” Which paradigm should we choose?\n\nğŸ¤© Actually, we can have â€œbothâ€!"
  },
  {
    "objectID": "posts/verl-intro/index.html#new-paradigm-hybrid-controller",
    "href": "posts/verl-intro/index.html#new-paradigm-hybrid-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.3 New Paradigm: Hybrid-Controller!",
    "text": "4.3 New Paradigm: Hybrid-Controller!\nğŸ’¡ Hybrid-Controller = Single-Controller + N x Multi-Controller\n\n\n\n\n\n\nFigureÂ 8: In the hybrid-controller, a single-controller manages multiple multi-controllers to process the dataflow."
  },
  {
    "objectID": "posts/verl-intro/index.html#implementation-in-verl",
    "href": "posts/verl-intro/index.html#implementation-in-verl",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.4 Implementation in verl",
    "text": "4.4 Implementation in verl\nEach call in the single-controller (e.g.Â critic.compute_values, actor.update_actor) is an RPC to a multi-controller worker group.\n\n\n\n\n\n\nListingÂ 2: PPO core code in single-controller.\n\n\nfor prompts in dataloader:\n    # Stage 1: Generation\n    batch = actor.generate_sequences(prompts)\n    # Stage 2: Experience Preparation\n    batch = reward.compute_reward(batch)\n    batch = reference.compute_log_prob(batch)\n    batch = critic.compute_values(batch)\n    batch = compute_advantage(batch, \"gae\")\n    # Stage 3: Training\n    critic.update_critic(batch)\n    actor.update_actor(batch)\n\n\n\n\n\n\n\nListingÂ 3: Example distributed code in multi-controller.\n\n\nclass CriticWorker(3DParallelWorker):\n  @register(dispatch_mode=3D_PROTO)\n  def compute_values(self, batch: DataProto):\n      values = self.critic.forward(batch)\n      batch.update(values=values)\n# ...\nclass ActorWorker(3DParallelWorker):\n  @register(dispatch_mode=3D_PROTO)\n  def update_actor(self, batch: DataProto):\n      loss = self.actor(batch)\n      loss.backward()\n\n\n\n\n\n\nThe register decorator utility manages the distributed data transfer, which also makes multi-controller programming easier."
  },
  {
    "objectID": "posts/verl-intro/index.html#async-engine-for-multi-turn-rollout",
    "href": "posts/verl-intro/index.html#async-engine-for-multi-turn-rollout",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "5.1 Async Engine for Multi-Turn Rollout",
    "text": "5.1 Async Engine for Multi-Turn Rollout\n\n\n\n\n\n\nFigureÂ 9: Synchronous vs.Â Asynchronous rollout.1\n\n\n\n\nSynchronous Engine: returns all the outputs in the batch at the same time\nAsynchronous Engine: returns each output as soon as it is ready\n\n\n\n\n\nImage Source: https://novasky-ai.notion.site/skyrl-v0â†©ï¸"
  },
  {
    "objectID": "posts/verl-intro/index.html#basic-capability-support",
    "href": "posts/verl-intro/index.html#basic-capability-support",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "5.2 Basic Capability Support",
    "text": "5.2 Basic Capability Support\n\nMulti-Modal: Qwen2.5-VL, Kimi-VL, etc.\nMulti-Turn & Tool Using: see progress at #1882\nâ€¦"
  },
  {
    "objectID": "posts/verl-intro/index.html#diverse-environments-tools-ongoing",
    "href": "posts/verl-intro/index.html#diverse-environments-tools-ongoing",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "5.3 Diverse Environments & Tools (Ongoing)",
    "text": "5.3 Diverse Environments & Tools (Ongoing)\nWelcome to discuss about / contribute to:\n\nOur ongoing RFC #1172\nIntegrating protocols like MCP\nIntegrating existing environments & tools, e.g.,\n\n\nKORGym @ ByteDance Seed (Shi et al. 2025)\nAtropos @ Nous Research (Dakota Mahan 2025)"
  },
  {
    "objectID": "posts/verl-intro/index.html#efficient-rl-with-huge-moe-like-deepseek-v3-671b-v0.4",
    "href": "posts/verl-intro/index.html#efficient-rl-with-huge-moe-like-deepseek-v3-671b-v0.4",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "6.1 Efficient RL with Huge MoE like DeepSeek-V3-671B (V0.4+)",
    "text": "6.1 Efficient RL with Huge MoE like DeepSeek-V3-671B (V0.4+)\nverl supports efficient RL training for huge MoE like DeepSeek-V3-671B, based on the following features:\n\nTraining: MoE models classes supporting diverse parallelism strategies like Expert Parallelism based on Megatron GPTModel\nInference: Multi-node inference\nHybrid: Parameter sharding manager for Megatron-Core V0.12 + latest inference engines"
  },
  {
    "objectID": "posts/verl-intro/index.html#q3-focuses",
    "href": "posts/verl-intro/index.html#q3-focuses",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "6.2 2025 Q3 Focuses",
    "text": "6.2 2025 Q3 Focuses\n\nMFU Optimization of huge MoE model training\nMore flexible RL architecture enabling different levels of asynchrony\nAgent recipes\n\n\nFor the most timely updates of important features, please keep an eye on verlâ€™s Roadmap."
  },
  {
    "objectID": "posts/verl-intro/index.html#references",
    "href": "posts/verl-intro/index.html#references",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "",
    "text": "References\n\n\nBarham, Paul, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, et al. 2022. â€œPathways: Asynchronous Distributed Dataflow for ML.â€ Proceedings of Machine Learning and Systems 4 (April): 430â€“49. https://proceedings.mlsys.org/paper_files/paper/2022/hash/37385144cac01dff38247ab11c119e3c-Abstract.html.\n\n\nDai, Josef, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. â€œSafe RLHF: Safe Reinforcement Learning from Human Feedback.â€ In. https://openreview.net/forum?id=TyFrPOKYXw.\n\n\nDakota Mahan, Teknium, Roger Jin. 2025. â€œAtropos - An Async First Environment Rollout Controller.â€ https://www.github.com/NousResearch/Atropos.\n\n\nDeepSeek-AI. 2025. â€œDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.â€ https://arxiv.org/abs/2501.12948.\n\n\nLi, Ziniu, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. 2024. â€œReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models.â€ In. https://openreview.net/forum?id=Stn8hXkpe6.\n\n\nLiang, Eric, Zhanghao Wu, Michael Luo, Sven Mika, Joseph E Gonzalez, and Ion Stoica. 2021. â€œRllib Flow: Distributed Reinforcement Learning Is a Dataflow Problem.â€ Advances in Neural Information Processing Systems 34: 5506â€“17.\n\n\nOpenAI. 2024. â€œLearning to Reason with LLMs.â€ OpenAI Blog. https://openai.com/index/learning-to-reason-with-llms/.\n\n\nâ€”â€”â€”. 2025. â€œIntroducing Deep Research.â€ OpenAI Blog. https://openai.com/index/introducing-deep-research/.\n\n\nSchaarschmidt, Michael, Sven Mika, Kai Fricke, and Eiko Yoneki. 2019. â€œRlgraph: Modular Computation Graphs for Deep Reinforcement Learning.â€ Proceedings of Machine Learning and Systems 1: 65â€“80.\n\n\nSchulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. â€œProximal Policy Optimization Algorithms.â€ https://arxiv.org/abs/1707.06347.\n\n\nSheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. â€œHybridFlow: A Flexible and Efficient RLHF Framework.â€ In Proceedings of the 20th European Conference on Computer Systems. EuroSys â€™25. Rotterdam, The Netherlands: ACM.\n\n\nShi, Jiajun, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, et al. 2025. â€œKORGym: A Dynamic Game Platform for LLM Reasoning Evaluation.â€ https://arxiv.org/abs/2505.14552."
  },
  {
    "objectID": "posts/verl-intro/index.html#sequence-packing",
    "href": "posts/verl-intro/index.html#sequence-packing",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "7.1 Sequence Packing",
    "text": "7.1 Sequence Packing\n\n\n\n\n\n\nFigureÂ 10: Tweaking the attention mask of an example packed sequence containing two data sequences.\n\n\n\n\nRemove padding tokens and packs multiple data sequences into a row\nTweak the attention mask & position IDs to avoid cross-contamination\n\n\nTo enable this, use use_remove_padding."
  },
  {
    "objectID": "posts/verl-intro/index.html#dp-balancing",
    "href": "posts/verl-intro/index.html#dp-balancing",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "7.2 DP Balancing",
    "text": "7.2 DP Balancing"
  },
  {
    "objectID": "posts/verl-intro/index.html#other-features",
    "href": "posts/verl-intro/index.html#other-features",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "7.3 Other Features",
    "text": "7.3 Other Features\n\nFull support for RL with AMD (ROCm Kernel) hardwares\nOptimizations: Gradient Checkpointing, Torch Compile, Liger Kernel, etc.\nâ€¦"
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-dataset",
    "href": "posts/verl-intro/index.html#customizing-the-dataset",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.1 Customizing the Dataset",
    "text": "8.1 Customizing the Dataset\nA canonical RL dataset in verl has the following fields:\n\nprompt: a list of messages {\"role\": \"...\", \"content\": \"...\"}\ndata_source: used to choose the reward function\nreward_model: a dict containing\n\n\"ground_truth\"\n\"style\" like \"model\" or \"rule\"\n\n(Optional) extra_info: a dict containing extra information\n\n\nFor VLM RL, verl expects fields \"images\" and/or \"videos\"\n\n\nFor examples, please check the examples/data_preprocess."
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-reward",
    "href": "posts/verl-intro/index.html#customizing-the-reward",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.2 Customizing the Reward",
    "text": "8.2 Customizing the Reward\nverl allows to define custom reward function via the custom_reward_function config:\n\n\n\nListingÂ 7: Config for custom reward function.\n\n\ncustom_reward_function:\n  path: null # path to the `.py` file containing the function definition\n  name: compute_score # the function name after `def`\nreward_model:\n  reward_manager: naive\n\n\n\n\nAn example CLI config could be:\n\n\n\nListingÂ 8: Example config for custom reward function.\n\n\n--custom_reward_function.path=./examples/reward_fn/custom_reward_fn.py \\\n--custom_reward_function.name=compute_score \\\n--reward_model.reward_manager=naive"
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-loss-function",
    "href": "posts/verl-intro/index.html#customizing-the-loss-function",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.3 Customizing the Loss Function",
    "text": "8.3 Customizing the Loss Function\nTo modify the loss function, the most convenient way is to\n\nsearch for the .backward() call\nmodify functions like compute_policy_loss\nor add loss terms like entropy_loss"
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-training-logic",
    "href": "posts/verl-intro/index.html#customizing-the-training-logic",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.4 Customizing the Training Logic",
    "text": "8.4 Customizing the Training Logic\nAs mentioned above, the main training logic is concentrated in the fit function of the trainer classes like RayPPOTrainer.\nFor example, the DAPORayTrainer class overrides the fit function to implement the â€œdynamic samplingâ€ feature:\n(See the next slide for the code â¡ï¸)"
  },
  {
    "objectID": "posts/verl-intro/index.html#presenter-contact",
    "href": "posts/verl-intro/index.html#presenter-contact",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "9.1 Presenter Contact",
    "text": "9.1 Presenter Contact\n\nEmail: tongyuxuan361@gmail.com\nWeChat / X: tongyx361"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#rl-as-dataflow-graph",
    "href": "posts/verl-tutorial/index.html#rl-as-dataflow-graph",
    "title": "verl Tutorial",
    "section": "1.1 RL as Dataflow Graph",
    "text": "1.1 RL as Dataflow Graph\n\n\n\n\n\nReinforcement Learning (RL) for LLM Post-Training can typically be modeled as a dataflow graph, consisting of:\n\nmultiple models: actor, critic, reference, reward model, etc.\nmultiple stages: generating, preparing experiences, training\nmultiple workloads: generation, inference, training"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#implementing-dataflow-graph-as-execution-pattern",
    "href": "posts/verl-tutorial/index.html#implementing-dataflow-graph-as-execution-pattern",
    "title": "verl Tutorial",
    "section": "1.2 Implementing Dataflow Graph as Execution Pattern",
    "text": "1.2 Implementing Dataflow Graph as Execution Pattern\nIn practice, we should implement the dataflow graph as execution pattern on GPU cluster.\n\n\n\n\n\n\nSpecifically, we:\n\ndesign the parallelism strategy and model placement to optimize the throughput\nwhile restricted by the temporal dependencies and device resources"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#entrypoint",
    "href": "posts/verl-tutorial/index.html#entrypoint",
    "title": "verl Tutorial",
    "section": "2.1 Entrypoint",
    "text": "2.1 Entrypoint\nverl uses a global resource pool and allocates all the workers (e.g., ActorRollout, Critic) to it by default.\n\n\n\nListingÂ 1: Simplified code for resource allocation in TaskRunner.run().\n\n\nglobal_pool_id = \"global_pool\"\nresource_pool_spec = {\n  global_pool_id: ([config.trainer.n_gpus_per_node] * config.trainer.nnodes),\n}\nmapping = {\n  Role.ActorRollout: global_pool_id, Role.Critic: global_pool_id,\n  Role.RefPolicy: global_pool_id, Role.RewardModel: global_pool_id,\n}\nresource_pool_manager = ResourcePoolManager(\n  resource_pool_spec=resource_pool_spec, mapping=mapping)\n# ...\ntrainer = RayPPOTrainer(config=config, \n                        resource_pool_manager=resource_pool_manager, # ...\n                       )\ntrainer.fit()"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#spawning-worker-groups",
    "href": "posts/verl-tutorial/index.html#spawning-worker-groups",
    "title": "verl Tutorial",
    "section": "2.2 Spawning Worker Groups",
    "text": "2.2 Spawning Worker Groups\n\nEach worker group corresponds to\n\na resource_pool (some GPUs);\none or more workers in class_dict.\n\nwg_dict.spawn() launches one process per GPU.\n\n\n\n\nListingÂ 2: Simplified code for spawning worker group processes in RayPPOTrainer.init_workers().\n\n\n# `resource_pool_to_cls` is a `dict` \n# mapping resource pools to worker classes.\nfor resource_pool, class_dict in self.resource_pool_to_cls.items():\n  # ...\n  wg_dict = self.ray_worker_group_cls(\n      resource_pool=resource_pool, # ...\n  )\n  spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n  all_wg.update(spawn_wg)\n  self.wg_dicts.append(wg_dict)"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#training-loop-single-controller",
    "href": "posts/verl-tutorial/index.html#training-loop-single-controller",
    "title": "verl Tutorial",
    "section": "2.3 Training Loop: Single-Controller",
    "text": "2.3 Training Loop: Single-Controller\nBetween worker procedures, verl adopts a single-controller paradigm to maximize the flexibility, which allows the users to\n\nfocus on the dataflow graph\nwithout worrying about the distributed implementation.\n\nverl runs the worker procedures sequentially within the global resource pool by default.\n\n\n\nListingÂ 3: Simplified code for training loop in RayPPOTrainer.fit().\n\n\nfor epoch in range(self.config.trainer.total_epochs):\n  for batch_dict in self.train_dataloader:\n    batch = DataProto.from_single_dict(batch_dict)\n    # Stage 1: Generating\n    gen_batch = batch.pop(...)\n    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n    # Stage 2: Preparing Experiences\n    old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\n    ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n    values = self.critic_wg.compute_values(batch)\n    reward_tensor = self.rm_wg.compute_rm_score(batch)\n    # Stage 3: Training\n    self.critic_wg.update_critic(batch)\n    self.actor_rollout_wg.update_actor(batch)"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#worker-procedure-multi-controller",
    "href": "posts/verl-tutorial/index.html#worker-procedure-multi-controller",
    "title": "verl Tutorial",
    "section": "2.4 Worker Procedure: Multi-Controller",
    "text": "2.4 Worker Procedure: Multi-Controller\nInside a worker procedure, verl adopts a multi-controller paradigm, i.e., SPMD (Single Program Multiple Data), to maximize the efficiency.\nIn SPMD, all the processes\n\nrun the same program,\nbut process diffrent data based on the distributed environment variables like RANK.\n\nSPMD is the programming model of most popular distributed methods, e.g.,\n\nData Parallelism: DDP, ZeRO, FSDP\nTensor Parallelism\nPipeline Parallelism\nSequence Parallelism"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#how-verl-manages-the-resources",
    "href": "posts/verl-tutorial/index.html#how-verl-manages-the-resources",
    "title": "verl Tutorial",
    "section": "3.1 How verl Manages the Resources",
    "text": "3.1 How verl Manages the Resources\nverl\n\nspawns a list of _workers, each of which is a Ray worker running on a GPU\nand sets the SPMD environment variables for each worker.\n\n\n\n\nListingÂ 4: Simplified code for initializing worker groups in RayPPOTrainer.init_workers().\n\n\ndef _init_with_resource_pool(self, resource_pool, ray_cls_with_init):\n  # ...\n  rank = -1\n  for pg_idx, pg in enumerate(sort_placement_group_by_node_ip(pgs)): # Node\n    for local_rank in range(local_world_size): # GPU\n      rank += 1\n      env_vars = {\n        'WORLD_SIZE': str(world_size), 'RANK': str(rank), # More env vars ...\n      }\n      ray_cls_with_init.update_options(\n        {'runtime_env': {'env_vars': env_vars}})\n      # ...\n      worker = ray_cls_with_init(placement_group=pg,\n                                 placement_group_bundle_idx=local_rank)\n      self._workers.append(worker)\n  # ..."
  },
  {
    "objectID": "posts/verl-tutorial/index.html#how-verl-defines-the-spmd-behavior",
    "href": "posts/verl-tutorial/index.html#how-verl-defines-the-spmd-behavior",
    "title": "verl Tutorial",
    "section": "3.2 How verl Defines the SPMD Behavior",
    "text": "3.2 How verl Defines the SPMD Behavior\nTaking the ActorRolloutRefWorker.update_actor() as an example:\n\n\n\nListingÂ 5: Simplified code for SPMD update_actor() in ActorRolloutRefWorker.\n\n\n@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\ndef update_actor(self, data: DataProto):\n  # NOTE: here, we already have only 1/WORLD_SIZE of the whole data\n  data = data.to(torch.cuda.current_device())\n  self.actor.update_policy(data=data)\n  self.actor_lr_scheduler.step()"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#references",
    "href": "posts/verl-tutorial/index.html#references",
    "title": "verl Tutorial",
    "section": "",
    "text": "References\n\n\nSheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. â€œHybridFlow: A Flexible and Efficient RLHF Framework.â€ arXiv Preprint arXiv: 2409.19256."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs åšå®¢",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nverl Tutorial\n\n\n\n\n\n\n\n\n\n\n\nYuxuan Tong\n\n\n\n\n\n\n\n\n\n\n\n\nIS with BET\n\n\nJustifying Importance Sampling with Best-Effort Trajectories\n\n\n\nWIP å°šåœ¨å®Œå–„\n\nEnglish è‹±æ–‡\n\nTechnical æŠ€æœ¯\n\n\n\nModern Large Language Model (LLM) Reinforcement Learning (RL) systems, such as Kimi k1.5 and AReaL, increasingly adopt Partial Rollout strategies to mitigate long-tail latency and simplify weight management. These systems generate Best-Effort Trajectories (BETs) â€“ sequences where the sampling policy may update mid-rollout â€“ thereby challenging the standard assumptions of Importance Sampling (IS) which typically rely on a consistent behavior policy.\nThis post provides a justification for applying Importance Sampling to BETs. We demonstrate that by constructing trajectory-wise behavior policies, we can maintain unbiased estimates despite the mixed-policy nature of the rollouts. Furthermore, leveraging variance bounds based on RÃ©nyi divergence, we argue that BETs â€“ being closer to the target policy than consistent but stale behavior policies â€“ are likelt to yield lower variance. Finally, we propose that Multiple Importance Sampling (MulIS) with the Balance Heuristic (BH) can be utilized to further exploit the distributional properties of these trajectories for better policy optimization.\n\n\n\n\n\nJan 7, 2026\n\n\nYuxuan Tong, Yingru Li, Guangming Sheng\n\n\n\n\n\n\n\n\n\n\n\n\nverl: Flexible and Efficient RL for LLMs\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2025\n\n\nYuxuan Tong (ç«¥é›¨è½©)\n\n\n\n\n\n\n\n\n\n\n\n\né‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–\n\n\nä¿®æ­£ GRPO å…¬å¼ä¸æµè¡Œ LLM RL æ¡†æ¶\n\n\n\nChinese ä¸­æ–‡\n\nTechnical æŠ€æœ¯\n\n\n\nå¯¹äº LLM RL ä¸­ç›¸å¯¹äºå‚è€ƒç­–ç•¥çš„ KL ä¼˜åŒ–ï¼ŒGRPO å…¬å¼\n\næ²¡æœ‰å¤„ç† KL é¡¹çš„ off-policy é—®é¢˜ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨å¤šè½®æ›´æ–°æ—¶é‡æ–°è®¡ç®— KL é¡¹å¹¶æ·»åŠ é‡è¦æ€§é‡‡æ ·ç³»æ•°è§£å†³\nå…ˆå°† KL ä¼°è®¡æ ·æœ¬é‡åº”ç”¨äºåŠ¨ä½œå¯¹æ•°æ¡ä»¶ä¼¼ç„¶å†æ±‚å’Œï¼Œè€Œéå…ˆæ±‚å’Œå¾—åˆ°æ¦‚ç‡å†åº”ç”¨ä¼°è®¡æ ·æœ¬é‡ï¼Œä¸ John Schulman â€œApproximating KL Divergenceâ€ åˆ†æä¸ç¬¦ï¼ˆå¯¹åº”å¯¼å‡ºçš„æ¢¯åº¦ä¹Ÿå¯èƒ½å› æ­¤è€Œé”™è¯¯ï¼‰\n\nç›®å‰æµè¡Œçš„ LLM RL æ¡†æ¶ï¼ˆTRLï¼ŒOpenRLHFï¼Œverlï¼‰ä¹Ÿæ²¡æœ‰é¿å…ä¸Šè¿°é—®é¢˜ï¼Œä¸”å­˜åœ¨å…¶ä»–é—®é¢˜ï¼š\n\nåœ¨è®¡ç®— KL loss é¡¹æ—¶é»˜è®¤ä¸å»é™¤ä»»ä½•æ¢¯åº¦ï¼Œå®é™…å¾—åˆ°çš„æ¢¯åº¦é€šå¸¸ä¸æ˜¯åœ¨ä¼˜åŒ– KL æ•£åº¦\nKL loss é¡¹çš„å¹³å‡æ“ä½œå­˜åœ¨é”™è¯¯ã€‚\n\næœ¬æ–‡åŸºäºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼ˆè€Œé banditï¼‰å»ºæ¨¡åˆ†æäº†ä¸Šè¿°é—®é¢˜ï¼Œå¹¶æä¾›äº†æ­£ç¡®çš„ KL loss / reward é¡¹å®ç°çš„æ•°å­¦æ¨å¯¼ä¸ä¸Šè¿°é—®é¢˜çš„ä¿®æ­£æ€è·¯ã€‚\n\n\n\n\n\nMar 9, 2025\n\n\nç«¥é›¨è½©\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/is-bet/index.html",
    "href": "posts/is-bet/index.html",
    "title": "IS with BET",
    "section": "",
    "text": "1 IS with BET?\n  \n  1.1 Importance Sampling (IS)\n  1.2 Best-Effort Trajectory (BET)\n  1.3 Partial Rollout\n  \n  1.3.1 Efficiency: Eliminating Long-Tail Bubbles\n  1.3.2 Simplicity: Only Maintaining One Version of Model Weights\n  \n  \n  2 IS with BET!\n  \n  2.1 IS with Global Behavior Policy \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\)\n  2.2 IS with Trajectory-wise Behavior Policy \\(\\mu_{i}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\)\n  2.3 Mixed Best-Effort Policy Might Have Lower Variance than Consistent Old Policy\n  2.4 Exploiting (Approximatly) Identical Distribution with Multiple IS\n  \n  Citation"
  },
  {
    "objectID": "posts/is-bet/index.html#importance-sampling-is",
    "href": "posts/is-bet/index.html#importance-sampling-is",
    "title": "IS with BET",
    "section": "1.1 Importance Sampling (IS)",
    "text": "1.1 Importance Sampling (IS)\nIt is a common practice to optimize LLM policy on off-policy data with Importance Sampling (IS):\n\\[\n\\hat{\\mathbb{E}}_{\\mu} = \\frac{p_{\\theta}(\\boldsymbol{\\tau})}{q_{\\mu}(\\boldsymbol{\\tau})}f(\\boldsymbol{\\tau}),\n\\]\nwhere \\(f(\\boldsymbol{\\tau})\\) can be the gradient of the objective function to be optimized, \\(p_{\\theta}(\\boldsymbol{\\tau})\\) is the trajectory distribution induced by the target policy \\(\\pi_{\\theta}\\) and \\(q_{\\mu}(\\boldsymbol{\\tau})\\) is the trajectory distribution induced by the behavior policy \\(\\mu\\)."
  },
  {
    "objectID": "posts/is-bet/index.html#best-effort-trajectory-bet",
    "href": "posts/is-bet/index.html#best-effort-trajectory-bet",
    "title": "IS with BET",
    "section": "1.2 Best-Effort Trajectory (BET)",
    "text": "1.2 Best-Effort Trajectory (BET)\nWe use Best-Effort Trajectories (BETs) to refer to the trajectories sampled in a Reinforcement Learning (RL) system where we always use the latest policy \\(\\pi_{\\theta_{t}}\\) at each moment to sample the actions.\nFormally, a BET can be defined as a trajectory \\(\\tau = (s_{0}, a_{0,\\pi_{\\theta_{n}}}, \\ldots, s_{t}, a_{t,\\pi_{\\theta_{n+m}}}, \\ldots, s_{T}, a_{T,\\pi_{\\theta_{n+m}}})\\), where \\(s_{t}\\) is the state at timestep \\(t\\) and \\(a_{t,\\pi_{\\theta_{n+m}}}\\) is the action sampled from the policy \\(\\pi_{\\theta_{n+m}}\\) that is the latest policy available in the system when sampling. So for \\(l &gt; m\\), \\(\\pi_{\\theta+{n+l}}\\) is usually more on-policy than \\(\\pi_{\\theta_{n+m}}\\).\nBETs are off-policy by definition, but are as close to on-policy as possible with the best effort in the system."
  },
  {
    "objectID": "posts/is-bet/index.html#partial-rollout",
    "href": "posts/is-bet/index.html#partial-rollout",
    "title": "IS with BET",
    "section": "1.3 Partial Rollout",
    "text": "1.3 Partial Rollout\nBETs are a natural result of a popular design choice called Partial Rollout in LLM RL systems nowadays, such as Kimi k1.5 (Kimi Team et al. 2025), AReaL (Fu et al. 2025) and PipelineRL (PichÃ© et al. 2025), etc., which can date back to a traditional Distributed RL system called SEED RL (Espeholt et al. 2020):\nThe ongoing trajectory rollouts are\n\naborted when\n\neither, in synchoronous systems like Kimi k1.5, there are enough samples collected for training, releasing the resources for the training engine to update the model weights,\nor, in asynchronous systems like AReaL and PipelineRL, a new version of model weights is produced by the trainer;\n\ncontinued with the latest version of model weights.\n\nPartial Rollout is motivated by its efficiency and simplicity. Let me explain in detail.\n\n1.3.1 Efficiency: Eliminating Long-Tail Bubbles\nThe earliest LLM RL systems Hu et al. (2025) all adopt synchoronous architectures, where the trainer always waits for all the trajectories are finished before updating the weights with these data.\nHowever, as the context length of LLMs scales up, the skewness of the trajectory length distribution becomes increasing heavy. If the distribution is very skewed but all the trajectories are required to finish in the same rollout stage, there can be only a few long-tail requests remaining in the system, causing severe under-utilization (typically &lt;30% in practice). (Sheng, Tong, et al. 2025)\nKimi k1.5 proposes to fix this issue by aborting all the ongoing rollouts once enough training samples are collected and directly update the weights with these data, instead of waiting for all the trajectories to finish, and then continue the rollouts with the new model weights.\n\n\n1.3.2 Simplicity: Only Maintaining One Version of Model Weights\nIn asynchornous RL systems with an experience buffer, it is troublesome to mamage multiple versions of model weights within the rollout engine.\nAReaL proposes to always only maintain the latest model weights across all the instances. Once a new version of model weights is produced by the trainer, all the rollouts of the stale policy are aborted and then continued with the latest policy.\nThis can be simply implemented by always loading the latest weights into all the inference engine instances, avoiding the bothering to manage the requests across each instance."
  },
  {
    "objectID": "posts/is-bet/index.html#is-with-global-behavior-policy-muboldsymbola-mid-boldsymbols",
    "href": "posts/is-bet/index.html#is-with-global-behavior-policy-muboldsymbola-mid-boldsymbols",
    "title": "IS with BET",
    "section": "2.1 IS with Global Behavior Policy \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\)",
    "text": "2.1 IS with Global Behavior Policy \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\)\nSince there is always an actual distribution we are sampling from, we can always formulate it as a global behavior policy \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\).\nIt is natural to try to use \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) for IS. So how can we calculate it?\nIt is easy to notice that, for each trajectory of an LLM policy, we can usually construct a behavior policy \\(\\forall t, \\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) satisfies that \\(\\mu(a_{t} \\mid s_{t}) = \\pi_{\\theta_{n+m}}(a_{t} \\mid s_{t})\\) at every timestep \\(t\\), since the LLM is usually auto-regressive and thus never revisits a past state within the same trajectory.\nHowever, it might be confusing when we also notice that, for the same state \\(s\\) in two different BETs \\(\\tau_{i}\\) and \\(\\tau_{j}\\), the same action \\(a\\) might be sampled from different policies \\(\\pi_{\\theta_{n+m}}\\) and \\(\\pi_{\\theta_{n+l}}\\) (\\(l \\neq m\\)) respectively. It is likely that \\(\\pi_{\\theta_{n+m}}(a_{t} \\mid s_{t}) \\neq \\pi_{\\theta_{n+l}}(a_{t} \\mid s_{t})\\), making it impossible to simply construct the same behavior policy for both BETs, i.e., \\(\\mu(a_{t} \\mid s_{t})=\\pi_{\\theta_{n+m}}(a_{t} \\mid s_{t})\\) constradicts \\(\\mu(a_{t} \\mid s_{t})=\\pi_{\\theta_{n+l}}(a_{t} \\mid s_{t})\\).\nSo where is the problem of the construction for \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) mentioned above?\nThe problem hidden here is that, \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) does not consider the probability distribution of which LLM policy \\(\\pi_{\\theta_{n+m}}\\) is used, but this distribution is intractable since it depends on the system dynamics."
  },
  {
    "objectID": "posts/is-bet/index.html#is-with-trajectory-wise-behavior-policy-mu_iboldsymbola-mid-boldsymbols",
    "href": "posts/is-bet/index.html#is-with-trajectory-wise-behavior-policy-mu_iboldsymbola-mid-boldsymbols",
    "title": "IS with BET",
    "section": "2.2 IS with Trajectory-wise Behavior Policy \\(\\mu_{i}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\)",
    "text": "2.2 IS with Trajectory-wise Behavior Policy \\(\\mu_{i}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\)\nA trivial solution is to construct a trajectory-wise behavior policy \\(\\mu_{i}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) for each trajectory \\(\\tau_{i}\\).\nNow for the counter-example mentioned above, \\(\\mu_i(\\boldsymbol{a} \\mid \\boldsymbol{s})=\\pi_{\\theta_{n+m}}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) and \\(\\mu_j(\\boldsymbol{a} \\mid \\boldsymbol{s})=\\pi_{\\theta_{n+l}}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) are obviously compatible.\nWith a batch of \\(N\\) trajectory samples sampled from each own behavior policy \\(\\boldsymbol{\\tau}_1 \\sim q_{\\mu_1}, \\ldots, \\boldsymbol{\\tau}_N \\sim q_{\\mu_N}\\), it is easy to formulate single-sample estimate \\(\\hat{\\mathbb{E}}_{\\mu_i} = \\frac{p_{\\theta}(\\boldsymbol{\\tau}_i)}{q_{\\mu_i}(\\boldsymbol{\\tau}_i)}f(\\boldsymbol{\\tau}_i)\\).\nNote that the single-sample estimate is already unbiased, i.e., the unbiasedness does not depend on the sample size.\nThe common practice to combine them for an estimate is to average the single-sample estimates \\(\\hat{\\mathbb{E}}_{\\mu_i}, i = 1, \\ldots, N\\), i.e., \\(\\hat{\\mathbb{E}}_{\\text{avg}} = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{\\mathbb{E}}_{\\mu_i}\\).\nDue to linearity of expectation, \\(\\hat{\\mathbb{E}}_{\\text{avg}}\\) is still unbiased.\nNow letâ€™s take a look at the variance. Let the variance of each single-sample estimate be \\(\\sigma^2_{\\mu_i}\\), since the samples are independent, the variance of the average estimate \\(\\hat{\\mathbb{E}}_{\\text{avg}}\\) is:\n\\[\n\\sigma^2_{\\text{avg}} = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sigma^2_{\\mu_i}\n\\]"
  },
  {
    "objectID": "posts/is-bet/index.html#mixed-best-effort-policy-might-have-lower-variance-than-consistent-old-policy",
    "href": "posts/is-bet/index.html#mixed-best-effort-policy-might-have-lower-variance-than-consistent-old-policy",
    "title": "IS with BET",
    "section": "2.3 Mixed Best-Effort Policy Might Have Lower Variance than Consistent Old Policy",
    "text": "2.3 Mixed Best-Effort Policy Might Have Lower Variance than Consistent Old Policy\nNow we wonder, given a trajectory \\(\\boldsymbol{\\tau}_{i}\\), which of the mixed best-effort policy \\(\\mu_{\\theta_{n},M}\\) using \\(\\pi_{\\theta_{n}},\\ldots,\\pi_{\\theta_{n+M}}\\) and the consistent old policy \\(\\pi_{\\theta_{n}}\\) is better for IS estimate, i.e., leads to a lower variance?\nMetelli et al. (2020) provided a family of bounds of the variance of IS estimate in terms of the RÃ©nyi divergence:\n\nLemma 1. Let \\(P\\) and \\(Q\\) be two probability measures on the measurable space \\((\\mathcal{X}, \\mathscr{F})\\) such that \\(P \\ll Q\\). Let \\(\\alpha \\in[1,+\\infty], \\mathbf{x}=\\left(x_1, x_2, \\ldots, x_N\\right)^T\\) be i.i.d. random variables sampled from \\(Q\\) and \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\) be a function with bounded \\(\\frac{2 \\alpha}{\\alpha-1}\\)-moment under \\(Q\\left(\\|f\\|_{Q, \\frac{2 \\alpha}{\\alpha-1}}&lt;+\\infty\\right)\\). Then, for any \\(N&gt;0\\), the variance of the IS estimator \\(\\widehat{\\mu}_{P / Q}\\) can be upper bounded as:\n\\[\\operatorname{Var}_{\\mathbf{x} \\sim Q}\\left[\\hat{\\mu}_{P / Q}\\right] \\leqslant \\frac{1}{N}\\|f\\|_{Q, \\frac{2 \\alpha}{\\alpha-1}}^2 d_{2 \\alpha}(P \\| Q)^{2-\\frac{1}{\\alpha}},\\]\nwhere we used the abbreviation \\(\\mathbf{x} \\sim Q\\) for denoting \\(x_i \\sim Q\\) for all \\(i=1,2, \\ldots, N\\) all independent.\nThis result generalizes Lemma 4.1 of Metelli et al.Â (2018), that can be recovered by setting \\(\\alpha=1\\) under the condition that \\(\\|f\\|_{\\infty}&lt;+\\infty\\) :\n\\[\\operatorname{Var}_{\\mathbf{x} \\sim Q}\\left[\\widehat{\\mu}_{P / Q}\\right] \\leqslant \\frac{1}{N}\\|f\\|_{\\infty}^2 d_2(P \\| Q) .\\]\n\nWhen \\(\\alpha = 1\\), the RÃ©nyi divergence is the Kullback-Leibler divergence widely used in RL analysis.\n\nWhen \\(P=Q\\) almost everywhere, we get \\(\\operatorname{Var}_{\\mathbf{x} \\sim Q}\\left[\\hat{\\mu}_{Q / Q}\\right] \\leqslant \\frac{1}{N}\\|f\\|_{\\infty}^2\\), a well-known upper bound to the variance of a Monte Carlo estimator. Recalling the definition of ESS (Equation 7) we can rewrite the previous bound as:\n\\[\\underset{\\mathbf{x} \\sim Q}{\\operatorname{Var}}\\left[\\hat{\\mu}_{P / Q}\\right] \\leqslant \\frac{\\|f\\|_{\\infty}^2}{\\operatorname{ESS}(P \\| Q)} .\\]\nThus, the variance scales with ESS instead of \\(N\\), justifying the definition of ESS.\n\nIn our context, \\(P\\) is the target distribution \\(p_{\\theta}\\) and \\(Q\\) is \\(\\mu_{\\theta_{n},M}\\) or \\(\\pi_{\\theta_{n}}\\).\nIntuitively, it is likely that \\(d_{2 \\alpha}(p_{\\theta} \\| q_{\\theta_{n},M}) &lt; d_{2 \\alpha}(p_{\\theta} \\| p_{\\theta_{n}})\\), since the newer the policy is, the more similar its induced distribution is to \\(p_{\\theta}\\).\nAs long as the \\(\\|f\\|_{q_{\\theta_{n},M}, \\frac{2 \\alpha}{\\alpha-1}}\\) is not much larger than \\(\\|f\\|_{p_{\\theta_{n}}, \\frac{2 \\alpha}{\\alpha-1}}\\), the estimate using \\(\\mu_{\\theta_{n},M}\\) has better guarantee than the estimate using \\(\\pi_{\\theta_{n}}\\).\nThese conjectures can be verified by empirical experiments. For example, Figure 6 and 7 by PichÃ© et al. (2025) show that \\(\\mu_{\\theta_{n},M}\\) has higher ESS and lower KL divergence relative to \\(p_{\\theta}\\) than \\(\\pi_{\\theta_{n}}\\), respectively.\nWe are also planning experiments to further verify these conjectures."
  },
  {
    "objectID": "posts/is-bet/index.html#exploiting-approximatly-identical-distribution-with-multiple-is",
    "href": "posts/is-bet/index.html#exploiting-approximatly-identical-distribution-with-multiple-is",
    "title": "IS with BET",
    "section": "2.4 Exploiting (Approximatly) Identical Distribution with Multiple IS",
    "text": "2.4 Exploiting (Approximatly) Identical Distribution with Multiple IS\nBesides the likely superiority for each single-sample estimate, the combination of multiple importance sampling has other properties that can be exploited.\nFormally, the combination can actually generalize from average weight \\(\\frac{1}{N}\\) to any â€œpartion of unityâ€, which is a collection of \\(J \\geqslant 1\\) weight functions \\(\\omega_j(\\boldsymbol{x}) \\geqslant 0\\) which satisfy \\(\\sum_{j=1}^J \\omega_j(\\boldsymbol{x})=1\\) for all \\(\\boldsymbol{x}\\). Different partitions of unity will lead to estimates that are all unbiased but with different variances.\nGiven the property that BETs of similar lengths usally conform to identical distributions (approximately, but this can be exact if we limit the timesteps where a trajectory can be aborted), we can reformulate the estimation as Multiple Importance Sampling (Owen 2013):\n\nSuppose that \\(\\boldsymbol{X}_{i j} \\sim q_j\\) for \\(i=1, \\ldots, n_j\\) and \\(j=1, \\ldots, J\\) and that \\(\\omega_j\\) are a partition of unity. The multiple importance sampling estimate is\n\\[\\widetilde{\\mu}_\\omega=\\sum_{j=1}^J \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\omega_j\\left(\\boldsymbol{X}_{i j}\\right) \\frac{f\\left(\\boldsymbol{X}_{i j}\\right) p\\left(\\boldsymbol{X}_{i j}\\right)}{q_j\\left(\\boldsymbol{X}_{i j}\\right)} .\\] (Owen 2013)\nNow assume that \\(q_j(\\boldsymbol{x})&gt;0\\) whenever \\(\\omega_j(\\boldsymbol{x}) p(\\boldsymbol{x}) f(\\boldsymbol{x}) \\neq 0\\). Then multiple importance sampling is unbiased, because \\[\\mathbb{E}\\left(\\widetilde{\\mu}_\\omega\\right)=\\sum_{j=1}^J \\mathbb{E}_{q_j}\\left(\\omega_j(\\boldsymbol{X}) \\frac{f(\\boldsymbol{X}) p(\\boldsymbol{X})}{q_j(\\boldsymbol{X})}\\right)=\\sum_{j=1}^J \\int \\omega_j(\\boldsymbol{x}) f(\\boldsymbol{x}) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}=\\mu .\\]\nAmong the proposals for functions \\(\\omega_j(\\boldsymbol{x})\\), the most studied one is the balance heuristic with \\(\\omega_j(\\boldsymbol{x}) \\propto n_j q_j(\\boldsymbol{x})\\), that is\n\\[\\omega_j(\\boldsymbol{x})=\\omega_j^{\\mathrm{BH}}(\\boldsymbol{x}) \\equiv \\frac{n_j q_j(\\boldsymbol{x})}{\\sum_{k=1}^J n_k q_k(\\boldsymbol{x})} .\\]\nBy construction \\(q_j(\\boldsymbol{x})&gt;0\\) holds whenever \\(\\left(\\omega_j^{\\mathrm{BH}} p f\\right)(\\boldsymbol{x}) \\neq 0\\). Let \\(n=\\sum_{j=1}^J n_j\\) and define \\(\\alpha_j=n_j / n\\). Then using the balance heuristic, \\(\\widetilde{\\mu}_{\\omega^{\\text {Ğ’Ğ }}}\\) simplifies to \\[\\widetilde{\\mu}_\\alpha=\\frac{1}{n} \\sum_{j=1}^J \\sum_{i=1}^{n_j} \\frac{f\\left(\\boldsymbol{X}_{i j}\\right) p\\left(\\boldsymbol{X}_{i j}\\right)}{\\sum_{j=1}^J \\alpha_j q_j\\left(\\boldsymbol{X}_{i j}\\right)} .\\]\nIn other words, multiple importance sampling, with weights from the balance heuristic reduces to the same estimator we would use in mixture importance sampling with mixture weights \\(\\alpha_j=n_j / n\\). Once again, the weight on a given sampled value \\(\\boldsymbol{X}_{i j}\\) does not depend on which mixture component it came from. The balance heuristic is nearly optimal in the following sense:\nTheorem 9.8. Let \\(n_j \\geqslant 1\\) be positive integers for \\(j=1, \\ldots, J\\). Let \\(\\omega_1, \\ldots, \\omega_J\\) be a partition of unity and let \\(\\omega^{\\mathrm{BH}}\\) be the balance heuristic. Suppose that \\(q_j(\\boldsymbol{x})&gt;\\) 0 whenever \\(\\omega_j(\\boldsymbol{x}) p(\\boldsymbol{x}) f(\\boldsymbol{x}) \\neq 0\\). Then\n\\[\\operatorname{Var}\\left(\\widetilde{\\mu}_{\\omega^{\\mathrm{BH}}}\\right) \\leqslant \\operatorname{Var}\\left(\\widetilde{\\mu}_\\omega\\right)+\\left(\\frac{1}{\\min _j n_j}-\\frac{1}{\\sum_j n_j}\\right) \\mu^2 .\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "",
    "text": "1 å¼•è¨€ï¼šGRPO å…¬å¼çš„â€œé”™è¯¯â€\n  2 æµè¡Œ LLM RL æ¡†æ¶ä¸­ on-policy KL ä¼˜åŒ–çš„å®ç°\n  \n  2.1 TRLï¼šKL reward é¡¹\n  2.2 OpenRLHF\n  \n  2.2.1 KL reward é¡¹\n  2.2.2 KL loss é¡¹\n  \n  2.3 verl\n  \n  2.3.1 KL reward é¡¹\n  2.3.2 KL loss é¡¹\n  \n  2.4 ä¸ºä»€ä¹ˆè¦å°† KL ä» reward ä¸­å‡å»\n  \n  2.4.1 KL reward çš„æµè¡Œåº”å½“æºè‡ª RLHF ä¸ InstructGPT\n  2.4.2 OpenAI è®ºæ–‡ä¸­ KL reward çš„å‡ºå¤„\n  2.4.3 KL reward æœ€æ—©çš„å‡ºå¤„\n  \n  \n  3 LLM RL ä¸­ KL ä¼˜åŒ–çš„æ•°å­¦å½¢å¼åŒ–\n  \n  3.1 RL ä¸­çš„ KL æ•£åº¦é€šå¸¸å®šä¹‰åœ¨è½¨è¿¹åˆ†å¸ƒä¸Š\n  3.2 å°†è½¨è¿¹å±•å¼€ä¸ºçŠ¶æ€-åŠ¨ä½œåºåˆ—\n  3.3 Markov å†³ç­–è¿‡ç¨‹ä¸­çš„ KL æ•£åº¦\n  3.4 è¯­è¨€æ¨¡å‹ä½œä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹\n  3.5 ä¼°è®¡ KL æ•£åº¦\n  \n  3.5.1 å‡ ä¹ä¸å¯èƒ½ç›´æ¥è®¡ç®— KL æ•£åº¦çš„çœŸå®å€¼\n  3.5.2 é€šå¸¸ä½¿ç”¨ Monte Carlo æ–¹æ³•ä¼°è®¡ KL æ•£åº¦\n  3.5.3 ä¸åŒçš„ KL ä¼°è®¡é‡\n  \n  \n  4 æµè¡Œ on-policy KL ä¼˜åŒ–å®ç°çš„æ•°å­¦å½¢å¼åŒ–\n  \n  4.1 åˆ†ææµè¡Œçš„ â€œKL loss é¡¹â€ å®ç°\n  \n  4.1.1 ä¸åŒ KL ä¼°è®¡é‡å¯¹åº”çš„ loss é¡¹å¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡çš„ä¸€èˆ¬å½¢å¼\n  4.1.2 \\(k_1\\) å¯¼å‡ºçš„æ¢¯åº¦ï¼šæœŸæœ›ä¸º 0\n  4.1.3 \\(k_2\\) å¯¼å‡ºçš„æ¢¯åº¦\n  4.1.4 \\(k_3\\) å¯¼å‡ºçš„æ¢¯åº¦\n  4.1.5 å°ç»“ï¼šæµè¡Œçš„ â€KL loss é¡¹â€œ å®ç°å¹¶ä¸åˆç†\n  \n  4.2 åˆ†ææµè¡Œçš„ â€œKL reward é¡¹â€œ å®ç°\n  \n  4.2.1 ç±»æ¯” PG ä¼˜åŒ– reward æ¥åˆ†æ KL reward çš„ä½œç”¨\n  4.2.2 ä¸åŒ KL ä¼°è®¡é‡å¯¼å‡ºçš„ reward é¡¹çš„ä½œç”¨\n  4.2.3 å°ç»“ï¼šåœ¨ on-policy è®¾ç½®ä¸‹ä¿®æ­£ GRPO ç›®æ ‡çš„ KL é¡¹\n  \n  \n  5 æ¨å¯¼ on-policy è®¾ç½®ä¸‹ KL æ•£åº¦çš„æ¢¯åº¦ä¼°è®¡\n  \n  5.1 åœ¨å·²çŸ¥ç¯å¢ƒä¸­ç®€åŒ– KL æ¢¯åº¦ä¼°è®¡\n  5.2 ç®€å†™ä¸º Contextual Bandit\n  5.3 è¿˜åŸä¸ºå·²çŸ¥ç¯å¢ƒå†³ç­–è¿‡ç¨‹\n  5.4 åˆ©ç”¨å› æœæ€§æŠ€å·§åŒ–ç®€ KL æ¢¯åº¦ä¼°è®¡\n  5.5 KL æ¢¯åº¦ä¼˜åŒ–å¯ä»¥å®ç°ä¸º KL æ ·æœ¬å€¼ reward\n  \n  6 off-policy è®¾ç½®ä¸‹å¦‚ä½•ä¼°è®¡ KL æ•£åº¦çš„æ¢¯åº¦\n  \n  6.1 æµè¡Œ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°å¿½ç•¥äº† off-policy é—®é¢˜\n  \n  6.1.1 TRL\n  6.1.2 OpenRLHF\n  6.1.3 verl\n  \n  6.2 åˆ©ç”¨é‡è¦æ€§é‡‡æ ·å¤„ç† off-policy è®¾ç½®\n  \n  7 ç»“è®ºï¼šå¦‚ä½•æ­£ç¡®åœ°åœ¨ RL ä¸­ä¼˜åŒ– KL æ•£åº¦\n  \n  7.1 ä¿®æ­£ GRPO å…¬å¼ä¸­çš„ KL é¡¹\n  7.2 ä¿®æ­£æµè¡Œ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°\n  \n  8 è®¨è®º\n  \n  8.1 å¯¹äº KL æ¢¯åº¦æ›´å¥½çš„ä¼°è®¡æ ·æœ¬é‡\n  8.2 KL-Regularized RL çš„ç†è®ºä¼˜åŠ¿\n  \n  9 é™„å½•\n  \n  9.1 ç›¸å…³å·¥ä½œ\n  9.2 å†™ä½œå¥‘æœºï¼šâ€œTRPO/PPO ä¸ GRPO ä¸­çš„ KL ä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿâ€\n  9.3 è‡´è°¢\n  9.4 å¼•ç”¨"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#trlkl-reward-é¡¹",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#trlkl-reward-é¡¹",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "2.1 TRLï¼šKL reward é¡¹",
    "text": "2.1 TRLï¼šKL reward é¡¹\nTRL è®¡ç®— KL å®šä¹‰ä¸­çš„æ ·æœ¬å€¼ \\(\\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,t})}{\\pi_{\\theta_{\\text{ref}}}(a_{i,t} \\mid s_{i,t})}\\)ï¼Œå¹¶å°†å…¶ä» reward ä¸­å‡å»ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 1ã€‚\n\n\n\nListingÂ 1: TRL è®¡ç®— KL æ ·æœ¬å€¼ \\(\\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,t})}{\\pi_{\\theta_{\\text{ref}}}(a_{i,t} \\mid s_{i,t})}\\) å¹¶ä» reward ä¸­å‡å»5\n\n\n# 4. compute rewards\nkl = logprobs - ref_logprobs\nnon_score_reward = -args.kl_coef * kl\nrewards = non_score_reward.clone()\n# ...\nrewards[[actual_start, actual_end]] += scores\n\n\n\nè¿™å¯èƒ½ä¼šå¼•èµ·ç–‘æƒ‘ï¼šä¸ºä»€ä¹ˆè¦å°† KL æ ·æœ¬å€¼ä» reward ä¸­å‡å»ï¼Ÿæˆ‘ä»¬å…ˆå°†å¯¹æ­¤çš„è®¨è®ºæ¨è¿Ÿåˆ° SectionÂ 2.4ã€‚\n\n\n\nhttps://github.com/huggingface/trlâ†©ï¸\nhttps://github.com/OpenRLHF/OpenRLHFâ†©ï¸\nhttps://github.com/volcengine/verlâ†©ï¸\nhttps://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#openrlhf",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#openrlhf",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "2.2 OpenRLHF",
    "text": "2.2 OpenRLHF\n\n2.2.1 KL reward é¡¹\nä¸ TRL ç±»ä¼¼ï¼ŒOpenRLHF æ”¯æŒè®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼ï¼Œå¹¶ä» reward ä¸­å‡å»ï¼Œä½†æä¾›äº†å¤šç§è®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼çš„æ–¹æ³•ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 2ã€‚\n\n\n\nListingÂ 2: OpenRLHF æ”¯æŒè®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼å¹¶ä» reward ä¸­å‡å» 6\n\n\ndef compute_approx_kl(\n    log_probs: torch.Tensor,\n    log_probs_base: torch.Tensor,\n    action_mask: Optional[torch.Tensor] = None,\n    kl_estimator: str = \"k1\",\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the approximate KL divergence between two distributions.\n    Schulman blog: http://joschu.net/blog/kl-approx.html\n\n    Args:\n        log_probs: Log probabilities of the new distribution.\n        log_probs_base: Log probabilities of the base distribution.\n        action_mask: Mask for actions.\n    \"\"\"\n\n    if kl_estimator == \"k1\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n\n    # The $k_2$ estimator is the non negative kl approximation in\n    # http://joschu.net/blog/kl-approx.html\n    # The k2_loss is approximately equivalent to the\n    # one-step KL divergence penalty with the $k_1$ estimator\n    # used in https://arxiv.org/abs/2310.10505.\n    if kl_estimator == \"k2\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n        log_ratio = log_ratio**2 / 2.0\n\n    # The $k_3$ estimator is the non negative kl approximation in\n    # http://joschu.net/blog/kl-approx.html\n    if kl_estimator == \"k3\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n        log_ratio = -log_ratio\n        log_ratio = log_ratio.exp() - 1 - log_ratio\n\n    return log_ratio\n\n\ndef compute_reward(\n    # ...\n    kl_coef: float,\n    kl: Union[torch.Tensor, list[torch.Tensor]],\n    # ...\n    num_actions: Optional[Union[int, list[int]]] = None,\n    # ...\n) -&gt; Union[torch.Tensor, list[torch.Tensor]]:\n    # ...\n    if action_mask is not None:\n        # ...\n    else:\n        # ...\n        reward = []\n        for i, (kl_seg, action_len) in enumerate(zip(kl, num_actions)):\n            kl_reward = -kl_coef * kl_seg\n            kl_reward[action_len - 1] += r[i]\n            reward.append(kl_reward)\n\n    return reward\n\n\n\n\n\n\nhttps://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88â†©ï¸\n\n\n\n\n2.2.2 KL loss é¡¹\næ­¤å¤–ï¼ŒOpenRLHF è¿˜æ”¯æŒè®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼ï¼Œå…ˆå¯¹åºåˆ—å†…éƒ¨çš„ token è®¡ç®—å‡å€¼ï¼Œå†åœ¨åºåˆ—ä¹‹é—´è®¡ç®—å‡å€¼ï¼Œå¹¶åŠ å…¥åˆ° loss ä¸­ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 3ã€‚\n\n\n\nListingÂ 3: OpenRLHF æ”¯æŒè®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼ï¼Œå…ˆå¯¹åºåˆ—å†…éƒ¨çš„ token è®¡ç®—å‡å€¼ï¼Œå†åœ¨åºåˆ—ä¹‹é—´è®¡ç®—å‡å€¼ï¼Œå¹¶åŠ å…¥åˆ° loss ä¸­ 7\n\n\ndef training_step_actor(self, experience: Experience) -&gt; Dict[str, float]:\n    self.actor.train()\n    # ...\n    if isinstance(experience.sequences, list):\n        # ...\n    else:\n        sequences = experience.sequences\n        old_action_log_probs = experience.action_log_probs\n        advantages = experience.advantages\n        num_actions = experience.action_mask.size(1)\n        packed_seq_lens = None\n        attention_mask = experience.attention_mask\n        if self.args.use_kl_loss and experience.base_action_log_probs is not None:\n            base_action_log_probs = experience.base_action_log_probs\n\n    # actor loss\n    action_log_probs, output = self.actor(\n        sequences,\n        num_actions,\n        # ...\n    )\n    # ...\n    # loss function\n    actor_loss = self.actor_loss_fn(\n        action_log_probs,\n        old_action_log_probs,\n        advantages,\n        # ...\n    )\n\n    if self.args.use_kl_loss:\n        if self.initial_model is not None:\n            kl = compute_approx_kl(\n                action_log_probs,\n                base_action_log_probs,\n                # ...\n                kl_estimator=self.args.kl_estimator,\n            )\n        else:\n            kl = torch.zeros_like(action_log_probs, dtype=action_log_probs.dtype, device=action_log_probs.device)\n\n        if not self.args.packing_samples:\n            kl_mean = masked_mean(kl, experience.action_mask, dim=-1)\n        else:\n            # ...\n\n        kl_loss = kl_mean.mean()\n        experience.info[\"kl\"] = kl_loss.item()\n    else:\n        kl_loss = 0\n    # ...\n    self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name=\"actor\")\n    # ...\n\n\n\n\n\n\nhttps://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#verl",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#verl",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "2.3 verl",
    "text": "2.3 verl\n\n2.3.1 KL reward é¡¹\nverl åŒæ ·æ”¯æŒè®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼å¹¶ä» reward ä¸­å‡å»ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 4ã€‚\n\n\n\nListingÂ 4: verl å°† KL ä¼°è®¡æ ·æœ¬å€¼ä» reward ä¸­å‡å» 8\n\n\ndef apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):\n    # ...\n    # compute kl between ref_policy and current policy\n    if 'ref_log_prob' in data.batch.keys():\n        kld = core_algos.kl_penalty(data.batch['old_log_probs'], data.batch['ref_log_prob'],\n                                    kl_penalty=kl_penalty)  # (batch_size, response_length)\n        kld = kld * response_mask\n        beta = kl_ctrl.value\n    else:\n        beta = 0\n        kld = torch.zeros_like(response_mask, dtype=torch.float32)\n\n    token_level_rewards = token_level_scores - beta * kld\n    # ...\n\n\n\n\n\n\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160â†©ï¸\n\n\n\n\n2.3.2 KL loss é¡¹\nverl ä¹Ÿæ”¯æŒè®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼ï¼Œå¯¹æ‰€æœ‰ token è®¡ç®—å‡å€¼ï¼Œå¹¶åŠ å…¥åˆ° loss ä¸­ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 5ã€‚\n\n\n\nListingÂ 5: verl è®¡ç®— KL ä¼°è®¡æ ·æœ¬å€¼ï¼Œå¯¹æ‰€æœ‰ token è®¡ç®—å‡å€¼ï¼Œå¹¶åŠ å…¥åˆ° loss ä¸­ 9\n\n\ndef update_policy(self, data: DataProto):\n    # make sure we are in training mode\n    self.actor_module.train()\n    # ...\n    for epoch in range(self.config.ppo_epochs):\n        for batch_idx, data in enumerate(dataloader):\n            # ...\n            self.actor_optimizer.zero_grad()\n\n            for data in micro_batches:\n                # ...\n                responses = data['responses']\n                # ...\n                old_log_prob = data['old_log_probs']\n                # ...\n\n                # all return: (bsz, response_length)\n                entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature)\n\n                pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=old_log_prob,\n                                                                                log_prob=log_prob,\n                                                                                # ...\n                                                                                )\n                # ...\n\n                # compute policy loss\n                policy_loss = pg_loss - entropy_loss * entropy_coeff\n\n                if self.config.use_kl_loss:\n                    ref_log_prob = data['ref_log_prob']\n                    # compute kl loss\n                    kld = core_algos.kl_penalty(logprob=log_prob,\n                                                ref_logprob=ref_log_prob,\n                                                kl_penalty=self.config.kl_loss_type)\n                    kl_loss = masked_mean(kld, response_mask)\n\n                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef\n                # ...\n                loss.backward()\n            # ...\n            grad_norm = self._optimizer_step()\n    # ...\n    self.actor_optimizer.zero_grad()\n    # ...\n\n\n\n\n\n\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-why-kl-reward",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-why-kl-reward",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "2.4 ä¸ºä»€ä¹ˆè¦å°† KL ä» reward ä¸­å‡å»",
    "text": "2.4 ä¸ºä»€ä¹ˆè¦å°† KL ä» reward ä¸­å‡å»\nå°† KL ä» reward ä¸­å‡å»çš„åšæ³•åº”å½“ä¸»è¦å‚è€ƒçš„æ˜¯ OpenAI æ­£å¼æå‡º RLHF çš„è®ºæ–‡ InstructGPT (Ouyang et al. 2022)ã€‚\n\n2.4.1 KL reward çš„æµè¡Œåº”å½“æºè‡ª RLHF ä¸ InstructGPT\nInstructGPT è®ºæ–‡ä¸­æåˆ°å…¶å‘ reward æ·»åŠ äº†ç›¸å¯¹äº SFT æ¨¡å‹çš„ KL æƒ©ç½šé¡¹ï¼Œä½†å¹¶æ²¡æœ‰æåˆ°ä¸ºä»€ä¹ˆå°† KL æ”¾åœ¨ reward è€Œé loss ä¸­ã€‚\n\nâ€¦ In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models â€œPPO.â€\nâ€¦\n\n\\[\n\\begin{aligned}\n\\text { objective }(\\phi)= & E_{(x, y) \\sim D_\\pi^{\\mathrm{RL}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {remin }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}\n\\]\n\nwhere \\(\\pi_\\phi^{\\mathrm{RL}}\\)is the learned RL policy,\\(\\pi^{\\mathrm{SFT}}\\) is the supervised trained model, and\\(D_{\\text {pretrain }}\\)is the pretraining distribution. The KL reward coefficient, \\(\\beta\\), and the pretraining loss coefficient, \\(\\gamma\\), control the strength of the KL penalty and pretraining gradients respectively. For â€œPPOâ€ models, \\(\\gamma\\) is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.\n\n\n\n2.4.2 OpenAI è®ºæ–‡ä¸­ KL reward çš„å‡ºå¤„\nç„¶è€Œï¼Œåœ¨OpenAI æ—©æœŸçš„ä¸€ç¯‡è®ºæ–‡ â€œLearning to summarize from human feedbackâ€ (Stiennon et al. 2020) ä¸­ï¼Œä»–ä»¬å°±å·²ç»é‡‡ç”¨äº† KL rewardï¼Œå¹¶æåŠäº†å‡ºå¤„ï¼š\n\nâ€¦ Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy \\(\\pi_\\phi^{\\mathrm{RL}}\\) with parameters \\(\\phi\\) and this original supervised model \\(\\pi^{\\mathrm{SFT}}\\), as previously done in [25]. The full reward \\(R\\) can be written as:\n\n\\[\nR(x, y)=r_\\theta(x, y)-\\beta \\log \\left[\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right]\n\\]\n\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesnâ€™t learn to produce outputs that are too different from those that the reward model has seen during training.\n\n\n\n2.4.3 KL reward æœ€æ—©çš„å‡ºå¤„\nSectionÂ 2.4.2 ä¸­ OpenAI å¼•ç”¨çš„ KL reward å‡ºå¤„ [25] æ˜¯ â€œWay Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialogâ€ (Jaques et al. 2019)ã€‚\nå®é™…ä¸Šï¼Œå…¶ä¸­å¼•å…¥ KL æ•£åº¦æ—¶ï¼Œæœ€åˆçš„å½¢å¼æ˜¯ loss é¡¹ï¼Œè€Œé reward é¡¹ï¼Œä½†å…¶æŒ‡å‡ºäº†ä¸¤è€…çš„ç­‰ä»·æ€§ï¼š\n\nRather than simply sample from the prior, we would like the \\(Q\\)-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior \\(p(y \\mid x)\\), and the \\(Q\\)-network policy \\(\\pi_\\theta\\), while still maximizing reward. Given a trajectory of actions, \\(\\tau=\\left\\{a_1, a_2, \\ldots a_{t-1}\\right\\}\\), let \\(q(\\tau)=\\prod_{t=1}^T \\pi_\\theta\\left(a_t, s_t\\right)\\)be the policy of our\\(Q\\)-learning algorithm at the trajectory level. Similarly, let \\(p(\\tau)=\\prod_{t=1}^T p\\left(a_t \\mid s_t\\right)\\)be the prior distribution over the trajectory, and\\(r(\\tau)\\) be the rewards. We seek to maximize the following KL-regularized objective:\n\n\\[\nL(q)=\\mathbb{E}_{q(\\tau)}[r(\\tau)] / c-D_{\\text{KL}}[q(\\tau) \\mid p(\\tau)]\n\\]\n\nSince \\(D_{\\text{KL}}[q \\mid p]=\\sum_x q(x)(\\log q(x)-\\log p(x))\\), we can see that this is equivalent to maximizing the following expected value function of the policy \\(\\pi_\\theta\\) at the action level:\n\n\\[\nQ^\\pi\\left(s_t, a_t\\right)=\\mathbb{E}_\\pi\\left[\\sum^T r\\left(s_{t^{\\prime}}, a_{t^{\\prime}}\\right) / c+\\log p\\left(a_{t^{\\prime}} \\mid s_{t^{\\prime}}\\right)-\\log \\pi\\left(a_{t^{\\prime}} \\mid s_{t^{\\prime}}\\right)\\right]\n\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#rl-ä¸­çš„-kl-æ•£åº¦é€šå¸¸å®šä¹‰åœ¨è½¨è¿¹åˆ†å¸ƒä¸Š",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#rl-ä¸­çš„-kl-æ•£åº¦é€šå¸¸å®šä¹‰åœ¨è½¨è¿¹åˆ†å¸ƒä¸Š",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "3.1 RL ä¸­çš„ KL æ•£åº¦é€šå¸¸å®šä¹‰åœ¨è½¨è¿¹åˆ†å¸ƒä¸Š",
    "text": "3.1 RL ä¸­çš„ KL æ•£åº¦é€šå¸¸å®šä¹‰åœ¨è½¨è¿¹åˆ†å¸ƒä¸Š\nGRPO å…¬å¼ (EquationÂ 1) ä¸­çš„ KL é¡¹å¯ä»¥å®šä¹‰ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & =\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)}\\right]\n\\end{aligned}\n\\tag{3}\\]\nå…¶ä¸­ \\(\\mathbf{\\tau}\\) æ˜¯è¡¨ç¤ºè½¨è¿¹ï¼ˆTrajectoryï¼‰çš„éšæœºå˜é‡ã€‚æ³¨æ„ï¼Œä¸ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼ŒPGï¼‰ä¼˜åŒ–è½¨è¿¹åˆ†å¸ƒä¸Šå¥–åŠ±çš„æœŸæœ›ç±»ä¼¼ï¼Œæˆ‘ä»¬åŒæ ·å¸Œæœ›åœ¨è½¨è¿¹åˆ†å¸ƒä¸Šä¼˜åŒ–æœ€æ–°ç­–ç•¥æ•´ä½“åˆ†å¸ƒ \\(p_{\\theta}\\) ä¸å‚è€ƒç­–ç•¥æ•´ä½“åˆ†å¸ƒ \\(p_{\\text{ref}}\\) çš„ KL æ•£åº¦ã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#å°†è½¨è¿¹å±•å¼€ä¸ºçŠ¶æ€-åŠ¨ä½œåºåˆ—",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#å°†è½¨è¿¹å±•å¼€ä¸ºçŠ¶æ€-åŠ¨ä½œåºåˆ—",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "3.2 å°†è½¨è¿¹å±•å¼€ä¸ºçŠ¶æ€-åŠ¨ä½œåºåˆ—",
    "text": "3.2 å°†è½¨è¿¹å±•å¼€ä¸ºçŠ¶æ€-åŠ¨ä½œåºåˆ—\nRL æ–‡çŒ®ä¸­é€šå¸¸ä¼šå°†è½¨è¿¹ \\(\\mathbf{\\tau}\\) å±•å¼€ä¸ºçŠ¶æ€-åŠ¨ä½œåºåˆ— \\(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\)ï¼š10\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & =\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|},\\right) \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|},, \\mathbf{a}_{|\\mathbf{\\tau}|},\\right)}{p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\log \\frac{p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)}{p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\right] \\\\\n\\end{aligned}\n\\tag{4}\\]\nå…¶ä¸­ \\(|\\mathbf{\\tau}|\\) ä¸ºè½¨è¿¹åŠ¨ä½œæ•°çš„éšæœºå˜é‡ã€‚\næ­¤å¤„åˆ©ç”¨äº†è”åˆæ¦‚ç‡çš„å±•å¼€ï¼Œä»¥ \\(p_{\\theta}\\) ä¸ºä¾‹ï¼š\n\\[\np_{\\theta}(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) = p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)\n\\tag{5}\\]\næ³¨æ„åŒºåˆ†æ•´ä½“æ¦‚ç‡åˆ†å¸ƒ \\(p_{\\theta}\\)ã€ç­–ç•¥ï¼ˆæ¡ä»¶ï¼‰æ¦‚ç‡åˆ†å¸ƒ \\(\\pi_{\\theta}\\) ä¸çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒ \\(p\\)ã€‚\n\n\n\nè¿™é‡Œæˆ‘ä»¬ç¦»å¼€äº† GRPO çš„ç¬¦å·ç³»ç»Ÿï¼Œæ¢ç”¨äº† RL æ–‡çŒ®ä¸­æ›´å¸¸è§çš„çŠ¶æ€-åŠ¨ä½œç¬¦å·ç³»ç»Ÿã€‚å®é™…ä¸Šï¼Œ\\(\\mathbf{q}\\) å¯¹åº”äº \\(\\mathbf{s}_1\\)ï¼Œè€Œ \\({\\mathbf{o}}\\) å¯¹åº”äº \\(\\mathbf{\\mathbf{a}_1, \\cdots, \\mathbf{s}_T, \\mathbf{a}_T}\\)ã€‚â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#markov-å†³ç­–è¿‡ç¨‹ä¸­çš„-kl-æ•£åº¦",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#markov-å†³ç­–è¿‡ç¨‹ä¸­çš„-kl-æ•£åº¦",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "3.3 Markov å†³ç­–è¿‡ç¨‹ä¸­çš„ KL æ•£åº¦",
    "text": "3.3 Markov å†³ç­–è¿‡ç¨‹ä¸­çš„ KL æ•£åº¦\nå®é™…ä¸Šï¼ŒRL æ–‡çŒ®ä¸­è¿˜ç»å¸¸å°†åºåˆ—å†³ç­–è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€é˜¶ Markov å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Process, MDP11ã€‚\nMarkov å†³ç­–è¿‡ç¨‹è¦æ±‚åºåˆ—ä¸­çš„æ¡ä»¶æ¦‚ç‡æ»¡è¶³ Markov æ€§è´¨ï¼Œå³åªä¾èµ–äºæœ€æ–°çš„ \\(n\\) ä¸ªå†å²çŠ¶æ€å’ŒåŠ¨ä½œï¼Œè€Œéå…¨éƒ¨çš„å†å²ä¿¡æ¯ï¼Œå¯¹åº”çš„è¿‡ç¨‹ç§°ä¸º \\(n\\) é˜¶ Markov è¿‡ç¨‹ã€‚ä»¥ \\(n=1\\) ä¸ºä¾‹ï¼š\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) & = \\pi(\\mathbf{a}_t \\mid \\mathbf{s}_t) \\\\\np(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) & = p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t) \\\\\n\\end{aligned}\n\\tag{6}\\]\nåˆ™ EquationÂ 5 ä¸­çš„è”åˆæ¦‚ç‡å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ä¸ºï¼š\n\\[\np(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) = p(s_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t)\n\\tag{7}\\]\nå¦‚æœè€ƒè™‘ä¸€é˜¶ Markov è¿‡ç¨‹ï¼Œåˆ™ EquationÂ 4 ä¸­çš„ KL å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] = & = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_t)}\\right] \\\\\n\\end{aligned}\n\\tag{8}\\]\n\n\n\nhttps://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8Bâ†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-lm-as-dp",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-lm-as-dp",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "3.4 è¯­è¨€æ¨¡å‹ä½œä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹",
    "text": "3.4 è¯­è¨€æ¨¡å‹ä½œä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹\nç›®å‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLanguage Model, LMï¼‰é€šå¸¸å»ºæ¨¡ä¸ºè‡ªå›å½’æ¨¡å‹ï¼Œå³å½“å‰ token çš„ç”Ÿæˆä¾èµ–äºæ‰€æœ‰ä¹‹å‰çš„ tokenã€‚\nå°½ç®¡åˆçœ‹èµ·æ¥ï¼Œè‡ªå›å½’æ¨¡å‹ä¼¼ä¹æ— æ³•æ»¡è¶³ Markov æ€§è´¨ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†è‡ªå›å½’æ¨¡å‹å»ºæ¨¡ä¸ºä¸€é˜¶ Markov è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼šä»¤ \\(s_1\\) è¡¨ç¤º prompt ä¸­çš„æ‰€æœ‰ tokenï¼Œå¯¹äº \\(t &gt;1\\)ï¼Œå¦‚æœä»¤ \\(s_t\\) è¡¨ç¤ºç¬¬ \\(t\\) ä¸ªåŠ¨ä½œ token å‰çš„æ‰€æœ‰ tokenï¼Œåˆ™è‡ªå›å½’æ¨¡å‹æ»¡è¶³ Markov æ€§è´¨ï¼Œå¦åˆ™ä¸ä¸€å®šã€‚\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å…ˆä»¤ \\(s_t\\) è¡¨ç¤ºå‰ \\(t\\) ä¸ª token ç»„æˆçš„åºåˆ—ï¼Œå³ä¸ä¾èµ–äº Markov æ€§è´¨ç»§ç»­æ¨å¯¼ï¼Œä»¥è·å¾—å°½å¯èƒ½é€šç”¨çš„ç»“è®ºã€‚åœ¨å¿…è¦æ—¶ï¼Œæˆ‘ä»¬ä¼šå†å¼•å…¥ Markov æ€§è´¨ã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ä¼°è®¡-kl-æ•£åº¦",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ä¼°è®¡-kl-æ•£åº¦",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "3.5 ä¼°è®¡ KL æ•£åº¦",
    "text": "3.5 ä¼°è®¡ KL æ•£åº¦\n\n3.5.1 å‡ ä¹ä¸å¯èƒ½ç›´æ¥è®¡ç®— KL æ•£åº¦çš„çœŸå®å€¼\nå®é™…å®ç°ä¸­ï¼Œæˆ‘ä»¬å‡ ä¹ä¸å¯èƒ½ç›´æ¥è®¡ç®—å‡º \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\)ï¼Œå› ä¸º RL ä¸­çš„ KL æ•£åº¦å®šä¹‰è¦å¯¹è½¨è¿¹ç©ºé—´æ±‚å‡å€¼ï¼Œè€Œè½¨è¿¹ç©ºé—´çš„å¤§å° \\(\\left|\\mathcal{T}\\right|\\) ä¸è½¨è¿¹æœ€å¤§é•¿åº¦ \\(T = \\max_{\\mathbf{\\tau} \\in \\mathcal{T}} |\\mathbf{\\tau}|\\) æˆæŒ‡æ•°å…³ç³»ï¼š \\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid  \\mathbf{s}_1, \\mathbf{a}_1, \\cdots,\\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots,\\mathbf{s}_t)}\\right] \\\\\n& = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta} (\\mathbf{\\tau}) \\left(\\sum_{t=1}^{|\\tau|} \\log \\frac{\\pi_{\\theta}(a_t \\mid  s_1, a_1, \\cdots, s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_1, a_1, \\cdots, s_t)}\\right) \\\\\n\\end{aligned}\n\\tag{9}\\]\n\n\n3.5.2 é€šå¸¸ä½¿ç”¨ Monte Carlo æ–¹æ³•ä¼°è®¡ KL æ•£åº¦\næ‰€ä»¥ï¼Œæˆ‘ä»¬é€šå¸¸åŸºäºè‹¥å¹²è½¨è¿¹æ ·æœ¬ä½¿ç”¨ Monte Carlo æ–¹æ³•12æ¥ä¼°è®¡ RL ä¸­çš„ KL æ•£åº¦ï¼Œä¾‹å¦‚ï¼š\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta} (\\mathbf{\\tau}) \\left(\\sum_{t=1}^{|\\tau|} \\log \\frac{\\pi_{\\theta}(a_t \\mid  s_1, a_1, \\cdots, s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_1, a_1, \\cdots, s_t)}\\right) \\\\\n& \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\sum_{t=1}^{|\\mathbf{\\tau_{i }}|} \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}\\right)\n\\end{aligned}\n\\tag{10}\\]\nå…¶ä¸­ï¼Œ\\(\\mathbf{\\tau_{i}} = \\left(\\mathbf{s}_{i,1}, \\mathbf{a}_{i,1}, \\cdots, \\mathbf{s}_{i,|\\mathbf{\\tau_{i}}|}, \\mathbf{a}_{i,|\\mathbf{\\tau_{i}}|}\\right) \\sim p_{\\theta}\\)ï¼Œ\\(N\\) ä¸ºä¼°è®¡ä½¿ç”¨çš„è½¨è¿¹æ ·æœ¬æ•°é‡ã€‚\n\n\n\nhttps://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95â†©ï¸\n\n\n\n\n3.5.3 ä¸åŒçš„ KL ä¼°è®¡é‡\nå®é™…ä¸Šï¼ŒMonte Carlo æ–¹æ³•å…è®¸ä½¿ç”¨æ ·æœ¬å¯¼å‡ºçš„ä¸åŒä¼°è®¡é‡ï¼Œè€Œä¸å¿…æ˜¯ç»Ÿè®¡é‡å®šä¹‰ä¸­çš„æ ·æœ¬é‡ã€‚ä¸åŒçš„ä¼°è®¡é‡æœ‰ä¸åŒçš„åå·®ï¼ˆBiasï¼‰å’Œæ–¹å·®ï¼ˆVarianceï¼‰ï¼Œä»è€Œæ„æˆäº†ä¼°è®¡é‡é€‰æ‹©ä¹‹é—´çš„æƒè¡¡ã€‚\nè®¾ KL ä¼°è®¡é‡ä¸º \\(k\\)ï¼Œåˆ™å¯¹åº”çš„ KL ä¼°è®¡å€¼ä¸º\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N} k(\\tau_i)\n\\end{aligned}\n\\tag{11}\\]\nä¾‹å¦‚ SectionÂ 2.2.1 æåˆ°ï¼ŒOpenRLHF å¼•å…¥äº† 3 ç§ KL æ•£åº¦çš„ä¼°è®¡æ–¹æ³•ï¼Œåˆ†åˆ«ç§°ä¸º k1, k2, k3ï¼Œè¿™åº”è¯¥æ˜¯ä¸»è¦å‚è€ƒäº† John Schulman çš„åšå®¢ â€œApproximating KL Divergenceâ€ã€‚\nverl åˆ™è€ƒè™‘äº†æ›´å¤šä¼°è®¡æ–¹æ³•ã€‚å®é™…ä¸Šï¼Œverl è¿˜è€ƒè™‘äº†ç›´æ¥è®¡ç®—æ¡ä»¶ KL æ•£åº¦13ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰å®ç°ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 6ã€‚\n\n\n\nListingÂ 6: verl çš„ KL æ•£åº¦ Monte Carlo ä¼°è®¡æ ·æœ¬å€¼14\n\n\ndef kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) -&gt; torch.FloatTensor:\n    # ...\n    if kl_penalty == \"kl\":\n        return logprob - ref_logprob\n\n    if kl_penalty == \"abs\":\n        return (logprob - ref_logprob).abs()\n\n    if kl_penalty == \"mse\":\n        return 0.5 * (logprob - ref_logprob).square()\n\n    # J. Schulman. Approximating kl divergence, 2020.\n    # # URL http://joschu.net/blog/kl-approx.html.\n    if kl_penalty == 'low_var_kl':\n        kl = ref_logprob - logprob\n        ratio = torch.exp(kl)\n        kld = (ratio - kl - 1).contiguous()\n        return torch.clamp(kld, min=-10, max=10)\n\n    if kl_penalty == \"full\":\n        # so, here logprob and ref_logprob should contain the logits for every token in vocabulary\n        raise NotImplementedError\n\n    raise NotImplementedError\n\n\n\nç”±äº \\(k_1\\)ã€\\(k_2\\)ã€\\(k_3\\) ä¸‰ç§ä¼°è®¡é‡æœ€ä¸ºæµè¡Œï¼Œæˆ‘ä»¬å°†ä»¥è¿™ä¸‰ç§ä¼°è®¡é‡ä¸ºä¾‹å±•å¼€åˆ†æã€‚\nè€ƒè™‘ \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} k_j(\\tau_i)\\)ï¼Œå…¶ä¸­ \\(\\tau_i \\sim p_{\\theta}\\)ï¼Œä»¤ \\(r = \\frac{\\pi_{\\text{ref}}(\\tau_i)}{\\pi_{\\theta}(\\tau_i)}\\)ï¼Œæ³¨æ„ï¼Œæ­¤å¤„ \\(r\\) å¹¶é KL å®šä¹‰ä¸­çš„æ ·æœ¬é‡ï¼Œè€Œæ˜¯å…¶å€’æ•°ï¼Œåˆ™ï¼š\n\\[\n\\begin{aligned}\nk_{1} & = - \\log r \\\\\nk_{2} & = \\frac{1}{2} (\\log r)^2 \\\\\nk_{3} & = (r - 1) - \\log r\n\\end{aligned}\n\\tag{12}\\]\n\n\n\nè¿™é‡Œçš„æ¡ä»¶ KL æ•£åº¦åªéœ€è¦éå†æ•´ä¸ªè¯è¡¨ï¼Œä»£ä»·å¯èƒ½æ˜¯å¯ä»¥æ¥å—çš„ã€‚â†©ï¸\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-loss-impl",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-loss-impl",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "4.1 åˆ†ææµè¡Œçš„ â€œKL loss é¡¹â€ å®ç°",
    "text": "4.1 åˆ†ææµè¡Œçš„ â€œKL loss é¡¹â€ å®ç°\nä¸Šè¿°æ¡†æ¶ä¸­ï¼ŒOpenRLHF ä¸ verl éƒ½å®ç°äº† â€œKL loss é¡¹â€ï¼Œå³å…ˆç›´æ¥è®¡ç®—å‡º KL ä¼°è®¡é‡å¹¶åŠ å…¥åˆ° loss ä¸­ï¼Œå†åå‘ä¼ æ’­å¾—åˆ°æ¢¯åº¦ï¼ŒæœŸé—´é»˜è®¤æ²¡æœ‰å»é™¤æ¢¯åº¦ã€‚\nç„¶è€Œï¼Œå¦‚ SectionÂ 1 æ‰€è¿°ï¼Œè¿™ä¸€åšæ³•æ˜¯é”™è¯¯çš„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†é€šè¿‡åˆ†æè¿™äº› â€œKL loss é¡¹â€ å®é™…å¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡ï¼Œè¯´æ˜å…¶é”™è¯¯ä¹‹å¤„ã€‚\n\n4.1.1 ä¸åŒ KL ä¼°è®¡é‡å¯¹åº”çš„ loss é¡¹å¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡çš„ä¸€èˆ¬å½¢å¼\nè§‚å¯Ÿ ListingÂ 3 è®¡ç®— â€œKL lossâ€ é¡¹çš„éƒ¨åˆ†ã€‚\n# ...\nkl = compute_approx_kl(\n    action_log_probs,\n    base_action_log_probs,\n    # ...\n    kl_estimator=self.args.kl_estimator,\n)\n# ...\nkl_mean = masked_mean(kl, experience.action_mask, dim=-1)\n# ...\nkl_loss = kl_mean.mean()\n# ...\nè¿™äº›ä»£ç ï¼š\n\nè®¡ç®—äº† klï¼Œå¯¹åº”å¯¹æ¯ä¸ªåŠ¨ä½œ token \\(a_{i,t}\\) è®¡ç®— â€œKL ä¼°è®¡é‡â€ \\(k\\)ã€‚\nè®¡ç®—äº† kl_meanï¼Œå¯¹åº”å¯¹æ¯ä¸ªè½¨è¿¹ \\(\\tau_i\\) è®¡ç®—å‡å€¼ \\(\\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|} k\\)ã€‚\nè®¡ç®—äº† kl_lossï¼Œå¯¹åº”å¯¹æ‰€æœ‰è½¨è¿¹æ ·æœ¬è®¡ç®—å‡å€¼ \\(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|} k\\)ã€‚\n\nç”±äºå…¶æ²¡æœ‰å»é™¤ä»»ä½•æ¢¯åº¦ï¼Œå› æ­¤å…¶å¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡å€¼ä¸º\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\frac{1}{|\\tau_i|} k \\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|}  \\nabla_{\\theta} k\n\\end{aligned}\n\\tag{13}\\]\nListingÂ 5 ä¸­ verl çš„å®ç°ç±»ä¼¼ï¼Œä½†ä¸åŒçš„æ˜¯å…¶å¹³å‡æ˜¯åœ¨æ‰€æœ‰ token ä¹‹é—´æ‰§è¡Œçš„ï¼Œå› æ­¤å¯¹åº”çš„æ¢¯åº¦ä¼°è®¡å€¼ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\left( \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|} \\sum_{i=1}^{N} k \\right) = \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|} \\sum_{i=1}^{N} \\nabla_{\\theta} k\n\\end{aligned}\n\\tag{14}\\]\næˆ‘ä»¬å°†å¹³å‡æ“ä½œä¸€èˆ¬åŒ–ä¸ºæƒé‡ \\(w_{\\mathbf{\\tau}}\\) ä¸ \\(w_{t}\\)ï¼Œåˆ™ä¸åŒ KL ä¼°è®¡é‡å¯¹åº”çš„ loss é¡¹å¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡å€¼çš„ä¸€èˆ¬å½¢å¼ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{N} w_{\\mathbf{\\tau}_i} \\sum_{t=1}^{|\\tau_i|} w_{t} \\nabla_{\\theta} k \\\\\n\\end{aligned}\n\\tag{15}\\]\nåˆ™\n\nOpenRLHF å¯¹åº” \\(w_{\\mathbf{\\tau}} = \\frac{1}{N}, w_{t} = \\frac{1}{|\\tau|}\\)ï¼›\nverl å¯¹åº” \\(w_{\\mathbf{\\tau}} = \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|}, w_{t} = 1\\)ã€‚\n\næ­¤å¤„ï¼Œæˆ‘ä»¬å…ˆä»¥ OpenRLHF çš„æ¢¯åº¦ä¼°è®¡ (EquationÂ 13) ä¸ºä¾‹ï¼Œåˆ†æä¸åŒ KL ä¼°è®¡é‡å¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡ï¼Œå…¶æ»¡è¶³ï¼š\n\\[\n\\mathbb{E}_{\\mathbf{\\tau}_i \\sim p_{\\theta}} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|}  \\nabla_{\\theta} k \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} k \\right]\n\\tag{16}\\]\næˆ‘ä»¬ä¼šåœ¨ SectionÂ 5 ä¸­æ¨å¯¼æ­£ç¡®çš„ KL æ¢¯åº¦ä¼°è®¡ã€‚\n\n\n4.1.2 \\(k_1\\) å¯¼å‡ºçš„æ¢¯åº¦ï¼šæœŸæœ›ä¸º 0\nå‘ EquationÂ 16 ä»£å…¥ \\(k = k_1 = - \\log r = \\log \\frac{1}{r} = \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\)ï¼Œå¯¼å‡ºçš„æ¢¯åº¦ä¼°è®¡ä¸º\n\\[\n\\begin{aligned}\n& \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta}\\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\left( \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) + \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t}) + \\nabla_{\\theta} \\log \\left( p(\\mathbf{s}_{1}) \\right) \\right) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\left( p(\\mathbf{s}_{1}) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t}) \\right) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_\\theta(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_{\\theta}(\\tau)\n\\end{aligned}\n\\tag{17}\\]\nåˆ™å…¶å¯¼å‡ºçš„æ¢¯åº¦æœŸæœ›æ»¡è¶³ï¼š\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\\right]\n& = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\frac{1}{|\\tau|} \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n& = \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n& = \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} \\nabla_{\\theta} p_{\\theta}(\\tau) \\\\\n& = \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\frac{1}{|\\tau|} \\\\\n& = \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\right]\n\\end{aligned}\n\\tag{18}\\]\næ­¤å¤„åˆ©ç”¨äº† \\(p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\frac{1}{p_{\\theta}(\\tau)} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} p_{\\theta}(\\tau)\\)ã€‚\næ‰€ä»¥ \\(k_1\\) loss é¡¹ä¼˜åŒ–çš„é‡æ˜¯ \\(\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\right]\\)ã€‚è¿™æ„å‘³ç€è¯¥ä¼˜åŒ–è¿‡ç¨‹ä¼šé™ä½é‡‡æ ·è½¨è¿¹çš„é•¿åº¦ã€‚\nç‰¹åˆ«åœ°ï¼Œå½“ä¸å¯¹åŒä¸€è½¨è¿¹ä¸­çš„ â€œ\\(k_1\\) ä¼°è®¡é‡â€æ±‚å‡å€¼ï¼Œè€Œæ˜¯æ±‚å’Œæ—¶ï¼Œå¯ä»¥ç›´æ¥å°† \\(\\frac{1}{|\\tau|}\\) è¿™ä¸€é¡¹æ›¿æ¢ä¸º \\(1\\)ï¼Œå¾—åˆ° \\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) = \\sum_{\\tau \\in \\mathcal{T}} \\nabla_{\\theta} p_{\\theta} = \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta} = \\nabla_{\\theta} 1 = 0\n\\tag{19}\\]15\nè¿™æ„å‘³ç€ä½¿ç”¨è¯¥æ¢¯åº¦æ›´æ–°å‚æ•°ï¼Œåœ¨å¹³å‡æ„ä¹‰ä¸Šä¸ä¼šå¼•èµ·å‚æ•°åŠå…¶å¯¼å‡ºçš„åˆ†å¸ƒæ”¹å˜ã€‚\næ— è®ºå“ªç§æƒ…å†µï¼Œ\\(k_1\\) å¯¼å‡ºçš„ä¼˜åŒ–é‡éƒ½éå¸¸å¥‡æ€ªï¼Œä¸å¤ªå¯èƒ½å‡ºäºå®ç°è€…çš„æœ¬æ„ã€‚\nåŒæ—¶ï¼Œå¯¹åŒä¸€è½¨è¿¹ä¸­çš„ KL ä¼°è®¡é‡æ±‚å‡å€¼è¿™ä¸€æ“ä½œï¼Œä¹Ÿå¾ˆæœ‰å¯èƒ½æ˜¯é”™è¯¯çš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¿½ç•¥è¿™ä¸€æ“ä½œï¼Œå³å°† \\(\\frac{1}{|\\tau|}\\) ä¸€é¡¹æ›¿æ¢ä¸º \\(1\\)ã€‚\n\n\n\næ­¤å¤„å¯¹æ•°ä¼¼ç„¶çš„æ¢¯åº¦çš„æœŸæœ›å€¼ä¸º 0ï¼Œæ˜¯ä¸€ä¸ªè‘—åçš„æ€§è´¨ï¼Œä¼šåœ¨æ¥ä¸‹æ¥é¢‘ç¹ç”¨åˆ°ã€‚â†©ï¸\n\n\n\n\n4.1.3 \\(k_2\\) å¯¼å‡ºçš„æ¢¯åº¦\nå‘ EquationÂ 16 ä»£å…¥ \\(k = k_2 = \\frac{1}{2} (\\log r)^2 = \\frac{1}{2} \\left(\\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\right)^2\\)ï¼Œå¯¼å‡ºçš„å•æ¡è½¨è¿¹ \\(\\mathbf{\\tau} \\sim p_{\\theta}\\) çš„æ¢¯åº¦ä¸º \\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k\\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta}  \\frac{1}{2} \\left(\\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}\\right)^2 \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\\\\n\\end{aligned}\n\\tag{20}\\]\næ˜¾ç„¶ï¼Œ\n\\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\\\\n\\neq & \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\right) \\\\\n=& \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\n\\end{aligned}\n\\tag{21}\\]\nç„¶è€Œï¼Œ\n\\[\n\\begin{aligned}\n& \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[  \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} \\right) \\nabla_{\\theta} p_{\\theta}(\\tau) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} \\left[ \\left( \\log p_{\\theta}(\\tau) \\right) \\nabla_{\\theta} p_{\\theta}(\\tau) - \\left( \\log p_{\\text{ref}}(\\tau) \\right) \\nabla_{\\theta} p_{\\theta}(\\tau) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}}  \\left[ \\nabla_{\\theta} (\\log p_{\\theta}(\\tau) - 1) p_{\\theta}(\\tau) -  \\nabla_{\\theta} \\log p_{\\text{ref}}(\\tau) p_{\\theta}(\\tau) \\right] \\\\\n=& \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}}  \\left[ (\\log p_{\\theta}(\\tau) - 1) p_{\\theta}(\\tau) - \\log p_{\\text{ref}}(\\tau) p_{\\theta}(\\tau) \\right] \\\\\n=& \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}}  p_{\\theta} \\left[ \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} - 1 \\right) \\right] \\\\\n=& \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[  \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} - 1 \\right) \\right] \\\\\n= & \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[  \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right] \\\\\n= & \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\n\\end{aligned}\n\\tag{22}\\]\næ­¤å¤„åˆ©ç”¨äº† \\(\\log p(x) \\nabla_{\\theta} p(x) = \\nabla_{\\theta} (\\log p(x) - 1) p(x)\\)\nå› æ­¤ï¼Œæœ€å°åŒ– \\(k_2\\) loss é¡¹ (EquationÂ 21) ï¼Œå¹¶éåœ¨ä¼˜åŒ– \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\)ã€‚\n\n\n4.1.4 \\(k_3\\) å¯¼å‡ºçš„æ¢¯åº¦\nå‘ EquationÂ 16 ä»£å…¥ \\(k = k_3 = (r - 1) - \\log r = (\\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} - 1) - \\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\)ï¼Œå¯¼å‡ºçš„å•æ¡è½¨è¿¹ \\(\\mathbf{\\tau} \\sim p_{\\theta}\\) çš„æ¢¯åº¦ä¸º \\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\left(\\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} - 1 - \\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\right) \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} - \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) - \\nabla_{\\theta} \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\\\\n=& - \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\right) - \\nabla_{\\theta} \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\\\\n=& - \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\right) + \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\\\\n\\end{aligned}\n\\tag{23}\\]\nå…¶ä¸­ï¼Œæ ¹æ® EquationÂ 19ï¼Œ\\(\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] = 0\\)ï¼Œä¸å¦¨ç›´æ¥çœç•¥ã€‚\nè€Œå‰©ä½™éƒ¨åˆ†ä¼¼ä¹å¾ˆéš¾é€šè¿‡æ¶ˆå» \\(\\pi_{\\theta}(\\mathbf{\\tau})\\) æ¥æå‡º \\(\\nabla_{\\theta}\\) å¹¶å‡†ç¡®åˆ†æã€‚ä½†æ˜¾ç„¶ä¹Ÿå¹¶éåœ¨ä¼˜åŒ– KL æ•£åº¦ã€‚\n\n\n4.1.5 å°ç»“ï¼šæµè¡Œçš„ â€KL loss é¡¹â€œ å®ç°å¹¶ä¸åˆç†\nç»¼ä¸Šæ‰€è¿°ï¼Œå¯¹äº OpenRLHF å®ç°çš„ â€œKL loss é¡¹â€ï¼Œ\n\nå¯¹åŒä¸€è½¨è¿¹å†…çš„ â€œKL ä¼°è®¡é‡â€ æ±‚å‡å€¼è¿™ä¸€æ“ä½œå¾ˆå¯èƒ½æ˜¯é”™è¯¯çš„ï¼Œæ­£ç¡®æ“ä½œåº”å½“ä¸ºæ±‚å’Œï¼Œå¯¹åº”äºæ ¹æ®å¯¹æ•°æ¡ä»¶æ¦‚ç‡æ±‚å¯¹æ•°è”åˆæ¦‚ç‡ã€‚\n\\(k_1\\) å¯¼å‡ºçš„æ¢¯åº¦\n\nè‹¥å¯¹åŒä¸€è½¨è¿¹å†…çš„ â€œKL ä¼°è®¡é‡â€ æ±‚å‡å€¼ï¼Œåˆ™ä¼šå¯¼è‡´è¾“å‡ºé•¿åº¦å‡å°ï¼Œ\nè€Œå¦‚æœä¿®æ­£ä¸ºæ±‚å’Œï¼Œåˆ™å…¶æœŸæœ›ä¸º 0ï¼Œåœ¨å¹³å‡æ„ä¹‰ä¸Šä¸æ”¹åˆ†å¸ƒã€‚\n\n\\(k_2\\)ï¼Œ\\(k_3\\) å¯¼å‡ºçš„æ¢¯åº¦åˆ™ååˆ†å¤æ‚ï¼Œéš¾ä»¥åˆ†æï¼Œä½†éƒ½å¹¶éåœ¨ä¼˜åŒ– KL æ•£åº¦ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå…¶é”™è¯¯åœ°å°† KL ä¼°è®¡æ ·æœ¬é‡åº”ç”¨äºåŠ¨ä½œå¯¹æ•°æ¡ä»¶ä¼¼ç„¶å¹¶æ±‚å’Œã€‚å›é¡¾ KL ä¼°è®¡é‡å…¬å¼ (EquationÂ 12) ï¼Œåº”å½“æ³¨æ„åˆ°è¿™äº›ä¼°è®¡é‡æ˜¯ç›´æ¥ä½œç”¨äºä¼¼ç„¶ \\(p_{\\theta}(\\mathbf{\\tau})\\)ï¼Œè€Œæ²¡æœ‰ä¿è¯ä½œç”¨äºæ¦‚ç‡åæ±‚ç§¯/å¯¹æ•°å’Œä»ç„¶æœ‰æ„ä¹‰ã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#åˆ†ææµè¡Œçš„-kl-reward-é¡¹-å®ç°",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#åˆ†ææµè¡Œçš„-kl-reward-é¡¹-å®ç°",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "4.2 åˆ†ææµè¡Œçš„ â€œKL reward é¡¹â€œ å®ç°",
    "text": "4.2 åˆ†ææµè¡Œçš„ â€œKL reward é¡¹â€œ å®ç°\n\n4.2.1 ç±»æ¯” PG ä¼˜åŒ– reward æ¥åˆ†æ KL reward çš„ä½œç”¨\nç”±äº PG ä¼˜åŒ–çš„å°±æ˜¯ rewardï¼Œå› æ­¤æˆ‘ä»¬ä¸å¦¨ä» PG çš„ä¼°è®¡å‡ºå‘ã€‚æœ€å¸¸ç”¨çš„ PG ä¼°è®¡æ–¹å¼åº”å½“æ˜¯ï¼š \\[\n\\nabla_\\theta \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[r(\\mathbf{\\tau})\\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right) \\hat{A}_t \\right]\n\\tag{24}\\]\nå…¶ä¸­ \\(\\hat{A}_t\\) ä¸ºä¼˜åŠ¿ï¼ˆAdvantageï¼‰çš„ä¼°è®¡é‡ã€‚\nä¸ºäº†æ–¹ä¾¿è§‚å¯Ÿ KL reward é¡¹å‘æŒ¥çš„ä½œç”¨ï¼Œæˆ‘ä»¬å°† \\(r_{\\mathbf{\\tau}}\\) å±•å¼€ï¼Œå¹¶ä¸å¦¨è€ƒè™‘ä¸€ä¸ªæ›´ç®€å•çš„ä¼°è®¡ï¼Œä¾‹å¦‚ï¼š\n\\[\n\\nabla_\\theta \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} r(\\mathbf{s}_t, \\mathbf{a}_t) \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right) \\sum_{t'=1}^{|\\tau|} r(s_{t'}, a_{t'}) \\right]\n\\tag{25}\\]\nç®€æ´èµ·è§ï¼Œè¿™é‡Œçœç•¥äº†è¯¥ä¼°è®¡æ–¹å¼æ­£ç¡®æ€§çš„è¯æ˜ï¼Œæœ‰å…´è¶£çš„è¯»è€…å¯ä»¥å‚è€ƒ UCB CS285 â€œPolicy Gradientâ€ ä¸€è®²16ã€‚\nç±»æ¯” \\(r_{t'}\\) å¯¼å‡ºçš„æ¢¯åº¦æœŸæœ›ï¼Œå°†è´Ÿçš„ KL æ ·æœ¬é‡ \\(- \\log \\frac{\\pi_\\theta\\left(a_t \\mid s_t \\right)}{\\pi_{\\text{ref}}\\left(a_t \\mid s_t \\right)}\\) åŠ å…¥ reward \\(r_{t'}\\) ä»£å…¥å…¶ä¸­ï¼Œå¯¼å‡ºçš„æ¢¯åº¦æœŸæœ›ä¸ºï¼š\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|}  \\left( \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t \\right) \\right) \\sum_{t'=1}^{|\\tau|} - \\log \\frac{\\pi_\\theta\\left(a_{t'} \\mid s_{t'} \\right)}{\\pi_{\\text{ref}}\\left(a_{t'} \\mid s_{t'} \\right)} \\right] = \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}\\right]\n\\tag{26}\\]\næ³¨æ„ï¼Œä»¥ä¸Šæ¨å¯¼å‡è®¾ RL ä¼˜åŒ–çš„åºåˆ—å†³ç­–è¿‡ç¨‹æ»¡è¶³ä¸€é˜¶ Markov æ€§è´¨ (EquationÂ 6)ã€‚\nå®é™…ä¸Šï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°ä»»æ„åºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå³è¦æ±‚æ¡ä»¶æ¦‚ç‡ä¾èµ–äºæ‰€æœ‰å†å²çŠ¶æ€å’ŒåŠ¨ä½œï¼Œåˆ™å¯¹åº”çš„ KL æ¢¯åº¦æœŸæœ›ä¸ºï¼š\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta}- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)} \\right] \\\\\n\\to& \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right] \\\\\n= & \\nabla_{\\theta} -  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{\\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{ \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{ p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) }{ p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) } \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{ p_\\theta\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}  \\right)}{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}  \\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta} \\left[ \\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n\\end{aligned}\n\\tag{27}\\]\nå¯è§ï¼Œè®¡ç®— KL æ ·æœ¬é‡å¹¶æ”¾å…¥ reward ä¸­ï¼Œå¯¼å‡ºçš„æ¢¯åº¦æœŸæœ›å³ä¸ºä¸¤ä¸ªåˆ†å¸ƒçš„ KL æ•£åº¦çš„è´Ÿæ¢¯åº¦ï¼Œåˆ™æœ€å¤§åŒ– rewardï¼Œå°±ä¼šæœ€å°åŒ– KL æ•£åº¦ï¼Œæ˜¯æ­£ç¡®çš„åšæ³•ã€‚\n\n\n\nhttps://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdfâ†©ï¸\n\n\n\n\n4.2.2 ä¸åŒ KL ä¼°è®¡é‡å¯¼å‡ºçš„ reward é¡¹çš„ä½œç”¨\nä¸éš¾æ³¨æ„åˆ°ï¼ŒSectionÂ 4.2.1 ä¸­çš„ KL æ ·æœ¬é‡å¯¹åº”äº \\(k_1\\) ä¼°è®¡é‡ã€‚\nä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜æ˜¯ï¼Œå¦‚æœå¯¹åŠ¨ä½œæ¡ä»¶ä¼¼ç„¶ä½¿ç”¨ \\(k_2\\) æˆ– \\(k_3\\) ç­‰å…¶ä»–ä¼°è®¡é‡ï¼Œä¼šå¾—åˆ°ä»€ä¹ˆç»“æœï¼Ÿ\n\\(k_2\\) æˆ– \\(k_3\\) ç­‰å…¶ä»–ä¼°è®¡é‡å¯¼è‡´çš„ä¸€ä¸ªé—®é¢˜ï¼Œæ±‚å’Œæ—¶é€šå¸¸æ— æ³•å¾—åˆ°è”åˆæ¦‚ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œå…¶ä»–ä¼°è®¡é‡åˆ†åˆ«åœ¨ä¼˜åŒ–\n\n\\(k_2\\): \\(- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{1}{2} \\left( \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right)^{2} \\right]\\)\n\\(k_3\\): \\(- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} (\\frac{\\pi_{\\text{ref}} \\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\theta}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} - 1 - \\log \\frac{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\theta}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}) \\right]\\)\n\næ˜¾ç„¶ï¼Œè¿™é‡Œçš„æ±‚å’Œæ— æ³•å¾—åˆ°è”åˆæ¦‚ç‡ï¼Œä¹Ÿå°±æ— æ³•å®ç°ç±»ä¼¼ EquationÂ 27 ä¸­çš„æ•ˆæœäº†ã€‚\n\n\n4.2.3 å°ç»“ï¼šåœ¨ on-policy è®¾ç½®ä¸‹ä¿®æ­£ GRPO ç›®æ ‡çš„ KL é¡¹\nè‹¥å¯¹åŠ¨ä½œå¯¹æ•°æ¡ä»¶ä¼¼ç„¶è®¡ç®— KL ä¼°è®¡æ ·æœ¬é‡ï¼Œåˆ™ç”±äºæ¶‰åŠåˆ°æ±‚å’Œï¼Œ\\(k_1\\) ä¹‹å¤–çš„ä¼°è®¡é‡é€šå¸¸æ²¡æœ‰è‰¯å¥½å®šä¹‰ã€‚\nä½†æ˜¯è‹¥æ”¾å¼ƒå¯¹åŠ¨ä½œæ¡ä»¶ä¼¼ç„¶è®¡ç®— KL ä¼°è®¡æ ·æœ¬é‡ï¼Œè€Œæ˜¯å¯¹æ±‚å’Œä¹‹åçš„å¯¹æ•°ï¼ˆæ¡ä»¶ï¼‰ä¼¼ç„¶è¿›è¡Œè®¡ç®—ï¼Œåˆ™åªéœ€æ»¡è¶³\n\\[\n\\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  k\\left(\\frac{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}\\right) \\right]\n\\approx \\nabla_{\\theta} - \\frac{1}{N} k\\left(\\frac{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}\\right)\n\\approx \\nabla_{\\theta} - \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\n\\tag{28}\\]\næš‚æ—¶ä¸è€ƒè™‘ off-policy é—®é¢˜ï¼Œæ ¹æ® EquationÂ 28, GRPO å…¬å¼ (EquationÂ 1, EquationÂ 2) åº”å½“ä¿®æ­£ KL é¡¹å¦‚ä¸‹ï¼š\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\left\\{ \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|} \\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]  \\right\\}  -\\beta k\\left( \\frac{\\prod_{t=1}^{|o_i|} \\pi_{\\text{ref}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\prod_{t=1}^{|o_i|} \\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\right)\n\\end{aligned}\n\\tag{29}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#åœ¨å·²çŸ¥ç¯å¢ƒä¸­ç®€åŒ–-kl-æ¢¯åº¦ä¼°è®¡",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#åœ¨å·²çŸ¥ç¯å¢ƒä¸­ç®€åŒ–-kl-æ¢¯åº¦ä¼°è®¡",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "5.1 åœ¨å·²çŸ¥ç¯å¢ƒä¸­ç®€åŒ– KL æ¢¯åº¦ä¼°è®¡",
    "text": "5.1 åœ¨å·²çŸ¥ç¯å¢ƒä¸­ç®€åŒ– KL æ¢¯åº¦ä¼°è®¡\nå®é™…ä¸Šï¼ŒLLM çš„è®¸å¤šä»»åŠ¡ä¸­ï¼Œç¯å¢ƒä¸­çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒå‡ä¸ºå·²çŸ¥çš„ï¼Œæœ‰æ—¶è¿˜å¯èƒ½æ˜¯ç¡®å®šæ€§çš„ï¼ˆDeterministicï¼‰ã€‚\nå½“çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒå·²çŸ¥æ—¶ï¼Œ\\(\\forall t, p_{\\theta}(a_1, \\cdots, s_t, a_t \\mid s_1)\\) éƒ½æ˜¯å¯ä»¥è®¡ç®—çš„ï¼Œåˆ™ KL æ•£åº¦å¯ä»¥ç›´æ¥å†™æˆï¼š\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\sum_{\\mathbf{\\tau} \\in \\mathcal{T}} p(\\mathbf{s}_1) p_{\\theta}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1) \\log \\frac{p_{\\theta}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1)}{p_{\\text{ref}}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1)}  \\\\\n\\end{aligned}\n\\tag{32}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ç®€å†™ä¸º-contextual-bandit",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ç®€å†™ä¸º-contextual-bandit",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "5.2 ç®€å†™ä¸º Contextual Bandit",
    "text": "5.2 ç®€å†™ä¸º Contextual Bandit\nä¸ºäº†æ–¹ä¾¿ä¹¦å†™ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å°†æ¨¡å‹ç®€åŒ–ä¸º contextual banditï¼Œå³ä»¤ \\(\\mathbf{s}_1 = \\mathbf{x} \\in \\mathcal{P}, (\\mathbf{a}_1, \\cdots, \\mathbf{s}_T, \\mathbf{a}_T) = \\mathbf{y} \\in \\mathcal{R}\\)ï¼Œå…¶ä¸­ \\(\\mathcal{P}, \\mathcal{R}\\) åˆ†åˆ«è¡¨ç¤º prompt / response ç©ºé—´ï¼Œåˆ™ KL æ•£åº¦å˜ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p_{\\theta}}\\left[\\log \\frac{\\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})}{\\pi_{\\text{ref}}(\\mathbf{y} \\mid \\mathbf{x})}\\right] \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p_{\\theta}(x, y) \\left(\\sum_{t=1}^{T} \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\n\\end{aligned}\n\\tag{33}\\]\nå…¶æ¢¯åº¦å˜ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\nabla_{\\theta} \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\nabla_{\\theta} \\left(\\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\\right)\n\\end{aligned}\n\\tag{34}\\]\nå…¶ä¸­æ¢¯åº¦é¡¹å¯ä»¥è¿›ä¸€æ­¥å±•å¼€ä¸ºï¼š\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\left(\\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\\right) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\pi_{\\theta}(y \\mid x) \\nabla_{\\theta} \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\pi_{\\theta}(y \\mid x) \\frac{1}{\\pi_\\theta(y \\mid x)} \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\n\\end{aligned}\n\\tag{35}\\]\nä»£å…¥å› KL æ¢¯åº¦è¡¨è¾¾å¼ï¼š\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)}{\\pi_{\\theta}(y \\mid x)} \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x) \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] + \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right]\n\\end{aligned}\n\\tag{36}\\]\nè¿™é‡Œä¸ºäº†é‡æ–°è·å¾—æœŸæœ›å½¢å¼ï¼Œå¼•å…¥äº† \\(1 = \\pi_{\\theta}(y \\mid x) / \\pi_{\\theta}(y \\mid x)\\)ï¼Œå¹¶åˆ©ç”¨äº† \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x) = \\frac{\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)}{\\pi_{\\theta}(y \\mid x)}\\) å’Œ \\(\\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] = 0\\)ã€‚\nè¿›è¡Œ Monte Carlo ä¼°è®¡ï¼š\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\log \\frac{\\pi_{\\theta}(y_i \\mid x_i)}{\\pi_{\\text{ref}}(y_i \\mid x_i)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y_i \\mid x_i)\n\\end{aligned}\n\\tag{37}\\]\nå…¶ä¸­ \\((\\mathbf{x}_i, \\mathbf{y}_i) \\sim p_{\\theta}\\)ã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#è¿˜åŸä¸ºå·²çŸ¥ç¯å¢ƒå†³ç­–è¿‡ç¨‹",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#è¿˜åŸä¸ºå·²çŸ¥ç¯å¢ƒå†³ç­–è¿‡ç¨‹",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "5.3 è¿˜åŸä¸ºå·²çŸ¥ç¯å¢ƒå†³ç­–è¿‡ç¨‹",
    "text": "5.3 è¿˜åŸä¸ºå·²çŸ¥ç¯å¢ƒå†³ç­–è¿‡ç¨‹\nå°†ä¸Šé¢çš„ KL æ¢¯åº¦è¡¨è¾¾å¼è¿˜åŸä¸ºå·²çŸ¥ç¯å¢ƒå†³ç­–è¿‡ç¨‹å»ºæ¨¡çš„å½¢å¼ï¼š\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\\\\n=& \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})}{\\pi_{\\text{ref}}(\\mathbf{y} \\mid \\mathbf{x})}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T}) \\sim p_{\\theta}} \\left[\\left(\\sum_{t=1}^{T} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)}\\right) \\left(\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)\\right)\\right]\n\\end{aligned}\n\\tag{38}\\]\nå¯¹åº”çš„ Monte Carlo ä¼°è®¡å¼ä¸ºï¼š\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\left(\\sum_{t=1}^{T}\\log \\frac{\\pi_{\\theta}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})}{\\pi_{\\text{ref}}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})}\\right) \\left(\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})\\right)\n\\end{aligned}\n\\tag{39}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#åˆ©ç”¨å› æœæ€§æŠ€å·§åŒ–ç®€-kl-æ¢¯åº¦ä¼°è®¡",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#åˆ©ç”¨å› æœæ€§æŠ€å·§åŒ–ç®€-kl-æ¢¯åº¦ä¼°è®¡",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "5.4 åˆ©ç”¨å› æœæ€§æŠ€å·§åŒ–ç®€ KL æ¢¯åº¦ä¼°è®¡17",
    "text": "5.4 åˆ©ç”¨å› æœæ€§æŠ€å·§åŒ–ç®€ KL æ¢¯åº¦ä¼°è®¡17\nå› æœæ€§æŠ€å·§ï¼ˆCausality Trickï¼‰æ˜¯åˆ†æåºåˆ—å†³ç­–è¿‡ç¨‹æ—¶ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„æŠ€å·§ï¼Œå…¶å……åˆ†åˆ©ç”¨äº†å› æœæ€§ä¸â€œå¯¹æ•°ï¼ˆæ¡ä»¶ï¼‰ä¼¼ç„¶çš„æ¢¯åº¦åœ¨ä¼¼ç„¶ï¼ˆæ¡ä»¶ï¼‰æ¦‚ç‡åˆ†å¸ƒä¸Šçš„æœŸæœ›ä¸º 0â€ è¿™ä¸¤ä¸ªæ€§è´¨ã€‚\nå¯¹äºä»»ä½• \\(0 \\leq t \\leq |\\tau|\\)ï¼Œæˆ‘ä»¬æœ‰ \\[\n\\begin{aligned}\n& \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) }\\left[\\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\sum_{a_t \\in \\mathcal{A}} \\pi_\\theta(a_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\\\\n=& \\sum_{a_j \\in \\mathcal{A}} \\pi_\\theta(a_j \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_j) \\cdot 0 \\\\\n=& 0\n\\end{aligned}\n\\tag{40}\\]\næ›´è¿›ä¸€æ­¥ï¼Œå¦‚æœ \\(\\mathbf{\\Psi}_{t'}\\) æ˜¯ä¸€ä¸ªä¸ \\(\\mathbf{a}_t, \\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}, \\ldots\\) ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œé‚£ä¹ˆ \\[\n\\begin{aligned}\n& \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{(\\mathbf{a}_t, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\mathbf{\\Psi}_{t'} \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\mathbb{E}_{\n    (\\mathbf{s}_{t+1}, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t})} \\left[\\mathbf{\\Psi}_{t'} \\right] \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\mathbf{\\Psi}_{t'} \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[\n            \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right]\n        \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[ \\mathbf{\\Psi}_{t'} \\cdot 0 \\right] \\\\\n=& 0\n\\end{aligned}\n\\tag{41}\\]\nå…¶ä¸­ï¼Œä¸ºäº†åˆ©ç”¨ EquationÂ 40 çš„ç»“è®ºï¼Œæˆ‘ä»¬åˆ©ç”¨äº†å…¨æœŸæœ›å®šå¾‹ï¼Œå³\n\\[\n\\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p} \\left[\\mathbf{x}\\right] = \\mathbb{E}_{\\mathbf{y} \\sim p} \\left[\\mathbb{E}_{\\mathbf{x} \\sim p(\\cdot \\mid \\mathbf{y})} [\\mathbf{x}] \\right]\n\\tag{42}\\]\næ¥å¼•å…¥æˆ‘ä»¬æƒ³è¦çš„æœŸæœ›ã€‚\n\\[\n\\begin{aligned}\n& \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\mathbf{\\Psi}_i \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\Psi_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_\\theta(s_1, a_1, \\cdots, s_t) \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) p_\\theta(s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\Psi_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\\\\n=& \\sum_{(s_{1}, a_{1}, \\cdots, s_{t})} p_\\theta(s_1, a_1, \\cdots, s_t)  \\sum_{(a_{t}, s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|})} \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) \\Psi_{t'} \\nabla_\\theta p_\\theta(s_{t+1}, \\cdots, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right)  \\\\\n=& \\sum_{(s_{1}, a_{1}, \\cdots, s_{t})} p_\\theta(s_1, a_1, \\cdots, s_t) \\sum_{a_t \\in \\mathcal{A}}  \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\sum_{(s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|})}  p_\\theta(s_{t+1}, \\cdots, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\Psi_{t'} \\\\\n\\end{aligned}\n\\tag{43}\\]\nè€ƒè™‘ Monte Carlo ä¼°è®¡å¼ EquationÂ 39 ä¸­çš„ä¼°è®¡é‡ï¼Œå°†å¯¹æ•°æ¡ä»¶ä¼¼ç„¶æ¢¯åº¦çš„æ±‚å’Œå±•å¼€ï¼Œè€ƒè™‘å…¶ä¸­ä»»æ„ä¸€é¡¹ä¹˜ç§¯çš„æœŸæœ›ï¼š\n\\[\n\\mathbb{E}_{\\mathbf{\\tau_{i}} \\sim p_{\\theta}} \\left[\n\\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})\n\\right]\n\\tag{44}\\]\nç”±äºåºåˆ—å†³ç­–è¿‡ç¨‹æ»¡è¶³å› æœæ€§ï¼Œå³ \\(\\forall t' &lt; t\\)ï¼Œ\\(\\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\) ç‹¬ç«‹äº \\(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\)ï¼Œåˆ™å¯ä»¤ \\(\\mathbf{\\Psi}_{t'} = \\nabla_{\\theta} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t'})}\\)ï¼Œå…¶ç‹¬ç«‹äº \\(\\mathbf{s}_{i, t}, \\mathbf{a}_{i, t}, \\ldots\\)ï¼Œåˆ©ç”¨ EquationÂ 43 çš„æ€§è´¨ï¼Œåˆ™æœ‰ \\[\n\\forall t' &lt; t, \\mathbb{E}_{\\mathbf{\\tau_{i}} \\sim p_{\\theta}} \\left[\n\\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})\n\\right] = 0\n\\tag{45}\\]\nå°† EquationÂ 45 ä»£å…¥ KL æ¢¯åº¦è¡¨è¾¾å¼ (EquationÂ 38) ï¼Œå³å¯ç®€åŒ–å¾—åˆ°ï¼š\n\\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] =  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{T} \\left(\\sum_{t'=t}^{T} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{t}) \\right]\n\\tag{46}\\]\nå¯¹åº”çš„ Monte Carlo ä¼°è®¡å¼ä¸ºï¼š\n\\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\left(\\sum_{t'=t}^{|\\tau_i|} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} \\mid s_{i, 1}, \\cdots, a_{i, t-1}, s_{i, t})\n\\tag{47}\\]\nåŒæ ·ï¼Œè¦ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†åœ¨åå‘ä¼ æ’­æ—¶è®¡ç®—è¯¥æ¢¯åº¦ä¼°è®¡å¼ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ å¯¹åº”çš„ loss å‡½æ•°ï¼š\n\\[\n\\mathcal{L}^{KL}_{\\theta} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\text{nograd}\\left (\\sum_{t'=t}^{|\\tau_i|} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\log \\pi_{\\theta}(a_{i, t} \\mid s_{i, 1}, \\cdots, a_{i, t-1}, s_{i, t})\n\\tag{48}\\]\nè¿™é‡Œä¹Ÿå¯ä»¥çœ‹åˆ°ï¼ŒKL loss é¡¹æ­£ç¡®çš„å®ç°è¦æ±‚ï¼š\n\nåœ¨åºåˆ—å†… token é—´ï¼Œå¯¹å¯¹æ•°æ¡ä»¶ä¼¼ç„¶å…ˆæ±‚å’Œï¼Œå¾—åˆ° KL æ ·æœ¬å€¼ï¼Œ\nå†åœ¨åºåˆ—é—´æ±‚å‡å€¼ã€‚\n\nå› æ­¤ OpenRLHF (EquationÂ 13) ä¸ verl (EquationÂ 14) çš„æƒé‡éƒ½æ˜¯é”™è¯¯çš„ã€‚\n\n\n\nhttps://www.wikiwand.com/en/articles/Policy_gradient_methodâ†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-grad-as-kl-reward",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-grad-as-kl-reward",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "5.5 KL æ¢¯åº¦ä¼˜åŒ–å¯ä»¥å®ç°ä¸º KL æ ·æœ¬å€¼ reward",
    "text": "5.5 KL æ¢¯åº¦ä¼˜åŒ–å¯ä»¥å®ç°ä¸º KL æ ·æœ¬å€¼ reward\nåœ¨ EquationÂ 46 ä¸­ï¼Œä»¤ \\(k\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\right) = \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'-1}, \\mathbf{s}_{t'})}\\)ï¼Œåˆ™æœ‰ï¼š \\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] =  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{T} \\left(\\sum_{t'=t}^{T} k\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\right) \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t-1}, \\mathbf{s}_{t}) \\right]\n\\tag{49}\\]\nä¸éš¾æ³¨æ„åˆ° EquationÂ 49 ä¸­ \\(k\\) ä¸ EquationÂ 25 ä¸­ reward \\(r\\) åœ¨å½¢å¼ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œè¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆå…ˆå‰çš„å·¥ä½œè¦å°† KL æ ·æœ¬å€¼æ”¾è¿› rewardã€‚\nç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ PG çš„å…¶ä»–æŠ€å·§ï¼Œè¿›ä¸€æ­¥å‡å°è¯¥ä¼°è®¡çš„æ–¹å·®ï¼Œä¾‹å¦‚å‡å» baseline ç­‰ã€‚æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥è¿›ä¸€æ­¥å‚è€ƒ UCB CS28518 ç­‰ææ–™ã€‚\n\n\n\nhttps://rail.eecs.berkeley.edu/deeprlcourse/â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#æµè¡Œ-llm-rl-æ¡†æ¶ä¸­çš„-kl-ä¼˜åŒ–å®ç°å¿½ç•¥äº†-off-policy-é—®é¢˜",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#æµè¡Œ-llm-rl-æ¡†æ¶ä¸­çš„-kl-ä¼˜åŒ–å®ç°å¿½ç•¥äº†-off-policy-é—®é¢˜",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "6.1 æµè¡Œ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°å¿½ç•¥äº† off-policy é—®é¢˜",
    "text": "6.1 æµè¡Œ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°å¿½ç•¥äº† off-policy é—®é¢˜\né—æ†¾çš„æ˜¯ï¼Œå¯¹äº KL ä¼˜åŒ–ï¼ŒGRPO ç­‰å·¥ä½œï¼Œä»¥åŠç›®å‰æµè¡Œçš„ LLM RL æ¡†æ¶ä¸­ï¼ŒåŒ…æ‹¬ TRLï¼Œéƒ½å¿½ç•¥äº† off-policy é—®é¢˜ï¼šå¯¹äº \\(\\pi_\\theta \\neq \\pi_{\\theta_{\\text{old}}}\\)ï¼Œå°½ç®¡æ²¡æœ‰æ¥è‡ªæœ€æ–°ç­–ç•¥ \\(p_{\\theta}\\) çš„æ ·æœ¬ï¼Œå´ä»ç„¶åœ¨ä½¿ç”¨åŸºäº on-policy è®¾ç½®çš„ä¼˜åŒ–æ–¹å¼ã€‚\n\n6.1.1 TRL\nTRL åœ¨ ListingÂ 1 ä¸­è®¡ç®— KL æ ·æœ¬å€¼ä½¿ç”¨çš„ logprobs åŠå…¶å¯¹åº”çš„è½¨è¿¹æ ·æœ¬å‡æ¥è‡ªé‡‡æ ·ç­–ç•¥ \\(\\pi_{\\theta_{\\text{old}}}\\)ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 7ã€‚\n\n\n\nListingÂ 7: TRL ä½¿ç”¨é‡‡æ ·æ ·æœ¬å¹¶ä½¿ç”¨ \\(\\pi_{\\theta_{\\text{old}}}\\) è®¡ç®—å¯¹æ•°ä¼¼ç„¶19\n\n\nqueries = data[\"input_ids\"].to(device)\n# ...\n\nwith unwrap_model_for_generation(\n    self.model, #...\n) as unwrapped_model:\n    query_responses, logitss = batch_generation(\n        unwrapped_model.policy,\n        queries,\n        # ...\n    )\n\n\nfor i in range(0, queries.shape[0], args.local_rollout_forward_batch_size):\n    # ...\n    logits = logitss[i : i + args.local_rollout_forward_batch_size]\n    logprob = selective_log_softmax(logits, response)\n\n\n\næ³¨æ„ï¼ŒåŸºäº \\(\\mathbf{\\tau} \\sim \\pi_{\\theta_{\\text{old}}}\\) è®¡ç®—çš„ KL æ ·æœ¬å€¼å¯ä»¥ç”¨äºä¼°è®¡ \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_{\\theta_{\\text{old}}} \\mid \\pi_{\\text{ref}}\\right]\\)ï¼Œåœ¨ç¬¬ä¸€æ¬¡æ›´æ–°æ—¶ï¼Œç”±äº \\(\\pi_\\theta = \\pi_{\\theta_{\\text{old}}}\\)ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥ç”¨äºä¼°è®¡ \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)ã€‚ä½†é—®é¢˜åœ¨äºï¼Œä»ç¬¬äºŒæ¬¡æ›´æ–°å¼€å§‹ï¼Œ\\(\\pi_\\theta \\neq \\pi_{\\theta_{\\text{old}}}\\)ï¼Œè€Œæˆ‘ä»¬ä»ç„¶å¸Œæœ›ä¼°è®¡ \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)ã€‚\néšåè¿›è¡Œå¤šè½® PPO æ›´æ–°æ—¶ï¼ŒTRL å¹¶æ²¡æœ‰åŸºäºå½“å‰ç­–ç•¥ \\(\\pi_{\\theta}\\) é‡æ–°ä¼°è®¡ \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 8ã€‚\n\n\n\nListingÂ 8: TRL PPO å¤šè½®æ›´æ–°\n\n\n# Do multiple epochs of PPO training, with a fresh random shuffle in each epoch\nfor ppo_epoch_idx in range(args.num_ppo_epochs):\n    b_inds = np.random.permutation(args.local_batch_size)\n    minibatch_idx = 0\n    for mini_batch_start in range(0, args.local_batch_size, args.local_mini_batch_size):\n        mini_batch_end = mini_batch_start + args.local_mini_batch_size\n        mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]\n        gradient_accumulation_idx = 0\n        for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):\n            with accelerator.accumulate(model):\n                micro_batch_end = micro_batch_start + args.per_device_train_batch_size\n                micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]\n                mb_advantage = advantages[micro_batch_inds]\n                mb_responses = responses[micro_batch_inds]\n                mb_query_responses = query_responses[micro_batch_inds]\n                mb_logprobs = logprobs[micro_batch_inds]\n                mb_return = returns[micro_batch_inds]\n                mb_values = values[micro_batch_inds]\n\n\n                output, vpred_temp = forward(model, mb_query_responses, processing_class.pad_token_id)\n                logits = output.logits[:, context_length - 1 : -1]\n                logits /= args.temperature + 1e-7\n                new_logprobs = selective_log_softmax(logits, mb_responses)\n                new_logprobs = torch.masked_fill(\n                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB\n                )\n                vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)\n                vpred = torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], 0)\n                vpredclipped = torch.clamp(\n                    vpred,\n                    mb_values - args.cliprange_value,\n                    mb_values + args.cliprange_value,\n                )\n                vf_losses1 = torch.square(vpred - mb_return)\n                vf_losses2 = torch.square(vpredclipped - mb_return)\n                vf_loss_max = torch.max(vf_losses1, vf_losses2)\n                vf_loss = 0.5 * masked_mean(vf_loss_max, ~padding_mask_p1[micro_batch_inds])\n                vf_clipfrac = masked_mean(\n                    (vf_losses2 &gt; vf_losses1).float(), ~padding_mask_p1[micro_batch_inds]\n                )\n                logprobs_diff = new_logprobs - mb_logprobs\n                ratio = torch.exp(logprobs_diff)\n                pg_losses = -mb_advantage * ratio\n                pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)\n                pg_loss_max = torch.max(pg_losses, pg_losses2)\n                pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])\n                loss = pg_loss + args.vf_coef * vf_loss\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n\n\n\n\n\n\nhttps://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432â†©ï¸\n\n\n\n\n6.1.2 OpenRLHF\nç±»ä¼¼åœ°ï¼ŒOpenRLHF åœ¨ ListingÂ 2 ä¸­è®¡ç®— KL æ ·æœ¬å€¼ä½¿ç”¨çš„ log_probs åœ¨ make_experience æ—¶è¢«è®¡ç®—ï¼Œå’Œå¯¹åº”çš„æ ·æœ¬ sequences éƒ½æ¥è‡ªé‡‡æ ·ç­–ç•¥ \\(\\pi_{\\theta_{\\text{old}}}\\)ï¼Œè€Œéå½“å‰ç­–ç•¥ \\(\\pi_{\\theta}\\)ã€‚å¯¹åº”ä»£ç å¯è§ ListingÂ 9ã€‚\n\n\n\nListingÂ 9: OpenRLHF é‡‡æ ·æ ·æœ¬å¹¶ä½¿ç”¨ \\(\\pi_{\\theta_{\\text{old}}}\\) è®¡ç®—å¯¹æ•°ä¼¼ç„¶\n\n\n# https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595\ndef make_experience(self, samples: Samples) -&gt; Experience:\n    \"\"\"\n    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.\n    \"\"\"\n    # ...\n    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680\n    action_log_probs = self.actor(\n        sequences,\n        num_actions,\n        # ...\n    )\n    # ...\n    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709\n    kl = compute_approx_kl(\n        action_log_probs,\n        base_action_log_probs,\n        # ...\n    )\n\n\n\nä» ListingÂ 3 å¯è§ï¼ŒOpenRLHF åœ¨å¤šæ¬¡æ›´æ–°ä¸­ï¼Œå¯¹äº KL rewardï¼Œå¹¶æ²¡æœ‰é‡æ–°è®¡ç®—ï¼Œè¿˜æ˜¯æ²¿ç”¨äº†åŸºäº \\(\\pi_{\\theta_{\\text{old}}}\\) çš„ KL æ ·æœ¬å€¼ã€‚æ³¨æ„ï¼Œè™½ç„¶å…¶ä¸­ KL loss é¡¹çš„è®¡ç®—ä½¿ç”¨äº†åŸºäº \\(\\pi_{\\theta}\\) è®¡ç®—çš„å¯¹æ•°ä¼¼ç„¶ï¼Œä½†å¦‚ SectionÂ 4.1 æ‰€è¿°ï¼ŒKL loss é¡¹çš„å®ç°é€šå¸¸æ˜¯é”™è¯¯çš„ï¼Œä¸”åŒæ ·ä¾èµ–äº on-policy è®¾ç½®ã€‚\n\n\n6.1.3 verl\nä» ListingÂ 4 å¯è§ï¼Œverl åŒæ ·ä½¿ç”¨ \\(\\pi_{\\theta_{\\text{old}}}\\) è®¡ç®— KL æ ·æœ¬å€¼ã€‚\nä» ListingÂ 5 å¯è§ï¼Œverl åœ¨å¤šæ¬¡æ›´æ–°ä¸­ï¼Œå¯¹äº KL rewardï¼Œä¹Ÿä¼šæ²¿ç”¨åŸºäº \\(\\pi_{\\theta_{\\text{old}}}\\) çš„ KL æ ·æœ¬å€¼ã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#åˆ©ç”¨é‡è¦æ€§é‡‡æ ·å¤„ç†-off-policy-è®¾ç½®",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#åˆ©ç”¨é‡è¦æ€§é‡‡æ ·å¤„ç†-off-policy-è®¾ç½®",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "6.2 åˆ©ç”¨é‡è¦æ€§é‡‡æ ·å¤„ç† off-policy è®¾ç½®",
    "text": "6.2 åˆ©ç”¨é‡è¦æ€§é‡‡æ ·å¤„ç† off-policy è®¾ç½®\noff-policy è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬æ²¡æœ‰æ¥è‡ªæœ€æ–°ç­–ç•¥ \\(\\pi_{\\theta}\\) çš„æ ·æœ¬ï¼Œè€Œåªèƒ½ä½¿ç”¨æ¥è‡ªé‡‡æ ·ç­–ç•¥ \\(\\pi_{\\theta_{\\text{old}}}\\) çš„æ ·æœ¬ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¸Œæœ›ä¼°è®¡ \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}} \\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)ã€‚\nç†Ÿæ‚‰ off-policy PG çš„è¯»è€…å¯èƒ½å·²ç»æƒ³åˆ°äº†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Samplingï¼ŒISï¼‰æŠ€å·§æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå³\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[f(\\mathbf{\\tau})\\right] = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) f(\\tau)  = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta_{\\text{old}}}(\\tau) \\frac{p_{\\theta}(\\tau)}{p_{\\theta_{\\text{old}}}(\\tau)} f(\\tau) = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}} \\left[\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})} f(\\mathbf{\\tau})\\right]\n\\tag{50}\\]\næ­¤å¤„ï¼Œé‡è¦æ€§é‡‡æ ·ç³»æ•° \\(\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})}\\) å¯ä»¥ä»¿ç…§ EquationÂ 5 å±•å¼€ä¸ºï¼š\n\\[\n\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})} = \\prod_{t=1}^{|\\mathbf{\\tau}|} \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}{\\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}\n\\tag{51}\\] 20\nåˆ©ç”¨é‡è¦æ€§é‡‡æ · (EquationÂ 50, EquationÂ 51) ï¼ŒKL æ¢¯åº¦è¡¨è¾¾å¼ EquationÂ 46 å¯ä»¥è½¬åŒ–ä¸ºï¼š\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}} \\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t}) \\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}}\\left[ \\frac{p_{\\theta}(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T})}{p_{\\theta_{\\text{old}}}(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T})}  \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})  \\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}}\\left[ \\left(\\prod_{t=1}^{|\\mathbf{\\tau}|} \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}\\right) \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t}) \\right]\n\\end{aligned}\n\\tag{52}\\]\nå¯¹åº”çš„ Monte Carlo ä¼°è®¡å¼ä¸ºï¼š\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n\\approx& \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\prod_{t=1}^{|\\mathbf{\\tau}_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t=1}^{|\\mathbf{\\tau}_{i}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1}) }{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t}) \\\\\n=& \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\mathbf{\\tau}_{i}|} \\left(\\left(\\prod_{t=1}^{|\\mathbf{\\tau}_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t'=t}^{|\\mathbf{\\tau}_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1}) }{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t})\n\\end{aligned}\n\\tag{53}\\]\nå¯¹åº”çš„ loss å‡½æ•°ä¸ºï¼š\n\\[\n\\mathcal{L}^{KL}_{\\theta} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_{i}|} \\text{nograd}\\left(\\left(\\prod_{t=1}^{|\\tau_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right)\\sum_{t'=t}^{|\\tau_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t})\n\\tag{54}\\]\nç±»ä¼¼ EquationÂ 49ï¼Œæˆ‘ä»¬å¯ä»¥ä»¤\n\\[\nk(\\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t}) = \\left(\\prod_{t=1}^{|\\tau_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t'=t}^{|\\tau_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}\n\\tag{55}\\]\næ³¨æ„ï¼ŒEquationÂ 55 ä¸­çš„ \\(k\\) éœ€è¦å¯¹äºæ¯ä¸ªæ–°çš„ \\(\\pi_{\\theta}\\) é‡æ–°è®¡ç®—ã€‚\n\n\n\nå®é™…è®¡ç®—ä¸­ï¼ŒEquationÂ 51 ç”±äºæ¶‰åŠåˆ° \\(|\\mathbf{\\tau}|\\) æ¬¡è¿ä¹˜ï¼Œæ–¹å·®å¤§ä¸”æ•°å€¼ç¨³å®šæ€§å·®ï¼Œéœ€è¦åˆ©ç”¨å› æœæ€§ã€è¿‘ä¼¼ç­‰æŠ€æœ¯æ¥åŒ–ç®€ã€‚æœ¬æ–‡ç›®å‰çœç•¥è¯¥éƒ¨åˆ†ï¼Œåç»­å°†ä¼šæ›´æ–°ç›¸å…³å†…å®¹ã€‚â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ä¿®æ­£-grpo-å…¬å¼ä¸­çš„-kl-é¡¹",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ä¿®æ­£-grpo-å…¬å¼ä¸­çš„-kl-é¡¹",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "7.1 ä¿®æ­£ GRPO å…¬å¼ä¸­çš„ KL é¡¹",
    "text": "7.1 ä¿®æ­£ GRPO å…¬å¼ä¸­çš„ KL é¡¹\nGRPO å…¬å¼ (EquationÂ 1, EquationÂ 2) å¯¹äº KL ä¼˜åŒ–ä¸»è¦å­˜åœ¨ä¸¤ä¸ªé”™è¯¯ï¼š\n\nå¿½ç•¥äº† KL ä¼˜åŒ–çš„ off-policy é—®é¢˜\nå…ˆå°† \\(k_{3}\\) ä¼°è®¡æ ·æœ¬é‡åº”ç”¨äºåŠ¨ä½œæ¡ä»¶ä¼¼ç„¶å†æ±‚å’Œï¼Œå¯¼è‡´å¾—åˆ°å¼‚å¸¸çš„æ¢¯åº¦\n\nå¯¹äºè¿™ä¸¤ä¸ªé—®é¢˜ï¼Œåœ¨ EquationÂ 29 çš„åŸºç¡€ä¸Šï¼Œä»¿ç…§ EquationÂ 55ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ­£ï¼š\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\left\\{ \\sum_{t=1}^{\\left|o_i\\right|} \\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old}}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right] \\right\\} -\\beta \\left(\\prod_{t=1}^{|o_{i}|}\\frac{\\pi_{\\theta}(o_{i, t} | q, o_{i,\\lt t})}{ \\pi_{\\theta_{\\text{old}}}(o_{i, t} | q, o_{i,\\lt t})}\\right) k\\left( \\frac{\\prod_{t=1}^{|o_i|} \\pi_{\\text{ref}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\prod_{t=1}^{|o_i|} \\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\right)\n\\end{aligned}\n\\tag{56}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ä¿®æ­£æµè¡Œ-llm-rl-æ¡†æ¶ä¸­çš„-kl-ä¼˜åŒ–å®ç°",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ä¿®æ­£æµè¡Œ-llm-rl-æ¡†æ¶ä¸­çš„-kl-ä¼˜åŒ–å®ç°",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "7.2 ä¿®æ­£æµè¡Œ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°",
    "text": "7.2 ä¿®æ­£æµè¡Œ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°\nç›®å‰æµè¡Œçš„ LLM RL æ¡†æ¶ä¸­çš„ KL ä¼˜åŒ–å®ç°ï¼Œé™¤äº† GRPO å…¬å¼ä¸­ä½“ç°çš„ä¸¤ä¸ªé—®é¢˜ä¹‹å¤–ï¼Œè¿˜å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š\n\nå®ç°å•ç‹¬çš„ KL loss é¡¹æ—¶ï¼Œé»˜è®¤ä¸å»é™¤ä»»ä½•æ¢¯åº¦ï¼Œï¼ˆè¿™å¯èƒ½æ˜¯è¯¯ä»¥ä¸ºç›´æ¥å‰å‘ä¼ æ’­ä¼°è®¡ KL æ•£åº¦ï¼Œå†åå‘ä¼ æ’­å°±èƒ½å¾—åˆ°æ­£ç¡®çš„æ¢¯åº¦å¯¼è‡´çš„ï¼‰\né”™è¯¯åœ°å®ç°äº†å¹³å‡æ“ä½œ\n\nå¯¹äºè¿™äº›é—®é¢˜ï¼Œå¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ€è·¯ä¿®æ­£ï¼š\n\nä¸º KL é¡¹æ·»åŠ é‡è¦æ€§é‡‡æ ·ï¼Œè¿™éœ€è¦ä»ç¬¬äºŒè½®æ›´æ–°å¼€å§‹ï¼Œæ¯æ¬¡åŸºäºæ–°çš„ \\(\\pi_\\theta\\) é‡æ–°è®¡ç®— KL loss / reward é¡¹ï¼ŒåŒ…æ‹¬é‡è¦æ€§é‡‡æ ·ç³»æ•°\nåº”ç”¨ KL ä¼°è®¡æ ·æœ¬é‡æ—¶ï¼Œå…ˆå¯¹äºåºåˆ—å†… token é—´çš„å¯¹æ•°æ¡ä»¶ä¼¼ç„¶æ±‚å’Œï¼Œå¾—åˆ°è½¨è¿¹è”åˆæ¦‚ç‡ï¼Œå†ä»£å…¥å…¬å¼\nå¦‚æœå¸Œæœ›åƒå¯¹äº reward ä¼˜åŒ–ä¸€æ ·ä½¿ç”¨åŸºçº¿ã€æŠ˜æ‰£ã€GAEç­‰æŠ€æœ¯ï¼Œå¯ä»¥æŒ‰ EquationÂ 55 å®ç°ä¸º KL reward é¡¹ï¼ˆå°½ç®¡è¿™äº›æŠ€æœ¯èƒŒåçš„è€ƒé‡å¹¶ä¸ä¸€å®šé€‚åˆ KL æ•£åº¦ï¼Œä¾‹å¦‚ reward æ˜¯å…è®¸è‡ªå®šä¹‰çš„ï¼Œä½† KL æ•£åº¦æœ‰æ˜ç¡®çš„å®šä¹‰ï¼‰\nå¦‚æœä¸å¸Œæœ›åº”ç”¨ reward ä¼˜åŒ–çš„å…¶ä»–æŠ€æœ¯ï¼Œå¯ä»¥æŒ‰ EquationÂ 54 å®ç°ä¸º KL loss é¡¹"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#å¯¹äº-kl-æ¢¯åº¦æ›´å¥½çš„ä¼°è®¡æ ·æœ¬é‡",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#å¯¹äº-kl-æ¢¯åº¦æ›´å¥½çš„ä¼°è®¡æ ·æœ¬é‡",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "8.1 å¯¹äº KL æ¢¯åº¦æ›´å¥½çš„ä¼°è®¡æ ·æœ¬é‡",
    "text": "8.1 å¯¹äº KL æ¢¯åº¦æ›´å¥½çš„ä¼°è®¡æ ·æœ¬é‡\nå¦‚ SectionÂ 5.5 æ‰€è¿°ï¼ŒPG ä½¿ç”¨äº†è®¸å¤šå…¶ä»–æŠ€æœ¯æ¥æ”¹è¿›å…¶æ¢¯åº¦ä¼°è®¡ï¼Œèƒ½å¦ä½¿ç”¨ç±»ä¼¼æŠ€æœ¯æ”¹è¿› KL æ¢¯åº¦ä¼°è®¡ï¼Ÿ\næ­¤å¤–ï¼ŒJohn Schulman çš„åšå®¢æ˜¯é’ˆå¯¹ä¼°è®¡ KL æ•£åº¦åˆ†æäº†ä¸åŒçš„ä¼°è®¡æ ·æœ¬é‡ã€‚ä½†è¿™äº›åˆ†æå¯¹äºä¼°è®¡ KL æ•£åº¦çš„æ¢¯åº¦æ˜¯å¦è¿˜æˆç«‹ï¼Ÿ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#kl-regularized-rl-çš„ç†è®ºä¼˜åŠ¿",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#kl-regularized-rl-çš„ç†è®ºä¼˜åŠ¿",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "8.2 KL-Regularized RL çš„ç†è®ºä¼˜åŠ¿",
    "text": "8.2 KL-Regularized RL çš„ç†è®ºä¼˜åŠ¿\næœ€è¿‘åŸºäºå¯éªŒè¯ reward çš„ RL éå¸¸æµè¡Œï¼Œå…¶å¾ˆå¤§ç¨‹åº¦ä¸Šé¿å…äº† reward hackingï¼Œç›´è§‰ä¸Šï¼Œæˆ‘ä»¬ä¼¼ä¹ä¸å†éœ€è¦ç›¸å¯¹äºå‚è€ƒç­–ç•¥çš„ KL æ­£åˆ™åŒ–ã€‚\nç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›å·¥ä½œæŒ‡å‡ºï¼ŒKL-Regularized RL åœ¨ç†è®ºä¸Šè¿˜æœ‰è®¸å¤šå…¶ä»–ä¼˜åŠ¿ã€‚ä¾‹å¦‚ Zhao et al. (2025) è¯æ˜äº† KL-regularized RL çš„ regret åªæœ‰ \\(\\mathcal{O}(\\log T)\\)ï¼Œè€Œå¸¸è§çš„åŸºäº contextual bandit æˆ– MDP å»ºæ¨¡çš„ RL æ–¹æ³• regret é€šå¸¸ä¸ä½äº \\(\\mathcal{O}(\\sqrt{T})\\)ã€‚ç²—æµ…åœ°è¯´ï¼Œè¿™æ˜¯å› ä¸º KL æ­£åˆ™åŒ–ç›®æ ‡é¡¹çš„å­˜åœ¨ï¼Œä½¿å¾— value åˆ†è§£æœ‰äº†ç‰¹åˆ«çš„æ€§è´¨ï¼Œä¾‹å¦‚å‡¸æ€§æ›´å¼ºã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ç›¸å…³å·¥ä½œ",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ç›¸å…³å·¥ä½œ",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "9.1 ç›¸å…³å·¥ä½œ",
    "text": "9.1 ç›¸å…³å·¥ä½œ\n\nä¸æœ¬æ–‡åŒæœŸä¹Ÿæœ‰è®¸å¤šç²¾å½©çš„è®¨è®ºï¼Œç”±äºç¬”è€…è¿˜æ²¡èƒ½é€šè¯»å…¨æ–‡ï¼Œæ­¤å¤„ä»…æä¾›é“¾æ¥ï¼Œä¸ä½œæ¦‚æ‹¬ï¼Œæ¬¢è¿æ„Ÿå…´è¶£çš„è¯»è€…è‡ªè¡Œé˜…è¯»ï¼š\n\nGRPO ä¸­çš„ KL Loss å®ç°ç»†èŠ‚é—®é¢˜ - Hongyu Zang @ çŸ¥ä¹\nk2 losså°±æ˜¯æ¯”k3 losså¥½ï¼ä»¥åŠGRPO_off-policy - Yiming Liu @ çŸ¥ä¹\n\n\n\n\nhttps://tongyx361.github.ioâ†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#å†™ä½œå¥‘æœºtrpoppo-ä¸-grpo-ä¸­çš„-kl-ä¸ºä»€ä¹ˆä¸ä¸€æ ·",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#å†™ä½œå¥‘æœºtrpoppo-ä¸-grpo-ä¸­çš„-kl-ä¸ºä»€ä¹ˆä¸ä¸€æ ·",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "9.2 å†™ä½œå¥‘æœºï¼šâ€œTRPO/PPO ä¸ GRPO ä¸­çš„ KL ä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿâ€",
    "text": "9.2 å†™ä½œå¥‘æœºï¼šâ€œTRPO/PPO ä¸ GRPO ä¸­çš„ KL ä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿâ€\n\nç¬”è€…å¯¹ RL ä¸­ KL ä¼˜åŒ–ç›¸å…³é—®é¢˜çš„æ€è€ƒä¸»è¦å¼€å§‹äº X ä¸Š Fanyi Pu æå‡ºäº†è¿™æ ·ä¸€ä¸ªé—®é¢˜22ï¼š\n\nA small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?\nTRPO (Schulman et al. 2015)\n\n\\[\n\\begin{aligned}\n& \\underset{\\theta}{\\text{maximize}}~L_{\\theta_{\\text {old }}}(\\theta) \\\\\n& \\text { subject to } \\bar{D}_{\\mathrm{KL}}^{\\rho_{\\theta_{\\text {old }}}}\\left(\\theta_{\\text {old }}, \\theta\\right) \\leq \\delta\n\\end{aligned}\n\\tag{57}\\]\n\nPPO (Schulman et al. 2017)\n\n\\[\nL^{K L P E N}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(\\mathbf{y}_t \\mid \\mathbf{x}_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(\\mathbf{y}_t \\mid \\mathbf{x}_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid \\mathbf{x}_t\\right), \\pi_\\theta\\left(\\cdot \\mid \\mathbf{x}_t\\right)\\right]\\right]\n\\tag{58}\\]\n\nGRPO (Shao et al. 2024)\n\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\right\\}\n\\end{aligned}\n\\tag{59}\\]\nè¿™ä¸ªé—®é¢˜æœ¬èº«çš„ç­”æ¡ˆæ˜¯éå¸¸ç®€å•çš„ã€‚\né¦–å…ˆï¼Œè¿™ä¸ªé—®é¢˜æ··æ·†äº†ä¸¤ç§ä¸åŒçš„ KL æƒ©ç½šé¡¹ï¼š\n\n\\(\\text{KL}[\\pi_{\\theta_{\\text{old}}},\\pi_{\\theta}]\\)ï¼Œå…¶ä½œç”¨æ˜¯çº¦æŸæœ€æ–°ç­–ç•¥ \\(\\pi_{\\theta}\\)ä¸è¦ç¦»é‡‡æ ·ç­–ç•¥\\(\\pi_{\\theta_{\\text{old}}}\\) å¤ªè¿œï¼Œé¿å…è¿‡å¤§çš„æ›´æ–°å¯¼è‡´ç­–ç•¥å´©æºƒï¼Œä»è€Œæ„æˆä¿¡ä»»åŸŸï¼ˆTrust Region, TRï¼‰ï¼Œä¹Ÿå°±æ˜¯ TRPO ä¸­çš„ TRã€‚è€Œ PPO ä½œä¸º TRPO çš„è¿‘ä¼¼å®ç°ï¼Œç»§æ‰¿äº†è¿™ä¸€ç‚¹ã€‚\n\\(\\text{KL}[\\pi_{\\theta},\\pi_{\\theta_{\\text{ref}}}]\\)ï¼Œå…¶ä½œç”¨æ˜¯çº¦æŸæœ€æ–°ç­–ç•¥ \\(\\pi_{\\theta}\\)ä¸è¦ç¦»å‚è€ƒç­–ç•¥\\(\\pi_{\\theta_{\\text{ref}}}\\) å¤ªè¿œï¼Œä»è€Œæ›´å……åˆ†åœ°åˆ©ç”¨å‚è€ƒç­–ç•¥ä¸­çš„å…ˆéªŒã€‚\n\nå¦å¤–ï¼Œè¿™ä¸ªé—®é¢˜å¿½ç•¥äº† TRPO/PPO å…¬å¼ä¸­çš„ KL æŸå¤±é¡¹ä¸ GRPO å…¬å¼ä¸­çš„ clip å‡½æ•°å®é™…ä¸Šæ˜¯å‡ºäºåŒä¸€ç›®çš„ï¼Œå³çº¦æŸ \\(\\text{KL}[\\pi_{\\theta_{\\text{old}}},\\pi_{\\theta}]\\)ã€‚å¦‚ PPO è®ºæ–‡ç¬¬ 3-4 èŠ‚æ‰€è¯´ï¼Œä¸¤è€…å¯ä»¥ç›¸äº’æ›¿ä»£æˆ–ç»“åˆä½¿ç”¨ï¼š\n\nLet \\(r_t(\\theta)\\) denote the probability ratio \\(r_{t}(\\theta)=\\frac{\\pi_{\\theta}\\left(a_t \\mid s_t\\right)}{\\left(\\pi_{\\theta_{\\text {old }}}\\left|a_t\\right| s_t\\right)}\\), so \\(r\\left(\\theta_{\\text{old}}\\right)=1\\). TRPO maximizes a â€œsurrogateâ€ objective\n\n\\[\nL^{\\text{CPI}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t\\right]=\\hat{\\mathbb{E}}_t\\left[r_t(\\theta) \\hat{A}_t\\right] .\n\\]\n\nâ€¦\nThe main objective we propose is the following:\n\n\\[\nL^{\\text{CLIP}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\n\\]\n\nwhere epsilon is a hyperparameter, say, \\(\\epsilon=0.2\\). The motivation for this objective is as follows. The first term inside the \\(\\min\\) is \\(L^{\\text{CPI}}\\). The second term, \\(\\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\), modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving \\(r_t\\) outside of the interval \\([1-\\epsilon, 1+\\epsilon]\\).\nâ€¦\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence \\(d_{\\text{targ}}\\) each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, weâ€™ve included it here because itâ€™s an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy update:\n\nUsing several epochs of minibatch SGD, optimize the KL-penalized objective\n\n\n\\[\nL^{\\text{KLPEN}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right]\n\\]\n\n\n\né¡ºå¸¦ï¼Œè¿˜å¯ä»¥ä»ä»¥ä¸‹è§’åº¦ç†è§£ä¸¤è€…çš„å…±é€šä¹‹å¤„ï¼šclip å‡½æ•°çº¦æŸçš„ \\(r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}\\)å°±æ˜¯\\(K L\\left[\\pi_{\\theta_{d d}}, \\pi_\\theta\\right]=\\mathbb{E}_{a_t \\sim \\pi_{\\theta_{d t}}\\left(\\cdot \\mid s_t\\right)}\\left[\\log \\frac{\\pi_{\\theta_{d t}}\\left(a_t \\mid s_t\\right)}{\\pi_\\theta\\left(a_t \\mid s_t\\right)}\\right]\\) ä¸­å¯¹å•ä¸ªæ ·æœ¬ \\((s_t, a_t)\\) çš„å€¼ä¸­ \\(\\log\\) çš„çœŸæ•°ã€‚\n\n\n\nhttps://x.com/pufanyi/status/1888845956684370202â†©ï¸"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#è‡´è°¢",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#è‡´è°¢",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "9.3 è‡´è°¢",
    "text": "9.3 è‡´è°¢\n\næ„Ÿè°¢ç‹æµ©ç„¶ã€YuMS å¯¹æœ¬æ–‡æä¾›çš„é‡è¦åé¦ˆã€‚\næ„Ÿè°¢ç”Ÿå¹¿æ˜ã€Wei Xiongã€åˆ˜ä»å½ªã€åˆ˜å¨ã€Weixun Wangã€Yiming Liuã€Haibin Lin ç­‰å…³äºç›¸å…³é—®é¢˜çš„æœ‰ç›Šè®¨è®ºä»¥åŠå¯¹äºæœ¬æ–‡çš„æœ‰ç›Šåé¦ˆã€‚\næ„Ÿè°¢ Cursor å’Œ Mathpix åœ¨ä¹¦å†™ LaTeX æ—¶æä¾›çš„å·¨å¤§å¸®åŠ©ã€‚"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#å¼•ç”¨",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#å¼•ç”¨",
    "title": "é‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–",
    "section": "9.4 å¼•ç”¨",
    "text": "9.4 å¼•ç”¨\n\nBibTeX:\n@online{tong2025kl,\n  author = {ç«¥é›¨è½©},\n  title = {é‡æ–°æ€è€ƒ {RL} ä¸­çš„ {KL} æ¢¯åº¦ä¼˜åŒ–},\n  year = {2025},\n  url = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},\n  urldate = {2025-03-09},\n  language = {Chinese},\n}\næ–‡æœ¬ï¼š\nç«¥é›¨è½©. 2025. â€œé‡æ–°æ€è€ƒ RL ä¸­çš„ KL æ¢¯åº¦ä¼˜åŒ–.â€ https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh."
  }
]