@misc{kimi2025k15,
  title         = {Kimi k1.5: Scaling Reinforcement Learning with LLMs},
  author        = {{Kimi Team} and Angang Du and Bofei Gao and Bowei Xing and Changjiu Jiang and Cheng Chen and Cheng Li and Chenjun Xiao and Chenzhuang Du and Chonghua Liao and Chuning Tang and Congcong Wang and Dehao Zhang and Enming Yuan and Enzhe Lu and Fengxiang Tang and Flood Sung and Guangda Wei and Guokun Lai and Haiqing Guo and Han Zhu and Hao Ding and Hao Hu and Hao Yang and Hao Zhang and Haotian Yao and Haotian Zhao and Haoyu Lu and Haoze Li and Haozhen Yu and Hongcheng Gao and Huabin Zheng and Huan Yuan and Jia Chen and Jianhang Guo and Jianlin Su and Jianzhou Wang and Jie Zhao and Jin Zhang and Jingyuan Liu and Junjie Yan and Junyan Wu and Lidong Shi and Ling Ye and Longhui Yu and Mengnan Dong and Neo Zhang and Ningchen Ma and Qiwei Pan and Qucheng Gong and Shaowei Liu and Shengling Ma and Shupeng Wei and Sihan Cao and Siying Huang and Tao Jiang and Weihao Gao and Weimin Xiong and Weiran He and Weixiao Huang and Weixin Xu and Wenhao Wu and Wenyang He and Xianghui Wei and Xianqing Jia and Xingzhe Wu and Xinran Xu and Xinxing Zu and Xinyu Zhou and Xuehai Pan and Y. Charles and Yang Li and Yangyang Hu and Yangyang Liu and Yanru Chen and Yejie Wang and Yibo Liu and Yidao Qin and Yifeng Liu and Ying Yang and Yiping Bao and Yulun Du and Yuxin Wu and Yuzhi Wang and Zaida Zhou and Zhaoji Wang and Zhaowei Li and Zhen Zhu and Zheng Zhang and Zhexu Wang and Zhilin Yang and Zhiqi Huang and Zihao Huang and Ziyao Xu and Zonghan Yang and Zongyu Lin},
  year          = {2025},
  eprint        = {2501.12599},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2501.12599}
}

@inproceedings{fu2025areal,
  title     = {{AREAL}: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning},
  author    = {Wei Fu and Jiaxuan Gao and Xujie Shen and Chen Zhu and Zhiyu Mei and Chuyi He and Shusheng Xu and Guo Wei and Jun Mei and WANG JIASHU and Tongkai Yang and Binhang Yuan and Yi Wu},
  booktitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  year      = {2025},
  url       = {https://openreview.net/forum?id=X9diEuva9R}
}

@misc{piché2025pipelinerl,
  title         = {PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generation},
  author        = {Alexandre Piché and Ehsan Kamalloo and Rafael Pardinas and Xiaoyin Chen and Dzmitry Bahdanau},
  year          = {2025},
  eprint        = {2509.19128},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2509.19128}
}

@inproceedings{espeholt2020seedrl,
  title     = {SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference},
  author    = {Lasse Espeholt and Raphaël Marinier and Piotr Stanczyk and Ke Wang and Marcin Michalski‎},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgvXlrKwH}
}

@misc{yao2023dschat,
  title         = {DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales},
  author        = {Zhewei Yao and Reza Yazdani Aminabadi and Olatunji Ruwase and Samyam Rajbhandari and Xiaoxia Wu and Ammar Ahmad Awan and Jeff Rasley and Minjia Zhang and Conglong Li and Connor Holmes and Zhongzhu Zhou and Michael Wyatt and Molly Smith and Lev Kurilenko and Heyang Qin and Masahiro Tanaka and Shuai Che and Shuaiwen Leon Song and Yuxiong He},
  year          = {2023},
  eprint        = {2308.01320},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2308.01320}
}

@inproceedings{shen2024nemoaligner,
  title     = {NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment},
  author    = {Gerald Shen and Zhilin Wang and Olivier Delalleau and Jiaqi Zeng and Yi Dong and Daniel Egert and Shengyang Sun and Jimmy J. Zhang and Sahil Jain and Ali Taghibakhshi and Markel Sanz Ausin and Ashwath Aithal and Oleksii Kuchaiev},
  booktitle = {First Conference on Language Modeling},
  year      = {2024},
  url       = {https://openreview.net/forum?id=yK2eGE8QVW}
}

@inproceedings{sheng2025hybridflow,
  author    = {Sheng, Guangming and Zhang, Chi and Ye, Zilingfeng and Wu, Xibin and Zhang, Wang and Zhang, Ru and Peng, Yanghua and Lin, Haibin and Wu, Chuan},
  title     = {HybridFlow: A Flexible and Efficient RLHF Framework},
  year      = {2025},
  isbn      = {9798400711961},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3689031.3696075},
  doi       = {10.1145/3689031.3696075},
  abstract  = {Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF data flow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53x~20.57\texttimes{} throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code is available at https://github.com/volcengine/verl},
  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
  pages     = {1279–1297},
  numpages  = {19},
  keywords  = {Distributed systems, Reinforcement Learning from Human Feedback},
  location  = {Rotterdam, Netherlands},
  series    = {EuroSys '25}
}

@inproceedings{hu2025openrlhf,
  title     = {{O}pen{RLHF}: A Ray-based Easy-to-use, Scalable and High-performance {RLHF} Framework},
  author    = {Hu, Jian  and
               Wu, Xibin  and
               Shen, Wei  and
               Liu, Jason Klein  and
               Wang, Weixun  and
               Jiang, Songlin  and
               Wang, Haoran  and
               Chen, Hao  and
               Chen, Bin  and
               Fang, Wenkai  and
               Xianyu  and
               Cao, Yu  and
               Xu, Haotian  and
               Liu, Yiming},
  editor    = {Habernal, Ivan  and
               Schulam, Peter  and
               Tiedemann, J{\"o}rg},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = nov,
  year      = {2025},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.emnlp-demos.48/},
  doi       = {10.18653/v1/2025.emnlp-demos.48},
  pages     = {656--666},
  isbn      = {979-8-89176-334-0},
  abstract  = {Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) significantly improve the alignment of human-AI values and further raise the upper bound of AI capabilities, particularly in reasoning-intensive, long-context Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or RLVR) frameworks commonly face challenges such as inference bottlenecks and complexity barriers, restricting their accessibility for newcomers. To bridge this gap, we introduce \textbf{OpenRLHF}, a user-friendly, scalable, and easy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, and HuggingFace Transformers, featuring a simplified design, clear code structure, and comprehensive documentation to facilitate entry for researchers and practitioners. Experimental results show that OpenRLHF achieves superior training efficiency with speedups ranging from 1.22{\texttimes} to 1.68{\texttimes} across different model sizes compared to state-of-the-art frameworks, while requiring significantly fewer lines of code for implementation. OpenRLHF is publicly available at \url{https://github.com/OpenRLHF/OpenRLHF}, and has already been adopted by leading institutions to accelerate RLHF research and learning.}
}

@misc{sheng2025laminar,
  title         = {Laminar: A Scalable Asynchronous RL Post-Training Framework},
  author        = {Guangming Sheng and Yuxuan Tong and Borui Wan and Wang Zhang and Chaobo Jia and Xibin Wu and Yuqi Wu and Xiang Li and Chi Zhang and Yanghua Peng and Haibin Lin and Xin Liu and Chuan Wu},
  year          = {2025},
  eprint        = {2510.12633},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2510.12633}
}

@article{metelli2020pois,
  author  = {Alberto Maria Metelli and Matteo Papini and Nico Montali and Marcello Restelli},
  title   = {Importance Sampling Techniques for Policy Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {141},
  pages   = {1--75},
  url     = {http://jmlr.org/papers/v21/20-124.html}
}

@book{owen2013mcbook,
  author    = {Art B. Owen},
  year      = 2013,
  title     = {Monte Carlo theory, methods and examples},
  publisher = {\url{https://artowen.su.domains/mc/}}
}