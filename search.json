[
  {
    "objectID": "posts/verl-intro/index.html#learning-to-reason-with-large-scale-rl",
    "href": "posts/verl-intro/index.html#learning-to-reason-with-large-scale-rl",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "1.1 Learning to Reason with Large-Scale RL",
    "text": "1.1 Learning to Reason with Large-Scale RL\n\n\n\nTable¬†1: Learning to reason with large-scale RL significantly boosts the performance of LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nLarge-Scale RL?\nAIME 2024\nMATH 500\nGPQA Diamond\nCode Forces\n\n\n\n\nGPT-4o (OpenAI 2024)\n‚ùå\n44.6\n60.3\n50.6\n&gt;11.0%\n\n\no1 (OpenAI 2024)\n‚úÖ\n74.4\n94.8\n77.3\n&gt;89.0%\n\n\nR1 (DeepSeek-AI 2025)\n‚úÖ\n79.8\n97.3\n71.5\n&gt;96.3%"
  },
  {
    "objectID": "posts/verl-intro/index.html#learning-as-agent-with-large-scale-rl",
    "href": "posts/verl-intro/index.html#learning-as-agent-with-large-scale-rl",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "1.2 Learning as Agent with Large-Scale RL",
    "text": "1.2 Learning as Agent with Large-Scale RL\nOpenAI (2025):\n\nDeep research independently discovers, reasons about, and consolidates insights from across the web.\nTo accomplish this, it was trained on real-world tasks requiring browser and Python tool use,\nusing the same reinforcement learning methods behind OpenAI o1, our first reasoning model.\n\nCheck OpenAI Deep Research‚Äôs demo video for more details."
  },
  {
    "objectID": "posts/verl-intro/index.html#rl-is-complex-dataflow",
    "href": "posts/verl-intro/index.html#rl-is-complex-dataflow",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.1 RL is Complex Dataflow",
    "text": "2.1 RL is Complex Dataflow\n\n\n\n\n\n\nFigure¬†1: Modelling three example RL algorithms (Schulman et al. 2017; Dai et al. 2023; Li et al. 2024) as dataflow graphs. (Source: Sheng et al. 2025)\n\n\n\nReinforcement Learning (RL) can be modelled as complex dataflow graph (Schaarschmidt et al. 2019; Liang et al. 2021; Sheng et al. 2025), consisting of:\n\nmultiple models: actor, critic, reference, reward model, etc.\nmultiple stages: generating, preparing experiences, training\nmultiple workloads: generation, inference, training"
  },
  {
    "objectID": "posts/verl-intro/index.html#llm-workloads-are-distributed",
    "href": "posts/verl-intro/index.html#llm-workloads-are-distributed",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.2 LLM Workloads Are Distributed",
    "text": "2.2 LLM Workloads Are Distributed\n\n\n\n\n\n\nFigure¬†2: LLM workloads are often distributed, involving many GPUs and complex parallelism strategies.\n\n\n\nLLM workloads often involves:\n\nmany GPUs\ncomplex parallelism strategies"
  },
  {
    "objectID": "posts/verl-intro/index.html#rl-with-llms-is-large-scale-distributed-dataflow",
    "href": "posts/verl-intro/index.html#rl-with-llms-is-large-scale-distributed-dataflow",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.3 RL with LLMs is Large-Scale Distributed Dataflow",
    "text": "2.3 RL with LLMs is Large-Scale Distributed Dataflow\n\n\n\n\n\n\nFigure¬†3: In RL with LLMs, each operator in the RL dataflow is a large-scale distributed computing workload itself."
  },
  {
    "objectID": "posts/verl-intro/index.html#constraints-data-dependencies-resource-limitations",
    "href": "posts/verl-intro/index.html#constraints-data-dependencies-resource-limitations",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "2.4 Constraints: Data Dependencies & Resource Limitations",
    "text": "2.4 Constraints: Data Dependencies & Resource Limitations\n\n\n\n\n\n\nFigure¬†4: Implementing RL algorithm with LLMs usually requires complex trade-offs between various constraints. (Sheng et al. 2025)"
  },
  {
    "objectID": "posts/verl-intro/index.html#flexibility-single-controller",
    "href": "posts/verl-intro/index.html#flexibility-single-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.1 Flexibility: ‚ÄúSingle-Controller‚Äù",
    "text": "3.1 Flexibility: ‚ÄúSingle-Controller‚Äù\n\n\n\n\n\n\n\n\nFigure¬†5: Dataflow of PPO with KL regularization, with data shown explicitly. (Source: Sheng et al. 2025)\n\n\n\n\n\n\n\nListing¬†1: PPO core code in a few lines in verl.\n\n\nfor prompts in dataloader:\n    # Stage 1: Generation\n    batch = actor.generate_sequences(prompts)\n    # Stage 2: Experience Preparation\n    batch = reward.compute_reward(batch)\n    batch = reference.compute_log_prob(batch)\n    batch = critic.compute_values(batch)\n    batch = compute_advantage(batch, \"gae\")\n    # Stage 3: Training\n    critic.update_critic(batch)\n    actor.update_actor(batch)\n\n\n\n\n\nProgramming interface based on the ‚Äúsingle-controller‚Äù paradigm\nRL algorithm core logic in a few lines of code!\nDiverse RL algorithms supported: PPO,¬†GRPO,¬†RLOO, ReMax,¬†PRIME,¬†DAPO, etc."
  },
  {
    "objectID": "posts/verl-intro/index.html#efficiency-multi-controller",
    "href": "posts/verl-intro/index.html#efficiency-multi-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.2 Efficiency: ‚ÄúMulti-Controller‚Äù",
    "text": "3.2 Efficiency: ‚ÄúMulti-Controller‚Äù\nverl is efficient for intra-operator with the ‚Äúmulti-controller‚Äù paradigm and features like:\n\n\nParallelism Algorithms:\n\nData Parallelism\nTensor Parallelism\nPipeline Parallelism\nContext / Sequence Parallelism\n\nEfficient Kernels:\n\nFlash Attention\nTorch Compile\nLiger Kernel\n\n\nTraining Backends:\n\nFSDP\nFSDP2\nMegatron\n\nGeneration Backends:\n\nvLLM\nSGLang\n‚Ä¶"
  },
  {
    "objectID": "posts/verl-intro/index.html#efficiency-hybrid-engine",
    "href": "posts/verl-intro/index.html#efficiency-hybrid-engine",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.3 Efficiency: ‚ÄúHybrid Engine‚Äù",
    "text": "3.3 Efficiency: ‚ÄúHybrid Engine‚Äù\nverl is efficient for inter-operator with the ‚Äúhybrid engine‚Äù paradigm, utilizing features like:\n\noffloading & reloading enables fully utilizing the GPU memory\nresharding enables switching for the optimal parallelism strategy\n\n\n\n\n\n\n\n\nFigure¬†6: Example of hybrid engine switching between workloads, changing DP for TP."
  },
  {
    "objectID": "posts/verl-intro/index.html#open-source-community",
    "href": "posts/verl-intro/index.html#open-source-community",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "3.4 Open-Source Community",
    "text": "3.4 Open-Source Community"
  },
  {
    "objectID": "posts/verl-intro/index.html#background-single-controller-vs.-multi-controller",
    "href": "posts/verl-intro/index.html#background-single-controller-vs.-multi-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.1 Background: Single-Controller vs.¬†Multi-Controller",
    "text": "4.1 Background: Single-Controller vs.¬†Multi-Controller\n\n\n\n\n\n\n\n\n\n\n\n(a) Single-Controller (MPMD)\n\n\n\n\n\n\n\n\n\n\n\n(b) Multi-Controller (SPMD)\n\n\n\n\n\n\n\nFigure¬†7: Single-Controller (Multi-Program-Multi-Data) vs.¬†Multi-Controller (Single-Program-Multi-Data) (Barham et al. 2022)\n\n\n\n\nSingle-Controller (MPMD): A centralized controller manages all the workers, running different programs.\nMulti-Controller (SPMD): Each worker has its own controller, running the same program with different data."
  },
  {
    "objectID": "posts/verl-intro/index.html#trade-off-single-controller-or-multi-controller",
    "href": "posts/verl-intro/index.html#trade-off-single-controller-or-multi-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.2 Trade-off: Single-Controller or Multi-Controller?",
    "text": "4.2 Trade-off: Single-Controller or Multi-Controller?\n\n\n\nTable¬†2: Trade-off between single-controller and multi-controller.\n\n\n\n\n\nParadigm\nPro\nCon\n\n\n\n\nSingle-Controller\nFlexible\nCommunication Overhead\n\n\nMulti-Controller\nEfficient\nComplex Programming\n\n\n\n\n\n\nü§î Which paradigm should we choose?\n\nü§© Actually, we can have ‚Äúboth‚Äù!"
  },
  {
    "objectID": "posts/verl-intro/index.html#new-paradigm-hybrid-controller",
    "href": "posts/verl-intro/index.html#new-paradigm-hybrid-controller",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.3 New Paradigm: Hybrid-Controller!",
    "text": "4.3 New Paradigm: Hybrid-Controller!\nüí° Hybrid-Controller = Single-Controller + N x Multi-Controller\n\n\n\n\n\n\nFigure¬†8: In the hybrid-controller, a single-controller manages multiple multi-controllers to process the dataflow."
  },
  {
    "objectID": "posts/verl-intro/index.html#implementation-in-verl",
    "href": "posts/verl-intro/index.html#implementation-in-verl",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "4.4 Implementation in verl",
    "text": "4.4 Implementation in verl\nEach call in the single-controller (e.g.¬†critic.compute_values, actor.update_actor) is an RPC to a multi-controller worker group.\n\n\n\n\n\n\nListing¬†2: PPO core code in single-controller.\n\n\nfor prompts in dataloader:\n    # Stage 1: Generation\n    batch = actor.generate_sequences(prompts)\n    # Stage 2: Experience Preparation\n    batch = reward.compute_reward(batch)\n    batch = reference.compute_log_prob(batch)\n    batch = critic.compute_values(batch)\n    batch = compute_advantage(batch, \"gae\")\n    # Stage 3: Training\n    critic.update_critic(batch)\n    actor.update_actor(batch)\n\n\n\n\n\n\n\nListing¬†3: Example distributed code in multi-controller.\n\n\nclass CriticWorker(3DParallelWorker):\n  @register(dispatch_mode=3D_PROTO)\n  def compute_values(self, batch: DataProto):\n      values = self.critic.forward(batch)\n      batch.update(values=values)\n# ...\nclass ActorWorker(3DParallelWorker):\n  @register(dispatch_mode=3D_PROTO)\n  def update_actor(self, batch: DataProto):\n      loss = self.actor(batch)\n      loss.backward()\n\n\n\n\n\n\nThe register decorator utility manages the distributed data transfer, which also makes multi-controller programming easier."
  },
  {
    "objectID": "posts/verl-intro/index.html#async-engine-for-multi-turn-rollout",
    "href": "posts/verl-intro/index.html#async-engine-for-multi-turn-rollout",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "5.1 Async Engine for Multi-Turn Rollout",
    "text": "5.1 Async Engine for Multi-Turn Rollout\n\n\n\n\n\n\nFigure¬†9: Synchronous vs.¬†Asynchronous rollout.1\n\n\n\n\nSynchronous Engine: returns all the outputs in the batch at the same time\nAsynchronous Engine: returns each output as soon as it is ready\n\n\n\n\n\nImage Source: https://novasky-ai.notion.site/skyrl-v0‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/verl-intro/index.html#basic-capability-support",
    "href": "posts/verl-intro/index.html#basic-capability-support",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "5.2 Basic Capability Support",
    "text": "5.2 Basic Capability Support\n\nMulti-Modal: Qwen2.5-VL, Kimi-VL, etc.\nMulti-Turn & Tool Using: see progress at #1882\n‚Ä¶"
  },
  {
    "objectID": "posts/verl-intro/index.html#diverse-environments-tools-ongoing",
    "href": "posts/verl-intro/index.html#diverse-environments-tools-ongoing",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "5.3 Diverse Environments & Tools (Ongoing)",
    "text": "5.3 Diverse Environments & Tools (Ongoing)\nWelcome to discuss about / contribute to:\n\nOur ongoing RFC #1172\nIntegrating protocols like MCP\nIntegrating existing environments & tools, e.g.,\n\n\nKORGym @ ByteDance Seed (Shi et al. 2025)\nAtropos @ Nous Research (Dakota Mahan 2025)"
  },
  {
    "objectID": "posts/verl-intro/index.html#efficient-rl-with-huge-moe-like-deepseek-v3-671b-v0.4",
    "href": "posts/verl-intro/index.html#efficient-rl-with-huge-moe-like-deepseek-v3-671b-v0.4",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "6.1 Efficient RL with Huge MoE like DeepSeek-V3-671B (V0.4+)",
    "text": "6.1 Efficient RL with Huge MoE like DeepSeek-V3-671B (V0.4+)\nverl supports efficient RL training for huge MoE like DeepSeek-V3-671B, based on the following features:\n\nTraining: MoE models classes supporting diverse parallelism strategies like Expert Parallelism based on Megatron GPTModel\nInference: Multi-node inference\nHybrid: Parameter sharding manager for Megatron-Core V0.12 + latest inference engines"
  },
  {
    "objectID": "posts/verl-intro/index.html#q3-focuses",
    "href": "posts/verl-intro/index.html#q3-focuses",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "6.2 2025 Q3 Focuses",
    "text": "6.2 2025 Q3 Focuses\n\nMFU Optimization of huge MoE model training\nMore flexible RL architecture enabling different levels of asynchrony\nAgent recipes\n\n\nFor the most timely updates of important features, please keep an eye on verl‚Äôs Roadmap."
  },
  {
    "objectID": "posts/verl-intro/index.html#references",
    "href": "posts/verl-intro/index.html#references",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "",
    "text": "References\n\n\nBarham, Paul, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, et al. 2022. ‚ÄúPathways: Asynchronous Distributed Dataflow for ML.‚Äù Proceedings of Machine Learning and Systems 4 (April): 430‚Äì49. https://proceedings.mlsys.org/paper_files/paper/2022/hash/37385144cac01dff38247ab11c119e3c-Abstract.html.\n\n\nDai, Josef, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. ‚ÄúSafe RLHF: Safe Reinforcement Learning from Human Feedback.‚Äù In. https://openreview.net/forum?id=TyFrPOKYXw.\n\n\nDakota Mahan, Teknium, Roger Jin. 2025. ‚ÄúAtropos - An Async First Environment Rollout Controller.‚Äù https://www.github.com/NousResearch/Atropos.\n\n\nDeepSeek-AI. 2025. ‚ÄúDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.‚Äù https://arxiv.org/abs/2501.12948.\n\n\nLi, Ziniu, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. 2024. ‚ÄúReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models.‚Äù In. https://openreview.net/forum?id=Stn8hXkpe6.\n\n\nLiang, Eric, Zhanghao Wu, Michael Luo, Sven Mika, Joseph E Gonzalez, and Ion Stoica. 2021. ‚ÄúRllib Flow: Distributed Reinforcement Learning Is a Dataflow Problem.‚Äù Advances in Neural Information Processing Systems 34: 5506‚Äì17.\n\n\nOpenAI. 2024. ‚ÄúLearning to Reason with LLMs.‚Äù OpenAI Blog. https://openai.com/index/learning-to-reason-with-llms/.\n\n\n‚Äî‚Äî‚Äî. 2025. ‚ÄúIntroducing Deep Research.‚Äù OpenAI Blog. https://openai.com/index/introducing-deep-research/.\n\n\nSchaarschmidt, Michael, Sven Mika, Kai Fricke, and Eiko Yoneki. 2019. ‚ÄúRlgraph: Modular Computation Graphs for Deep Reinforcement Learning.‚Äù Proceedings of Machine Learning and Systems 1: 65‚Äì80.\n\n\nSchulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. ‚ÄúProximal Policy Optimization Algorithms.‚Äù https://arxiv.org/abs/1707.06347.\n\n\nSheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. ‚ÄúHybridFlow: A Flexible and Efficient RLHF Framework.‚Äù In Proceedings of the 20th European Conference on Computer Systems. EuroSys ‚Äô25. Rotterdam, The Netherlands: ACM.\n\n\nShi, Jiajun, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, et al. 2025. ‚ÄúKORGym: A Dynamic Game Platform for LLM Reasoning Evaluation.‚Äù https://arxiv.org/abs/2505.14552."
  },
  {
    "objectID": "posts/verl-intro/index.html#sequence-packing",
    "href": "posts/verl-intro/index.html#sequence-packing",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "7.1 Sequence Packing",
    "text": "7.1 Sequence Packing\n\n\n\n\n\n\nFigure¬†10: Tweaking the attention mask of an example packed sequence containing two data sequences.\n\n\n\n\nRemove padding tokens and packs multiple data sequences into a row\nTweak the attention mask & position IDs to avoid cross-contamination\n\n\nTo enable this, use use_remove_padding."
  },
  {
    "objectID": "posts/verl-intro/index.html#dp-balancing",
    "href": "posts/verl-intro/index.html#dp-balancing",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "7.2 DP Balancing",
    "text": "7.2 DP Balancing"
  },
  {
    "objectID": "posts/verl-intro/index.html#other-features",
    "href": "posts/verl-intro/index.html#other-features",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "7.3 Other Features",
    "text": "7.3 Other Features\n\nFull support for RL with AMD (ROCm Kernel) hardwares\nOptimizations: Gradient Checkpointing, Torch Compile, Liger Kernel, etc.\n‚Ä¶"
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-dataset",
    "href": "posts/verl-intro/index.html#customizing-the-dataset",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.1 Customizing the Dataset",
    "text": "8.1 Customizing the Dataset\nA canonical RL dataset in verl has the following fields:\n\nprompt: a list of messages {\"role\": \"...\", \"content\": \"...\"}\ndata_source: used to choose the reward function\nreward_model: a dict containing\n\n\"ground_truth\"\n\"style\" like \"model\" or \"rule\"\n\n(Optional) extra_info: a dict containing extra information\n\n\nFor VLM RL, verl expects fields \"images\" and/or \"videos\"\n\n\nFor examples, please check the examples/data_preprocess."
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-reward",
    "href": "posts/verl-intro/index.html#customizing-the-reward",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.2 Customizing the Reward",
    "text": "8.2 Customizing the Reward\nverl allows to define custom reward function via the custom_reward_function config:\n\n\n\nListing¬†7: Config for custom reward function.\n\n\ncustom_reward_function:\n  path: null # path to the `.py` file containing the function definition\n  name: compute_score # the function name after `def`\nreward_model:\n  reward_manager: naive\n\n\n\n\nAn example CLI config could be:\n\n\n\nListing¬†8: Example config for custom reward function.\n\n\n--custom_reward_function.path=./examples/reward_fn/custom_reward_fn.py \\\n--custom_reward_function.name=compute_score \\\n--reward_model.reward_manager=naive"
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-loss-function",
    "href": "posts/verl-intro/index.html#customizing-the-loss-function",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.3 Customizing the Loss Function",
    "text": "8.3 Customizing the Loss Function\nTo modify the loss function, the most convenient way is to\n\nsearch for the .backward() call\nmodify functions like compute_policy_loss\nor add loss terms like entropy_loss"
  },
  {
    "objectID": "posts/verl-intro/index.html#customizing-the-training-logic",
    "href": "posts/verl-intro/index.html#customizing-the-training-logic",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "8.4 Customizing the Training Logic",
    "text": "8.4 Customizing the Training Logic\nAs mentioned above, the main training logic is concentrated in the fit function of the trainer classes like RayPPOTrainer.\nFor example, the DAPORayTrainer class overrides the fit function to implement the ‚Äúdynamic sampling‚Äù feature:\n(See the next slide for the code ‚û°Ô∏è)"
  },
  {
    "objectID": "posts/verl-intro/index.html#presenter-contact",
    "href": "posts/verl-intro/index.html#presenter-contact",
    "title": "verl: Flexible and Efficient RL for LLMs",
    "section": "9.1 Presenter Contact",
    "text": "9.1 Presenter Contact\n\nEmail: tongyuxuan361@gmail.com\nWeChat / X: tongyx361"
  },
  {
    "objectID": "posts/isdr/index.html",
    "href": "posts/isdr/index.html",
    "title": "[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems",
    "section": "",
    "text": "1 Off-Policy Data in Fully-Utilized LLM RL Systems\n  \n  1.1 Sliding Latest Policy Trajectories (SLAPTs)\n  1.2 Multiple Consistent Stale Policy Trajectories (MCSPTs)\n  \n  2 Importance Sampling with Off-Policy Data in (Fully-Utilized) LLM RL Systems\n  \n  2.1 Simple but Intractable: Global Behavior Policy \\(\\mu^{*}(\\cdot \\mid \\boldsymbol{s})\\)\n  2.2 IS with MCSPTs\n  \n  2.2.1 Mixture Importance Sampling\n  2.2.2 Multiple Importance Sampling\n  2.2.3 Balance Heuristic\n  \n  2.3 IS with SLAPTs\n  \n  2.3.1 Trajectory-dependent Behavior Policy \\(\\mu_{i}(\\cdot \\mid \\boldsymbol{s})\\)\n  2.3.2 Variance of Single-Sample Estimate ‚Äì MCSPT vs.¬†SLAPT\n  2.3.3 Minimizing Variance by Optimizing the Weighting\n  \n  \n  Citation"
  },
  {
    "objectID": "posts/isdr/index.html#sliding-latest-policy-trajectories-slapts",
    "href": "posts/isdr/index.html#sliding-latest-policy-trajectories-slapts",
    "title": "[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems",
    "section": "1.1 Sliding Latest Policy Trajectories (SLAPTs)",
    "text": "1.1 Sliding Latest Policy Trajectories (SLAPTs)\nWe use Sliding Latest Policy Trajectories to refer to such batches of trajectories that are sampled in an RL system where we always use the latest policy \\(\\pi_{\\theta_{t}}\\) at each moment to sample the actions.\nFormally, a SLAPT can be defined as a trajectory \\(\\tau = (s_{0}, a_{0,\\pi_{\\theta_{n}}}, \\ldots, s_{t}, a_{t,\\pi_{\\theta_{n+m}}}, \\ldots, s_{T}, a_{T,\\pi_{\\theta_{n+M}}})\\), where \\(s_{t}\\) is the state at timestep \\(t\\) and \\(a_{t,\\pi_{\\theta_{n+m}}}\\) is the action sampled from the policy \\(\\pi_{\\theta_{n+m}}\\) that is the latest policy available in the system when sampling. So for \\(l &gt; m\\), \\(\\pi_{\\theta_{n+l}}\\) is usually more on-policy than \\(\\pi_{\\theta_{n+m}}\\).\nSLAPTs in a batch usually have different policy compositions, depending on the system dynamics.\nSLAPTs are a natural result of a popular design choice called Partial Rollout in LLM RL systems nowadays, such as Kimi k1.5 (Kimi Team et al. 2025), AReaL (Fu et al. 2025) and PipelineRL (Pich√© et al. 2025), etc., which can date back to a traditional distributed RL system called SEED RL (Espeholt et al. 2020):\nThe ongoing trajectory rollouts are\n\naborted when\n\neither, in synchoronous systems like Kimi k1.5, there are enough samples collected for training, releasing the resources for the training engine to update the model weights,\nor, in asynchronous systems like AReaL and PipelineRL, a new version of model weights is produced by the trainer;\n\ncontinued with the latest version of model weights.\n\nPartial Rollout is motivated by its efficiency and simplicity. Let me explain in detail.\nThe earliest LLM RL systems Hu et al. (2025) all adopt synchoronous architectures, where the trainer always waits for all the trajectories are finished before updating the weights with these data. However, as the context length of LLMs scales up, the skewness of the trajectory length distribution becomes increasing heavy. If the distribution is very skewed but all the trajectories are required to finish in the same rollout stage, there can be only a few long-tail requests remaining in the system, causing severe under-utilization (typically &lt;30% in practice).\nKimi k1.5 proposes to fix this issue by aborting all the ongoing rollouts once enough training samples are collected and directly update the weights with these data, instead of waiting for all the trajectories to finish, and then continue the rollouts with the new model weights.\nIn asynchornous RL systems with an experience buffer, it is troublesome to mamage multiple versions of model weights within the rollout engine.\nAReaL proposes to always only maintain the latest model weights across all the instances. Once a new version of model weights is produced by the trainer, all the rollouts of the stale policy are aborted and then continued with the latest policy.\nThis can be simply implemented by always loading the latest weights into all the inference engine instances, avoiding the bothering to manage the requests across each instance.\nDespite Partial Rollout‚Äôs efficiency and simplicity, there have been worries about mixing multiple policies within a single trajectory, since most previous works formulate IS with a single consistent behavior policy \\(\\mu(\\cdot \\mid \\boldsymbol{s})\\) while such formulation is rather under-explored. We defer a more detailed discussion about this to Section¬†2."
  },
  {
    "objectID": "posts/isdr/index.html#multiple-consistent-stale-policy-trajectories-mcspts",
    "href": "posts/isdr/index.html#multiple-consistent-stale-policy-trajectories-mcspts",
    "title": "[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems",
    "section": "1.2 Multiple Consistent Stale Policy Trajectories (MCSPTs)",
    "text": "1.2 Multiple Consistent Stale Policy Trajectories (MCSPTs)\nWe use Multiple Consistent Stale Policy Trajectories (MCSPTs) to refer to such batches of trajectories that\n\neach of them is sampled with a consistent stale policy \\(\\pi_{\\theta_{n}}\\),\nwhile there might be different \\(n\\) for different trajectories.\n\nThe consistency of policy within a trajectory makes the formulation simpler, which is widely used in traditional distributed RL systems like IMPALA (Espeholt et al. 2018).\nIn contrast, MCSPT sampling is more difficult to implement efficiently against long-tail bubbles in LLM RL systems, because this requires managing multiple versions of model weights at the same time within the rollout engine and dynamically transferring the requests of various lengths.\nSheng, Tong, et al. (2025) implement an LLM RL system that samples MCSPTs with full utilization of the hardware resources."
  },
  {
    "objectID": "posts/isdr/index.html#sec-intractable-global-behav-policy",
    "href": "posts/isdr/index.html#sec-intractable-global-behav-policy",
    "title": "[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems",
    "section": "2.1 Simple but Intractable: Global Behavior Policy \\(\\mu^{*}(\\cdot \\mid \\boldsymbol{s})\\)",
    "text": "2.1 Simple but Intractable: Global Behavior Policy \\(\\mu^{*}(\\cdot \\mid \\boldsymbol{s})\\)\nSince there is always an actual distribution we are sampling from, we can always formulate the IS estimate with a global behavior policy \\(\\mu^{*}(\\cdot \\mid \\boldsymbol{s})\\).\nThe most direct thought is to use \\(\\mu^{*}(\\cdot \\mid \\boldsymbol{s})\\) for IS in practice. But this requires us to calculate the probabilities under it.\nFu et al. (2025) Proposition 1 resorts to constructing a behavior policy \\(\\mu(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) that satisfies \\(\\mu^{*}(a_{t} \\mid s_{t}) = \\pi_{\\theta_{n+m}}(a_{t} \\mid s_{t})\\) for each \\((s_{t}, a_{t})\\) pair in the trajectory samples. This sounds reasonable for LLM RL since the LLM is usually auto-regressive and thus never revisits a past state within the same trajectory.\nHowever, it might be confusing when we also notice that, the same state \\(s\\) might appear in two different trajectory samples \\(\\tau_{i}\\) and \\(\\tau_{j}\\), especially for the initial states \\(s_{0}\\), i.e., the prompts, and the same action \\(a\\) might be sampled from different policies \\(\\pi_{\\theta_{n+m}}\\) and \\(\\pi_{\\theta_{n+l}}\\) (\\(l \\neq m\\)) respectively. It is very likely that \\(\\pi_{\\theta_{n+m}}(a \\mid s) \\neq \\pi_{\\theta_{n+l}}(a \\mid s)\\), making it infeasible to simply construct the same behavior policy for both trajectory samples, i.e., \\(\\mu^{*}(a \\mid s)=\\pi_{\\theta_{n+m}}(a \\mid s)\\) constradicts \\(\\mu(a \\mid s)=\\pi_{\\theta_{n+l}}(a \\mid s)\\).\nSo where is the problem in the construction for \\(\\mu(\\cdot \\mid \\boldsymbol{s})\\) mentioned above?\nThe problem hidden here is that, \\(\\mu(\\cdot \\mid \\boldsymbol{s})\\) does not consider the probability distribution of which LLM policy \\(\\pi_{\\theta_{n+m}}\\) is used. Furthermore, this distribution is actually intractable since it depends on the system dynamics."
  },
  {
    "objectID": "posts/isdr/index.html#sec-is-mcspt",
    "href": "posts/isdr/index.html#sec-is-mcspt",
    "title": "[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems",
    "section": "2.2 IS with MCSPTs",
    "text": "2.2 IS with MCSPTs\nIS with MCSPTs is simpler to formulate since each trajectory is sampled with a consistent stale policy \\(\\pi_{\\theta_{n}}\\), and we only need to correctly formulate the policy used by each trajectory.\n\n2.2.1 Mixture Importance Sampling\nIn some simple cases, the distribution of which policy is used, i.e., the hyper-policy distribution, is known, where we can formulate the IS estimate as Mixture Importance Sampling (Owen 2013).\n\n\n2.2.2 Multiple Importance Sampling\nHowever, in more general cases, we can only know ad hoc that the number of trajectories sampled from each policy \\(\\pi_{\\theta_{j}}\\) is \\(n_{j}\\), where \\(N = \\sum_{j} n_{j}\\). For such cases, we can formulate the IS estimate as Multiple Importance Sampling (Owen 2013):\n\nSuppose that \\(\\boldsymbol{X}_{i j} \\sim q_j\\) for \\(i=1, \\ldots, n_j\\) and \\(j=1, \\ldots, J\\) and that \\(\\omega_j\\) are a partition of unity. The multiple importance sampling estimate is\n\\[\\widetilde{\\mu}_\\omega=\\sum_{j=1}^J \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\omega_j\\left(\\boldsymbol{X}_{i j}\\right) \\frac{f\\left(\\boldsymbol{X}_{i j}\\right) p\\left(\\boldsymbol{X}_{i j}\\right)}{q_j\\left(\\boldsymbol{X}_{i j}\\right)} .\\]\nNow assume that \\(q_j(\\boldsymbol{x})&gt;0\\) whenever \\(\\omega_j(\\boldsymbol{x}) p(\\boldsymbol{x}) f(\\boldsymbol{x}) \\neq 0\\). Then multiple importance sampling is unbiased, because \\[\\mathbb{E}\\left(\\widetilde{\\mu}_\\omega\\right)=\\sum_{j=1}^J \\mathbb{E}_{q_j}\\left(\\omega_j(\\boldsymbol{X}) \\frac{f(\\boldsymbol{X}) p(\\boldsymbol{X})}{q_j(\\boldsymbol{X})}\\right)=\\sum_{j=1}^J \\int \\omega_j(\\boldsymbol{x}) f(\\boldsymbol{x}) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}=\\mu .\\]\n\n\n\n2.2.3 Balance Heuristic\nThe natural problem following is how to choose the partition of unity \\(\\omega_j\\):\n\nAmong the proposals for functions \\(\\omega_j(\\boldsymbol{x})\\), the most studied one is the balance heuristic with \\(\\omega_j(\\boldsymbol{x}) \\propto n_j q_j(\\boldsymbol{x})\\), that is\n\\[\\omega_j(\\boldsymbol{x})=\\omega_j^{\\mathrm{BH}}(\\boldsymbol{x}) \\equiv \\frac{n_j q_j(\\boldsymbol{x})}{\\sum_{k=1}^J n_k q_k(\\boldsymbol{x})} .\\]\nBy construction \\(q_j(\\boldsymbol{x})&gt;0\\) holds whenever \\(\\left(\\omega_j^{\\mathrm{BH}} p f\\right)(\\boldsymbol{x}) \\neq 0\\). Let \\(n=\\sum_{j=1}^J n_j\\) and define \\(\\alpha_j=n_j / n\\). Then using the balance heuristic, \\(\\widetilde{\\mu}_{\\omega^{\\text {–í–ù }}}\\) simplifies to \\[\\widetilde{\\mu}_\\alpha=\\frac{1}{n} \\sum_{j=1}^J \\sum_{i=1}^{n_j} \\frac{f\\left(\\boldsymbol{X}_{i j}\\right) p\\left(\\boldsymbol{X}_{i j}\\right)}{\\sum_{j=1}^J \\alpha_j q_j\\left(\\boldsymbol{X}_{i j}\\right)} .\\]\nIn other words, multiple importance sampling, with weights from the balance heuristic reduces to the same estimator we would use in mixture importance sampling with mixture weights \\(\\alpha_j=n_j / n\\). Once again, the weight on a given sampled value \\(\\boldsymbol{X}_{i j}\\) does not depend on which mixture component it came from. The balance heuristic is nearly optimal in the following sense:\nTheorem 9.8. Let \\(n_j \\geqslant 1\\) be positive integers for \\(j=1, \\ldots, J\\). Let \\(\\omega_1, \\ldots, \\omega_J\\) be a partition of unity and let \\(\\omega^{\\mathrm{BH}}\\) be the balance heuristic. Suppose that \\(q_j(\\boldsymbol{x})&gt;\\) 0 whenever \\(\\omega_j(\\boldsymbol{x}) p(\\boldsymbol{x}) f(\\boldsymbol{x}) \\neq 0\\). Then\n\\[\\operatorname{Var}\\left(\\widetilde{\\mu}_{\\omega^{\\mathrm{BH}}}\\right) \\leqslant \\operatorname{Var}\\left(\\widetilde{\\mu}_\\omega\\right)+\\left(\\frac{1}{\\min _j n_j}-\\frac{1}{\\sum_j n_j}\\right) \\mu^2 .\\]\n(Owen 2013)\n\nThe heuristic behind the balance heuristic to make \\(\\omega_j^{\\mathrm{BH}} \\propto n_{j}q_j(\\boldsymbol{x})\\) can be also understood as the more samples we have from a policy, the more information we have about it, thus the more weight it should have."
  },
  {
    "objectID": "posts/isdr/index.html#is-with-slapts",
    "href": "posts/isdr/index.html#is-with-slapts",
    "title": "[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems",
    "section": "2.3 IS with SLAPTs",
    "text": "2.3 IS with SLAPTs\nThe IS with SLAPTs is more difficult to formulate since we need to also consider the policy composition within a trajectory. As far as we know, the related discussion is in absense in previous works including Espeholt et al. (2020) that first proposed the usage of SLAPTs.\nIn this section, we try to discuss the formulation and some properties of IS with SLAPTs that might be useful in practice.\n\n2.3.1 Trajectory-dependent Behavior Policy \\(\\mu_{i}(\\cdot \\mid \\boldsymbol{s})\\)\nA general but trivial formulation for IS estimate with a batch of trajectory samples is to make the behavior policy trajectory-dependent, i.e., use \\(\\mu_{i}(\\boldsymbol{a} \\mid \\boldsymbol{s})\\) for each trajectory \\(\\tau_{i}\\).\nNow for the counter-example mentioned in Section¬†2.1, \\(\\mu_i(a \\mid s)=\\pi_{\\theta_{n+m}}(a \\mid s)\\) and \\(\\mu_j(a \\mid s)=\\pi_{\\theta_{n+l}}(a \\mid s)\\) are obviously compatible.\nWith a batch of \\(N\\) trajectory samples sampled from each own behavior policy \\(\\boldsymbol{\\tau}_1 \\sim q_{\\mu_1}, \\ldots, \\boldsymbol{\\tau}_N \\sim q_{\\mu_N}\\), we can only use the special case of the batch estimate Equation¬†1 with \\(N=1\\), i.e., the single-sample estimate\n\\[\n\\hat{\\mathbb{E}}_{\\mu_i}(\\boldsymbol{\\tau}_i) = \\prod_{t=0}^{T-1} \\frac{\\pi_{\\theta_{n+m}}(\\boldsymbol{a}_{t} \\mid \\boldsymbol{s}_{t})}{\\mu_i(\\boldsymbol{a}_{t} \\mid \\boldsymbol{s}_{t})}f(\\boldsymbol{\\tau}_i)\n\\tag{3}\\]\nNote that the single-sample estimate is also unbiased, i.e., the unbiasedness of Equation¬†1 does not depend on the sample size \\(N\\).\nThe common practice to combine them into an estimate with the batch of samples is to average the single-sample estimates Equation¬†3, i.e.,\n\\[\n\\hat{\\mathbb{E}}_{\\text{avg}} = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{\\mathbb{E}}_{\\mu_i} = \\frac{1}{N} \\sum_{i=1}^{N} \\prod_{t=0}^{T-1} \\frac{\\pi_{\\theta_{n+m}}(\\boldsymbol{a}_{t} \\mid \\boldsymbol{s}_{t})}{\\mu_i(\\boldsymbol{a}_{t} \\mid \\boldsymbol{s}_{t})}f(\\boldsymbol{\\tau}_i)\n\\tag{4}\\]\nWith the linearity of expectation, it is obvious to see the unbiasedness of the average estimate \\(\\hat{\\mathbb{E}}_{\\text{avg}}\\).\nNow let‚Äôs take a look at the variance. Let the variance of each single-sample estimate be \\(\\sigma^2_{\\mu_i}\\), since the samples are independent, the variance of \\(\\hat{\\mathbb{E}}_{\\text{avg}}\\) is:\n\\[\n\\sigma^2_{\\text{avg}} = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sigma^2_{\\mu_i}\n\\tag{5}\\]\nGiven the batch variance is determined by composing the variances of the single-sample estimates, we can first try to analyze the variance of the single-sample estimate.\n\n\n2.3.2 Variance of Single-Sample Estimate ‚Äì MCSPT vs.¬†SLAPT\nMCSPT and SLAPT just form two different cases of the single-sample estimate. We might wonder that, given a trajectory \\(\\boldsymbol{\\tau}_{i}\\), which of\n\nthe consistent old policy \\(\\pi_{\\theta_{n+m}}\\) and\nthe sliding latest policy \\(\\mu_{\\theta_{n},M}\\) using \\(\\pi_{\\theta_{n}},\\ldots,\\pi_{\\theta_{n+M}}\\) successively\n\nis better for the unbiased single-sample IS estimate Equation¬†3, i.e., leads to a lower variance?\nMetelli et al. (2020) provided a family of bounds of the variance of IS estimate in terms of the R√©nyi divergence:\n\nLemma 1. Let \\(P\\) and \\(Q\\) be two probability measures on the measurable space \\((\\mathcal{X}, \\mathscr{F})\\) such that \\(P \\ll Q\\). Let \\(\\alpha \\in[1,+\\infty], \\mathbf{x}=\\left(x_1, x_2, \\ldots, x_N\\right)^T\\) be i.i.d. random variables sampled from \\(Q\\) and \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\) be a function with bounded \\(\\frac{2 \\alpha}{\\alpha-1}\\)-moment under \\(Q\\left(\\|f\\|_{Q, \\frac{2 \\alpha}{\\alpha-1}}&lt;+\\infty\\right)\\). Then, for any \\(N&gt;0\\), the variance of the IS estimator \\(\\widehat{\\mu}_{P / Q}\\) can be upper bounded as:\n\\[\\operatorname{Var}_{\\mathbf{x} \\sim Q}\\left[\\hat{\\mu}_{P / Q}\\right] \\leqslant \\frac{1}{N}\\|f\\|_{Q, \\frac{2 \\alpha}{\\alpha-1}}^2 d_{2 \\alpha}(P \\| Q)^{2-\\frac{1}{\\alpha}},\\]\nwhere we used the abbreviation \\(\\mathbf{x} \\sim Q\\) for denoting \\(x_i \\sim Q\\) for all \\(i=1,2, \\ldots, N\\) all independent.\nThis result generalizes Lemma 4.1 of Metelli et al.¬†(2018), that can be recovered by setting \\(\\alpha=1\\) under the condition that \\(\\|f\\|_{\\infty}&lt;+\\infty\\) :\n\\[\\operatorname{Var}_{\\mathbf{x} \\sim Q}\\left[\\widehat{\\mu}_{P / Q}\\right] \\leqslant \\frac{1}{N}\\|f\\|_{\\infty}^2 d_2(P \\| Q) .\\]\n\nWhen \\(\\alpha = 1\\), the R√©nyi divergence is the Kullback-Leibler divergence widely used in RL analysis.\n\nWhen \\(P=Q\\) almost everywhere, we get \\(\\operatorname{Var}_{\\mathbf{x} \\sim Q}\\left[\\hat{\\mu}_{Q / Q}\\right] \\leqslant \\frac{1}{N}\\|f\\|_{\\infty}^2\\), a well-known upper bound to the variance of a Monte Carlo estimator. Recalling the definition of ESS (Equation 7) we can rewrite the previous bound as:\n\\[\\underset{\\mathbf{x} \\sim Q}{\\operatorname{Var}}\\left[\\hat{\\mu}_{P / Q}\\right] \\leqslant \\frac{\\|f\\|_{\\infty}^2}{\\operatorname{ESS}(P \\| Q)} .\\]\nThus, the variance scales with ESS instead of \\(N\\), justifying the definition of ESS.\n\nIn our context, \\(P\\) is the target distribution \\(p_{\\theta}\\) and \\(Q\\) is \\(\\mu_{\\theta_{n},M}\\) or \\(\\pi_{\\theta_{n}}\\).\nIt is possible that \\(d_{2 \\alpha}(p_{\\theta} \\| q_{\\theta_{n},M}) &lt; d_{2 \\alpha}(p_{\\theta} \\| p_{\\theta_{n}})\\), since the newer the policy is, the more similar its induced distribution is to \\(p_{\\theta}\\). Then as long as the \\(\\|f\\|_{q_{\\theta_{n},M}, \\frac{2 \\alpha}{\\alpha-1}}\\) is not much larger than \\(\\|f\\|_{p_{\\theta_{n}}, \\frac{2 \\alpha}{\\alpha-1}}\\), the estimate using \\(\\mu_{\\theta_{n},M}\\) has better guarantee than the estimate using \\(\\pi_{\\theta_{n}}\\).\nIn practice, the final effectiveness of the IS estimate usually relies on empirical diagnostic metrics like Effective Sample Size (ESS). For example, Pich√© et al. (2025) measured the ESS of different sampling policies they used.\n\n\n2.3.3 Minimizing Variance by Optimizing the Weighting\nBeyond the comparison on each single-sample estimate, we can also consider the optimal weighting of them. In Equation¬†4, we use the simplest average weight \\(\\frac{1}{N}\\) for each single-sample estimate. But we can actually use any other weight functions \\(\\omega_i(\\boldsymbol{x})\\) to combine them, as long as they form a partition of unity, i.e., a collection of \\(J \\geqslant 1\\) weight functions \\(\\omega_j(\\boldsymbol{x}) \\geqslant 0\\) which satisfy \\(\\sum_{j=1}^J \\omega_j(\\boldsymbol{x})=1\\) for all \\(\\boldsymbol{x}\\). Different partitions of unity will lead to estimates that are all unbiased but with different variances.\nThe optimal weighting should be still an open problem that is beyond our scope. However, there is still some properties that we can exploit from the SLAPT structure. For example, in practical systems, SLAPTs of similar lengths often conform to identical distributions (approximately, but this can be exact if we limit the timesteps where a trajectory can be aborted), we can also apply the formulation of Multiple IS to SLAPT like in Section¬†2.2. This also supports the comparability of SLAPT and MCSPT."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs ÂçöÂÆ¢",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nverl Tutorial\n\n\n\n\n\n\n\n\n\n\n\nYuxuan Tong\n\n\n\n\n\n\n\n\n\n\n\n\n[WIP] Importance Sampling Done Right with Off-Policy Data in Fully-Utilized LLM RL Systems\n\n\n\n\n\n\nEnglish Ëã±Êñá\n\nTechnical ÊäÄÊúØ\n\n\n\nIn the pursuit of maximizing hardware utilization for Large Language Model (LLM) Reinforcement Learning, systems introduces some new sampling strategies like Partial Rollout and Asynchornous Rollout. This introduces complex off-policy data structures, which we categorize into Sliding Latest Policy Trajectories (SLAPTs) and Multiple Consistent Stale Policy Trajectories (MCSPTs). This post discusses the properties of applying Importance Sampling (IS) to these distinct data forms, combining theoretical notations like Multiple Importance Sampling and R√©nyi divergence and practical observations like the (approximate) indentical distribution structure of SLAPTs of similar lengths.\n\n\n\n\n\nJan 7, 2026\n\n\nYuxuan Tong, Yingru Li, Guangming Sheng\n\n\n\n\n\n\n\n\n\n\n\n\nverl: Flexible and Efficient RL for LLMs\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2025\n\n\nYuxuan Tong (Á´•Èõ®ËΩ©)\n\n\n\n\n\n\n\n\n\n\n\n\nÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ\n\n\n‰øÆÊ≠£ GRPO ÂÖ¨Âºè‰∏éÊµÅË°å LLM RL Ê°ÜÊû∂\n\n\n\nChinese ‰∏≠Êñá\n\nTechnical ÊäÄÊúØ\n\n\n\nÂØπ‰∫é LLM RL ‰∏≠Áõ∏ÂØπ‰∫éÂèÇËÄÉÁ≠ñÁï•ÁöÑ KL ‰ºòÂåñÔºåGRPO ÂÖ¨Âºè\n\nÊ≤°ÊúâÂ§ÑÁêÜ KL È°πÁöÑ off-policy ÈóÆÈ¢òÔºåËøôÂèØ‰ª•ÈÄöËøáÂú®Â§öËΩÆÊõ¥Êñ∞Êó∂ÈáçÊñ∞ËÆ°ÁÆó KL È°πÂπ∂Ê∑ªÂä†ÈáçË¶ÅÊÄßÈááÊ†∑Á≥ªÊï∞Ëß£ÂÜ≥\nÂÖàÂ∞Ü KL ‰º∞ËÆ°Ê†∑Êú¨ÈáèÂ∫îÁî®‰∫éÂä®‰ΩúÂØπÊï∞Êù°‰ª∂‰ººÁÑ∂ÂÜçÊ±ÇÂíåÔºåËÄåÈùûÂÖàÊ±ÇÂíåÂæóÂà∞Ê¶ÇÁéáÂÜçÂ∫îÁî®‰º∞ËÆ°Ê†∑Êú¨ÈáèÔºå‰∏é John Schulman ‚ÄúApproximating KL Divergence‚Äù ÂàÜÊûê‰∏çÁ¨¶ÔºàÂØπÂ∫îÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰πüÂèØËÉΩÂõ†Ê≠§ËÄåÈîôËØØÔºâ\n\nÁõÆÂâçÊµÅË°åÁöÑ LLM RL Ê°ÜÊû∂ÔºàTRLÔºåOpenRLHFÔºåverlÔºâ‰πüÊ≤°ÊúâÈÅøÂÖç‰∏äËø∞ÈóÆÈ¢òÔºå‰∏îÂ≠òÂú®ÂÖ∂‰ªñÈóÆÈ¢òÔºö\n\nÂú®ËÆ°ÁÆó KL loss È°πÊó∂ÈªòËÆ§‰∏çÂéªÈô§‰ªª‰ΩïÊ¢ØÂ∫¶ÔºåÂÆûÈôÖÂæóÂà∞ÁöÑÊ¢ØÂ∫¶ÈÄöÂ∏∏‰∏çÊòØÂú®‰ºòÂåñ KL Êï£Â∫¶\nKL loss È°πÁöÑÂπ≥ÂùáÊìç‰ΩúÂ≠òÂú®ÈîôËØØ„ÄÇ\n\nÊú¨ÊñáÂü∫‰∫éÂ∫èÂàóÂÜ≥Á≠ñËøáÁ®ãÔºàËÄåÈùû banditÔºâÂª∫Ê®°ÂàÜÊûê‰∫Ü‰∏äËø∞ÈóÆÈ¢òÔºåÂπ∂Êèê‰æõ‰∫ÜÊ≠£Á°ÆÁöÑ KL loss / reward È°πÂÆûÁé∞ÁöÑÊï∞Â≠¶Êé®ÂØº‰∏é‰∏äËø∞ÈóÆÈ¢òÁöÑ‰øÆÊ≠£ÊÄùË∑Ø„ÄÇ\n\n\n\n\n\nMar 9, 2025\n\n\nÁ´•Èõ®ËΩ©\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#rl-as-dataflow-graph",
    "href": "posts/verl-tutorial/index.html#rl-as-dataflow-graph",
    "title": "verl Tutorial",
    "section": "1.1 RL as Dataflow Graph",
    "text": "1.1 RL as Dataflow Graph\n\n\n\n\n\nReinforcement Learning (RL) for LLM Post-Training can typically be modeled as a dataflow graph, consisting of:\n\nmultiple models: actor, critic, reference, reward model, etc.\nmultiple stages: generating, preparing experiences, training\nmultiple workloads: generation, inference, training"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#implementing-dataflow-graph-as-execution-pattern",
    "href": "posts/verl-tutorial/index.html#implementing-dataflow-graph-as-execution-pattern",
    "title": "verl Tutorial",
    "section": "1.2 Implementing Dataflow Graph as Execution Pattern",
    "text": "1.2 Implementing Dataflow Graph as Execution Pattern\nIn practice, we should implement the dataflow graph as execution pattern on GPU cluster.\n\n\n\n\n\n\nSpecifically, we:\n\ndesign the parallelism strategy and model placement to optimize the throughput\nwhile restricted by the temporal dependencies and device resources"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#entrypoint",
    "href": "posts/verl-tutorial/index.html#entrypoint",
    "title": "verl Tutorial",
    "section": "2.1 Entrypoint",
    "text": "2.1 Entrypoint\nverl uses a global resource pool and allocates all the workers (e.g., ActorRollout, Critic) to it by default.\n\n\n\nListing¬†1: Simplified code for resource allocation in TaskRunner.run().\n\n\nglobal_pool_id = \"global_pool\"\nresource_pool_spec = {\n  global_pool_id: ([config.trainer.n_gpus_per_node] * config.trainer.nnodes),\n}\nmapping = {\n  Role.ActorRollout: global_pool_id, Role.Critic: global_pool_id,\n  Role.RefPolicy: global_pool_id, Role.RewardModel: global_pool_id,\n}\nresource_pool_manager = ResourcePoolManager(\n  resource_pool_spec=resource_pool_spec, mapping=mapping)\n# ...\ntrainer = RayPPOTrainer(config=config, \n                        resource_pool_manager=resource_pool_manager, # ...\n                       )\ntrainer.fit()"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#spawning-worker-groups",
    "href": "posts/verl-tutorial/index.html#spawning-worker-groups",
    "title": "verl Tutorial",
    "section": "2.2 Spawning Worker Groups",
    "text": "2.2 Spawning Worker Groups\n\nEach worker group corresponds to\n\na resource_pool (some GPUs);\none or more workers in class_dict.\n\nwg_dict.spawn() launches one process per GPU.\n\n\n\n\nListing¬†2: Simplified code for spawning worker group processes in RayPPOTrainer.init_workers().\n\n\n# `resource_pool_to_cls` is a `dict` \n# mapping resource pools to worker classes.\nfor resource_pool, class_dict in self.resource_pool_to_cls.items():\n  # ...\n  wg_dict = self.ray_worker_group_cls(\n      resource_pool=resource_pool, # ...\n  )\n  spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n  all_wg.update(spawn_wg)\n  self.wg_dicts.append(wg_dict)"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#training-loop-single-controller",
    "href": "posts/verl-tutorial/index.html#training-loop-single-controller",
    "title": "verl Tutorial",
    "section": "2.3 Training Loop: Single-Controller",
    "text": "2.3 Training Loop: Single-Controller\nBetween worker procedures, verl adopts a single-controller paradigm to maximize the flexibility, which allows the users to\n\nfocus on the dataflow graph\nwithout worrying about the distributed implementation.\n\nverl runs the worker procedures sequentially within the global resource pool by default.\n\n\n\nListing¬†3: Simplified code for training loop in RayPPOTrainer.fit().\n\n\nfor epoch in range(self.config.trainer.total_epochs):\n  for batch_dict in self.train_dataloader:\n    batch = DataProto.from_single_dict(batch_dict)\n    # Stage 1: Generating\n    gen_batch = batch.pop(...)\n    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n    # Stage 2: Preparing Experiences\n    old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\n    ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n    values = self.critic_wg.compute_values(batch)\n    reward_tensor = self.rm_wg.compute_rm_score(batch)\n    # Stage 3: Training\n    self.critic_wg.update_critic(batch)\n    self.actor_rollout_wg.update_actor(batch)"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#worker-procedure-multi-controller",
    "href": "posts/verl-tutorial/index.html#worker-procedure-multi-controller",
    "title": "verl Tutorial",
    "section": "2.4 Worker Procedure: Multi-Controller",
    "text": "2.4 Worker Procedure: Multi-Controller\nInside a worker procedure, verl adopts a multi-controller paradigm, i.e., SPMD (Single Program Multiple Data), to maximize the efficiency.\nIn SPMD, all the processes\n\nrun the same program,\nbut process diffrent data based on the distributed environment variables like RANK.\n\nSPMD is the programming model of most popular distributed methods, e.g.,\n\nData Parallelism: DDP, ZeRO, FSDP\nTensor Parallelism\nPipeline Parallelism\nSequence Parallelism"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#how-verl-manages-the-resources",
    "href": "posts/verl-tutorial/index.html#how-verl-manages-the-resources",
    "title": "verl Tutorial",
    "section": "3.1 How verl Manages the Resources",
    "text": "3.1 How verl Manages the Resources\nverl\n\nspawns a list of _workers, each of which is a Ray worker running on a GPU\nand sets the SPMD environment variables for each worker.\n\n\n\n\nListing¬†4: Simplified code for initializing worker groups in RayPPOTrainer.init_workers().\n\n\ndef _init_with_resource_pool(self, resource_pool, ray_cls_with_init):\n  # ...\n  rank = -1\n  for pg_idx, pg in enumerate(sort_placement_group_by_node_ip(pgs)): # Node\n    for local_rank in range(local_world_size): # GPU\n      rank += 1\n      env_vars = {\n        'WORLD_SIZE': str(world_size), 'RANK': str(rank), # More env vars ...\n      }\n      ray_cls_with_init.update_options(\n        {'runtime_env': {'env_vars': env_vars}})\n      # ...\n      worker = ray_cls_with_init(placement_group=pg,\n                                 placement_group_bundle_idx=local_rank)\n      self._workers.append(worker)\n  # ..."
  },
  {
    "objectID": "posts/verl-tutorial/index.html#how-verl-defines-the-spmd-behavior",
    "href": "posts/verl-tutorial/index.html#how-verl-defines-the-spmd-behavior",
    "title": "verl Tutorial",
    "section": "3.2 How verl Defines the SPMD Behavior",
    "text": "3.2 How verl Defines the SPMD Behavior\nTaking the ActorRolloutRefWorker.update_actor() as an example:\n\n\n\nListing¬†5: Simplified code for SPMD update_actor() in ActorRolloutRefWorker.\n\n\n@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\ndef update_actor(self, data: DataProto):\n  # NOTE: here, we already have only 1/WORLD_SIZE of the whole data\n  data = data.to(torch.cuda.current_device())\n  self.actor.update_policy(data=data)\n  self.actor_lr_scheduler.step()"
  },
  {
    "objectID": "posts/verl-tutorial/index.html#references",
    "href": "posts/verl-tutorial/index.html#references",
    "title": "verl Tutorial",
    "section": "",
    "text": "References\n\n\nSheng, Guangming, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. ‚ÄúHybridFlow: A Flexible and Efficient RLHF Framework.‚Äù arXiv Preprint arXiv: 2409.19256."
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "",
    "text": "1 ÂºïË®ÄÔºöGRPO ÂÖ¨ÂºèÁöÑ‚ÄúÈîôËØØ‚Äù\n  2 ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ on-policy KL ‰ºòÂåñÁöÑÂÆûÁé∞\n  \n  2.1 TRLÔºöKL reward È°π\n  2.2 OpenRLHF\n  \n  2.2.1 KL reward È°π\n  2.2.2 KL loss È°π\n  \n  2.3 verl\n  \n  2.3.1 KL reward È°π\n  2.3.2 KL loss È°π\n  \n  2.4 ‰∏∫‰ªÄ‰πàË¶ÅÂ∞Ü KL ‰ªé reward ‰∏≠ÂáèÂéª\n  \n  2.4.1 KL reward ÁöÑÊµÅË°åÂ∫îÂΩìÊ∫êËá™ RLHF ‰∏é InstructGPT\n  2.4.2 OpenAI ËÆ∫Êñá‰∏≠ KL reward ÁöÑÂá∫Â§Ñ\n  2.4.3 KL reward ÊúÄÊó©ÁöÑÂá∫Â§Ñ\n  \n  \n  3 LLM RL ‰∏≠ KL ‰ºòÂåñÁöÑÊï∞Â≠¶ÂΩ¢ÂºèÂåñ\n  \n  3.1 RL ‰∏≠ÁöÑ KL Êï£Â∫¶ÈÄöÂ∏∏ÂÆö‰πâÂú®ËΩ®ËøπÂàÜÂ∏É‰∏ä\n  3.2 Â∞ÜËΩ®ËøπÂ±ïÂºÄ‰∏∫Áä∂ÊÄÅ-Âä®‰ΩúÂ∫èÂàó\n  3.3 Markov ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÁöÑ KL Êï£Â∫¶\n  3.4 ËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Â∫èÂàóÂÜ≥Á≠ñËøáÁ®ã\n  3.5 ‰º∞ËÆ° KL Êï£Â∫¶\n  \n  3.5.1 Âá†‰πé‰∏çÂèØËÉΩÁõ¥Êé•ËÆ°ÁÆó KL Êï£Â∫¶ÁöÑÁúüÂÆûÂÄº\n  3.5.2 ÈÄöÂ∏∏‰ΩøÁî® Monte Carlo ÊñπÊ≥ï‰º∞ËÆ° KL Êï£Â∫¶\n  3.5.3 ‰∏çÂêåÁöÑ KL ‰º∞ËÆ°Èáè\n  \n  \n  4 ÊµÅË°å on-policy KL ‰ºòÂåñÂÆûÁé∞ÁöÑÊï∞Â≠¶ÂΩ¢ÂºèÂåñ\n  \n  4.1 ÂàÜÊûêÊµÅË°åÁöÑ ‚ÄúKL loss È°π‚Äù ÂÆûÁé∞\n  \n  4.1.1 ‰∏çÂêå KL ‰º∞ËÆ°ÈáèÂØπÂ∫îÁöÑ loss È°πÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÁöÑ‰∏ÄËà¨ÂΩ¢Âºè\n  4.1.2 \\(k_1\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÔºöÊúüÊúõ‰∏∫ 0\n  4.1.3 \\(k_2\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶\n  4.1.4 \\(k_3\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶\n  4.1.5 Â∞èÁªìÔºöÊµÅË°åÁöÑ ‚ÄùKL loss È°π‚Äú ÂÆûÁé∞Âπ∂‰∏çÂêàÁêÜ\n  \n  4.2 ÂàÜÊûêÊµÅË°åÁöÑ ‚ÄúKL reward È°π‚Äú ÂÆûÁé∞\n  \n  4.2.1 Á±ªÊØî PG ‰ºòÂåñ reward Êù•ÂàÜÊûê KL reward ÁöÑ‰ΩúÁî®\n  4.2.2 ‰∏çÂêå KL ‰º∞ËÆ°ÈáèÂØºÂá∫ÁöÑ reward È°πÁöÑ‰ΩúÁî®\n  4.2.3 Â∞èÁªìÔºöÂú® on-policy ËÆæÁΩÆ‰∏ã‰øÆÊ≠£ GRPO ÁõÆÊ†áÁöÑ KL È°π\n  \n  \n  5 Êé®ÂØº on-policy ËÆæÁΩÆ‰∏ã KL Êï£Â∫¶ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°\n  \n  5.1 Âú®Â∑≤Áü•ÁéØÂ¢É‰∏≠ÁÆÄÂåñ KL Ê¢ØÂ∫¶‰º∞ËÆ°\n  5.2 ÁÆÄÂÜô‰∏∫ Contextual Bandit\n  5.3 ËøòÂéü‰∏∫Â∑≤Áü•ÁéØÂ¢ÉÂÜ≥Á≠ñËøáÁ®ã\n  5.4 Âà©Áî®Âõ†ÊûúÊÄßÊäÄÂ∑ßÂåñÁÆÄ KL Ê¢ØÂ∫¶‰º∞ËÆ°\n  5.5 KL Ê¢ØÂ∫¶‰ºòÂåñÂèØ‰ª•ÂÆûÁé∞‰∏∫ KL Ê†∑Êú¨ÂÄº reward\n  \n  6 off-policy ËÆæÁΩÆ‰∏ãÂ¶Ç‰Ωï‰º∞ËÆ° KL Êï£Â∫¶ÁöÑÊ¢ØÂ∫¶\n  \n  6.1 ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞ÂøΩÁï•‰∫Ü off-policy ÈóÆÈ¢ò\n  \n  6.1.1 TRL\n  6.1.2 OpenRLHF\n  6.1.3 verl\n  \n  6.2 Âà©Áî®ÈáçË¶ÅÊÄßÈááÊ†∑Â§ÑÁêÜ off-policy ËÆæÁΩÆ\n  \n  7 ÁªìËÆ∫ÔºöÂ¶Ç‰ΩïÊ≠£Á°ÆÂú∞Âú® RL ‰∏≠‰ºòÂåñ KL Êï£Â∫¶\n  \n  7.1 ‰øÆÊ≠£ GRPO ÂÖ¨Âºè‰∏≠ÁöÑ KL È°π\n  7.2 ‰øÆÊ≠£ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞\n  \n  8 ËÆ®ËÆ∫\n  \n  8.1 ÂØπ‰∫é KL Ê¢ØÂ∫¶Êõ¥Â•ΩÁöÑ‰º∞ËÆ°Ê†∑Êú¨Èáè\n  8.2 KL-Regularized RL ÁöÑÁêÜËÆ∫‰ºòÂäø\n  \n  9 ÈôÑÂΩï\n  \n  9.1 Áõ∏ÂÖ≥Â∑•‰Ωú\n  9.2 ÂÜô‰ΩúÂ•ëÊú∫Ôºö‚ÄúTRPO/PPO ‰∏é GRPO ‰∏≠ÁöÑ KL ‰∏∫‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑Ôºü‚Äù\n  9.3 Ëá¥Ë∞¢\n  9.4 ÂºïÁî®"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#trlkl-reward-È°π",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#trlkl-reward-È°π",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "2.1 TRLÔºöKL reward È°π",
    "text": "2.1 TRLÔºöKL reward È°π\nTRL ËÆ°ÁÆó KL ÂÆö‰πâ‰∏≠ÁöÑÊ†∑Êú¨ÂÄº \\(\\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,t})}{\\pi_{\\theta_{\\text{ref}}}(a_{i,t} \\mid s_{i,t})}\\)ÔºåÂπ∂Â∞ÜÂÖ∂‰ªé reward ‰∏≠ÂáèÂéª„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†1„ÄÇ\n\n\n\nListing¬†1: TRL ËÆ°ÁÆó KL Ê†∑Êú¨ÂÄº \\(\\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,t})}{\\pi_{\\theta_{\\text{ref}}}(a_{i,t} \\mid s_{i,t})}\\) Âπ∂‰ªé reward ‰∏≠ÂáèÂéª5\n\n\n# 4. compute rewards\nkl = logprobs - ref_logprobs\nnon_score_reward = -args.kl_coef * kl\nrewards = non_score_reward.clone()\n# ...\nrewards[[actual_start, actual_end]] += scores\n\n\n\nËøôÂèØËÉΩ‰ºöÂºïËµ∑ÁñëÊÉëÔºö‰∏∫‰ªÄ‰πàË¶ÅÂ∞Ü KL Ê†∑Êú¨ÂÄº‰ªé reward ‰∏≠ÂáèÂéªÔºüÊàë‰ª¨ÂÖàÂ∞ÜÂØπÊ≠§ÁöÑËÆ®ËÆ∫Êé®ËøüÂà∞ Section¬†2.4„ÄÇ\n\n\n\nhttps://github.com/huggingface/trl‚Ü©Ô∏é\nhttps://github.com/OpenRLHF/OpenRLHF‚Ü©Ô∏é\nhttps://github.com/volcengine/verl‚Ü©Ô∏é\nhttps://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L500-506‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#openrlhf",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#openrlhf",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "2.2 OpenRLHF",
    "text": "2.2 OpenRLHF\n\n2.2.1 KL reward È°π\n‰∏é TRL Á±ª‰ººÔºåOpenRLHF ÊîØÊåÅËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÔºåÂπ∂‰ªé reward ‰∏≠ÂáèÂéªÔºå‰ΩÜÊèê‰æõ‰∫ÜÂ§öÁßçËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÁöÑÊñπÊ≥ï„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†2„ÄÇ\n\n\n\nListing¬†2: OpenRLHF ÊîØÊåÅËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÂπ∂‰ªé reward ‰∏≠ÂáèÂéª 6\n\n\ndef compute_approx_kl(\n    log_probs: torch.Tensor,\n    log_probs_base: torch.Tensor,\n    action_mask: Optional[torch.Tensor] = None,\n    kl_estimator: str = \"k1\",\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the approximate KL divergence between two distributions.\n    Schulman blog: http://joschu.net/blog/kl-approx.html\n\n    Args:\n        log_probs: Log probabilities of the new distribution.\n        log_probs_base: Log probabilities of the base distribution.\n        action_mask: Mask for actions.\n    \"\"\"\n\n    if kl_estimator == \"k1\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n\n    # The $k_2$ estimator is the non negative kl approximation in\n    # http://joschu.net/blog/kl-approx.html\n    # The k2_loss is approximately equivalent to the\n    # one-step KL divergence penalty with the $k_1$ estimator\n    # used in https://arxiv.org/abs/2310.10505.\n    if kl_estimator == \"k2\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n        log_ratio = log_ratio**2 / 2.0\n\n    # The $k_3$ estimator is the non negative kl approximation in\n    # http://joschu.net/blog/kl-approx.html\n    if kl_estimator == \"k3\":\n        log_ratio = log_probs.float() - log_probs_base.float()\n        if action_mask is not None:\n            log_ratio = log_ratio * action_mask\n        log_ratio = -log_ratio\n        log_ratio = log_ratio.exp() - 1 - log_ratio\n\n    return log_ratio\n\n\ndef compute_reward(\n    # ...\n    kl_coef: float,\n    kl: Union[torch.Tensor, list[torch.Tensor]],\n    # ...\n    num_actions: Optional[Union[int, list[int]]] = None,\n    # ...\n) -&gt; Union[torch.Tensor, list[torch.Tensor]]:\n    # ...\n    if action_mask is not None:\n        # ...\n    else:\n        # ...\n        reward = []\n        for i, (kl_seg, action_len) in enumerate(zip(kl, num_actions)):\n            kl_reward = -kl_coef * kl_seg\n            kl_reward[action_len - 1] += r[i]\n            reward.append(kl_reward)\n\n    return reward\n\n\n\n\n\n\nhttps://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/models/utils.py#L7-L88‚Ü©Ô∏é\n\n\n\n\n2.2.2 KL loss È°π\nÊ≠§Â§ñÔºåOpenRLHF ËøòÊîØÊåÅËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÔºåÂÖàÂØπÂ∫èÂàóÂÜÖÈÉ®ÁöÑ token ËÆ°ÁÆóÂùáÂÄºÔºåÂÜçÂú®Â∫èÂàó‰πãÈó¥ËÆ°ÁÆóÂùáÂÄºÔºåÂπ∂Âä†ÂÖ•Âà∞ loss ‰∏≠„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†3„ÄÇ\n\n\n\nListing¬†3: OpenRLHF ÊîØÊåÅËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÔºåÂÖàÂØπÂ∫èÂàóÂÜÖÈÉ®ÁöÑ token ËÆ°ÁÆóÂùáÂÄºÔºåÂÜçÂú®Â∫èÂàó‰πãÈó¥ËÆ°ÁÆóÂùáÂÄºÔºåÂπ∂Âä†ÂÖ•Âà∞ loss ‰∏≠ 7\n\n\ndef training_step_actor(self, experience: Experience) -&gt; Dict[str, float]:\n    self.actor.train()\n    # ...\n    if isinstance(experience.sequences, list):\n        # ...\n    else:\n        sequences = experience.sequences\n        old_action_log_probs = experience.action_log_probs\n        advantages = experience.advantages\n        num_actions = experience.action_mask.size(1)\n        packed_seq_lens = None\n        attention_mask = experience.attention_mask\n        if self.args.use_kl_loss and experience.base_action_log_probs is not None:\n            base_action_log_probs = experience.base_action_log_probs\n\n    # actor loss\n    action_log_probs, output = self.actor(\n        sequences,\n        num_actions,\n        # ...\n    )\n    # ...\n    # loss function\n    actor_loss = self.actor_loss_fn(\n        action_log_probs,\n        old_action_log_probs,\n        advantages,\n        # ...\n    )\n\n    if self.args.use_kl_loss:\n        if self.initial_model is not None:\n            kl = compute_approx_kl(\n                action_log_probs,\n                base_action_log_probs,\n                # ...\n                kl_estimator=self.args.kl_estimator,\n            )\n        else:\n            kl = torch.zeros_like(action_log_probs, dtype=action_log_probs.dtype, device=action_log_probs.device)\n\n        if not self.args.packing_samples:\n            kl_mean = masked_mean(kl, experience.action_mask, dim=-1)\n        else:\n            # ...\n\n        kl_loss = kl_mean.mean()\n        experience.info[\"kl\"] = kl_loss.item()\n    else:\n        kl_loss = 0\n    # ...\n    self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name=\"actor\")\n    # ...\n\n\n\n\n\n\nhttps://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_trainer.py#L337-L470‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#verl",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#verl",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "2.3 verl",
    "text": "2.3 verl\n\n2.3.1 KL reward È°π\nverl ÂêåÊ†∑ÊîØÊåÅËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÂπ∂‰ªé reward ‰∏≠ÂáèÂéª„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†4„ÄÇ\n\n\n\nListing¬†4: verl Â∞Ü KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄº‰ªé reward ‰∏≠ÂáèÂéª 8\n\n\ndef apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):\n    # ...\n    # compute kl between ref_policy and current policy\n    if 'ref_log_prob' in data.batch.keys():\n        kld = core_algos.kl_penalty(data.batch['old_log_probs'], data.batch['ref_log_prob'],\n                                    kl_penalty=kl_penalty)  # (batch_size, response_length)\n        kld = kld * response_mask\n        beta = kl_ctrl.value\n    else:\n        beta = 0\n        kld = torch.zeros_like(response_mask, dtype=torch.float32)\n\n    token_level_rewards = token_level_scores - beta * kld\n    # ...\n\n\n\n\n\n\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/ray_trainer.py#L131-L160‚Ü©Ô∏é\n\n\n\n\n2.3.2 KL loss È°π\nverl ‰πüÊîØÊåÅËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÔºåÂØπÊâÄÊúâ token ËÆ°ÁÆóÂùáÂÄºÔºåÂπ∂Âä†ÂÖ•Âà∞ loss ‰∏≠„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†5„ÄÇ\n\n\n\nListing¬†5: verl ËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÂÄºÔºåÂØπÊâÄÊúâ token ËÆ°ÁÆóÂùáÂÄºÔºåÂπ∂Âä†ÂÖ•Âà∞ loss ‰∏≠ 9\n\n\ndef update_policy(self, data: DataProto):\n    # make sure we are in training mode\n    self.actor_module.train()\n    # ...\n    for epoch in range(self.config.ppo_epochs):\n        for batch_idx, data in enumerate(dataloader):\n            # ...\n            self.actor_optimizer.zero_grad()\n\n            for data in micro_batches:\n                # ...\n                responses = data['responses']\n                # ...\n                old_log_prob = data['old_log_probs']\n                # ...\n\n                # all return: (bsz, response_length)\n                entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature)\n\n                pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=old_log_prob,\n                                                                                log_prob=log_prob,\n                                                                                # ...\n                                                                                )\n                # ...\n\n                # compute policy loss\n                policy_loss = pg_loss - entropy_loss * entropy_coeff\n\n                if self.config.use_kl_loss:\n                    ref_log_prob = data['ref_log_prob']\n                    # compute kl loss\n                    kld = core_algos.kl_penalty(logprob=log_prob,\n                                                ref_logprob=ref_log_prob,\n                                                kl_penalty=self.config.kl_loss_type)\n                    kl_loss = masked_mean(kld, response_mask)\n\n                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef\n                # ...\n                loss.backward()\n            # ...\n            grad_norm = self._optimizer_step()\n    # ...\n    self.actor_optimizer.zero_grad()\n    # ...\n\n\n\n\n\n\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/workers/actor/dp_actor.py#L226-L327‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-why-kl-reward",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-why-kl-reward",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "2.4 ‰∏∫‰ªÄ‰πàË¶ÅÂ∞Ü KL ‰ªé reward ‰∏≠ÂáèÂéª",
    "text": "2.4 ‰∏∫‰ªÄ‰πàË¶ÅÂ∞Ü KL ‰ªé reward ‰∏≠ÂáèÂéª\nÂ∞Ü KL ‰ªé reward ‰∏≠ÂáèÂéªÁöÑÂÅöÊ≥ïÂ∫îÂΩì‰∏ªË¶ÅÂèÇËÄÉÁöÑÊòØ OpenAI Ê≠£ÂºèÊèêÂá∫ RLHF ÁöÑËÆ∫Êñá InstructGPT (Ouyang et al. 2022)„ÄÇ\n\n2.4.1 KL reward ÁöÑÊµÅË°åÂ∫îÂΩìÊ∫êËá™ RLHF ‰∏é InstructGPT\nInstructGPT ËÆ∫Êñá‰∏≠ÊèêÂà∞ÂÖ∂Âêë reward Ê∑ªÂä†‰∫ÜÁõ∏ÂØπ‰∫é SFT Ê®°ÂûãÁöÑ KL ÊÉ©ÁΩöÈ°πÔºå‰ΩÜÂπ∂Ê≤°ÊúâÊèêÂà∞‰∏∫‰ªÄ‰πàÂ∞Ü KL ÊîæÂú® reward ËÄåÈùû loss ‰∏≠„ÄÇ\n\n‚Ä¶ In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models ‚ÄúPPO.‚Äù\n‚Ä¶\n\n\\[\n\\begin{aligned}\n\\text { objective }(\\phi)= & E_{(x, y) \\sim D_\\pi^{\\mathrm{RL}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\\n& \\gamma E_{x \\sim D_{\\text {remin }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\n\\end{aligned}\n\\]\n\nwhere \\(\\pi_\\phi^{\\mathrm{RL}}\\)is the learned RL policy,\\(\\pi^{\\mathrm{SFT}}\\) is the supervised trained model, and\\(D_{\\text {pretrain }}\\)is the pretraining distribution. The KL reward coefficient, \\(\\beta\\), and the pretraining loss coefficient, \\(\\gamma\\), control the strength of the KL penalty and pretraining gradients respectively. For ‚ÄúPPO‚Äù models, \\(\\gamma\\) is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.\n\n\n\n2.4.2 OpenAI ËÆ∫Êñá‰∏≠ KL reward ÁöÑÂá∫Â§Ñ\nÁÑ∂ËÄåÔºåÂú®OpenAI Êó©ÊúüÁöÑ‰∏ÄÁØáËÆ∫Êñá ‚ÄúLearning to summarize from human feedback‚Äù (Stiennon et al. 2020) ‰∏≠Ôºå‰ªñ‰ª¨Â∞±Â∑≤ÁªèÈááÁî®‰∫Ü KL rewardÔºåÂπ∂ÊèêÂèä‰∫ÜÂá∫Â§ÑÔºö\n\n‚Ä¶ Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy \\(\\pi_\\phi^{\\mathrm{RL}}\\) with parameters \\(\\phi\\) and this original supervised model \\(\\pi^{\\mathrm{SFT}}\\), as previously done in [25]. The full reward \\(R\\) can be written as:\n\n\\[\nR(x, y)=r_\\theta(x, y)-\\beta \\log \\left[\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right]\n\\]\n\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collaPsing to a single mode. Second, it ensures the policy doesn‚Äôt learn to produce outputs that are too different from those that the reward model has seen during training.\n\n\n\n2.4.3 KL reward ÊúÄÊó©ÁöÑÂá∫Â§Ñ\nSection¬†2.4.2 ‰∏≠ OpenAI ÂºïÁî®ÁöÑ KL reward Âá∫Â§Ñ [25] ÊòØ ‚ÄúWay Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog‚Äù (Jaques et al. 2019)„ÄÇ\nÂÆûÈôÖ‰∏äÔºåÂÖ∂‰∏≠ÂºïÂÖ• KL Êï£Â∫¶Êó∂ÔºåÊúÄÂàùÁöÑÂΩ¢ÂºèÊòØ loss È°πÔºåËÄåÈùû reward È°πÔºå‰ΩÜÂÖ∂ÊåáÂá∫‰∫Ü‰∏§ËÄÖÁöÑÁ≠â‰ª∑ÊÄßÔºö\n\nRather than simply sample from the prior, we would like the \\(Q\\)-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior \\(p(y \\mid x)\\), and the \\(Q\\)-network policy \\(\\pi_\\theta\\), while still maximizing reward. Given a trajectory of actions, \\(\\tau=\\left\\{a_1, a_2, \\ldots a_{t-1}\\right\\}\\), let \\(q(\\tau)=\\prod_{t=1}^T \\pi_\\theta\\left(a_t, s_t\\right)\\)be the policy of our\\(Q\\)-learning algorithm at the trajectory level. Similarly, let \\(p(\\tau)=\\prod_{t=1}^T p\\left(a_t \\mid s_t\\right)\\)be the prior distribution over the trajectory, and\\(r(\\tau)\\) be the rewards. We seek to maximize the following KL-regularized objective:\n\n\\[\nL(q)=\\mathbb{E}_{q(\\tau)}[r(\\tau)] / c-D_{\\text{KL}}[q(\\tau) \\mid p(\\tau)]\n\\]\n\nSince \\(D_{\\text{KL}}[q \\mid p]=\\sum_x q(x)(\\log q(x)-\\log p(x))\\), we can see that this is equivalent to maximizing the following expected value function of the policy \\(\\pi_\\theta\\) at the action level:\n\n\\[\nQ^\\pi\\left(s_t, a_t\\right)=\\mathbb{E}_\\pi\\left[\\sum^T r\\left(s_{t^{\\prime}}, a_{t^{\\prime}}\\right) / c+\\log p\\left(a_{t^{\\prime}} \\mid s_{t^{\\prime}}\\right)-\\log \\pi\\left(a_{t^{\\prime}} \\mid s_{t^{\\prime}}\\right)\\right]\n\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#rl-‰∏≠ÁöÑ-kl-Êï£Â∫¶ÈÄöÂ∏∏ÂÆö‰πâÂú®ËΩ®ËøπÂàÜÂ∏É‰∏ä",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#rl-‰∏≠ÁöÑ-kl-Êï£Â∫¶ÈÄöÂ∏∏ÂÆö‰πâÂú®ËΩ®ËøπÂàÜÂ∏É‰∏ä",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "3.1 RL ‰∏≠ÁöÑ KL Êï£Â∫¶ÈÄöÂ∏∏ÂÆö‰πâÂú®ËΩ®ËøπÂàÜÂ∏É‰∏ä",
    "text": "3.1 RL ‰∏≠ÁöÑ KL Êï£Â∫¶ÈÄöÂ∏∏ÂÆö‰πâÂú®ËΩ®ËøπÂàÜÂ∏É‰∏ä\nGRPO ÂÖ¨Âºè (Equation¬†1) ‰∏≠ÁöÑ KL È°πÂèØ‰ª•ÂÆö‰πâ‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & =\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)}\\right]\n\\end{aligned}\n\\tag{3}\\]\nÂÖ∂‰∏≠ \\(\\mathbf{\\tau}\\) ÊòØË°®Á§∫ËΩ®ËøπÔºàTrajectoryÔºâÁöÑÈöèÊú∫ÂèòÈáè„ÄÇÊ≥®ÊÑèÔºå‰∏éÁ≠ñÁï•Ê¢ØÂ∫¶ÔºàPolicy GradientÔºåPGÔºâ‰ºòÂåñËΩ®ËøπÂàÜÂ∏É‰∏äÂ•ñÂä±ÁöÑÊúüÊúõÁ±ª‰ººÔºåÊàë‰ª¨ÂêåÊ†∑Â∏åÊúõÂú®ËΩ®ËøπÂàÜÂ∏É‰∏ä‰ºòÂåñÊúÄÊñ∞Á≠ñÁï•Êï¥‰ΩìÂàÜÂ∏É \\(p_{\\theta}\\) ‰∏éÂèÇËÄÉÁ≠ñÁï•Êï¥‰ΩìÂàÜÂ∏É \\(p_{\\text{ref}}\\) ÁöÑ KL Êï£Â∫¶„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#Â∞ÜËΩ®ËøπÂ±ïÂºÄ‰∏∫Áä∂ÊÄÅ-Âä®‰ΩúÂ∫èÂàó",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#Â∞ÜËΩ®ËøπÂ±ïÂºÄ‰∏∫Áä∂ÊÄÅ-Âä®‰ΩúÂ∫èÂàó",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "3.2 Â∞ÜËΩ®ËøπÂ±ïÂºÄ‰∏∫Áä∂ÊÄÅ-Âä®‰ΩúÂ∫èÂàó",
    "text": "3.2 Â∞ÜËΩ®ËøπÂ±ïÂºÄ‰∏∫Áä∂ÊÄÅ-Âä®‰ΩúÂ∫èÂàó\nRL ÊñáÁåÆ‰∏≠ÈÄöÂ∏∏‰ºöÂ∞ÜËΩ®Ëøπ \\(\\mathbf{\\tau}\\) Â±ïÂºÄ‰∏∫Áä∂ÊÄÅ-Âä®‰ΩúÂ∫èÂàó \\(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\)Ôºö10\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & =\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|},\\right) \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|},, \\mathbf{a}_{|\\mathbf{\\tau}|},\\right)}{p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\log \\frac{p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)}{p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\right] \\\\\n\\end{aligned}\n\\tag{4}\\]\nÂÖ∂‰∏≠ \\(|\\mathbf{\\tau}|\\) ‰∏∫ËΩ®ËøπÂä®‰ΩúÊï∞ÁöÑÈöèÊú∫ÂèòÈáè„ÄÇ\nÊ≠§Â§ÑÂà©Áî®‰∫ÜËÅîÂêàÊ¶ÇÁéáÁöÑÂ±ïÂºÄÔºå‰ª• \\(p_{\\theta}\\) ‰∏∫‰æãÔºö\n\\[\np_{\\theta}(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) = p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t)\n\\tag{5}\\]\nÊ≥®ÊÑèÂå∫ÂàÜÊï¥‰ΩìÊ¶ÇÁéáÂàÜÂ∏É \\(p_{\\theta}\\)„ÄÅÁ≠ñÁï•ÔºàÊù°‰ª∂ÔºâÊ¶ÇÁéáÂàÜÂ∏É \\(\\pi_{\\theta}\\) ‰∏éÁä∂ÊÄÅËΩ¨ÁßªÊ¶ÇÁéáÂàÜÂ∏É \\(p\\)„ÄÇ\n\n\n\nËøôÈáåÊàë‰ª¨Á¶ªÂºÄ‰∫Ü GRPO ÁöÑÁ¨¶Âè∑Á≥ªÁªüÔºåÊç¢Áî®‰∫Ü RL ÊñáÁåÆ‰∏≠Êõ¥Â∏∏ËßÅÁöÑÁä∂ÊÄÅ-Âä®‰ΩúÁ¨¶Âè∑Á≥ªÁªü„ÄÇÂÆûÈôÖ‰∏äÔºå\\(\\mathbf{q}\\) ÂØπÂ∫î‰∫é \\(\\mathbf{s}_1\\)ÔºåËÄå \\({\\mathbf{o}}\\) ÂØπÂ∫î‰∫é \\(\\mathbf{\\mathbf{a}_1, \\cdots, \\mathbf{s}_T, \\mathbf{a}_T}\\)„ÄÇ‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#markov-ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÁöÑ-kl-Êï£Â∫¶",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#markov-ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÁöÑ-kl-Êï£Â∫¶",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "3.3 Markov ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÁöÑ KL Êï£Â∫¶",
    "text": "3.3 Markov ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÁöÑ KL Êï£Â∫¶\nÂÆûÈôÖ‰∏äÔºåRL ÊñáÁåÆ‰∏≠ËøòÁªèÂ∏∏Â∞ÜÂ∫èÂàóÂÜ≥Á≠ñËøáÁ®ãÂª∫Ê®°‰∏∫‰∏ÄÈò∂ Markov ÂÜ≥Á≠ñËøáÁ®ãÔºàMarkov Decision Process, MDP11„ÄÇ\nMarkov ÂÜ≥Á≠ñËøáÁ®ãË¶ÅÊ±ÇÂ∫èÂàó‰∏≠ÁöÑÊù°‰ª∂Ê¶ÇÁéáÊª°Ë∂≥ Markov ÊÄßË¥®ÔºåÂç≥Âè™‰æùËµñ‰∫éÊúÄÊñ∞ÁöÑ \\(n\\) ‰∏™ÂéÜÂè≤Áä∂ÊÄÅÂíåÂä®‰ΩúÔºåËÄåÈùûÂÖ®ÈÉ®ÁöÑÂéÜÂè≤‰ø°ÊÅØÔºåÂØπÂ∫îÁöÑËøáÁ®ãÁß∞‰∏∫ \\(n\\) Èò∂ Markov ËøáÁ®ã„ÄÇ‰ª• \\(n=1\\) ‰∏∫‰æãÔºö\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) & = \\pi(\\mathbf{a}_t \\mid \\mathbf{s}_t) \\\\\np(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) & = p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t) \\\\\n\\end{aligned}\n\\tag{6}\\]\nÂàô Equation¬†5 ‰∏≠ÁöÑËÅîÂêàÊ¶ÇÁéáÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÁÆÄÂåñ‰∏∫Ôºö\n\\[\np(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) = p(s_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_t) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t)\n\\tag{7}\\]\nÂ¶ÇÊûúËÄÉËôë‰∏ÄÈò∂ Markov ËøáÁ®ãÔºåÂàô Equation¬†4 ‰∏≠ÁöÑ KL ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÁÆÄÂåñ‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] = & = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\right] \\\\\n& = \\mathbb{E}_{\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}\\right) \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_t)}\\right] \\\\\n\\end{aligned}\n\\tag{8}\\]\n\n\n\nhttps://www.wikiwand.com/zh-cn/articles/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-lm-as-dp",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-lm-as-dp",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "3.4 ËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Â∫èÂàóÂÜ≥Á≠ñËøáÁ®ã",
    "text": "3.4 ËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Â∫èÂàóÂÜ≥Á≠ñËøáÁ®ã\nÁõÆÂâçÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàLanguage Model, LMÔºâÈÄöÂ∏∏Âª∫Ê®°‰∏∫Ëá™ÂõûÂΩíÊ®°ÂûãÔºåÂç≥ÂΩìÂâç token ÁöÑÁîüÊàê‰æùËµñ‰∫éÊâÄÊúâ‰πãÂâçÁöÑ token„ÄÇ\nÂ∞ΩÁÆ°ÂàùÁúãËµ∑Êù•ÔºåËá™ÂõûÂΩíÊ®°Âûã‰ºº‰πéÊó†Ê≥ïÊª°Ë∂≥ Markov ÊÄßË¥®Ôºå‰ΩÜÂÆûÈôÖ‰∏äÊàë‰ª¨‰πüÂèØ‰ª•Â∞ÜËá™ÂõûÂΩíÊ®°ÂûãÂª∫Ê®°‰∏∫‰∏ÄÈò∂ Markov ËøáÁ®ã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºö‰ª§ \\(s_1\\) Ë°®Á§∫ prompt ‰∏≠ÁöÑÊâÄÊúâ tokenÔºåÂØπ‰∫é \\(t &gt;1\\)ÔºåÂ¶ÇÊûú‰ª§ \\(s_t\\) Ë°®Á§∫Á¨¨ \\(t\\) ‰∏™Âä®‰Ωú token ÂâçÁöÑÊâÄÊúâ tokenÔºåÂàôËá™ÂõûÂΩíÊ®°ÂûãÊª°Ë∂≥ Markov ÊÄßË¥®ÔºåÂê¶Âàô‰∏ç‰∏ÄÂÆö„ÄÇ\nÊé•‰∏ãÊù•ÔºåÊàë‰ª¨ÂÖà‰ª§ \\(s_t\\) Ë°®Á§∫Ââç \\(t\\) ‰∏™ token ÁªÑÊàêÁöÑÂ∫èÂàóÔºåÂç≥‰∏ç‰æùËµñ‰∫é Markov ÊÄßË¥®ÁªßÁª≠Êé®ÂØºÔºå‰ª•Ëé∑ÂæóÂ∞ΩÂèØËÉΩÈÄöÁî®ÁöÑÁªìËÆ∫„ÄÇÂú®ÂøÖË¶ÅÊó∂ÔºåÊàë‰ª¨‰ºöÂÜçÂºïÂÖ• Markov ÊÄßË¥®„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#‰º∞ËÆ°-kl-Êï£Â∫¶",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#‰º∞ËÆ°-kl-Êï£Â∫¶",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "3.5 ‰º∞ËÆ° KL Êï£Â∫¶",
    "text": "3.5 ‰º∞ËÆ° KL Êï£Â∫¶\n\n3.5.1 Âá†‰πé‰∏çÂèØËÉΩÁõ¥Êé•ËÆ°ÁÆó KL Êï£Â∫¶ÁöÑÁúüÂÆûÂÄº\nÂÆûÈôÖÂÆûÁé∞‰∏≠ÔºåÊàë‰ª¨Âá†‰πé‰∏çÂèØËÉΩÁõ¥Êé•ËÆ°ÁÆóÂá∫ \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\)ÔºåÂõ†‰∏∫ RL ‰∏≠ÁöÑ KL Êï£Â∫¶ÂÆö‰πâË¶ÅÂØπËΩ®ËøπÁ©∫Èó¥Ê±ÇÂùáÂÄºÔºåËÄåËΩ®ËøπÁ©∫Èó¥ÁöÑÂ§ßÂ∞è \\(\\left|\\mathcal{T}\\right|\\) ‰∏éËΩ®ËøπÊúÄÂ§ßÈïøÂ∫¶ \\(T = \\max_{\\mathbf{\\tau} \\in \\mathcal{T}} |\\mathbf{\\tau}|\\) ÊàêÊåáÊï∞ÂÖ≥Á≥ªÔºö \\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_t \\mid  \\mathbf{s}_1, \\mathbf{a}_1, \\cdots,\\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots,\\mathbf{s}_t)}\\right] \\\\\n& = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta} (\\mathbf{\\tau}) \\left(\\sum_{t=1}^{|\\tau|} \\log \\frac{\\pi_{\\theta}(a_t \\mid  s_1, a_1, \\cdots, s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_1, a_1, \\cdots, s_t)}\\right) \\\\\n\\end{aligned}\n\\tag{9}\\]\n\n\n3.5.2 ÈÄöÂ∏∏‰ΩøÁî® Monte Carlo ÊñπÊ≥ï‰º∞ËÆ° KL Êï£Â∫¶\nÊâÄ‰ª•ÔºåÊàë‰ª¨ÈÄöÂ∏∏Âü∫‰∫éËã•Âπ≤ËΩ®ËøπÊ†∑Êú¨‰ΩøÁî® Monte Carlo ÊñπÊ≥ï12Êù•‰º∞ËÆ° RL ‰∏≠ÁöÑ KL Êï£Â∫¶Ôºå‰æãÂ¶ÇÔºö\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta} (\\mathbf{\\tau}) \\left(\\sum_{t=1}^{|\\tau|} \\log \\frac{\\pi_{\\theta}(a_t \\mid  s_1, a_1, \\cdots, s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_1, a_1, \\cdots, s_t)}\\right) \\\\\n& \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\sum_{t=1}^{|\\mathbf{\\tau_{i }}|} \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}\\right)\n\\end{aligned}\n\\tag{10}\\]\nÂÖ∂‰∏≠Ôºå\\(\\mathbf{\\tau_{i}} = \\left(\\mathbf{s}_{i,1}, \\mathbf{a}_{i,1}, \\cdots, \\mathbf{s}_{i,|\\mathbf{\\tau_{i}}|}, \\mathbf{a}_{i,|\\mathbf{\\tau_{i}}|}\\right) \\sim p_{\\theta}\\)Ôºå\\(N\\) ‰∏∫‰º∞ËÆ°‰ΩøÁî®ÁöÑËΩ®ËøπÊ†∑Êú¨Êï∞Èáè„ÄÇ\n\n\n\nhttps://www.wikiwand.com/zh-hans/articles/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95‚Ü©Ô∏é\n\n\n\n\n3.5.3 ‰∏çÂêåÁöÑ KL ‰º∞ËÆ°Èáè\nÂÆûÈôÖ‰∏äÔºåMonte Carlo ÊñπÊ≥ïÂÖÅËÆ∏‰ΩøÁî®Ê†∑Êú¨ÂØºÂá∫ÁöÑ‰∏çÂêå‰º∞ËÆ°ÈáèÔºåËÄå‰∏çÂøÖÊòØÁªüËÆ°ÈáèÂÆö‰πâ‰∏≠ÁöÑÊ†∑Êú¨Èáè„ÄÇ‰∏çÂêåÁöÑ‰º∞ËÆ°ÈáèÊúâ‰∏çÂêåÁöÑÂÅèÂ∑ÆÔºàBiasÔºâÂíåÊñπÂ∑ÆÔºàVarianceÔºâÔºå‰ªéËÄåÊûÑÊàê‰∫Ü‰º∞ËÆ°ÈáèÈÄâÊã©‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ\nËÆæ KL ‰º∞ËÆ°Èáè‰∏∫ \\(k\\)ÔºåÂàôÂØπÂ∫îÁöÑ KL ‰º∞ËÆ°ÂÄº‰∏∫\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N} k(\\tau_i)\n\\end{aligned}\n\\tag{11}\\]\n‰æãÂ¶Ç Section¬†2.2.1 ÊèêÂà∞ÔºåOpenRLHF ÂºïÂÖ•‰∫Ü 3 Áßç KL Êï£Â∫¶ÁöÑ‰º∞ËÆ°ÊñπÊ≥ïÔºåÂàÜÂà´Áß∞‰∏∫ k1, k2, k3ÔºåËøôÂ∫îËØ•ÊòØ‰∏ªË¶ÅÂèÇËÄÉ‰∫Ü John Schulman ÁöÑÂçöÂÆ¢ ‚ÄúApproximating KL Divergence‚Äù„ÄÇ\nverl ÂàôËÄÉËôë‰∫ÜÊõ¥Â§ö‰º∞ËÆ°ÊñπÊ≥ï„ÄÇÂÆûÈôÖ‰∏äÔºåverl ËøòËÄÉËôë‰∫ÜÁõ¥Êé•ËÆ°ÁÆóÊù°‰ª∂ KL Êï£Â∫¶13Ôºå‰ΩÜÁõÆÂâçËøòÊ≤°ÊúâÂÆûÁé∞„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†6„ÄÇ\n\n\n\nListing¬†6: verl ÁöÑ KL Êï£Â∫¶ Monte Carlo ‰º∞ËÆ°Ê†∑Êú¨ÂÄº14\n\n\ndef kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty) -&gt; torch.FloatTensor:\n    # ...\n    if kl_penalty == \"kl\":\n        return logprob - ref_logprob\n\n    if kl_penalty == \"abs\":\n        return (logprob - ref_logprob).abs()\n\n    if kl_penalty == \"mse\":\n        return 0.5 * (logprob - ref_logprob).square()\n\n    # J. Schulman. Approximating kl divergence, 2020.\n    # # URL http://joschu.net/blog/kl-approx.html.\n    if kl_penalty == 'low_var_kl':\n        kl = ref_logprob - logprob\n        ratio = torch.exp(kl)\n        kld = (ratio - kl - 1).contiguous()\n        return torch.clamp(kld, min=-10, max=10)\n\n    if kl_penalty == \"full\":\n        # so, here logprob and ref_logprob should contain the logits for every token in vocabulary\n        raise NotImplementedError\n\n    raise NotImplementedError\n\n\n\nÁî±‰∫é \\(k_1\\)„ÄÅ\\(k_2\\)„ÄÅ\\(k_3\\) ‰∏âÁßç‰º∞ËÆ°ÈáèÊúÄ‰∏∫ÊµÅË°åÔºåÊàë‰ª¨Â∞Ü‰ª•Ëøô‰∏âÁßç‰º∞ËÆ°Èáè‰∏∫‰æãÂ±ïÂºÄÂàÜÊûê„ÄÇ\nËÄÉËôë \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} k_j(\\tau_i)\\)ÔºåÂÖ∂‰∏≠ \\(\\tau_i \\sim p_{\\theta}\\)Ôºå‰ª§ \\(r = \\frac{\\pi_{\\text{ref}}(\\tau_i)}{\\pi_{\\theta}(\\tau_i)}\\)ÔºåÊ≥®ÊÑèÔºåÊ≠§Â§Ñ \\(r\\) Âπ∂Èùû KL ÂÆö‰πâ‰∏≠ÁöÑÊ†∑Êú¨ÈáèÔºåËÄåÊòØÂÖ∂ÂÄíÊï∞ÔºåÂàôÔºö\n\\[\n\\begin{aligned}\nk_{1} & = - \\log r \\\\\nk_{2} & = \\frac{1}{2} (\\log r)^2 \\\\\nk_{3} & = (r - 1) - \\log r\n\\end{aligned}\n\\tag{12}\\]\n\n\n\nËøôÈáåÁöÑÊù°‰ª∂ KL Êï£Â∫¶Âè™ÈúÄË¶ÅÈÅçÂéÜÊï¥‰∏™ËØçË°®Ôºå‰ª£‰ª∑ÂèØËÉΩÊòØÂèØ‰ª•Êé•ÂèóÁöÑ„ÄÇ‚Ü©Ô∏é\nhttps://github.com/volcengine/verl/blob/f8acd9017b4db4eead1f34beb39fce9c39143194/verl/trainer/ppo/core_algos.py#L351-L383‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-loss-impl",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-loss-impl",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "4.1 ÂàÜÊûêÊµÅË°åÁöÑ ‚ÄúKL loss È°π‚Äù ÂÆûÁé∞",
    "text": "4.1 ÂàÜÊûêÊµÅË°åÁöÑ ‚ÄúKL loss È°π‚Äù ÂÆûÁé∞\n‰∏äËø∞Ê°ÜÊû∂‰∏≠ÔºåOpenRLHF ‰∏é verl ÈÉΩÂÆûÁé∞‰∫Ü ‚ÄúKL loss È°π‚ÄùÔºåÂç≥ÂÖàÁõ¥Êé•ËÆ°ÁÆóÂá∫ KL ‰º∞ËÆ°ÈáèÂπ∂Âä†ÂÖ•Âà∞ loss ‰∏≠ÔºåÂÜçÂèçÂêë‰º†Êí≠ÂæóÂà∞Ê¢ØÂ∫¶ÔºåÊúüÈó¥ÈªòËÆ§Ê≤°ÊúâÂéªÈô§Ê¢ØÂ∫¶„ÄÇ\nÁÑ∂ËÄåÔºåÂ¶Ç Section¬†1 ÊâÄËø∞ÔºåËøô‰∏ÄÂÅöÊ≥ïÊòØÈîôËØØÁöÑÔºåÊé•‰∏ãÊù•Êàë‰ª¨Â∞ÜÈÄöËøáÂàÜÊûêËøô‰∫õ ‚ÄúKL loss È°π‚Äù ÂÆûÈôÖÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÔºåËØ¥ÊòéÂÖ∂ÈîôËØØ‰πãÂ§Ñ„ÄÇ\n\n4.1.1 ‰∏çÂêå KL ‰º∞ËÆ°ÈáèÂØπÂ∫îÁöÑ loss È°πÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÁöÑ‰∏ÄËà¨ÂΩ¢Âºè\nËßÇÂØü Listing¬†3 ËÆ°ÁÆó ‚ÄúKL loss‚Äù È°πÁöÑÈÉ®ÂàÜ„ÄÇ\n# ...\nkl = compute_approx_kl(\n    action_log_probs,\n    base_action_log_probs,\n    # ...\n    kl_estimator=self.args.kl_estimator,\n)\n# ...\nkl_mean = masked_mean(kl, experience.action_mask, dim=-1)\n# ...\nkl_loss = kl_mean.mean()\n# ...\nËøô‰∫õ‰ª£Á†ÅÔºö\n\nËÆ°ÁÆó‰∫Ü klÔºåÂØπÂ∫îÂØπÊØè‰∏™Âä®‰Ωú token \\(a_{i,t}\\) ËÆ°ÁÆó ‚ÄúKL ‰º∞ËÆ°Èáè‚Äù \\(k\\)„ÄÇ\nËÆ°ÁÆó‰∫Ü kl_meanÔºåÂØπÂ∫îÂØπÊØè‰∏™ËΩ®Ëøπ \\(\\tau_i\\) ËÆ°ÁÆóÂùáÂÄº \\(\\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|} k\\)„ÄÇ\nËÆ°ÁÆó‰∫Ü kl_lossÔºåÂØπÂ∫îÂØπÊâÄÊúâËΩ®ËøπÊ†∑Êú¨ËÆ°ÁÆóÂùáÂÄº \\(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|} k\\)„ÄÇ\n\nÁî±‰∫éÂÖ∂Ê≤°ÊúâÂéªÈô§‰ªª‰ΩïÊ¢ØÂ∫¶ÔºåÂõ†Ê≠§ÂÖ∂ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÂÄº‰∏∫\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\frac{1}{|\\tau_i|} k \\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|}  \\nabla_{\\theta} k\n\\end{aligned}\n\\tag{13}\\]\nListing¬†5 ‰∏≠ verl ÁöÑÂÆûÁé∞Á±ª‰ººÔºå‰ΩÜ‰∏çÂêåÁöÑÊòØÂÖ∂Âπ≥ÂùáÊòØÂú®ÊâÄÊúâ token ‰πãÈó¥ÊâßË°åÁöÑÔºåÂõ†Ê≠§ÂØπÂ∫îÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÂÄº‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\left( \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|} \\sum_{i=1}^{N} k \\right) = \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|} \\sum_{i=1}^{N} \\nabla_{\\theta} k\n\\end{aligned}\n\\tag{14}\\]\nÊàë‰ª¨Â∞ÜÂπ≥ÂùáÊìç‰Ωú‰∏ÄËà¨Âåñ‰∏∫ÊùÉÈáç \\(w_{\\mathbf{\\tau}}\\) ‰∏é \\(w_{t}\\)ÔºåÂàô‰∏çÂêå KL ‰º∞ËÆ°ÈáèÂØπÂ∫îÁöÑ loss È°πÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÂÄºÁöÑ‰∏ÄËà¨ÂΩ¢Âºè‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{N} w_{\\mathbf{\\tau}_i} \\sum_{t=1}^{|\\tau_i|} w_{t} \\nabla_{\\theta} k \\\\\n\\end{aligned}\n\\tag{15}\\]\nÂàô\n\nOpenRLHF ÂØπÂ∫î \\(w_{\\mathbf{\\tau}} = \\frac{1}{N}, w_{t} = \\frac{1}{|\\tau|}\\)Ôºõ\nverl ÂØπÂ∫î \\(w_{\\mathbf{\\tau}} = \\frac{1}{\\sum_{i=1}^{N} |\\tau_i|}, w_{t} = 1\\)„ÄÇ\n\nÊ≠§Â§ÑÔºåÊàë‰ª¨ÂÖà‰ª• OpenRLHF ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ° (Equation¬†13) ‰∏∫‰æãÔºåÂàÜÊûê‰∏çÂêå KL ‰º∞ËÆ°ÈáèÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÔºåÂÖ∂Êª°Ë∂≥Ôºö\n\\[\n\\mathbb{E}_{\\mathbf{\\tau}_i \\sim p_{\\theta}} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|\\tau_i|} \\sum_{t=1}^{|\\tau_i|}  \\nabla_{\\theta} k \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} k \\right]\n\\tag{16}\\]\nÊàë‰ª¨‰ºöÂú® Section¬†5 ‰∏≠Êé®ÂØºÊ≠£Á°ÆÁöÑ KL Ê¢ØÂ∫¶‰º∞ËÆ°„ÄÇ\n\n\n4.1.2 \\(k_1\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÔºöÊúüÊúõ‰∏∫ 0\nÂêë Equation¬†16 ‰ª£ÂÖ• \\(k = k_1 = - \\log r = \\log \\frac{1}{r} = \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\)ÔºåÂØºÂá∫ÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°‰∏∫\n\\[\n\\begin{aligned}\n& \\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta}\\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\left( \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) + \\nabla_{\\theta} \\log \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t}) + \\nabla_{\\theta} \\log \\left( p(\\mathbf{s}_{1}) \\right) \\right) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\left( p(\\mathbf{s}_{1}) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t}) \\right) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_\\theta(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\\\\n=&\\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_{\\theta}(\\tau)\n\\end{aligned}\n\\tag{17}\\]\nÂàôÂÖ∂ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÊúüÊúõÊª°Ë∂≥Ôºö\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\\right]\n& = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\frac{1}{|\\tau|} \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n& = \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n& = \\sum_{\\tau \\in \\mathcal{T}} \\frac{1}{|\\tau|} \\nabla_{\\theta} p_{\\theta}(\\tau) \\\\\n& = \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\frac{1}{|\\tau|} \\\\\n& = \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\right]\n\\end{aligned}\n\\tag{18}\\]\nÊ≠§Â§ÑÂà©Áî®‰∫Ü \\(p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\frac{1}{p_{\\theta}(\\tau)} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} p_{\\theta}(\\tau)\\)„ÄÇ\nÊâÄ‰ª• \\(k_1\\) loss È°π‰ºòÂåñÁöÑÈáèÊòØ \\(\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\frac{1}{|\\mathbf{\\tau}|} \\right]\\)„ÄÇËøôÊÑèÂë≥ÁùÄËØ•‰ºòÂåñËøáÁ®ã‰ºöÈôç‰ΩéÈááÊ†∑ËΩ®ËøπÁöÑÈïøÂ∫¶„ÄÇ\nÁâπÂà´Âú∞ÔºåÂΩì‰∏çÂØπÂêå‰∏ÄËΩ®Ëøπ‰∏≠ÁöÑ ‚Äú\\(k_1\\) ‰º∞ËÆ°Èáè‚ÄùÊ±ÇÂùáÂÄºÔºåËÄåÊòØÊ±ÇÂíåÊó∂ÔºåÂèØ‰ª•Áõ¥Êé•Â∞Ü \\(\\frac{1}{|\\tau|}\\) Ëøô‰∏ÄÈ°πÊõøÊç¢‰∏∫ \\(1\\)ÔºåÂæóÂà∞ \\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) = \\sum_{\\tau \\in \\mathcal{T}} \\nabla_{\\theta} p_{\\theta} = \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta} = \\nabla_{\\theta} 1 = 0\n\\tag{19}\\]15\nËøôÊÑèÂë≥ÁùÄ‰ΩøÁî®ËØ•Ê¢ØÂ∫¶Êõ¥Êñ∞ÂèÇÊï∞ÔºåÂú®Âπ≥ÂùáÊÑè‰πâ‰∏ä‰∏ç‰ºöÂºïËµ∑ÂèÇÊï∞ÂèäÂÖ∂ÂØºÂá∫ÁöÑÂàÜÂ∏ÉÊîπÂèò„ÄÇ\nÊó†ËÆ∫Âì™ÁßçÊÉÖÂÜµÔºå\\(k_1\\) ÂØºÂá∫ÁöÑ‰ºòÂåñÈáèÈÉΩÈùûÂ∏∏Â•áÊÄ™Ôºå‰∏çÂ§™ÂèØËÉΩÂá∫‰∫éÂÆûÁé∞ËÄÖÁöÑÊú¨ÊÑè„ÄÇ\nÂêåÊó∂ÔºåÂØπÂêå‰∏ÄËΩ®Ëøπ‰∏≠ÁöÑ KL ‰º∞ËÆ°ÈáèÊ±ÇÂùáÂÄºËøô‰∏ÄÊìç‰ΩúÔºå‰πüÂæàÊúâÂèØËÉΩÊòØÈîôËØØÁöÑ„ÄÇÊé•‰∏ãÊù•ÔºåÊàë‰ª¨Â∞ÜÂøΩÁï•Ëøô‰∏ÄÊìç‰ΩúÔºåÂç≥Â∞Ü \\(\\frac{1}{|\\tau|}\\) ‰∏ÄÈ°πÊõøÊç¢‰∏∫ \\(1\\)„ÄÇ\n\n\n\nÊ≠§Â§ÑÂØπÊï∞‰ººÁÑ∂ÁöÑÊ¢ØÂ∫¶ÁöÑÊúüÊúõÂÄº‰∏∫ 0ÔºåÊòØ‰∏Ä‰∏™ËëóÂêçÁöÑÊÄßË¥®Ôºå‰ºöÂú®Êé•‰∏ãÊù•È¢ëÁπÅÁî®Âà∞„ÄÇ‚Ü©Ô∏é\n\n\n\n\n4.1.3 \\(k_2\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶\nÂêë Equation¬†16 ‰ª£ÂÖ• \\(k = k_2 = \\frac{1}{2} (\\log r)^2 = \\frac{1}{2} \\left(\\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\right)^2\\)ÔºåÂØºÂá∫ÁöÑÂçïÊù°ËΩ®Ëøπ \\(\\mathbf{\\tau} \\sim p_{\\theta}\\) ÁöÑÊ¢ØÂ∫¶‰∏∫ \\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k\\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta}  \\frac{1}{2} \\left(\\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}\\right)^2 \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\frac{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\\\\n\\end{aligned}\n\\tag{20}\\]\nÊòæÁÑ∂Ôºå\n\\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left( \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\\\\n\\neq & \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\log \\frac{\\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})}{\\pi_{\\text{ref}}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t})} \\right) \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} \\mid s_{i,1}, a_{i,1}, \\cdots, s_{i,t}) \\right) \\\\\n=& \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau})\n\\end{aligned}\n\\tag{21}\\]\nÁÑ∂ËÄåÔºå\n\\[\n\\begin{aligned}\n& \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[  \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} \\right) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} \\right) \\nabla_{\\theta} p_{\\theta}(\\tau) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} \\left[ \\left( \\log p_{\\theta}(\\tau) \\right) \\nabla_{\\theta} p_{\\theta}(\\tau) - \\left( \\log p_{\\text{ref}}(\\tau) \\right) \\nabla_{\\theta} p_{\\theta}(\\tau) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}}  \\left[ \\nabla_{\\theta} (\\log p_{\\theta}(\\tau) - 1) p_{\\theta}(\\tau) -  \\nabla_{\\theta} \\log p_{\\text{ref}}(\\tau) p_{\\theta}(\\tau) \\right] \\\\\n=& \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}}  \\left[ (\\log p_{\\theta}(\\tau) - 1) p_{\\theta}(\\tau) - \\log p_{\\text{ref}}(\\tau) p_{\\theta}(\\tau) \\right] \\\\\n=& \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}}  p_{\\theta} \\left[ \\left( \\log \\frac{p_{\\theta}(\\tau)}{p_{\\text{ref}}(\\tau)} - 1 \\right) \\right] \\\\\n=& \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[  \\left( \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} - 1 \\right) \\right] \\\\\n= & \\nabla_{\\theta} \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[  \\log \\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\text{ref}}(\\mathbf{\\tau})} \\right] \\\\\n= & \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\n\\end{aligned}\n\\tag{22}\\]\nÊ≠§Â§ÑÂà©Áî®‰∫Ü \\(\\log p(x) \\nabla_{\\theta} p(x) = \\nabla_{\\theta} (\\log p(x) - 1) p(x)\\)\nÂõ†Ê≠§ÔºåÊúÄÂ∞èÂåñ \\(k_2\\) loss È°π (Equation¬†21) ÔºåÂπ∂ÈùûÂú®‰ºòÂåñ \\(\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\)„ÄÇ\n\n\n4.1.4 \\(k_3\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶\nÂêë Equation¬†16 ‰ª£ÂÖ• \\(k = k_3 = (r - 1) - \\log r = (\\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} - 1) - \\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\)ÔºåÂØºÂá∫ÁöÑÂçïÊù°ËΩ®Ëøπ \\(\\mathbf{\\tau} \\sim p_{\\theta}\\) ÁöÑÊ¢ØÂ∫¶‰∏∫ \\[\n\\begin{aligned}\n& \\sum_{t=1}^{|\\mathbf{\\tau}|}  \\nabla_{\\theta} k \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} \\nabla_{\\theta} \\left(\\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} - 1 - \\log \\frac{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}\\right) \\\\\n=& \\sum_{t=1}^{|\\mathbf{\\tau}|} - \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) - \\nabla_{\\theta} \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\\\\n=& - \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\right) - \\nabla_{\\theta} \\log \\frac{p_{\\text{ref}}(\\mathbf{\\tau})}{p_{\\theta}(\\mathbf{\\tau})} \\\\\n=& - \\left( \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{ \\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})}{\\pi_{\\theta}^{2}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t})} \\nabla_{\\theta}  \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\right) + \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\\\\n\\end{aligned}\n\\tag{23}\\]\nÂÖ∂‰∏≠ÔºåÊ†πÊçÆ Equation¬†19Ôºå\\(\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(\\mathbf{\\tau}) \\right] = 0\\)Ôºå‰∏çÂ¶®Áõ¥Êé•ÁúÅÁï•„ÄÇ\nËÄåÂâ©‰ΩôÈÉ®ÂàÜ‰ºº‰πéÂæàÈöæÈÄöËøáÊ∂àÂéª \\(\\pi_{\\theta}(\\mathbf{\\tau})\\) Êù•ÊèêÂá∫ \\(\\nabla_{\\theta}\\) Âπ∂ÂáÜÁ°ÆÂàÜÊûê„ÄÇ‰ΩÜÊòæÁÑ∂‰πüÂπ∂ÈùûÂú®‰ºòÂåñ KL Êï£Â∫¶„ÄÇ\n\n\n4.1.5 Â∞èÁªìÔºöÊµÅË°åÁöÑ ‚ÄùKL loss È°π‚Äú ÂÆûÁé∞Âπ∂‰∏çÂêàÁêÜ\nÁªº‰∏äÊâÄËø∞ÔºåÂØπ‰∫é OpenRLHF ÂÆûÁé∞ÁöÑ ‚ÄúKL loss È°π‚ÄùÔºå\n\nÂØπÂêå‰∏ÄËΩ®ËøπÂÜÖÁöÑ ‚ÄúKL ‰º∞ËÆ°Èáè‚Äù Ê±ÇÂùáÂÄºËøô‰∏ÄÊìç‰ΩúÂæàÂèØËÉΩÊòØÈîôËØØÁöÑÔºåÊ≠£Á°ÆÊìç‰ΩúÂ∫îÂΩì‰∏∫Ê±ÇÂíåÔºåÂØπÂ∫î‰∫éÊ†πÊçÆÂØπÊï∞Êù°‰ª∂Ê¶ÇÁéáÊ±ÇÂØπÊï∞ËÅîÂêàÊ¶ÇÁéá„ÄÇ\n\\(k_1\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶\n\nËã•ÂØπÂêå‰∏ÄËΩ®ËøπÂÜÖÁöÑ ‚ÄúKL ‰º∞ËÆ°Èáè‚Äù Ê±ÇÂùáÂÄºÔºåÂàô‰ºöÂØºËá¥ËæìÂá∫ÈïøÂ∫¶ÂáèÂ∞èÔºå\nËÄåÂ¶ÇÊûú‰øÆÊ≠£‰∏∫Ê±ÇÂíåÔºåÂàôÂÖ∂ÊúüÊúõ‰∏∫ 0ÔºåÂú®Âπ≥ÂùáÊÑè‰πâ‰∏ä‰∏çÊîπÂàÜÂ∏É„ÄÇ\n\n\\(k_2\\)Ôºå\\(k_3\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÂàôÂçÅÂàÜÂ§çÊùÇÔºåÈöæ‰ª•ÂàÜÊûêÔºå‰ΩÜÈÉΩÂπ∂ÈùûÂú®‰ºòÂåñ KL Êï£Â∫¶ÔºåËøôÂèØËÉΩÊòØÂõ†‰∏∫ÂÖ∂ÈîôËØØÂú∞Â∞Ü KL ‰º∞ËÆ°Ê†∑Êú¨ÈáèÂ∫îÁî®‰∫éÂä®‰ΩúÂØπÊï∞Êù°‰ª∂‰ººÁÑ∂Âπ∂Ê±ÇÂíå„ÄÇÂõûÈ°æ KL ‰º∞ËÆ°ÈáèÂÖ¨Âºè (Equation¬†12) ÔºåÂ∫îÂΩìÊ≥®ÊÑèÂà∞Ëøô‰∫õ‰º∞ËÆ°ÈáèÊòØÁõ¥Êé•‰ΩúÁî®‰∫é‰ººÁÑ∂ \\(p_{\\theta}(\\mathbf{\\tau})\\)ÔºåËÄåÊ≤°Êúâ‰øùËØÅ‰ΩúÁî®‰∫éÊ¶ÇÁéáÂêéÊ±ÇÁßØ/ÂØπÊï∞Âíå‰ªçÁÑ∂ÊúâÊÑè‰πâ„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂàÜÊûêÊµÅË°åÁöÑ-kl-reward-È°π-ÂÆûÁé∞",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂàÜÊûêÊµÅË°åÁöÑ-kl-reward-È°π-ÂÆûÁé∞",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "4.2 ÂàÜÊûêÊµÅË°åÁöÑ ‚ÄúKL reward È°π‚Äú ÂÆûÁé∞",
    "text": "4.2 ÂàÜÊûêÊµÅË°åÁöÑ ‚ÄúKL reward È°π‚Äú ÂÆûÁé∞\n\n4.2.1 Á±ªÊØî PG ‰ºòÂåñ reward Êù•ÂàÜÊûê KL reward ÁöÑ‰ΩúÁî®\nÁî±‰∫é PG ‰ºòÂåñÁöÑÂ∞±ÊòØ rewardÔºåÂõ†Ê≠§Êàë‰ª¨‰∏çÂ¶®‰ªé PG ÁöÑ‰º∞ËÆ°Âá∫Âèë„ÄÇÊúÄÂ∏∏Áî®ÁöÑ PG ‰º∞ËÆ°ÊñπÂºèÂ∫îÂΩìÊòØÔºö \\[\n\\nabla_\\theta \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[r(\\mathbf{\\tau})\\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right) \\hat{A}_t \\right]\n\\tag{24}\\]\nÂÖ∂‰∏≠ \\(\\hat{A}_t\\) ‰∏∫‰ºòÂäøÔºàAdvantageÔºâÁöÑ‰º∞ËÆ°Èáè„ÄÇ\n‰∏∫‰∫ÜÊñπ‰æøËßÇÂØü KL reward È°πÂèëÊå•ÁöÑ‰ΩúÁî®ÔºåÊàë‰ª¨Â∞Ü \\(r_{\\mathbf{\\tau}}\\) Â±ïÂºÄÔºåÂπ∂‰∏çÂ¶®ËÄÉËôë‰∏Ä‰∏™Êõ¥ÁÆÄÂçïÁöÑ‰º∞ËÆ°Ôºå‰æãÂ¶ÇÔºö\n\\[\n\\nabla_\\theta \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} r(\\mathbf{s}_t, \\mathbf{a}_t) \\right] = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right) \\sum_{t'=1}^{|\\tau|} r(s_{t'}, a_{t'}) \\right]\n\\tag{25}\\]\nÁÆÄÊ¥ÅËµ∑ËßÅÔºåËøôÈáåÁúÅÁï•‰∫ÜËØ•‰º∞ËÆ°ÊñπÂºèÊ≠£Á°ÆÊÄßÁöÑËØÅÊòéÔºåÊúâÂÖ¥Ë∂£ÁöÑËØªËÄÖÂèØ‰ª•ÂèÇËÄÉ UCB CS285 ‚ÄúPolicy Gradient‚Äù ‰∏ÄËÆ≤16„ÄÇ\nÁ±ªÊØî \\(r_{t'}\\) ÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÊúüÊúõÔºåÂ∞ÜË¥üÁöÑ KL Ê†∑Êú¨Èáè \\(- \\log \\frac{\\pi_\\theta\\left(a_t \\mid s_t \\right)}{\\pi_{\\text{ref}}\\left(a_t \\mid s_t \\right)}\\) Âä†ÂÖ• reward \\(r_{t'}\\) ‰ª£ÂÖ•ÂÖ∂‰∏≠ÔºåÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÊúüÊúõ‰∏∫Ôºö\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\tau|}  \\left( \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t \\right) \\right) \\sum_{t'=1}^{|\\tau|} - \\log \\frac{\\pi_\\theta\\left(a_{t'} \\mid s_{t'} \\right)}{\\pi_{\\text{ref}}\\left(a_{t'} \\mid s_{t'} \\right)} \\right] = \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}\\right]\n\\tag{26}\\]\nÊ≥®ÊÑèÔºå‰ª•‰∏äÊé®ÂØºÂÅáËÆæ RL ‰ºòÂåñÁöÑÂ∫èÂàóÂÜ≥Á≠ñËøáÁ®ãÊª°Ë∂≥‰∏ÄÈò∂ Markov ÊÄßË¥® (Equation¬†6)„ÄÇ\nÂÆûÈôÖ‰∏äÔºåËøòÂèØ‰ª•Êâ©Â±ïÂà∞‰ªªÊÑèÂ∫èÂàóÂÜ≥Á≠ñËøáÁ®ãÔºåÂç≥Ë¶ÅÊ±ÇÊù°‰ª∂Ê¶ÇÁéá‰æùËµñ‰∫éÊâÄÊúâÂéÜÂè≤Áä∂ÊÄÅÂíåÂä®‰ΩúÔºåÂàôÂØπÂ∫îÁöÑ KL Ê¢ØÂ∫¶ÊúüÊúõ‰∏∫Ôºö\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta}- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t \\right)} \\right] \\\\\n\\to& \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right] \\\\\n= & \\nabla_{\\theta} -  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{\\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{ \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{ p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) }{ p(\\mathbf{s}_1) \\prod_{t=1}^{|\\mathbf{\\tau}|} \\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right) \\prod_{t=1}^{|\\mathbf{\\tau}|-1} p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t) } \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  \\log \\frac{ p_\\theta\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}  \\right)}{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}  \\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta} \\left[ \\log \\frac{p_{\\theta}\\left(\\mathbf{\\tau}\\right)}{p_{\\text{ref}}\\left(\\mathbf{\\tau}\\right)} \\right] \\\\\n= & \\nabla_{\\theta} - \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n\\end{aligned}\n\\tag{27}\\]\nÂèØËßÅÔºåËÆ°ÁÆó KL Ê†∑Êú¨ÈáèÂπ∂ÊîæÂÖ• reward ‰∏≠ÔºåÂØºÂá∫ÁöÑÊ¢ØÂ∫¶ÊúüÊúõÂç≥‰∏∫‰∏§‰∏™ÂàÜÂ∏ÉÁöÑ KL Êï£Â∫¶ÁöÑË¥üÊ¢ØÂ∫¶ÔºåÂàôÊúÄÂ§ßÂåñ rewardÔºåÂ∞±‰ºöÊúÄÂ∞èÂåñ KL Êï£Â∫¶ÔºåÊòØÊ≠£Á°ÆÁöÑÂÅöÊ≥ï„ÄÇ\n\n\n\nhttps://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-5.pdf‚Ü©Ô∏é\n\n\n\n\n4.2.2 ‰∏çÂêå KL ‰º∞ËÆ°ÈáèÂØºÂá∫ÁöÑ reward È°πÁöÑ‰ΩúÁî®\n‰∏çÈöæÊ≥®ÊÑèÂà∞ÔºåSection¬†4.2.1 ‰∏≠ÁöÑ KL Ê†∑Êú¨ÈáèÂØπÂ∫î‰∫é \\(k_1\\) ‰º∞ËÆ°Èáè„ÄÇ\n‰∏Ä‰∏™Ëá™ÁÑ∂ÁöÑÈóÆÈ¢òÊòØÔºåÂ¶ÇÊûúÂØπÂä®‰ΩúÊù°‰ª∂‰ººÁÑ∂‰ΩøÁî® \\(k_2\\) Êàñ \\(k_3\\) Á≠âÂÖ∂‰ªñ‰º∞ËÆ°ÈáèÔºå‰ºöÂæóÂà∞‰ªÄ‰πàÁªìÊûúÔºü\n\\(k_2\\) Êàñ \\(k_3\\) Á≠âÂÖ∂‰ªñ‰º∞ËÆ°ÈáèÂØºËá¥ÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊ±ÇÂíåÊó∂ÈÄöÂ∏∏Êó†Ê≥ïÂæóÂà∞ËÅîÂêàÊ¶ÇÁéá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÖ∂‰ªñ‰º∞ËÆ°ÈáèÂàÜÂà´Âú®‰ºòÂåñ\n\n\\(k_2\\): \\(- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} \\frac{1}{2} \\left( \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} \\right)^{2} \\right]\\)\n\\(k_3\\): \\(- \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[ \\sum_{t=1}^{|\\mathbf{\\tau}|} (\\frac{\\pi_{\\text{ref}} \\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\theta}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)} - 1 - \\log \\frac{\\pi_{\\text{ref}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}{\\pi_{\\theta}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t \\right)}) \\right]\\)\n\nÊòæÁÑ∂ÔºåËøôÈáåÁöÑÊ±ÇÂíåÊó†Ê≥ïÂæóÂà∞ËÅîÂêàÊ¶ÇÁéáÔºå‰πüÂ∞±Êó†Ê≥ïÂÆûÁé∞Á±ª‰ºº Equation¬†27 ‰∏≠ÁöÑÊïàÊûú‰∫Ü„ÄÇ\n\n\n4.2.3 Â∞èÁªìÔºöÂú® on-policy ËÆæÁΩÆ‰∏ã‰øÆÊ≠£ GRPO ÁõÆÊ†áÁöÑ KL È°π\nËã•ÂØπÂä®‰ΩúÂØπÊï∞Êù°‰ª∂‰ººÁÑ∂ËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÈáèÔºåÂàôÁî±‰∫éÊ∂âÂèäÂà∞Ê±ÇÂíåÔºå\\(k_1\\) ‰πãÂ§ñÁöÑ‰º∞ËÆ°ÈáèÈÄöÂ∏∏Ê≤°ÊúâËâØÂ•ΩÂÆö‰πâ„ÄÇ\n‰ΩÜÊòØËã•ÊîæÂºÉÂØπÂä®‰ΩúÊù°‰ª∂‰ººÁÑ∂ËÆ°ÁÆó KL ‰º∞ËÆ°Ê†∑Êú¨ÈáèÔºåËÄåÊòØÂØπÊ±ÇÂíå‰πãÂêéÁöÑÂØπÊï∞ÔºàÊù°‰ª∂Ôºâ‰ººÁÑ∂ËøõË°åËÆ°ÁÆóÔºåÂàôÂè™ÈúÄÊª°Ë∂≥\n\\[\n\\nabla_{\\theta} - \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[  k\\left(\\frac{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}\\right) \\right]\n\\approx \\nabla_{\\theta} - \\frac{1}{N} k\\left(\\frac{ p_{\\text{ref}}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}{ p_{\\theta}\\left(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t, \\mathbf{a}_t  \\right)}\\right)\n\\approx \\nabla_{\\theta} - \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\n\\tag{28}\\]\nÊöÇÊó∂‰∏çËÄÉËôë off-policy ÈóÆÈ¢òÔºåÊ†πÊçÆ Equation¬†28, GRPO ÂÖ¨Âºè (Equation¬†1, Equation¬†2) Â∫îÂΩì‰øÆÊ≠£ KL È°πÂ¶Ç‰∏ãÔºö\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\left\\{ \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|} \\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]  \\right\\}  -\\beta k\\left( \\frac{\\prod_{t=1}^{|o_i|} \\pi_{\\text{ref}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\prod_{t=1}^{|o_i|} \\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\right)\n\\end{aligned}\n\\tag{29}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#Âú®Â∑≤Áü•ÁéØÂ¢É‰∏≠ÁÆÄÂåñ-kl-Ê¢ØÂ∫¶‰º∞ËÆ°",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#Âú®Â∑≤Áü•ÁéØÂ¢É‰∏≠ÁÆÄÂåñ-kl-Ê¢ØÂ∫¶‰º∞ËÆ°",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "5.1 Âú®Â∑≤Áü•ÁéØÂ¢É‰∏≠ÁÆÄÂåñ KL Ê¢ØÂ∫¶‰º∞ËÆ°",
    "text": "5.1 Âú®Â∑≤Áü•ÁéØÂ¢É‰∏≠ÁÆÄÂåñ KL Ê¢ØÂ∫¶‰º∞ËÆ°\nÂÆûÈôÖ‰∏äÔºåLLM ÁöÑËÆ∏Â§ö‰ªªÂä°‰∏≠ÔºåÁéØÂ¢É‰∏≠ÁöÑÁä∂ÊÄÅËΩ¨ÁßªÊ¶ÇÁéáÂàÜÂ∏ÉÂùá‰∏∫Â∑≤Áü•ÁöÑÔºåÊúâÊó∂ËøòÂèØËÉΩÊòØÁ°ÆÂÆöÊÄßÁöÑÔºàDeterministicÔºâ„ÄÇ\nÂΩìÁä∂ÊÄÅËΩ¨ÁßªÊ¶ÇÁéáÂàÜÂ∏ÉÂ∑≤Áü•Êó∂Ôºå\\(\\forall t, p_{\\theta}(a_1, \\cdots, s_t, a_t \\mid s_1)\\) ÈÉΩÊòØÂèØ‰ª•ËÆ°ÁÆóÁöÑÔºåÂàô KL Êï£Â∫¶ÂèØ‰ª•Áõ¥Êé•ÂÜôÊàêÔºö\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\sum_{\\mathbf{\\tau} \\in \\mathcal{T}} p(\\mathbf{s}_1) p_{\\theta}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1) \\log \\frac{p_{\\theta}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1)}{p_{\\text{ref}}(\\mathbf{a}_1, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|} \\mid \\mathbf{s}_1)}  \\\\\n\\end{aligned}\n\\tag{32}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÁÆÄÂÜô‰∏∫-contextual-bandit",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÁÆÄÂÜô‰∏∫-contextual-bandit",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "5.2 ÁÆÄÂÜô‰∏∫ Contextual Bandit",
    "text": "5.2 ÁÆÄÂÜô‰∏∫ Contextual Bandit\n‰∏∫‰∫ÜÊñπ‰æø‰π¶ÂÜôÔºåÊàë‰ª¨ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â∞ÜÊ®°ÂûãÁÆÄÂåñ‰∏∫ contextual banditÔºåÂç≥‰ª§ \\(\\mathbf{s}_1 = \\mathbf{x} \\in \\mathcal{P}, (\\mathbf{a}_1, \\cdots, \\mathbf{s}_T, \\mathbf{a}_T) = \\mathbf{y} \\in \\mathcal{R}\\)ÔºåÂÖ∂‰∏≠ \\(\\mathcal{P}, \\mathcal{R}\\) ÂàÜÂà´Ë°®Á§∫ prompt / response Á©∫Èó¥ÔºåÂàô KL Êï£Â∫¶Âèò‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p_{\\theta}}\\left[\\log \\frac{\\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})}{\\pi_{\\text{ref}}(\\mathbf{y} \\mid \\mathbf{x})}\\right] \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p_{\\theta}(x, y) \\left(\\sum_{t=1}^{T} \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\n\\end{aligned}\n\\tag{33}\\]\nÂÖ∂Ê¢ØÂ∫¶Âèò‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & = \\nabla_{\\theta} \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n& = \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\nabla_{\\theta} \\left(\\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\\right)\n\\end{aligned}\n\\tag{34}\\]\nÂÖ∂‰∏≠Ê¢ØÂ∫¶È°πÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â±ïÂºÄ‰∏∫Ôºö\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\left(\\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right)\\right) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\pi_{\\theta}(y \\mid x) \\nabla_{\\theta} \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\pi_{\\theta}(y \\mid x) \\frac{1}{\\pi_\\theta(y \\mid x)} \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\left(\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\\right) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) + \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)\n\\end{aligned}\n\\tag{35}\\]\n‰ª£ÂÖ•Âõû KL Ê¢ØÂ∫¶Ë°®ËææÂºèÔºö\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\pi_{\\theta}(y \\mid x) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)}{\\pi_{\\theta}(y \\mid x)} \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\\\\n=& \\sum_{(x, y) \\in \\mathcal{T}} p(s) \\pi_{\\theta}(y \\mid x) \\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x) \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + 1\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] + \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] \\\\\n=& \\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right]\n\\end{aligned}\n\\tag{36}\\]\nËøôÈáå‰∏∫‰∫ÜÈáçÊñ∞Ëé∑ÂæóÊúüÊúõÂΩ¢ÂºèÔºåÂºïÂÖ•‰∫Ü \\(1 = \\pi_{\\theta}(y \\mid x) / \\pi_{\\theta}(y \\mid x)\\)ÔºåÂπ∂Âà©Áî®‰∫Ü \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x) = \\frac{\\nabla_{\\theta} \\pi_{\\theta}(y \\mid x)}{\\pi_{\\theta}(y \\mid x)}\\) Âíå \\(\\mathbb{E}_{(x, y) \\sim p_{\\theta}} \\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(y \\mid x)\\right] = 0\\)„ÄÇ\nËøõË°å Monte Carlo ‰º∞ËÆ°Ôºö\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\log \\frac{\\pi_{\\theta}(y_i \\mid x_i)}{\\pi_{\\text{ref}}(y_i \\mid x_i)}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(y_i \\mid x_i)\n\\end{aligned}\n\\tag{37}\\]\nÂÖ∂‰∏≠ \\((\\mathbf{x}_i, \\mathbf{y}_i) \\sim p_{\\theta}\\)„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ËøòÂéü‰∏∫Â∑≤Áü•ÁéØÂ¢ÉÂÜ≥Á≠ñËøáÁ®ã",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ËøòÂéü‰∏∫Â∑≤Áü•ÁéØÂ¢ÉÂÜ≥Á≠ñËøáÁ®ã",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "5.3 ËøòÂéü‰∏∫Â∑≤Áü•ÁéØÂ¢ÉÂÜ≥Á≠ñËøáÁ®ã",
    "text": "5.3 ËøòÂéü‰∏∫Â∑≤Áü•ÁéØÂ¢ÉÂÜ≥Á≠ñËøáÁ®ã\nÂ∞Ü‰∏äÈù¢ÁöÑ KL Ê¢ØÂ∫¶Ë°®ËææÂºèËøòÂéü‰∏∫Â∑≤Áü•ÁéØÂ¢ÉÂÜ≥Á≠ñËøáÁ®ãÂª∫Ê®°ÁöÑÂΩ¢ÂºèÔºö\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right]\\\\\n=& \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p_{\\theta}} \\left[\\left(\\log \\frac{\\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})}{\\pi_{\\text{ref}}(\\mathbf{y} \\mid \\mathbf{x})}\\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T}) \\sim p_{\\theta}} \\left[\\left(\\sum_{t=1}^{T} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)}{\\pi_{\\text{ref}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)}\\right) \\left(\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_t)\\right)\\right]\n\\end{aligned}\n\\tag{38}\\]\nÂØπÂ∫îÁöÑ Monte Carlo ‰º∞ËÆ°Âºè‰∏∫Ôºö\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] & \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\left(\\sum_{t=1}^{T}\\log \\frac{\\pi_{\\theta}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})}{\\pi_{\\text{ref}}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})}\\right) \\left(\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} \\mid s_{1, t}, \\cdots, a_{i, t-1}, s_{i, t})\\right)\n\\end{aligned}\n\\tag{39}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#Âà©Áî®Âõ†ÊûúÊÄßÊäÄÂ∑ßÂåñÁÆÄ-kl-Ê¢ØÂ∫¶‰º∞ËÆ°",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#Âà©Áî®Âõ†ÊûúÊÄßÊäÄÂ∑ßÂåñÁÆÄ-kl-Ê¢ØÂ∫¶‰º∞ËÆ°",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "5.4 Âà©Áî®Âõ†ÊûúÊÄßÊäÄÂ∑ßÂåñÁÆÄ KL Ê¢ØÂ∫¶‰º∞ËÆ°17",
    "text": "5.4 Âà©Áî®Âõ†ÊûúÊÄßÊäÄÂ∑ßÂåñÁÆÄ KL Ê¢ØÂ∫¶‰º∞ËÆ°17\nÂõ†ÊûúÊÄßÊäÄÂ∑ßÔºàCausality TrickÔºâÊòØÂàÜÊûêÂ∫èÂàóÂÜ≥Á≠ñËøáÁ®ãÊó∂‰∏Ä‰∏™ÈùûÂ∏∏ÊúâÁî®ÁöÑÊäÄÂ∑ßÔºåÂÖ∂ÂÖÖÂàÜÂà©Áî®‰∫ÜÂõ†ÊûúÊÄß‰∏é‚ÄúÂØπÊï∞ÔºàÊù°‰ª∂Ôºâ‰ººÁÑ∂ÁöÑÊ¢ØÂ∫¶Âú®‰ººÁÑ∂ÔºàÊù°‰ª∂ÔºâÊ¶ÇÁéáÂàÜÂ∏É‰∏äÁöÑÊúüÊúõ‰∏∫ 0‚Äù Ëøô‰∏§‰∏™ÊÄßË¥®„ÄÇ\nÂØπ‰∫é‰ªª‰Ωï \\(0 \\leq t \\leq |\\tau|\\)ÔºåÊàë‰ª¨Êúâ \\[\n\\begin{aligned}\n& \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) }\\left[\\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\sum_{a_t \\in \\mathcal{A}} \\pi_\\theta(a_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\\\\n=& \\sum_{a_j \\in \\mathcal{A}} \\pi_\\theta(a_j \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_j) \\cdot 0 \\\\\n=& 0\n\\end{aligned}\n\\tag{40}\\]\nÊõ¥Ëøõ‰∏ÄÊ≠•ÔºåÂ¶ÇÊûú \\(\\mathbf{\\Psi}_{t'}\\) ÊòØ‰∏Ä‰∏™‰∏é \\(\\mathbf{a}_t, \\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}, \\ldots\\) Áã¨Á´ãÁöÑÈöèÊú∫ÂèòÈáèÔºåÈÇ£‰πà \\[\n\\begin{aligned}\n& \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{(\\mathbf{a}_t, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\mathbf{\\Psi}_{t'} \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\mathbb{E}_{\n    (\\mathbf{s}_{t+1}, \\cdots, \\mathbf{s}_{|\\mathbf{\\tau}|}, \\mathbf{a}_{|\\mathbf{\\tau}|}) \\sim p_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}, \\mathbf{a}_{t})} \\left[\\mathbf{\\Psi}_{t'} \\right] \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t}) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t} )} \\left[ \\mathbf{\\Psi}_{t'} \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\right]\n\\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[\n            \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right]\n        \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[ \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t)}\\left[\\mathbf{\\Psi}_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\right] \\\\\n=& \\mathbb{E}_{(\\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t) \\sim p_\\theta} \\left[ \\mathbf{\\Psi}_{t'} \\cdot 0 \\right] \\\\\n=& 0\n\\end{aligned}\n\\tag{41}\\]\nÂÖ∂‰∏≠Ôºå‰∏∫‰∫ÜÂà©Áî® Equation¬†40 ÁöÑÁªìËÆ∫ÔºåÊàë‰ª¨Âà©Áî®‰∫ÜÂÖ®ÊúüÊúõÂÆöÂæãÔºåÂç≥\n\\[\n\\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim p} \\left[\\mathbf{x}\\right] = \\mathbb{E}_{\\mathbf{y} \\sim p} \\left[\\mathbb{E}_{\\mathbf{x} \\sim p(\\cdot \\mid \\mathbf{y})} [\\mathbf{x}] \\right]\n\\tag{42}\\]\nÊù•ÂºïÂÖ•Êàë‰ª¨ÊÉ≥Ë¶ÅÁöÑÊúüÊúõ„ÄÇ\n\\[\n\\begin{aligned}\n& \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\mathbf{\\Psi}_i \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_1, \\mathbf{a}_1, \\cdots, \\mathbf{s}_t\\right) \\right] \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) \\Psi_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\\\\n=& \\sum_{\\tau \\in \\mathcal{T}} p_\\theta(s_1, a_1, \\cdots, s_t) \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) p_\\theta(s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\Psi_{t'} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\\\\n=& \\sum_{(s_{1}, a_{1}, \\cdots, s_{t})} p_\\theta(s_1, a_1, \\cdots, s_t)  \\sum_{(a_{t}, s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|})} \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) \\Psi_{t'} \\nabla_\\theta p_\\theta(s_{t+1}, \\cdots, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right)  \\\\\n=& \\sum_{(s_{1}, a_{1}, \\cdots, s_{t})} p_\\theta(s_1, a_1, \\cdots, s_t) \\sum_{a_t \\in \\mathcal{A}}  \\pi_\\theta(a_t \\mid s_1, a_1, \\cdots, s_t) \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_1, a_1, \\cdots, s_t\\right) \\sum_{(s_{t+1}, \\cdots, s_{|\\tau|}, a_{|\\tau|})}  p_\\theta(s_{t+1}, \\cdots, a_{|\\tau|} \\mid s_1, a_1, \\cdots, s_t, a_t) \\Psi_{t'} \\\\\n\\end{aligned}\n\\tag{43}\\]\nËÄÉËôë Monte Carlo ‰º∞ËÆ°Âºè Equation¬†39 ‰∏≠ÁöÑ‰º∞ËÆ°ÈáèÔºåÂ∞ÜÂØπÊï∞Êù°‰ª∂‰ººÁÑ∂Ê¢ØÂ∫¶ÁöÑÊ±ÇÂíåÂ±ïÂºÄÔºåËÄÉËôëÂÖ∂‰∏≠‰ªªÊÑè‰∏ÄÈ°π‰πòÁßØÁöÑÊúüÊúõÔºö\n\\[\n\\mathbb{E}_{\\mathbf{\\tau_{i}} \\sim p_{\\theta}} \\left[\n\\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})\n\\right]\n\\tag{44}\\]\nÁî±‰∫éÂ∫èÂàóÂÜ≥Á≠ñËøáÁ®ãÊª°Ë∂≥Âõ†ÊûúÊÄßÔºåÂç≥ \\(\\forall t' &lt; t\\)Ôºå\\(\\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\) Áã¨Á´ã‰∫é \\(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\)ÔºåÂàôÂèØ‰ª§ \\(\\mathbf{\\Psi}_{t'} = \\nabla_{\\theta} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t'})}\\)ÔºåÂÖ∂Áã¨Á´ã‰∫é \\(\\mathbf{s}_{i, t}, \\mathbf{a}_{i, t}, \\ldots\\)ÔºåÂà©Áî® Equation¬†43 ÁöÑÊÄßË¥®ÔºåÂàôÊúâ \\[\n\\forall t' &lt; t, \\mathbb{E}_{\\mathbf{\\tau_{i}} \\sim p_{\\theta}} \\left[\n\\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'})} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})\n\\right] = 0\n\\tag{45}\\]\nÂ∞Ü Equation¬†45 ‰ª£ÂÖ• KL Ê¢ØÂ∫¶Ë°®ËææÂºè (Equation¬†38) ÔºåÂç≥ÂèØÁÆÄÂåñÂæóÂà∞Ôºö\n\\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] =  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{T} \\left(\\sum_{t'=t}^{T} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{t}) \\right]\n\\tag{46}\\]\nÂØπÂ∫îÁöÑ Monte Carlo ‰º∞ËÆ°Âºè‰∏∫Ôºö\n\\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\left(\\sum_{t'=t}^{|\\tau_i|} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} \\mid s_{i, 1}, \\cdots, a_{i, t-1}, s_{i, t})\n\\tag{47}\\]\nÂêåÊ†∑ÔºåË¶Å‰ΩøÁî®Ëá™Âä®ÂæÆÂàÜÂú®ÂèçÂêë‰º†Êí≠Êó∂ËÆ°ÁÆóËØ•Ê¢ØÂ∫¶‰º∞ËÆ°ÂºèÔºåÊàë‰ª¨ÈúÄË¶ÅÊûÑÈÄ†ÂØπÂ∫îÁöÑ loss ÂáΩÊï∞Ôºö\n\\[\n\\mathcal{L}^{KL}_{\\theta} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_i|} \\text{nograd}\\left (\\sum_{t'=t}^{|\\tau_i|} \\log \\frac{\\pi_{\\theta}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})}{\\pi_{\\text{ref}}(a_{i, t'} \\mid s_{i, 1}, \\cdots, a_{i, t'-1}, s_{i, t'})} \\right) \\log \\pi_{\\theta}(a_{i, t} \\mid s_{i, 1}, \\cdots, a_{i, t-1}, s_{i, t})\n\\tag{48}\\]\nËøôÈáå‰πüÂèØ‰ª•ÁúãÂà∞ÔºåKL loss È°πÊ≠£Á°ÆÁöÑÂÆûÁé∞Ë¶ÅÊ±ÇÔºö\n\nÂú®Â∫èÂàóÂÜÖ token Èó¥ÔºåÂØπÂØπÊï∞Êù°‰ª∂‰ººÁÑ∂ÂÖàÊ±ÇÂíåÔºåÂæóÂà∞ KL Ê†∑Êú¨ÂÄºÔºå\nÂÜçÂú®Â∫èÂàóÈó¥Ê±ÇÂùáÂÄº„ÄÇ\n\nÂõ†Ê≠§ OpenRLHF (Equation¬†13) ‰∏é verl (Equation¬†14) ÁöÑÊùÉÈáçÈÉΩÊòØÈîôËØØÁöÑ„ÄÇ\n\n\n\nhttps://www.wikiwand.com/en/articles/Policy_gradient_method‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-grad-as-kl-reward",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#sec-kl-grad-as-kl-reward",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "5.5 KL Ê¢ØÂ∫¶‰ºòÂåñÂèØ‰ª•ÂÆûÁé∞‰∏∫ KL Ê†∑Êú¨ÂÄº reward",
    "text": "5.5 KL Ê¢ØÂ∫¶‰ºòÂåñÂèØ‰ª•ÂÆûÁé∞‰∏∫ KL Ê†∑Êú¨ÂÄº reward\nÂú® Equation¬†46 ‰∏≠Ôºå‰ª§ \\(k\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\right) = \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'-1}, \\mathbf{s}_{t'})}\\)ÔºåÂàôÊúâÔºö \\[\n\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] =  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_\\theta}\\left[\\sum_{t=1}^{T} \\left(\\sum_{t'=t}^{T} k\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t'}, \\mathbf{a}_{t'}\\right) \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{t-1}, \\mathbf{s}_{t}) \\right]\n\\tag{49}\\]\n‰∏çÈöæÊ≥®ÊÑèÂà∞ Equation¬†49 ‰∏≠ \\(k\\) ‰∏é Equation¬†25 ‰∏≠ reward \\(r\\) Âú®ÂΩ¢Âºè‰∏äÁöÑÁõ∏‰ººÊÄßÔºåËøô‰πüËß£Èáä‰∫Ü‰∏∫‰ªÄ‰πàÂÖàÂâçÁöÑÂ∑•‰ΩúË¶ÅÂ∞Ü KL Ê†∑Êú¨ÂÄºÊîæËøõ reward„ÄÇ\nÁ±ª‰ººÂú∞ÔºåÊàë‰ª¨ÂèØ‰ª•Âà©Áî® PG ÁöÑÂÖ∂‰ªñÊäÄÂ∑ßÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞èËØ•‰º∞ËÆ°ÁöÑÊñπÂ∑ÆÔºå‰æãÂ¶ÇÂáèÂéª baseline Á≠â„ÄÇÊÑüÂÖ¥Ë∂£ÁöÑËØªËÄÖÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂèÇËÄÉ UCB CS28518 Á≠âÊùêÊñô„ÄÇ\n\n\n\nhttps://rail.eecs.berkeley.edu/deeprlcourse/‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÊµÅË°å-llm-rl-Ê°ÜÊû∂‰∏≠ÁöÑ-kl-‰ºòÂåñÂÆûÁé∞ÂøΩÁï•‰∫Ü-off-policy-ÈóÆÈ¢ò",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÊµÅË°å-llm-rl-Ê°ÜÊû∂‰∏≠ÁöÑ-kl-‰ºòÂåñÂÆûÁé∞ÂøΩÁï•‰∫Ü-off-policy-ÈóÆÈ¢ò",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "6.1 ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞ÂøΩÁï•‰∫Ü off-policy ÈóÆÈ¢ò",
    "text": "6.1 ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞ÂøΩÁï•‰∫Ü off-policy ÈóÆÈ¢ò\nÈÅóÊÜæÁöÑÊòØÔºåÂØπ‰∫é KL ‰ºòÂåñÔºåGRPO Á≠âÂ∑•‰ΩúÔºå‰ª•ÂèäÁõÆÂâçÊµÅË°åÁöÑ LLM RL Ê°ÜÊû∂‰∏≠ÔºåÂåÖÊã¨ TRLÔºåÈÉΩÂøΩÁï•‰∫Ü off-policy ÈóÆÈ¢òÔºöÂØπ‰∫é \\(\\pi_\\theta \\neq \\pi_{\\theta_{\\text{old}}}\\)ÔºåÂ∞ΩÁÆ°Ê≤°ÊúâÊù•Ëá™ÊúÄÊñ∞Á≠ñÁï• \\(p_{\\theta}\\) ÁöÑÊ†∑Êú¨ÔºåÂç¥‰ªçÁÑ∂Âú®‰ΩøÁî®Âü∫‰∫é on-policy ËÆæÁΩÆÁöÑ‰ºòÂåñÊñπÂºè„ÄÇ\n\n6.1.1 TRL\nTRL Âú® Listing¬†1 ‰∏≠ËÆ°ÁÆó KL Ê†∑Êú¨ÂÄº‰ΩøÁî®ÁöÑ logprobs ÂèäÂÖ∂ÂØπÂ∫îÁöÑËΩ®ËøπÊ†∑Êú¨ÂùáÊù•Ëá™ÈááÊ†∑Á≠ñÁï• \\(\\pi_{\\theta_{\\text{old}}}\\)„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†7„ÄÇ\n\n\n\nListing¬†7: TRL ‰ΩøÁî®ÈááÊ†∑Ê†∑Êú¨Âπ∂‰ΩøÁî® \\(\\pi_{\\theta_{\\text{old}}}\\) ËÆ°ÁÆóÂØπÊï∞‰ººÁÑ∂19\n\n\nqueries = data[\"input_ids\"].to(device)\n# ...\n\nwith unwrap_model_for_generation(\n    self.model, #...\n) as unwrapped_model:\n    query_responses, logitss = batch_generation(\n        unwrapped_model.policy,\n        queries,\n        # ...\n    )\n\n\nfor i in range(0, queries.shape[0], args.local_rollout_forward_batch_size):\n    # ...\n    logits = logitss[i : i + args.local_rollout_forward_batch_size]\n    logprob = selective_log_softmax(logits, response)\n\n\n\nÊ≥®ÊÑèÔºåÂü∫‰∫é \\(\\mathbf{\\tau} \\sim \\pi_{\\theta_{\\text{old}}}\\) ËÆ°ÁÆóÁöÑ KL Ê†∑Êú¨ÂÄºÂèØ‰ª•Áî®‰∫é‰º∞ËÆ° \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_{\\theta_{\\text{old}}} \\mid \\pi_{\\text{ref}}\\right]\\)ÔºåÂú®Á¨¨‰∏ÄÊ¨°Êõ¥Êñ∞Êó∂ÔºåÁî±‰∫é \\(\\pi_\\theta = \\pi_{\\theta_{\\text{old}}}\\)ÔºåÊâÄ‰ª•‰πüÂèØ‰ª•Áî®‰∫é‰º∞ËÆ° \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)„ÄÇ‰ΩÜÈóÆÈ¢òÂú®‰∫éÔºå‰ªéÁ¨¨‰∫åÊ¨°Êõ¥Êñ∞ÂºÄÂßãÔºå\\(\\pi_\\theta \\neq \\pi_{\\theta_{\\text{old}}}\\)ÔºåËÄåÊàë‰ª¨‰ªçÁÑ∂Â∏åÊúõ‰º∞ËÆ° \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)„ÄÇ\nÈöèÂêéËøõË°åÂ§öËΩÆ PPO Êõ¥Êñ∞Êó∂ÔºåTRL Âπ∂Ê≤°ÊúâÂü∫‰∫éÂΩìÂâçÁ≠ñÁï• \\(\\pi_{\\theta}\\) ÈáçÊñ∞‰º∞ËÆ° \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†8„ÄÇ\n\n\n\nListing¬†8: TRL PPO Â§öËΩÆÊõ¥Êñ∞\n\n\n# Do multiple epochs of PPO training, with a fresh random shuffle in each epoch\nfor ppo_epoch_idx in range(args.num_ppo_epochs):\n    b_inds = np.random.permutation(args.local_batch_size)\n    minibatch_idx = 0\n    for mini_batch_start in range(0, args.local_batch_size, args.local_mini_batch_size):\n        mini_batch_end = mini_batch_start + args.local_mini_batch_size\n        mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]\n        gradient_accumulation_idx = 0\n        for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):\n            with accelerator.accumulate(model):\n                micro_batch_end = micro_batch_start + args.per_device_train_batch_size\n                micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]\n                mb_advantage = advantages[micro_batch_inds]\n                mb_responses = responses[micro_batch_inds]\n                mb_query_responses = query_responses[micro_batch_inds]\n                mb_logprobs = logprobs[micro_batch_inds]\n                mb_return = returns[micro_batch_inds]\n                mb_values = values[micro_batch_inds]\n\n\n                output, vpred_temp = forward(model, mb_query_responses, processing_class.pad_token_id)\n                logits = output.logits[:, context_length - 1 : -1]\n                logits /= args.temperature + 1e-7\n                new_logprobs = selective_log_softmax(logits, mb_responses)\n                new_logprobs = torch.masked_fill(\n                    new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB\n                )\n                vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)\n                vpred = torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], 0)\n                vpredclipped = torch.clamp(\n                    vpred,\n                    mb_values - args.cliprange_value,\n                    mb_values + args.cliprange_value,\n                )\n                vf_losses1 = torch.square(vpred - mb_return)\n                vf_losses2 = torch.square(vpredclipped - mb_return)\n                vf_loss_max = torch.max(vf_losses1, vf_losses2)\n                vf_loss = 0.5 * masked_mean(vf_loss_max, ~padding_mask_p1[micro_batch_inds])\n                vf_clipfrac = masked_mean(\n                    (vf_losses2 &gt; vf_losses1).float(), ~padding_mask_p1[micro_batch_inds]\n                )\n                logprobs_diff = new_logprobs - mb_logprobs\n                ratio = torch.exp(logprobs_diff)\n                pg_losses = -mb_advantage * ratio\n                pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)\n                pg_loss_max = torch.max(pg_losses, pg_losses2)\n                pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])\n                loss = pg_loss + args.vf_coef * vf_loss\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n\n\n\n\n\n\nhttps://github.com/huggingface/trl/blob/e3244d2d096ff1e2e248c931d06d39e165e20623/trl/trainer/ppo_trainer.py#L406-L432‚Ü©Ô∏é\n\n\n\n\n6.1.2 OpenRLHF\nÁ±ª‰ººÂú∞ÔºåOpenRLHF Âú® Listing¬†2 ‰∏≠ËÆ°ÁÆó KL Ê†∑Êú¨ÂÄº‰ΩøÁî®ÁöÑ log_probs Âú® make_experience Êó∂Ë¢´ËÆ°ÁÆóÔºåÂíåÂØπÂ∫îÁöÑÊ†∑Êú¨ sequences ÈÉΩÊù•Ëá™ÈááÊ†∑Á≠ñÁï• \\(\\pi_{\\theta_{\\text{old}}}\\)ÔºåËÄåÈùûÂΩìÂâçÁ≠ñÁï• \\(\\pi_{\\theta}\\)„ÄÇÂØπÂ∫î‰ª£Á†ÅÂèØËßÅ Listing¬†9„ÄÇ\n\n\n\nListing¬†9: OpenRLHF ÈááÊ†∑Ê†∑Êú¨Âπ∂‰ΩøÁî® \\(\\pi_{\\theta_{\\text{old}}}\\) ËÆ°ÁÆóÂØπÊï∞‰ººÁÑ∂\n\n\n# https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L592-L595\ndef make_experience(self, samples: Samples) -&gt; Experience:\n    \"\"\"\n    Turn samples into experience by calculating logprobs, values, rewards, and kl divergence.\n    \"\"\"\n    # ...\n    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L673-L680\n    action_log_probs = self.actor(\n        sequences,\n        num_actions,\n        # ...\n    )\n    # ...\n    # https://github.com/OpenRLHF/OpenRLHF/blob/cdcabf3548ed67f7454eed4fb70905ac8faa8694/openrlhf/trainer/ppo_utils/experience_maker.py#L704-L709\n    kl = compute_approx_kl(\n        action_log_probs,\n        base_action_log_probs,\n        # ...\n    )\n\n\n\n‰ªé Listing¬†3 ÂèØËßÅÔºåOpenRLHF Âú®Â§öÊ¨°Êõ¥Êñ∞‰∏≠ÔºåÂØπ‰∫é KL rewardÔºåÂπ∂Ê≤°ÊúâÈáçÊñ∞ËÆ°ÁÆóÔºåËøòÊòØÊ≤øÁî®‰∫ÜÂü∫‰∫é \\(\\pi_{\\theta_{\\text{old}}}\\) ÁöÑ KL Ê†∑Êú¨ÂÄº„ÄÇÊ≥®ÊÑèÔºåËôΩÁÑ∂ÂÖ∂‰∏≠ KL loss È°πÁöÑËÆ°ÁÆó‰ΩøÁî®‰∫ÜÂü∫‰∫é \\(\\pi_{\\theta}\\) ËÆ°ÁÆóÁöÑÂØπÊï∞‰ººÁÑ∂Ôºå‰ΩÜÂ¶Ç Section¬†4.1 ÊâÄËø∞ÔºåKL loss È°πÁöÑÂÆûÁé∞ÈÄöÂ∏∏ÊòØÈîôËØØÁöÑÔºå‰∏îÂêåÊ†∑‰æùËµñ‰∫é on-policy ËÆæÁΩÆ„ÄÇ\n\n\n6.1.3 verl\n‰ªé Listing¬†4 ÂèØËßÅÔºåverl ÂêåÊ†∑‰ΩøÁî® \\(\\pi_{\\theta_{\\text{old}}}\\) ËÆ°ÁÆó KL Ê†∑Êú¨ÂÄº„ÄÇ\n‰ªé Listing¬†5 ÂèØËßÅÔºåverl Âú®Â§öÊ¨°Êõ¥Êñ∞‰∏≠ÔºåÂØπ‰∫é KL rewardÔºå‰πü‰ºöÊ≤øÁî®Âü∫‰∫é \\(\\pi_{\\theta_{\\text{old}}}\\) ÁöÑ KL Ê†∑Êú¨ÂÄº„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#Âà©Áî®ÈáçË¶ÅÊÄßÈááÊ†∑Â§ÑÁêÜ-off-policy-ËÆæÁΩÆ",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#Âà©Áî®ÈáçË¶ÅÊÄßÈááÊ†∑Â§ÑÁêÜ-off-policy-ËÆæÁΩÆ",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "6.2 Âà©Áî®ÈáçË¶ÅÊÄßÈááÊ†∑Â§ÑÁêÜ off-policy ËÆæÁΩÆ",
    "text": "6.2 Âà©Áî®ÈáçË¶ÅÊÄßÈááÊ†∑Â§ÑÁêÜ off-policy ËÆæÁΩÆ\noff-policy ËÆæÁΩÆ‰∏ãÔºåÊàë‰ª¨Ê≤°ÊúâÊù•Ëá™ÊúÄÊñ∞Á≠ñÁï• \\(\\pi_{\\theta}\\) ÁöÑÊ†∑Êú¨ÔºåËÄåÂè™ËÉΩ‰ΩøÁî®Êù•Ëá™ÈááÊ†∑Á≠ñÁï• \\(\\pi_{\\theta_{\\text{old}}}\\) ÁöÑÊ†∑Êú¨Ôºå‰ΩÜÊàë‰ª¨‰ªçÁÑ∂Â∏åÊúõ‰º∞ËÆ° \\(\\nabla_{\\theta} \\mathbb{D}_{\\text{KL}} \\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\)„ÄÇ\nÁÜüÊÇâ off-policy PG ÁöÑËØªËÄÖÂèØËÉΩÂ∑≤ÁªèÊÉ≥Âà∞‰∫ÜÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®ÈáçË¶ÅÊÄßÈááÊ†∑ÔºàImportance SamplingÔºåISÔºâÊäÄÂ∑ßÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÂç≥\n\\[\n\\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}} \\left[f(\\mathbf{\\tau})\\right] = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta}(\\tau) f(\\tau)  = \\sum_{\\tau \\in \\mathcal{T}} p_{\\theta_{\\text{old}}}(\\tau) \\frac{p_{\\theta}(\\tau)}{p_{\\theta_{\\text{old}}}(\\tau)} f(\\tau) = \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}} \\left[\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})} f(\\mathbf{\\tau})\\right]\n\\tag{50}\\]\nÊ≠§Â§ÑÔºåÈáçË¶ÅÊÄßÈááÊ†∑Á≥ªÊï∞ \\(\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})}\\) ÂèØ‰ª•‰ªøÁÖß Equation¬†5 Â±ïÂºÄ‰∏∫Ôºö\n\\[\n\\frac{p_{\\theta}(\\mathbf{\\tau})}{p_{\\theta_{\\text{old}}}(\\mathbf{\\tau})} = \\prod_{t=1}^{|\\mathbf{\\tau}|} \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}{\\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}\n\\tag{51}\\] 20\nÂà©Áî®ÈáçË¶ÅÊÄßÈááÊ†∑ (Equation¬†50, Equation¬†51) ÔºåKL Ê¢ØÂ∫¶Ë°®ËææÂºè Equation¬†46 ÂèØ‰ª•ËΩ¨Âåñ‰∏∫Ôºö\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}} \\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta}}\\left[\\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t}) \\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}}\\left[ \\frac{p_{\\theta}(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T})}{p_{\\theta_{\\text{old}}}(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T})}  \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})  \\right] \\\\\n=&  \\mathbb{E}_{\\mathbf{\\tau} \\sim p_{\\theta_{\\text{old}}}}\\left[ \\left(\\prod_{t=1}^{|\\mathbf{\\tau}|} \\frac{\\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t})}\\right) \\sum_{t=1}^{|\\mathbf{\\tau}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})}{\\pi_{\\text{ref}}(\\mathbf{a}_{t'} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t'-1}, \\mathbf{s}_{t'})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{t} \\mid \\mathbf{s}_{1}, \\cdots, \\mathbf{a}_{t-1}, \\mathbf{s}_{t}) \\right]\n\\end{aligned}\n\\tag{52}\\]\nÂØπÂ∫îÁöÑ Monte Carlo ‰º∞ËÆ°Âºè‰∏∫Ôºö\n\\[\n\\begin{aligned}\n& \\nabla_{\\theta} \\mathbb{D}_{\\text{KL}}\\left[\\pi_\\theta \\| \\pi_{\\text{ref}}\\right] \\\\\n\\approx& \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\prod_{t=1}^{|\\mathbf{\\tau}_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t=1}^{|\\mathbf{\\tau}_{i}|} \\left(\\sum_{t'=t}^{|\\mathbf{\\tau}_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1}) }{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t'-1}, \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t}) \\\\\n=& \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\mathbf{\\tau}_{i}|} \\left(\\left(\\prod_{t=1}^{|\\mathbf{\\tau}_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t'=t}^{|\\mathbf{\\tau}_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1}) }{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t})\n\\end{aligned}\n\\tag{53}\\]\nÂØπÂ∫îÁöÑ loss ÂáΩÊï∞‰∏∫Ôºö\n\\[\n\\mathcal{L}^{KL}_{\\theta} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{|\\tau_{i}|} \\text{nograd}\\left(\\left(\\prod_{t=1}^{|\\tau_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right)\\sum_{t'=t}^{|\\tau_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})} \\right) \\log \\pi_{\\theta}(\\mathbf{a}_{i, t} \\mid \\mathbf{s}_{i, t})\n\\tag{54}\\]\nÁ±ª‰ºº Equation¬†49ÔºåÊàë‰ª¨ÂèØ‰ª•‰ª§\n\\[\nk(\\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t}) = \\left(\\prod_{t=1}^{|\\tau_{i}|}\\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}{ \\pi_{\\theta_{\\text{old}}}(\\mathbf{a}_{i, t} | \\mathbf{s}_{i, 1}, \\cdots, \\mathbf{a}_{i, t-1}, \\mathbf{s}_{i, t})}\\right) \\sum_{t'=t}^{|\\tau_{i}|} \\log \\frac{\\pi_{\\theta}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}{\\pi_{\\text{ref}}(\\mathbf{a}_{i, t'} | \\mathbf{s}_{i, t'}, \\cdots, \\mathbf{a}_{i, t-1})}\n\\tag{55}\\]\nÊ≥®ÊÑèÔºåEquation¬†55 ‰∏≠ÁöÑ \\(k\\) ÈúÄË¶ÅÂØπ‰∫éÊØè‰∏™Êñ∞ÁöÑ \\(\\pi_{\\theta}\\) ÈáçÊñ∞ËÆ°ÁÆó„ÄÇ\n\n\n\nÂÆûÈôÖËÆ°ÁÆó‰∏≠ÔºåEquation¬†51 Áî±‰∫éÊ∂âÂèäÂà∞ \\(|\\mathbf{\\tau}|\\) Ê¨°Ëøû‰πòÔºåÊñπÂ∑ÆÂ§ß‰∏îÊï∞ÂÄºÁ®≥ÂÆöÊÄßÂ∑ÆÔºåÈúÄË¶ÅÂà©Áî®Âõ†ÊûúÊÄß„ÄÅËøë‰ººÁ≠âÊäÄÊúØÊù•ÂåñÁÆÄ„ÄÇÊú¨ÊñáÁõÆÂâçÁúÅÁï•ËØ•ÈÉ®ÂàÜÔºåÂêéÁª≠Â∞Ü‰ºöÊõ¥Êñ∞Áõ∏ÂÖ≥ÂÜÖÂÆπ„ÄÇ‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#‰øÆÊ≠£-grpo-ÂÖ¨Âºè‰∏≠ÁöÑ-kl-È°π",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#‰øÆÊ≠£-grpo-ÂÖ¨Âºè‰∏≠ÁöÑ-kl-È°π",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "7.1 ‰øÆÊ≠£ GRPO ÂÖ¨Âºè‰∏≠ÁöÑ KL È°π",
    "text": "7.1 ‰øÆÊ≠£ GRPO ÂÖ¨Âºè‰∏≠ÁöÑ KL È°π\nGRPO ÂÖ¨Âºè (Equation¬†1, Equation¬†2) ÂØπ‰∫é KL ‰ºòÂåñ‰∏ªË¶ÅÂ≠òÂú®‰∏§‰∏™ÈîôËØØÔºö\n\nÂøΩÁï•‰∫Ü KL ‰ºòÂåñÁöÑ off-policy ÈóÆÈ¢ò\nÂÖàÂ∞Ü \\(k_{3}\\) ‰º∞ËÆ°Ê†∑Êú¨ÈáèÂ∫îÁî®‰∫éÂä®‰ΩúÊù°‰ª∂‰ººÁÑ∂ÂÜçÊ±ÇÂíåÔºåÂØºËá¥ÂæóÂà∞ÂºÇÂ∏∏ÁöÑÊ¢ØÂ∫¶\n\nÂØπ‰∫éËøô‰∏§‰∏™ÈóÆÈ¢òÔºåÂú® Equation¬†29 ÁöÑÂü∫Á°Ä‰∏äÔºå‰ªøÁÖß Equation¬†55ÔºåÊàë‰ª¨ÂèØ‰ª•ÊåâÂ¶Ç‰∏ãÊñπÂºè‰øÆÊ≠£Ôºö\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\left\\{ \\sum_{t=1}^{\\left|o_i\\right|} \\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old}}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right] \\right\\} -\\beta \\left(\\prod_{t=1}^{|o_{i}|}\\frac{\\pi_{\\theta}(o_{i, t} | q, o_{i,\\lt t})}{ \\pi_{\\theta_{\\text{old}}}(o_{i, t} | q, o_{i,\\lt t})}\\right) k\\left( \\frac{\\prod_{t=1}^{|o_i|} \\pi_{\\text{ref}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\prod_{t=1}^{|o_i|} \\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\right)\n\\end{aligned}\n\\tag{56}\\]"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#‰øÆÊ≠£ÊµÅË°å-llm-rl-Ê°ÜÊû∂‰∏≠ÁöÑ-kl-‰ºòÂåñÂÆûÁé∞",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#‰øÆÊ≠£ÊµÅË°å-llm-rl-Ê°ÜÊû∂‰∏≠ÁöÑ-kl-‰ºòÂåñÂÆûÁé∞",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "7.2 ‰øÆÊ≠£ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞",
    "text": "7.2 ‰øÆÊ≠£ÊµÅË°å LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞\nÁõÆÂâçÊµÅË°åÁöÑ LLM RL Ê°ÜÊû∂‰∏≠ÁöÑ KL ‰ºòÂåñÂÆûÁé∞ÔºåÈô§‰∫Ü GRPO ÂÖ¨Âºè‰∏≠‰ΩìÁé∞ÁöÑ‰∏§‰∏™ÈóÆÈ¢ò‰πãÂ§ñÔºåËøòÂ≠òÂú®‰ª•‰∏ãÈóÆÈ¢òÔºö\n\nÂÆûÁé∞ÂçïÁã¨ÁöÑ KL loss È°πÊó∂ÔºåÈªòËÆ§‰∏çÂéªÈô§‰ªª‰ΩïÊ¢ØÂ∫¶ÔºåÔºàËøôÂèØËÉΩÊòØËØØ‰ª•‰∏∫Áõ¥Êé•ÂâçÂêë‰º†Êí≠‰º∞ËÆ° KL Êï£Â∫¶ÔºåÂÜçÂèçÂêë‰º†Êí≠Â∞±ËÉΩÂæóÂà∞Ê≠£Á°ÆÁöÑÊ¢ØÂ∫¶ÂØºËá¥ÁöÑÔºâ\nÈîôËØØÂú∞ÂÆûÁé∞‰∫ÜÂπ≥ÂùáÊìç‰Ωú\n\nÂØπ‰∫éËøô‰∫õÈóÆÈ¢òÔºåÂèØ‰ª•ÊåâÁÖßÂ¶Ç‰∏ãÊÄùË∑Ø‰øÆÊ≠£Ôºö\n\n‰∏∫ KL È°πÊ∑ªÂä†ÈáçË¶ÅÊÄßÈááÊ†∑ÔºåËøôÈúÄË¶Å‰ªéÁ¨¨‰∫åËΩÆÊõ¥Êñ∞ÂºÄÂßãÔºåÊØèÊ¨°Âü∫‰∫éÊñ∞ÁöÑ \\(\\pi_\\theta\\) ÈáçÊñ∞ËÆ°ÁÆó KL loss / reward È°πÔºåÂåÖÊã¨ÈáçË¶ÅÊÄßÈááÊ†∑Á≥ªÊï∞\nÂ∫îÁî® KL ‰º∞ËÆ°Ê†∑Êú¨ÈáèÊó∂ÔºåÂÖàÂØπ‰∫éÂ∫èÂàóÂÜÖ token Èó¥ÁöÑÂØπÊï∞Êù°‰ª∂‰ººÁÑ∂Ê±ÇÂíåÔºåÂæóÂà∞ËΩ®ËøπËÅîÂêàÊ¶ÇÁéáÔºåÂÜç‰ª£ÂÖ•ÂÖ¨Âºè\nÂ¶ÇÊûúÂ∏åÊúõÂÉèÂØπ‰∫é reward ‰ºòÂåñ‰∏ÄÊ†∑‰ΩøÁî®Âü∫Á∫ø„ÄÅÊäòÊâ£„ÄÅGAEÁ≠âÊäÄÊúØÔºåÂèØ‰ª•Êåâ Equation¬†55 ÂÆûÁé∞‰∏∫ KL reward È°πÔºàÂ∞ΩÁÆ°Ëøô‰∫õÊäÄÊúØËÉåÂêéÁöÑËÄÉÈáèÂπ∂‰∏ç‰∏ÄÂÆöÈÄÇÂêà KL Êï£Â∫¶Ôºå‰æãÂ¶Ç reward ÊòØÂÖÅËÆ∏Ëá™ÂÆö‰πâÁöÑÔºå‰ΩÜ KL Êï£Â∫¶ÊúâÊòéÁ°ÆÁöÑÂÆö‰πâÔºâ\nÂ¶ÇÊûú‰∏çÂ∏åÊúõÂ∫îÁî® reward ‰ºòÂåñÁöÑÂÖ∂‰ªñÊäÄÊúØÔºåÂèØ‰ª•Êåâ Equation¬†54 ÂÆûÁé∞‰∏∫ KL loss È°π"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂØπ‰∫é-kl-Ê¢ØÂ∫¶Êõ¥Â•ΩÁöÑ‰º∞ËÆ°Ê†∑Êú¨Èáè",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂØπ‰∫é-kl-Ê¢ØÂ∫¶Êõ¥Â•ΩÁöÑ‰º∞ËÆ°Ê†∑Êú¨Èáè",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "8.1 ÂØπ‰∫é KL Ê¢ØÂ∫¶Êõ¥Â•ΩÁöÑ‰º∞ËÆ°Ê†∑Êú¨Èáè",
    "text": "8.1 ÂØπ‰∫é KL Ê¢ØÂ∫¶Êõ¥Â•ΩÁöÑ‰º∞ËÆ°Ê†∑Êú¨Èáè\nÂ¶Ç Section¬†5.5 ÊâÄËø∞ÔºåPG ‰ΩøÁî®‰∫ÜËÆ∏Â§öÂÖ∂‰ªñÊäÄÊúØÊù•ÊîπËøõÂÖ∂Ê¢ØÂ∫¶‰º∞ËÆ°ÔºåËÉΩÂê¶‰ΩøÁî®Á±ª‰ººÊäÄÊúØÊîπËøõ KL Ê¢ØÂ∫¶‰º∞ËÆ°Ôºü\nÊ≠§Â§ñÔºåJohn Schulman ÁöÑÂçöÂÆ¢ÊòØÈíàÂØπ‰º∞ËÆ° KL Êï£Â∫¶ÂàÜÊûê‰∫Ü‰∏çÂêåÁöÑ‰º∞ËÆ°Ê†∑Êú¨Èáè„ÄÇ‰ΩÜËøô‰∫õÂàÜÊûêÂØπ‰∫é‰º∞ËÆ° KL Êï£Â∫¶ÁöÑÊ¢ØÂ∫¶ÊòØÂê¶ËøòÊàêÁ´ãÔºü"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#kl-regularized-rl-ÁöÑÁêÜËÆ∫‰ºòÂäø",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#kl-regularized-rl-ÁöÑÁêÜËÆ∫‰ºòÂäø",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "8.2 KL-Regularized RL ÁöÑÁêÜËÆ∫‰ºòÂäø",
    "text": "8.2 KL-Regularized RL ÁöÑÁêÜËÆ∫‰ºòÂäø\nÊúÄËøëÂü∫‰∫éÂèØÈ™åËØÅ reward ÁöÑ RL ÈùûÂ∏∏ÊµÅË°åÔºåÂÖ∂ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÈÅøÂÖç‰∫Ü reward hackingÔºåÁõ¥Ëßâ‰∏äÔºåÊàë‰ª¨‰ºº‰πé‰∏çÂÜçÈúÄË¶ÅÁõ∏ÂØπ‰∫éÂèÇËÄÉÁ≠ñÁï•ÁöÑ KL Ê≠£ÂàôÂåñ„ÄÇ\nÁÑ∂ËÄåÔºå‰πüÊúâ‰∏Ä‰∫õÂ∑•‰ΩúÊåáÂá∫ÔºåKL-Regularized RL Âú®ÁêÜËÆ∫‰∏äËøòÊúâËÆ∏Â§öÂÖ∂‰ªñ‰ºòÂäø„ÄÇ‰æãÂ¶Ç Zhao et al. (2025) ËØÅÊòé‰∫Ü KL-regularized RL ÁöÑ regret Âè™Êúâ \\(\\mathcal{O}(\\log T)\\)ÔºåËÄåÂ∏∏ËßÅÁöÑÂü∫‰∫é contextual bandit Êàñ MDP Âª∫Ê®°ÁöÑ RL ÊñπÊ≥ï regret ÈÄöÂ∏∏‰∏ç‰Ωé‰∫é \\(\\mathcal{O}(\\sqrt{T})\\)„ÄÇÁ≤óÊµÖÂú∞ËØ¥ÔºåËøôÊòØÂõ†‰∏∫ KL Ê≠£ÂàôÂåñÁõÆÊ†áÈ°πÁöÑÂ≠òÂú®Ôºå‰ΩøÂæó value ÂàÜËß£Êúâ‰∫ÜÁâπÂà´ÁöÑÊÄßË¥®Ôºå‰æãÂ¶ÇÂá∏ÊÄßÊõ¥Âº∫„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#Áõ∏ÂÖ≥Â∑•‰Ωú",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#Áõ∏ÂÖ≥Â∑•‰Ωú",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "9.1 Áõ∏ÂÖ≥Â∑•‰Ωú",
    "text": "9.1 Áõ∏ÂÖ≥Â∑•‰Ωú\n\n‰∏éÊú¨ÊñáÂêåÊúü‰πüÊúâËÆ∏Â§öÁ≤æÂΩ©ÁöÑËÆ®ËÆ∫ÔºåÁî±‰∫éÁ¨îËÄÖËøòÊ≤°ËÉΩÈÄöËØªÂÖ®ÊñáÔºåÊ≠§Â§Ñ‰ªÖÊèê‰æõÈìæÊé•Ôºå‰∏ç‰ΩúÊ¶ÇÊã¨ÔºåÊ¨¢ËøéÊÑüÂÖ¥Ë∂£ÁöÑËØªËÄÖËá™Ë°åÈòÖËØªÔºö\n\nGRPO ‰∏≠ÁöÑ KL Loss ÂÆûÁé∞ÁªÜËäÇÈóÆÈ¢ò - Hongyu Zang @ Áü•‰πé\nk2 lossÂ∞±ÊòØÊØîk3 lossÂ•ΩÔºÅ‰ª•ÂèäGRPO_off-policy - Yiming Liu @ Áü•‰πé\n\n\n\n\nhttps://tongyx361.github.io‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂÜô‰ΩúÂ•ëÊú∫trpoppo-‰∏é-grpo-‰∏≠ÁöÑ-kl-‰∏∫‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂÜô‰ΩúÂ•ëÊú∫trpoppo-‰∏é-grpo-‰∏≠ÁöÑ-kl-‰∏∫‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "9.2 ÂÜô‰ΩúÂ•ëÊú∫Ôºö‚ÄúTRPO/PPO ‰∏é GRPO ‰∏≠ÁöÑ KL ‰∏∫‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑Ôºü‚Äù",
    "text": "9.2 ÂÜô‰ΩúÂ•ëÊú∫Ôºö‚ÄúTRPO/PPO ‰∏é GRPO ‰∏≠ÁöÑ KL ‰∏∫‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑Ôºü‚Äù\n\nÁ¨îËÄÖÂØπ RL ‰∏≠ KL ‰ºòÂåñÁõ∏ÂÖ≥ÈóÆÈ¢òÁöÑÊÄùËÄÉ‰∏ªË¶ÅÂºÄÂßã‰∫é X ‰∏ä Fanyi Pu ÊèêÂá∫‰∫ÜËøôÊ†∑‰∏Ä‰∏™ÈóÆÈ¢ò22Ôºö\n\nA small question about GRPO: I noticed that the KL divergence in GRPO is written as KL(new || old), while TRPO and PPO use KL(old || new) as the constraint/penalty. Is there a difference between the two? Would modifying this part have any impact?\nTRPO (Schulman et al. 2015)\n\n\\[\n\\begin{aligned}\n& \\underset{\\theta}{\\text{maximize}}~L_{\\theta_{\\text {old }}}(\\theta) \\\\\n& \\text { subject to } \\bar{D}_{\\mathrm{KL}}^{\\rho_{\\theta_{\\text {old }}}}\\left(\\theta_{\\text {old }}, \\theta\\right) \\leq \\delta\n\\end{aligned}\n\\tag{57}\\]\n\nPPO (Schulman et al. 2017)\n\n\\[\nL^{K L P E N}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(\\mathbf{y}_t \\mid \\mathbf{x}_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(\\mathbf{y}_t \\mid \\mathbf{x}_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid \\mathbf{x}_t\\right), \\pi_\\theta\\left(\\cdot \\mid \\mathbf{x}_t\\right)\\right]\\right]\n\\tag{58}\\]\n\nGRPO (Shao et al. 2024)\n\n\\[\n\\begin{aligned}\n& \\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\\n& \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)} \\hat{A}_{i, t}, \\text{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,\\lt t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta \\mid \\pi_{\\text{ref}}\\right]\\right\\}\n\\end{aligned}\n\\tag{59}\\]\nËøô‰∏™ÈóÆÈ¢òÊú¨Ë∫´ÁöÑÁ≠îÊ°àÊòØÈùûÂ∏∏ÁÆÄÂçïÁöÑ„ÄÇ\nÈ¶ñÂÖàÔºåËøô‰∏™ÈóÆÈ¢òÊ∑∑Ê∑Ü‰∫Ü‰∏§Áßç‰∏çÂêåÁöÑ KL ÊÉ©ÁΩöÈ°πÔºö\n\n\\(\\text{KL}[\\pi_{\\theta_{\\text{old}}},\\pi_{\\theta}]\\)ÔºåÂÖ∂‰ΩúÁî®ÊòØÁ∫¶ÊùüÊúÄÊñ∞Á≠ñÁï• \\(\\pi_{\\theta}\\)‰∏çË¶ÅÁ¶ªÈááÊ†∑Á≠ñÁï•\\(\\pi_{\\theta_{\\text{old}}}\\) Â§™ËøúÔºåÈÅøÂÖçËøáÂ§ßÁöÑÊõ¥Êñ∞ÂØºËá¥Á≠ñÁï•Â¥©Ê∫ÉÔºå‰ªéËÄåÊûÑÊàê‰ø°‰ªªÂüüÔºàTrust Region, TRÔºâÔºå‰πüÂ∞±ÊòØ TRPO ‰∏≠ÁöÑ TR„ÄÇËÄå PPO ‰Ωú‰∏∫ TRPO ÁöÑËøë‰ººÂÆûÁé∞ÔºåÁªßÊâø‰∫ÜËøô‰∏ÄÁÇπ„ÄÇ\n\\(\\text{KL}[\\pi_{\\theta},\\pi_{\\theta_{\\text{ref}}}]\\)ÔºåÂÖ∂‰ΩúÁî®ÊòØÁ∫¶ÊùüÊúÄÊñ∞Á≠ñÁï• \\(\\pi_{\\theta}\\)‰∏çË¶ÅÁ¶ªÂèÇËÄÉÁ≠ñÁï•\\(\\pi_{\\theta_{\\text{ref}}}\\) Â§™ËøúÔºå‰ªéËÄåÊõ¥ÂÖÖÂàÜÂú∞Âà©Áî®ÂèÇËÄÉÁ≠ñÁï•‰∏≠ÁöÑÂÖàÈ™å„ÄÇ\n\nÂè¶Â§ñÔºåËøô‰∏™ÈóÆÈ¢òÂøΩÁï•‰∫Ü TRPO/PPO ÂÖ¨Âºè‰∏≠ÁöÑ KL ÊçüÂ§±È°π‰∏é GRPO ÂÖ¨Âºè‰∏≠ÁöÑ clip ÂáΩÊï∞ÂÆûÈôÖ‰∏äÊòØÂá∫‰∫éÂêå‰∏ÄÁõÆÁöÑÔºåÂç≥Á∫¶Êùü \\(\\text{KL}[\\pi_{\\theta_{\\text{old}}},\\pi_{\\theta}]\\)„ÄÇÂ¶Ç PPO ËÆ∫ÊñáÁ¨¨ 3-4 ËäÇÊâÄËØ¥Ôºå‰∏§ËÄÖÂèØ‰ª•Áõ∏‰∫íÊõø‰ª£ÊàñÁªìÂêà‰ΩøÁî®Ôºö\n\nLet \\(r_t(\\theta)\\) denote the probability ratio \\(r_{t}(\\theta)=\\frac{\\pi_{\\theta}\\left(a_t \\mid s_t\\right)}{\\left(\\pi_{\\theta_{\\text {old }}}\\left|a_t\\right| s_t\\right)}\\), so \\(r\\left(\\theta_{\\text{old}}\\right)=1\\). TRPO maximizes a ‚Äúsurrogate‚Äù objective\n\n\\[\nL^{\\text{CPI}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t\\right]=\\hat{\\mathbb{E}}_t\\left[r_t(\\theta) \\hat{A}_t\\right] .\n\\]\n\n‚Ä¶\nThe main objective we propose is the following:\n\n\\[\nL^{\\text{CLIP}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\n\\]\n\nwhere epsilon is a hyperparameter, say, \\(\\epsilon=0.2\\). The motivation for this objective is as follows. The first term inside the \\(\\min\\) is \\(L^{\\text{CPI}}\\). The second term, \\(\\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\), modifies the surrogate objective by clipping the probability ratio, which removes the incentive for moving \\(r_t\\) outside of the interval \\([1-\\epsilon, 1+\\epsilon]\\).\n‚Ä¶\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in addition to it, is to use a penalty on KL divergence, and to adapt the penalty coefficient so that we achieve some target value of the KL divergence \\(d_{\\text{targ}}\\) each policy update. In our experiments, we found that the KL penalty performed worse than the clipped surrogate objective, however, we‚Äôve included it here because it‚Äôs an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy update:\n\nUsing several epochs of minibatch SGD, optimize the KL-penalized objective\n\n\n\\[\nL^{\\text{KLPEN}}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right]\n\\]\n\n\n\nÈ°∫Â∏¶ÔºåËøòÂèØ‰ª•‰ªé‰ª•‰∏ãËßíÂ∫¶ÁêÜËß£‰∏§ËÄÖÁöÑÂÖ±ÈÄö‰πãÂ§ÑÔºöclip ÂáΩÊï∞Á∫¶ÊùüÁöÑ \\(r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}\\)Â∞±ÊòØ\\(K L\\left[\\pi_{\\theta_{d d}}, \\pi_\\theta\\right]=\\mathbb{E}_{a_t \\sim \\pi_{\\theta_{d t}}\\left(\\cdot \\mid s_t\\right)}\\left[\\log \\frac{\\pi_{\\theta_{d t}}\\left(a_t \\mid s_t\\right)}{\\pi_\\theta\\left(a_t \\mid s_t\\right)}\\right]\\) ‰∏≠ÂØπÂçï‰∏™Ê†∑Êú¨ \\((s_t, a_t)\\) ÁöÑÂÄº‰∏≠ \\(\\log\\) ÁöÑÁúüÊï∞„ÄÇ\n\n\n\nhttps://x.com/pufanyi/status/1888845956684370202‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#Ëá¥Ë∞¢",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#Ëá¥Ë∞¢",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "9.3 Ëá¥Ë∞¢",
    "text": "9.3 Ëá¥Ë∞¢\n\nÊÑüË∞¢ÁéãÊµ©ÁÑ∂„ÄÅYuMS ÂØπÊú¨ÊñáÊèê‰æõÁöÑÈáçË¶ÅÂèçÈ¶à„ÄÇ\nÊÑüË∞¢ÁîüÂπøÊòé„ÄÅWei Xiong„ÄÅÂàò‰ªÅÂΩ™„ÄÅÂàòÂ®Å„ÄÅWeixun Wang„ÄÅYiming Liu„ÄÅHaibin Lin Á≠âÂÖ≥‰∫éÁõ∏ÂÖ≥ÈóÆÈ¢òÁöÑÊúâÁõäËÆ®ËÆ∫‰ª•ÂèäÂØπ‰∫éÊú¨ÊñáÁöÑÊúâÁõäÂèçÈ¶à„ÄÇ\nÊÑüË∞¢ Cursor Âíå Mathpix Âú®‰π¶ÂÜô LaTeX Êó∂Êèê‰æõÁöÑÂ∑®Â§ßÂ∏ÆÂä©„ÄÇ"
  },
  {
    "objectID": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂºïÁî®",
    "href": "posts/kl-rel-to-ref-in-rl-zh/index.html#ÂºïÁî®",
    "title": "ÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ",
    "section": "9.4 ÂºïÁî®",
    "text": "9.4 ÂºïÁî®\n\nBibTeX:\n@article{tong2025kl,\n  author = {Á´•Èõ®ËΩ©},\n  title = {ÈáçÊñ∞ÊÄùËÄÉ {RL} ‰∏≠ÁöÑ {KL} Ê¢ØÂ∫¶‰ºòÂåñ},\n  journal = {Blog},\n  year = {2025},\n  url = {https://tongyx361.github.io/posts/kl-rel-to-ref-in-rl-zh},\n  language = {Chinese},\n}\nÊñáÊú¨Ôºö\nÁ´•Èõ®ËΩ©. 2025. ‚ÄúÈáçÊñ∞ÊÄùËÄÉ RL ‰∏≠ÁöÑ KL Ê¢ØÂ∫¶‰ºòÂåñ.‚Äù https://tongyx361.github.io/blogs/posts/kl-rel-to-ref-in-rl-zh."
  }
]